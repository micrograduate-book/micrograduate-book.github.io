{"version":3,"kind":"Notebook","sha256":"719e54f7ba077e9cbf8e75e8ccc6e9b666dd3a22e73cf9647a47054701f780ce","slug":"micrograduate.makemore1","location":"/micrograduate/makemore1.ipynb","dependencies":[],"frontmatter":{"title":"2. makemore (part 1): implementing a bigram character-level language model","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"micrograduate-env","language":"python"},"github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","source_url":"https://github.com/ckaraneen/micrograduate/blob/main/micrograduate/makemore1.ipynb","edit_url":"https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore1.ipynb","exports":[{"format":"ipynb","filename":"makemore1.ipynb","url":"/build/makemore1-b8ffaf704180e087bc88a5c3d14c98ac.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt","key":"wBMzBJbn4V"},{"type":"outputs","id":"g23KS2MOqJ3DPam0xVXBz","children":[],"key":"VUGidq8fVM"}],"key":"ez5eScrt7P"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Intro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NhDuLhVYWL"}],"identifier":"intro","label":"Intro","html_id":"intro","implicit":true,"key":"UDtYx6XUIY"}],"key":"S7OLDwVjpv"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Just like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W3DUXJ7EHm"},{"type":"link","url":"#1.-micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"USlNtVy5GI"}],"urlSource":"#1.-micrograd","key":"b3tyovo5l3"},{"type":"text","value":" before it, here, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Psre2ro8G4"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"step-by-step","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iBvJAJ0CET"}],"key":"i6kp4KF8D3"},{"type":"text","value":" with everything ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q3EREgN4BC"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"spelled-out","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wb3lTSc7br"}],"key":"flYUrE1xSH"},{"type":"text","value":", we will build ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NQ0Fel7iWR"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YyzEDOdHAi"}],"key":"Le9MjSgOMw"},{"type":"text","value":": a bigram character-level language model. We’re going to build it out slowly and together! But what is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FdOI7FDOdg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZpNTz0KrKc"}],"key":"BkoIrBLXxV"},{"type":"text","value":"? As the name suggests, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"udI3zvxAlo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dAwNJjX79l"}],"key":"tKIxKWWS8E"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hQNH64ZFpS"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makes more","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zn2bq2Fbrz"}],"key":"KYGBpfiM9s"},{"type":"text","value":" of things that you give it. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V20r2SjGmb"},{"type":"link","url":"/build/names-172ee1f909c88cb582ff89a82018cc5d.txt","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"names.txt","key":"pXwccFxI29"}],"urlSource":"names.txt","static":true,"protocol":"file","key":"Xv2t3iesgj"},{"type":"text","value":" is an example dataset. Specifically, it is a very large list of different names. If you train ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iXhC5H7J4q"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UYzxu345Xw"}],"key":"hGOhQxQskg"},{"type":"text","value":" on this dataset, it will learn to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tycd1mCaT7"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"make more","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QIUIhhwCIS"}],"key":"Yc6MbfPewB"},{"type":"text","value":" of name-like things, basically more unique names! So, maybe if you have a baby and you’re looking for a new, cool-sounding unique name, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cj1vaH6nb7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Oz2e7xZpBa"}],"key":"ALsDuFJF62"},{"type":"text","value":" might help you. Here are some examples of such names that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jL8Yqcz2CI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gE9hfCYjJS"}],"key":"yK7ysU2fSD"},{"type":"text","value":" will be able to generate:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fHjFFnvPQy"}],"key":"Y00GvGkRpc"}],"key":"r1H7AQ8vXT"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"text","value":"dontell\nkhylum\ncamatena\naeriline\nnajlah\nsherrith\nryel\nirmi\ntaislee\nmortaz\nakarli\nmaxfelynn\nbiolett\nzendy\nlaisa\nhalliliana\ngoralynn\nbrodynn\nromima\nchiyomin\nloghlyn\nmelichae\nmahmed\nirot\nhelicha\nbesdy\nebokun\nlucianno","position":{"start":{"line":1,"column":1},"end":{"line":30,"column":1}},"key":"eQzTamOLvE"}],"key":"C967Fr2LUe"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"dontell","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JFb1o4EbGw"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xPAz4HsynG"},{"type":"inlineCode","value":"irot","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z0VUyeFF81"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GP7RXkMaOb"},{"type":"inlineCode","value":"zendy","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V6YV5Mc4za"},{"type":"text","value":", and so on, you name it! So under the hood, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vHzCVXYxfv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hcB7apSwbQ"}],"key":"n4SXXX213P"},{"type":"text","value":" is a character-level language model. That means that it’s treating every single line (i.e. name) of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bCM2KnHcpN"},{"type":"link","url":"/build/names-172ee1f909c88cb582ff89a82018cc5d.txt","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"its training dataset","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SjDpSD7gfz"}],"urlSource":"names.txt","static":true,"protocol":"file","key":"ylm95QuuQY"},{"type":"text","value":" as an example. And each example is treated as a sequence of individual characters. For instance, it treats the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OoKOq3tvug"},{"type":"inlineCode","value":"reese","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oY6sTJ4Lsa"},{"type":"text","value":" as the sequence of characters: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yB1nVFCXUw"},{"type":"inlineCode","value":"r","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TjptRMDCFH"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ldn2OpqZjQ"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k5DD0K7YGZ"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NyHSfm7Bsl"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WWjIoYc4mH"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"saZS7YjgVm"},{"type":"inlineCode","value":"s","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pP2x6Tsr8n"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ie5RodLu5E"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fyo98duY0L"},{"type":"text","value":". That is the level on which we are building out ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l5KmPaqKrG"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dCgDJJBFbO"}],"key":"M0IiCnDFrm"},{"type":"text","value":". Basically, its purpose is this: given a character, it can predict the next character in the sequence based upon the names that it has seen so far. Now, we’re actually going to implement a large number of character-level language models, following a few key innovations:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M7cMMTe6vm"}],"key":"KeMUay4QY8"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Bigram (one character predicts the next one with a lookup table of counts)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"kp6lh8OghA"}],"key":"xLNATxRxpo"}],"key":"cQnQtpuRJP"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"MLP, following ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"MrB73vbuAR"},{"type":"link","url":"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Bengio et al. 2003","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"jdhAJt4PYN"}],"urlSource":"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf","key":"PjnPhsXAeE"}],"key":"ynbW3TQ4Gk"}],"key":"iyjVsy71YX"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"CNN, following ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"DZbD4v9zqs"},{"type":"link","url":"https://arxiv.org/abs/1609.03499","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"DeepMind WaveNet 2016","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AIJK0Htxd5"}],"urlSource":"https://arxiv.org/abs/1609.03499","key":"KzoQRLNPCv"},{"type":"text","value":" (in progress...)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"x4qBIwRwoB"}],"key":"pOF3JqQVdg"}],"key":"A3ZM5iLGSj"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RNN, following ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Q6CPo2s2pH"},{"type":"link","url":"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Mikolov et al. 2010","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"F8CY3i54xH"}],"urlSource":"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf","key":"ECklP4Hvcu"}],"key":"bhVomVwjkm"}],"key":"zInInDJd4R"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LSTM, following ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"QR28TVCHjJ"},{"type":"link","url":"https://arxiv.org/abs/1308.0850","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Graves et al. 2014","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PBknMTgxUS"}],"urlSource":"https://arxiv.org/abs/1308.0850","key":"DNzC8ft6h3"}],"key":"GpoA21tpgB"}],"key":"OWZqp411KN"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"GRU, following ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"Wf9HQT2rbR"},{"type":"link","url":"https://arxiv.org/abs/1409.1259","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Kyunghyun Cho et al. 2014","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"oRrZ3N8Kou"}],"urlSource":"https://arxiv.org/abs/1409.1259","key":"ESVXEwpdQB"}],"key":"anX47nLX6x"}],"key":"XTaFYapYRi"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Transformer, following ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"gtLh2zbTPg"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Vaswani et al. 2017","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Xgln8oCMS2"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"LkaFYg5qLp"}],"key":"K2ohd1gUui"}],"key":"pwXWrTuji0"}],"key":"RNsCWWmV9a"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"In fact, the transformer we are going to build will be the equivalent of ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"CoPqKXEU7Y"},{"type":"link","url":"https://en.wikipedia.org/wiki/GPT-2","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"GPT-2","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Rw0Pp1vJpU"}],"urlSource":"https://en.wikipedia.org/wiki/GPT-2","data":{"page":"GPT-2","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"ZlbBZRbHqS"},{"type":"text","value":". Kind of a big deal, since it’s a modern network and by the end of this guide you’ll actually understand how it works at the level of characters. Later on, we will probably spend some time on the word level, so we can generate documents of words, not just segments of characters. And then we’re probably going to go into image and image-text networks such as ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"dVapfUSnpH"},{"type":"link","url":"https://en.wikipedia.org/wiki/DALL-E","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"DALL-E","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"qR405TPXrf"}],"urlSource":"https://en.wikipedia.org/wiki/DALL-E","data":{"page":"DALL-E","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"CHcgpKaYs7"},{"type":"text","value":", ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"jXoOfeEhME"},{"type":"link","url":"https://en.wikipedia.org/wiki/Stable_Diffusion","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Stable Diffusion","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"CszkHTy5Sm"}],"urlSource":"https://en.wikipedia.org/wiki/Stable_Diffusion","data":{"page":"Stable_Diffusion","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"Hf3yDc5qkv"},{"type":"text","value":", and so on. But first, let’s jump into character-level modeling.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"sFnPdHfzzg"}],"key":"noJ6Kfbyag"}],"key":"U4PxlBYdcM"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Building a bigram language model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Kwdg33FW1J"}],"identifier":"building-a-bigram-language-model","label":"Building a bigram language model","html_id":"building-a-bigram-language-model","implicit":true,"key":"J64s4IfFxN"}],"key":"qzB846zHY2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s start by reading all the names into a list:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UFIgu4MIts"}],"key":"CHuOMDwa9B"}],"key":"XNueiNSFPs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words = open(\"names.txt\").read().splitlines()","key":"P2TB97J94R"},{"type":"outputs","id":"stvx7vvtu9SrXoLDpefaJ","children":[],"key":"j7NIOZ8JAm"}],"key":"ao9Vr2dGuu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words[:10]","key":"JA43bs4tUB"},{"type":"outputs","id":"GvxZhtAhqzo0pv4GG0cMQ","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']","content_type":"text/plain"}}},"children":[],"key":"txe2qqZSya"}],"key":"PATNYzoUim"}],"key":"qBPloDuJJb"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we want to learn a bit more about this dataset.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"syelVeHGcT"}],"key":"rn61qfprQX"}],"key":"zPNygLlM1Q"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(words)","key":"hArXl51yeo"},{"type":"outputs","id":"SVfeMt63L9ozFHu__lL0j","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"32033","content_type":"text/plain"}}},"children":[],"key":"rQSRXfQzUj"}],"key":"ZdlVMwLcCP"}],"key":"Da2H1rEvcF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(min(words, key=len))  # shortest","key":"cKCpxmUZ1X"},{"type":"outputs","id":"GS2FsktP2Enzepr33qnEG","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":5,"metadata":{},"data":{"text/plain":{"content":"2","content_type":"text/plain"}}},"children":[],"key":"wsKzVjsboO"}],"key":"Q3FAoVLXmQ"}],"key":"H9NoAdVpnt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(max(words, key=len))  # longest","key":"jtRqy9kHB7"},{"type":"outputs","id":"B6xw1gdoESOqz8vYY9elS","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":6,"metadata":{},"data":{"text/plain":{"content":"15","content_type":"text/plain"}}},"children":[],"key":"WFS52k1qvs"}],"key":"SLm4oJaj12"}],"key":"LC8Yxof99k"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s think through our very first language model. A character-level language model is predicting the next character in the sequence given already some concrete sequence of characters before it. What we have to realize here is that every single word like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M3hFHBs9vy"},{"type":"inlineCode","value":"isabella","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pkqjZk6AEP"},{"type":"text","value":" is actually quite a few examples packed in that single word. Because, let’s think: what is a word telling us really? It’s saying that the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JAFHFa1bSb"},{"type":"inlineCode","value":"i","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H5JkR7J4Hq"},{"type":"text","value":" is a very likely character to come first in the sequence that constitutes a name. The character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uHpfcfmlnx"},{"type":"inlineCode","value":"s","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AwX9DQeWiI"},{"type":"text","value":" is likely to come after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GIOThOF8Xt"},{"type":"inlineCode","value":"i","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uoUvS4yYFe"},{"type":"text","value":", the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ImAzxPziJ1"},{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vQPGw1YvzV"},{"type":"text","value":" is likely to come after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BEti68RqjO"},{"type":"inlineCode","value":"is","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j3scphCytD"},{"type":"text","value":", the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hSfPgtnTyG"},{"type":"inlineCode","value":"b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iKQbtMkoQh"},{"type":"text","value":" is likely to come after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V5GxnLzOO9"},{"type":"inlineCode","value":"isa","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AnOmA7HuqF"},{"type":"text","value":", and so on all the way to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VxiiAlk2n0"},{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pkzcHjlrMv"},{"type":"text","value":" following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CVZAkQfjM1"},{"type":"inlineCode","value":"isabell","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U4Q0SD9Rg0"},{"type":"text","value":". And then there’s one more important piece of information in here. And that is that after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WeY9fPl4mt"},{"type":"inlineCode","value":"isabella","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HlmNhxAJJf"},{"type":"text","value":", the word is very likely to end. So, time to build our first network: a bigram language model. In these, we are working with two characters at a time. So, we are only looking for one character we are given and we are trying to predict the next character in a sequence. For example, in the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IQvyGefBk2"},{"type":"inlineCode","value":"charlotte","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W0mrlA5ZOX"},{"type":"text","value":", we ask: what characters are likely to follow ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AK8j4YvuZn"},{"type":"inlineCode","value":"r","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NovXiVZlAh"},{"type":"text","value":"?  In the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O0MsV3nqJz"},{"type":"inlineCode","value":"sophia","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mx7B9PhS4Q"},{"type":"text","value":": we ask what characters are likely to follow ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UZxkGzNfzZ"},{"type":"inlineCode","value":"p","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gNbDl9MN8R"},{"type":"text","value":"? And so on. This mean we are just modeling that local structure. Meaning, we only look at the previous character, even though there might be a lot of useful information before it. This is a very simple model, which is why it’s a great place to start! We can learn about the statistics of which characters are likely to follow which other characters by counting. So by iterating over all names, we can count how often each consecutive pair (bigram) of characters appears.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QmkmL1Oo6e"}],"key":"vFUEPyaqZo"}],"key":"MyvrTvSMWm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"b = {}\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0) + 1","key":"Cen1OXn6Dc"},{"type":"outputs","id":"JYY3gkS7MgQwlBUghcl9_","children":[],"key":"fQoYyN5EvU"}],"key":"BTLJQ1uVbO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Notice that we have also added the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qsNTV50G1S"},{"type":"inlineCode","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gtqaZMzeSc"},{"type":"text","value":" to signify the start and end of each word. And obviously, the variable ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XXzFILzuYN"},{"type":"inlineCode","value":"b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U2n3R4eFEl"},{"type":"text","value":" now holds the statistics of the entire dataset.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"go3u3mURVy"}],"key":"yEPIQQixgE"}],"key":"SWa2Vfy6yD"},{"type":"block","kind":"notebook-code","data":{"scrolled":true,"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"sorted(b.items(), key=lambda tup: tup[1], reverse=True)","visibility":"show","key":"hZq8IGsFBu"},{"type":"outputs","id":"bnOJqVYwnbGGCwpf8StdP","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":8,"metadata":{},"data":{"text/plain":{"content":"[(('n', '.'), 6763),\n (('a', '.'), 6640),\n (('a', 'n'), 5438),\n (('.', 'a'), 4410),\n (('e', '.'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('.', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('.', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '.'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('.', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '.'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('.', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '.'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('.', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('.', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('.', 'l'), 1572),\n (('.', 'c'), 1542),\n (('.', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '.'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '.'), 1314),\n (('.', 't'), 1308),\n (('.', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '.'), 1169),\n (('.', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('.', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('.', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '.'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('.', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('.', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('.', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '.'), 516),\n (('d', '.'), 516),\n (('.', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '.'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('.', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('.', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('.', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '.'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('.', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '.'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '.'), 160),\n (('u', '.'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('.', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '.'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '.'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '.'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('.', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '.'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '.'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('.', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '.'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '.'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '.'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '.'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]","content_type":"text/plain"}}},"children":[],"key":"M3c0VvwuvR"}],"visibility":"show","key":"SGYQJJ4nmM"}],"visibility":"show","key":"niRD23gEBG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And this is the sorted list of counts of the individual bigrams across all the words in the dataset! Now let’s convert our current bigram-to-occurence-frequency map into a bigram counts array, where every row index represents the first character and every column index represents the second character of each bigram. Before doing so, we must first find a way to convert each character into a unique integer index:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WvYzpT7X0d"}],"key":"PPLIJCBmEE"}],"key":"OdddETdCKV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"chars = [\".\"] + sorted(list(set(\"\".join(words))))\nctoi = {c: i for i, c in enumerate(chars)}\nprint(ctoi)","key":"ba1W8hhyBq"},{"type":"outputs","id":"gjv-dHTrXVBPP0QCJJ14Q","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"},"children":[],"key":"FOFmq9QrOp"}],"key":"cinRGjtVv1"}],"key":"PhxKNrQAH7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now that we have a character-to-index map, we may construct our bigram counts array ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wflRC7QQjK"},{"type":"inlineCode","value":"N","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c6facwk7Ew"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kK2o4EdhjL"}],"key":"T2Jgwd6d51"}],"key":"qkzOjZWBEf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\n\nnchars = len(chars)\nN = torch.zeros(nchars, nchars, dtype=torch.int32)\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        N[ctoi[ch1], ctoi[ch2]] += 1","key":"lz1WEmAIiY"},{"type":"outputs","id":"vF4UAQEbhS2aMJIKqeNyo","children":[],"key":"qaRCzqrtay"}],"key":"DptHhMC9lH"},{"type":"block","kind":"notebook-code","data":{"scrolled":true,"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"N","visibility":"show","key":"jOV9aEJwKP"},{"type":"outputs","id":"dhnkz_eZZhpztotCS7O8k","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":11,"metadata":{},"data":{"text/plain":{"content":"tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n          134,  535,  929],\n        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n          182, 2050,  435],\n        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n            0,   83,    0],\n        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n            3,  104,    4],\n        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n            0,  317,    1],\n        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n          132, 1070,  181],\n        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n            0,   14,    2],\n        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n            0,   31,    1],\n        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n            0,  213,   20],\n        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n           89,  779,  277],\n        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n            0,   10,    0],\n        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n            0,  379,    2],\n        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n            0, 1588,   10],\n        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n            0,  287,   11],\n        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n            6,  465,  145],\n        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n           45,  103,   54],\n        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n            0,   12,    0],\n        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n            0,    0,    0],\n        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n            3,  773,   23],\n        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n            0,  215,   10],\n        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n            2,  341,  105],\n        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n           34,   13,   45],\n        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n            0,  121,    0],\n        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n            0,   73,    1],\n        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n           38,   30,   19],\n        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n           28,   23,   78],\n        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n            1,  147,   45]], dtype=torch.int32)","content_type":"text/plain"}}},"children":[],"key":"CD15tLVqke"}],"visibility":"show","key":"svuUrKqgX7"}],"visibility":"show","key":"EKvBLsQ23W"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Done! Of course, this looks like a mess. So let’s visualize it better.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hbeqpbplxO"}],"key":"X4GgJb1cRs"}],"key":"K7ZCkU3ph0"},{"type":"block","kind":"notebook-code","data":{"scrolled":true,"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\n\nitoc = {i: c for c, i in ctoi.items()}\nplt.figure(figsize=(16, 16))\nplt.imshow(N, cmap=\"Blues\")\nfor i in range(27):\n    for j in range(27):\n        chstr = itoc[i] + itoc[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\nplt.axis(\"off\")","visibility":"show","key":"HGUk4BlLzQ"},{"type":"outputs","id":"lEgh966SE7OGhvAhMJ7la","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":12,"metadata":{},"data":{"text/plain":{"content":"(np.float64(-0.5), np.float64(26.5), np.float64(26.5), np.float64(-0.5))","content_type":"text/plain"}}},"children":[],"key":"yxNXBz2QWt"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"2e90789e8ce84917a2a5d630a82c41ef\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"b09bbce13ad8931ca3f31925d5c24351","path":"/build/b09bbce13ad8931ca3f31925d5c24351.png"},"text/html":{"content_type":"text/html","hash":"b7ac7932598006a180542adb878232b1","path":"/build/b7ac7932598006a180542adb878232b1.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"rv4lDXlWLg"}],"visibility":"show","key":"YZ7n8t5bZ8"}],"visibility":"show","key":"w6VrlMdnme"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The color-graded bigram counts array! Looks good. This array actually has all the necessary information for us to start sampling from this bigram character language model. Let’s just start by sampling the start character (of course) of each name: the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a702XpJlsK"},{"type":"inlineCode","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uxbkUPIxdF"},{"type":"text","value":" character. The first row tells us how often each other character follows it. In other words, the first row tells us how often each character is the first character of a word:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B6A2TfLD63"}],"key":"O05mJc7lNE"}],"key":"pPCNvPT7Pl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"N[0]","key":"tnbvUZJTTB"},{"type":"outputs","id":"u_thnYAD23bWPEcUgohJL","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":13,"metadata":{},"data":{"text/plain":{"content":"tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)","content_type":"text/plain"}}},"children":[],"key":"S55jVTiLWx"}],"key":"RnOK6Dt5Ij"}],"key":"nGNvOlWv11"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To get the probability of each of character being the first:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R4Q9T6BdoG"}],"key":"oHyHiyY6xm"}],"key":"y0M5omQ3z2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"p = N[0].float()\np = p / p.sum()\np","key":"AK8P5W7uqY"},{"type":"outputs","id":"90gIal5S6UHwY8UYe96g4","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/plain":{"content":"tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])","content_type":"text/plain"}}},"children":[],"key":"tclMXIjp8a"}],"key":"BaOa13LsLX"}],"key":"gBJ4UTEVKD"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Each value of this probability distribution corresponds simply to the probability of the corresponding character being the first character of a word. And of course it sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uwfxvzQ88D"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PYOsCWLn6X"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OznoAJRcEC"}],"key":"MXr8YN0JVG"}],"key":"XTEXeG7ZL6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert p.sum() == 1","key":"IBP3MqAWiP"},{"type":"outputs","id":"QsbGL-GqYrLSryqekBTip","children":[],"key":"rkf4x6c2iS"}],"key":"v94ie2yblJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we’ll sample numbers according to this probability distribution using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QOmabI1cUv"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.multinomial","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i8lAZg73Ht"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial","key":"rN5O5GwIlS"},{"type":"text","value":". And to do so deterministically we are going to use a generator. So, let’s take a brief detour and test out how to sample. First we create a probability distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bpAUs9jqan"}],"key":"tmMeKGmQiX"}],"key":"owrElDmvcB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"SEED = 2147483647\ng = torch.Generator().manual_seed(SEED)\nptest = torch.rand(3, generator=g)\nptest = ptest / ptest.sum()\nptest","key":"PW26ZIyQvh"},{"type":"outputs","id":"ZAuBV6XAK335ztBfmD2wn","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":16,"metadata":{},"data":{"text/plain":{"content":"tensor([0.6064, 0.3033, 0.0903])","content_type":"text/plain"}}},"children":[],"key":"APKnIVvpGt"}],"key":"CsydoaZqS3"}],"key":"VwZbJHx0iG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then, we sample from this distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DgFKdhR4o4"}],"key":"EPEamqNnGi"}],"key":"CuYoaD72Mo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"s = torch.multinomial(ptest, num_samples=100, replacement=True, generator=g)\ns","key":"o6m2eqUWfz"},{"type":"outputs","id":"VCx6qr76DJjIGOLKoBDLR","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":17,"metadata":{},"data":{"text/plain":{"content":"tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,\n        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,\n        0, 1, 1, 1])","content_type":"text/plain"}}},"children":[],"key":"L8uZERnVt5"}],"key":"hM0HP7AYIG"}],"key":"hTt59I5zju"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Simple. Now, notice that it outputs the same tensor however many times you run the cells. That’s because we have set a fixed seed and passed the generator object to the functions. Now, notice the output of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s9JIK01duT"},{"type":"inlineCode","value":"torch.multinomial","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nvjcyMxqxu"},{"type":"text","value":". What we expect is that around ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r2D59wnSjt"},{"type":"inlineMath","value":"60.64\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>60.64</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">60.64\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">60.64%</span></span></span></span>","key":"EpTWTEsvvw"},{"type":"text","value":" of the numbers to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UDMMPaC55O"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m34u5TGdaD"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QJ6yt6WlbV"},{"type":"inlineMath","value":"30.33\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>30.33</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">30.33\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">30.33%</span></span></span></span>","key":"ZTDuRxRggZ"},{"type":"text","value":" to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Iz4wfP1Y0u"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WqZdT0lbQ5"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PFQjdk0UrN"},{"type":"inlineMath","value":"9.03\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>9.03</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">9.03\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">9.03%</span></span></span></span>","key":"cGPo6z89t0"},{"type":"text","value":" to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hrEUq8QSB5"},{"type":"inlineCode","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e6KBAeMqR2"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zcOBdwxwYR"}],"key":"aH1anX8gM0"}],"key":"vMdMdDMzYP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sbc = torch.bincount(s)\nfor i in [0, 1, 2]:\n    print(f\"Ratio of {i}: {sbc[i]/sbc.sum()}\")","key":"CUiNmhWTPp"},{"type":"outputs","id":"rcqVnhhUrYOedSm4XvPa7","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Ratio of 0: 0.6100000143051147\nRatio of 1: 0.33000001311302185\nRatio of 2: 0.05999999865889549\n"},"children":[],"key":"ZjHL1sTkhr"}],"key":"jJl4xF4uTh"}],"key":"iSFmULqUDS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Not too far away from what we expected! But, if we increase the number of samples, we will get much closer to the probabilities of our distribution. Try it out! The more samples we take, the more the actual occurence ratios match the probabilities of the distribution the numbers were sampled from. Now, it’s time to sample from our initial character probability distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n1GCDzbnXg"}],"key":"zTRPTzQwUV"}],"key":"Hn90R7znX3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nidx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nitoc[idx]","key":"upYLS4ynyJ"},{"type":"outputs","id":"8UgcfHyumsEMtYsdPfPaV","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":19,"metadata":{},"data":{"text/plain":{"content":"'j'","content_type":"text/plain"}}},"children":[],"key":"B7hEfA7SxG"}],"key":"nXQmCcMkWv"}],"key":"GPgJ1JKiXN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We are now ready to write out our name generator.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mQXrq50Doq"}],"key":"bShcaxwAjG"}],"key":"xEPmOPzNxz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nP = N.float()\nP = P / P.sum(\n    1, keepdim=True\n)  # sum over the column dimension and keep column dimension\nfor i in range(20):\n    out = []\n    idx = 0\n    while True:\n        p = P[idx]\n        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[idx])\n        if idx == 0:\n            break\n    print(\"\".join(out))","key":"ykfbBUNXYj"},{"type":"outputs","id":"ZZVDwHvWOo4L7v1N3dMMX","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"junide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\ndedainrwieta.\nssonielylarte.\nfaveumerifontume.\nphynslenaruani.\ncore.\nyaenon.\nka.\njabdinerimikimaynin.\nanaasn.\nssorionsush.\n"},"children":[],"key":"jqbmS6lndb"}],"key":"J7rBmgyoyn"}],"key":"Yh0P0rwlnY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"It works! It yields names. Well, kinda. Some look name-like enough but most are just terrible. Lol. This is a bigrams model for you! To recap, we trained a bigrams language model essentially just by counting how frequently any pairing of characters occurs and then normalizing so that we get a nice probability distribution. Really, the elements of array ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YU9YGbLPDf"},{"type":"inlineCode","value":"P","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GWJNNYQPQz"},{"type":"text","value":" are the parameters of our model that summarize the statistics of these bigrams. We train the model and iteratively sample the next character and feed it in each time and get the next character. But how do we evaluate our model? We can do so, by looking at the probability of each bigram.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t7j8u5wTeN"}],"key":"cdTnpEIiTq"}],"key":"hEI8K7EJpw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"for w in words[:3]:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = ctoi[ch1]\n        ix2 = ctoi[ch2]\n        prob = P[ix1, ix2]\n        print(f\"{ch1}{ch2}: {prob:.4f}\")","key":"hZXbipMLU7"},{"type":"outputs","id":"UQnGPGL82E3Fd_7J_w_GH","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".e: 0.0478\nem: 0.0377\nmm: 0.0253\nma: 0.3899\na.: 0.1960\n.o: 0.0123\nol: 0.0780\nli: 0.1777\niv: 0.0152\nvi: 0.3541\nia: 0.1381\na.: 0.1960\n.a: 0.1377\nav: 0.0246\nva: 0.2495\na.: 0.1960\n"},"children":[],"key":"GW5nXnRZFm"}],"key":"p3tKYv7jF7"}],"key":"XP6DalBpfT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here we are looking at the probabilities that the model assigns to every bigram in the dataset. Just keep in mind that we have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RfLt5UiDp8"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CkY0t1hA3D"},{"type":"text","value":" characters, so if everything was equally likely we would expect all probabilities to be:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yD2V7F9SbO"}],"key":"kyqSAQizDG"}],"key":"FWMy6pAa4C"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"1/27","key":"dWl03tw2a6"},{"type":"outputs","id":"wpHgk_sv7U7k3aF_YnVpp","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":22,"metadata":{},"data":{"text/plain":{"content":"0.037037037037037035","content_type":"text/plain"}}},"children":[],"key":"moDE4mS7Ox"}],"key":"Kn30e85uX0"}],"key":"TtKV9xwMLC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Since they are not and we have mostly higher probabilities, it means that our model has learned something useful. In an ideal case, we would expect the bigram probabilities to be near ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HaWb83DTSh"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AEbwjkeAju"},{"type":"text","value":" (perfect prediction probability). Now, when you look at the literature of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gok1HPh9nq"},{"type":"link","url":"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"maximum likelihood estimation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FoMedA2QRo"}],"urlSource":"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation","data":{"page":"Maximum_likelihood_estimation","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"wPi2VSYFX5"},{"type":"text","value":", statistical modelling and so on, you’ll see that what’s typically used here is something called the likelihood: the product of all the above probabilities. This gives us the probability of the entire dataset assigned by the model that you made. But, because the product of these probabilities is an unwieldly, very tiny number to work with (think ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IgV1qW8y8G"},{"type":"inlineMath","value":"0.0478 \\times 0.0377 \\times 0.0253 \\times ...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.0478</mn><mo>×</mo><mn>0.0377</mn><mo>×</mo><mn>0.0253</mn><mo>×</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">0.0478 \\times 0.0377 \\times 0.0253 \\times ...</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">0.0478</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">0.0377</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">0.0253</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.1056em;\"></span><span class=\"mord\">...</span></span></span></span>","key":"ZoZPbKVdCi"},{"type":"text","value":"), for convenience, what people usually work with is not the likelihood, but the log-likelihood. The log, as you can see:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BslFIuwwL6"}],"key":"hkNLLBiN5P"}],"key":"UJXXq5AaCs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\n\nx = np.arange(0.01, 1.0, 0.01)\ny = np.log(x)\nplt.figure()\nplt.plot(x, y)","key":"EYslnhPRI3"},{"type":"outputs","id":"3NxheCef9vSv_nb22lDu4","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8cff68e9d3544a6aa5001375df6992e9\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"d97c0c2cfc87a8e517face2402198721","path":"/build/d97c0c2cfc87a8e517face2402198721.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=640.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"HEYPalifvd"}],"key":"uwmonzPotY"}],"key":"jeVVDMpb38"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"is a monotonic transformation of the probability, where if you pass in probability ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IsXM9d88LQ"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qPFH4EXPvF"},{"type":"text","value":" you get log-probability of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lkBMTb5uBd"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PLm6AeLAYB"},{"type":"text","value":", and as the probabilities you pass in decrease, the log-probability decreases all the way to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ObpgS0Qz5t"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">-\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord\">−</span><span class=\"mord\">∞</span></span></span></span>","key":"lSiLvWoXxw"},{"type":"text","value":" as the probability approaches ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WjJ12j3rpF"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ePTLhNVw12"},{"type":"text","value":". Therefore, let’s also add the log probability in our loop to see what that looks like:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V2mnGlHeVG"}],"key":"bS5PTWjLF8"}],"key":"RcbS5ZXHFZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def test_model(iterable, print_probs=True, calc_ll=False, print_nll=False):\n    if print_nll:\n        calc_ll = True\n    log_likelihood = 0.0\n    n = 0\n    for w in iterable:\n        chs = [\".\"] + list(w) + [\".\"]\n        for ch1, ch2 in zip(chs, chs[1:]):\n            prob = P[ctoi[ch1], ctoi[ch2]]\n            logprob = torch.log(prob)\n            if calc_ll:\n                log_likelihood += logprob.item()\n                n += 1\n            if print_probs:\n                print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n    if calc_ll:\n        print(f\"{log_likelihood=}\")\n    if print_nll:\n        nll = -log_likelihood\n        print(f\"{nll=}\")\n        print(f\"loss={nll/n}\")\n    return log_likelihood\n\n\n_ = test_model(words[:3])","key":"mXQk2cYjPf"},{"type":"outputs","id":"DflC5H0Q5ldeWJn7jA0u5","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\n"},"children":[],"key":"gOxjWF7EVc"}],"key":"lIU8NOIxDz"}],"key":"L6l1sLuMHP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, for higher probabilities we get closer and closer to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ln6CNivUQr"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tCqKzKB3tj"},{"type":"text","value":", but lower probabilities gives us a more negative number. And so to calculate the log-likelihood, we just sum up all the log probabilities:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HiUZbvnnlf"}],"key":"N44QEtXfBU"}],"key":"itOwlxHTwU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"log_likelihood = test_model(words[:3], calc_ll=True)","key":"YhtmxoBRTV"},{"type":"outputs","id":"VOnhbj7gJ4_jqc07OfXCO","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\nlog_likelihood=-38.785636603832245\n"},"children":[],"key":"YM2s0P9OD7"}],"key":"XellT6uxeq"}],"key":"f9s3t4RWxu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, how high can log-likelihood get? As high as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MR64cC5LTy"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Py3NKAgbA2"},{"type":"text","value":"! So, when all the probabilities are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cd2VyDrAB3"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WqvYNx1oaE"},{"type":"text","value":", it will be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uZ7mdbg0Bq"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PlrZbSep4f"},{"type":"text","value":". But the further away from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XID2kxLBSj"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MjEk3iu6nJ"},{"type":"text","value":" they are, the more negative the log-likehood will get. Now, we don’t actually like this because we are looking to define here is a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IxykCiAKHk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sWccqhL1o1"}],"key":"shNgBXbVwz"},{"type":"text","value":" function, that has the semantics where high is bad and low is good, since we are trying to minimize it. Any ideas? Well, we actually just need to invert the log-likelihood, aka take the negative log-likelihood (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nrD1gcdFFt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kByC57085m"}],"key":"jq8pDmLrfC"},{"type":"text","value":"):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S6YqoNuSsO"}],"key":"ANflM5N5ph"}],"key":"qagTF90skN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"nll = -log_likelihood\nprint(f'{nll=}')","key":"UiWYKvj9kg"},{"type":"outputs","id":"z-RAUY8ddPvFaXp1gA1ny","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"nll=38.785636603832245\n"},"children":[],"key":"kHC070wP35"}],"key":"R89ezCbFfQ"}],"key":"gjz54bR0QC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"su6jeJ3edg"}],"key":"gOiEluYZwB"},{"type":"text","value":" is a very nice loss function because the lowest it can get is zero and the higher it is the worse off the predictions are that we are making. People also usually like to see the average of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EXH2iEgTx9"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XMINTwzKA6"}],"key":"sU7x58w7s2"},{"type":"text","value":" instead of just the sum:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s3dSTPaW6t"}],"key":"tgGmcV4wdT"}],"key":"Uz0pm1wli5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"test_model(words[:3], print_probs=False, calc_ll=True, print_nll=True);","key":"HvcBr4EYN9"},{"type":"outputs","id":"MAdYH8TKGaQ5NOQFhkv5l","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"log_likelihood=-38.785636603832245\nnll=38.785636603832245\nloss=2.4241022877395153\n"},"children":[],"key":"iWerppT2Am"}],"key":"a4D40JxWrM"}],"key":"CMnglpnkd3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kND5YfbaGf"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ur7okTbHUC"}],"key":"dkcGVVpPdo"},{"type":"text","value":" function for the training set assigned by the model yields a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dWGuPpIq1D"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HBnDsn37P8"}],"key":"vgpKxt98iN"},{"type":"text","value":" of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y7BcygRWc7"},{"type":"text","value":"2.424","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SYhAsyOFZk"},{"type":"text","value":". The lower it is, the better off we are. The higher it is, the worse off we are. So, the job of training is produce a high-quality model, by finding the parameters that minimize the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rhA2hhYSoO"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jAOlGT2LkS"}],"key":"vvh73SAA40"},{"type":"text","value":". In this case, ones that minimize the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eIQa9SpGj6"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n0QVhMfzBV"}],"key":"dwhTUWVYbR"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PC69ieovcu"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TzOpYkbBPd"}],"key":"NN9eQS2vWj"},{"type":"text","value":". To summarize, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sjAE7QeOCA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"goal","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zRB0rIazqm"}],"key":"ONQOCf5nXW"},{"type":"text","value":" is to maximize likelihood of the data ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MAyWaqUtHp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"w.r.t.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D0vAKgQWk6"}],"key":"QtGEHakVrz"},{"type":"text","value":" model parameters (in our statistical modeling case these are the bigram probabilities), which is:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x93PM5hQwg"}],"key":"qwt9hvw3nN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"equivalent to maximizing the log-likelihood (because the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"PplBwMJcK4"},{"type":"inlineMath","value":"\\log","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>log</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\log</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span></span></span></span>","key":"aqIFZQexyY"},{"type":"text","value":" function is monotonic)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"fhjlTLSW3r"}],"key":"rArd2WSFn6"}],"key":"MXdqzvrjS6"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"equivalent to minimizing the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xeIaD8Q3zO"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Cq5Ni9mvS5"}],"key":"vmghlomo14"}],"key":"u8wQ7gD06h"}],"key":"uGGlq8dPe5"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"equivalent to minimizing the average ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"D7WiXRPkKM"},{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"xNUX23wKST"}],"key":"YbkndMKm9c"}],"key":"augUH4SpKo"}],"key":"WklWTpC94Z"}],"key":"sDrALjYUE1"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"The lower the ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"tNbWL00gvO"},{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Va6jooYTEH"}],"key":"CKHdTPV5qM"},{"type":"text","value":" ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"B0InqBtmA3"},{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"j4S3tSQat9"}],"key":"mqM8CHQWMQ"},{"type":"text","value":" the better, since that would mean assigning high probabilities. Remember: ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"i9pAWUwmLp"},{"type":"inlineMath","value":"\\log(a \\cdot b \\cdot c) = \\log(a) + \\log(b) + \\log(c)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>a</mi><mo>⋅</mo><mi>b</mi><mo>⋅</mo><mi>c</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\log(a \\cdot b \\cdot c) = \\log(a) + \\log(b) + \\log(c)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">c</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">b</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">c</span><span class=\"mclose\">)</span></span></span></span>","key":"C5YaN837Ja"},{"type":"text","value":". Also, keep in mind that here we store the probabilities in a table format. But in what’s coming up, these numbers will not be kept explicitly but they will be calculated by a ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Zd5CDtCRZA"},{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"fUPAtkjEHs"}],"key":"htWymMMNef"},{"type":"text","value":" and we will change its parameters to maximize the likelihood of these probabilities. Let’s now test out our model with a random name:","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"IB2iz6zdKK"}],"key":"fDjAGBZ4tT"}],"key":"kdFkJ5lEJx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"test_model(iterable=['christosqj'], calc_ll=True, print_nll=True);","key":"ujeiamO49I"},{"type":"outputs","id":"fbLnKqFHmvvWpuKXk1a9X","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".c: 0.0481 -3.0337\nch: 0.1880 -1.6713\nhr: 0.0268 -3.6199\nri: 0.2388 -1.4320\nis: 0.0743 -2.5990\nst: 0.0944 -2.3605\nto: 0.1197 -2.1224\nos: 0.0635 -2.7563\nsq: 0.0001 -9.0004\nqj: 0.0000 -inf\nj.: 0.0245 -3.7098\nlog_likelihood=-inf\nnll=inf\nloss=inf\n"},"children":[],"key":"mLM19IFmdf"}],"key":"a9qAWboqn9"}],"key":"ePNvAPJIgB"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, the probability of the bigram ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ixpVDO7dg6"},{"type":"inlineCode","value":"sq","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"juuBAGyZWL"},{"type":"text","value":" is super low. Whereas the probability for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n3I49U1WWn"},{"type":"inlineCode","value":"qj","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wgrxXNguoq"},{"type":"text","value":", since it is never encountered in our training data (see our bigram count table!), is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pMgusFHCyM"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p2XVPkZQLd"},{"type":"text","value":", which predictably yields a log-probability of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DrvHDpXI2R"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">-\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord\">−</span><span class=\"mord\">∞</span></span></span></span>","key":"zaqluvSUr8"},{"type":"text","value":", which in turn causes the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OWmu4WN1SS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lg0YIxl0Tc"}],"key":"uedR0Z2asN"},{"type":"text","value":" to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xvwuc3i5JH"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">-\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord\">−</span><span class=\"mord\">∞</span></span></span></span>","key":"WXkvF6reTJ"},{"type":"text","value":". What this means is that this model is exactly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kpT1yep8A9"},{"type":"inlineMath","value":"0 \\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">0 \\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">0%</span></span></span></span>","key":"I6LiapjzPh"},{"type":"text","value":" likely to predict this name (infinite ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FODqFyVgcA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UvJCXI7g5D"}],"key":"agBhXF3XZy"},{"type":"text","value":"). If you look up the table you see that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oDQpGVIMKt"},{"type":"inlineCode","value":"q","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X6DSekftsD"},{"type":"text","value":" is followed by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w0tqImCa2u"},{"type":"inlineCode","value":"j","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kpf2Vb4hsr"},{"type":"text","value":" zero times. This kind of behavior people don’t usually like too much, so there is a simple trick to alleviate it: model smoothing. It involves adding some fake counts to the bigram counts array so that never is there a bigram with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TC9fBkNw0R"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QAYkEiRcsC"},{"type":"text","value":" counts (and therefore ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wF6gdfJ0WH"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NSbgSVXwcP"},{"type":"text","value":" probability). This ensures that there are no zeros in our bigram counts matrix. E.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hSf4j1cTtn"}],"key":"NXPoMFvPF5"}],"key":"HdFgo1ZVyK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"P = (N + 1).float()\nP = P / P.sum(\n    1, keepdim=True\n)  # sum over the column dimension and keep column dimension\ntest_model(iterable=[\"christosqj\"], calc_ll=True, print_nll=True)","key":"PslZuTFr2v"},{"type":"outputs","id":"cSWiJLsa6p84_gUwXM047","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".c: 0.0481 -3.0339\nch: 0.1869 -1.6774\nhr: 0.0268 -3.6185\nri: 0.2384 -1.4338\nis: 0.0743 -2.5998\nst: 0.0942 -2.3625\nto: 0.1193 -2.1257\nos: 0.0634 -2.7578\nsq: 0.0002 -8.3105\nqj: 0.0033 -5.7004\nj.: 0.0246 -3.7051\nlog_likelihood=-37.32549834251404\nnll=37.32549834251404\nloss=3.393227122046731\n"},"children":[],"key":"fted0ysZQ8"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":29,"metadata":{},"data":{"text/plain":{"content":"-37.32549834251404","content_type":"text/plain"}}},"children":[],"key":"JqjiITORrf"}],"key":"l37sRcJosa"}],"key":"SvkzHYWHUc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we avoid getting a loss of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nWYmWgSXBv"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">-\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord\">−</span><span class=\"mord\">∞</span></span></span></span>","key":"HpSntX0igE"},{"type":"text","value":". Cool! So we’ve now trained a respectable bigram character-level language model. We trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions. We saw that we can also then use those parameters of this model to perform sampling of new words (sample new names according to these distributions) and evaluate the quality of this model which is summarized by a single number: the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kBeJhHAOW3"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ALRFixVU6B"}],"key":"A8gNFvCS6Y"},{"type":"text","value":". And the lower this number is, the better the model is because it is giving high probabilities to the actual mixed characters of all the bigrams in our training set. Great! We basically, counted and then normalized those counts, which is sensible enough.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"othBLHngc2"}],"key":"jYpKhA1mWw"}],"key":"s1Ic37a9js"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Casting the model as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l0KbiGoYX8"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xH95V2HTlP"}],"key":"OMizs7Rg91"}],"identifier":"casting-the-model-as-a-nn","label":"Casting the model as a nn","html_id":"casting-the-model-as-a-nn","implicit":true,"key":"xV72lPD61f"}],"key":"CrNizE4lmj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s now try a different approach by casting such a bigram language model into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GElvK15xid"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mvXnRCOFXO"}],"key":"aMHcHjdsN5"},{"type":"text","value":" framework to achieve the same goal. Our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DmPtSyZMwy"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HCyswjE5zU"}],"key":"WWgJjG5vlH"},{"type":"text","value":" is still going to be a bigram character-level language model. It will receive a single character as an input that will pass through a bunch of weighted neurons and then output the probability distribution over the next character in the sequence. It’s going to make guesses about what character is going to follow the input character. In addition, we’ll be able to evaluate any setting of the parameters of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XVQz4XeaWT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VQnftDg6Mx"}],"key":"vgYDbnreaG"},{"type":"text","value":", since we have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vxzQOq9Bvo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tQXYU94315"}],"key":"VBTt8Vx9bE"},{"type":"text","value":" function. Basically, we’re going to take a look at the probabilities distributions our model assigns for our next character and find the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JQcwL678tD"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RdPt6pAcou"}],"key":"ti2uesfH6Q"},{"type":"text","value":" between those and the labels (which are the character that we expect to come next in the bigram). By doing so, we can use gradient-based optimization to tune the weights of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MfRKfl4S5G"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WEgshlwj27"}],"key":"Bftzv19bwZ"},{"type":"text","value":" that give us the output probabilities. Let’s begin this alternative approach by first constructing our dataset:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZECORG8wps"}],"key":"C23olL9I5v"}],"key":"g9u2vU7Iog"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create training dataset of bigrams (x, y)\nxs, ys = [], []\nfor w in words[:1]:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2)\n        xs.append(ctoi[ch1])\n        ys.append(ctoi[ch2])\n# Convert to pytorch tensor (https://pytorch.org/docs/stable/generated/torch.tensor.html)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)","key":"lVqoQH8IuT"},{"type":"outputs","id":"KmshTyqosYUk-_eAXX47_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":". e\ne m\nm m\nm a\na .\n"},"children":[],"key":"f9jG2vzg0n"}],"key":"sSPm0w3xke"}],"key":"imBmMV6rEl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xs","key":"AKX96glwJ0"},{"type":"outputs","id":"kRN35-TeMf6FBLOJFtRNE","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":31,"metadata":{},"data":{"text/plain":{"content":"tensor([ 0,  5, 13, 13,  1])","content_type":"text/plain"}}},"children":[],"key":"iQt2SJpyaH"}],"key":"JrnvFstosb"}],"key":"ddIgspEtow"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ys","key":"z5mByYaNT5"},{"type":"outputs","id":"HO_lHFeq1e5kyuXvLYmnF","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":32,"metadata":{},"data":{"text/plain":{"content":"tensor([ 5, 13, 13,  1,  0])","content_type":"text/plain"}}},"children":[],"key":"acEJDFPC5J"}],"key":"hD7JqzUrIx"}],"key":"muBg48TUNE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, how do we pass each character into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KyoaYEQuc0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b8yP4vDkhL"}],"key":"P3brvhT1rw"},{"type":"text","value":"? ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jZ4quTLA7Q"},{"type":"link","url":"https://en.wikipedia.org/wiki/One-hot","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"One-hot encoding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UOtcN85qE7"}],"urlSource":"https://en.wikipedia.org/wiki/One-hot","data":{"page":"One-hot","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"esOYqXgdjY"},{"type":"text","value":"! With this encoding, each integer is encoded with bits.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"II1anT5862"}],"key":"wRXJcEa1qc"}],"key":"DEa1Kf58AA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch.nn.functional as F\n\nxenc = F.one_hot(xs, num_classes=27).float()\nxenc","key":"BqvYATiPRL"},{"type":"outputs","id":"T_FuOJonj2LHgfPus4aRN","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":33,"metadata":{},"data":{"text/plain":{"content":"tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])","content_type":"text/plain"}}},"children":[],"key":"bX5M2ix7Im"}],"key":"kOsKjQZCDV"}],"key":"BKkRLKvRCf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xenc.shape","key":"duxrD9Wery"},{"type":"outputs","id":"MjM47WajcLjbBTt3DsJ3A","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":34,"metadata":{},"data":{"text/plain":{"content":"torch.Size([5, 27])","content_type":"text/plain"}}},"children":[],"key":"dhqQUK0VQi"}],"key":"YXprpk12lx"}],"key":"KOAwZtoD42"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.imshow(xenc);","key":"FSzaWsYYh7"},{"type":"outputs","id":"NyyWWum15R45HNt0aFcT4","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"958200875f3d4e87b042cf5e9da17908\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"d18c6e0069b4dcc50d26f37021326cac","path":"/build/d18c6e0069b4dcc50d26f37021326cac.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=640.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"JnFKTw7pmq"}],"key":"tpGUyyL3QD"}],"key":"IFEEJryYne"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s create our neuron:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wNIn97BYox"}],"key":"YFMIDVjgoS"}],"key":"walOrEfB6a"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W = torch.randn((27, 1))\nxenc @ W","key":"SgTtgcnypo"},{"type":"outputs","id":"tucxSZxA6mcfAjX9-Hh72","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":36,"metadata":{},"data":{"text/plain":{"content":"tensor([[-0.3270],\n        [-1.4539],\n        [-0.8739],\n        [-0.8739],\n        [ 1.4181]])","content_type":"text/plain"}}},"children":[],"key":"upjg9d0dCm"}],"key":"IDgYI5Wins"}],"key":"VL2PFyVSiX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Our neuron receives one character of size ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DicIOGCGiM"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mrjvVQLJdD"},{"type":"text","value":" and spits out ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eUCxwXkhYH"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gs8FgsjIts"},{"type":"text","value":" output value. However, as you can see, since PyTorch supports matrix multiplication, our neuron can receive ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I0vMZGNYi8"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yBkTlmDpGB"},{"type":"text","value":" characters of size ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OSIx5jd8q2"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ptEciwP38K"},{"type":"text","value":" in parallel and output each character’s output in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"odVMMiLms3"},{"type":"inlineMath","value":"5 \\times 1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>5</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">5 \\times 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span>","key":"WODwIbZEeS"},{"type":"text","value":" matrix (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wxhZmVyyYl"},{"type":"inlineMath","value":"[5 \\times 27] \\cdot [27 \\times 1] \\rightarrow [5 \\times 1]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>5</mn><mo>×</mo><mn>27</mn><mo stretchy=\"false\">]</mo><mo>⋅</mo><mo stretchy=\"false\">[</mo><mn>27</mn><mo>×</mo><mn>1</mn><mo stretchy=\"false\">]</mo><mo>→</mo><mo stretchy=\"false\">[</mo><mn>5</mn><mo>×</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[5 \\times 27] \\cdot [27 \\times 1] \\rightarrow [5 \\times 1]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">27</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span></span></span></span>","key":"BPfVmIF3iY"},{"type":"text","value":"). Now, let’s pass our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e4nJ9P0edw"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UvO5fFAveZ"},{"type":"text","value":" characters as inputs through ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R3nEUkVhlk"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X91Z1At6xN"},{"type":"text","value":" neurons instead of just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xYlVe4nHkX"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OGFpWIXlHs"},{"type":"text","value":" neuron:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nEnGCgVGsa"}],"key":"mcIbz4g9a8"}],"key":"RXknC0XLuS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W = torch.randn((27, 27))\nxenc @ W","key":"Zp2QUxr9sQ"},{"type":"outputs","id":"8z5akYAuO6DcwpxJ-X7bW","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":37,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0.9979,  0.2825,  1.1355,  0.3798, -0.2801,  0.0672, -1.1496,  2.1393,\n         -0.2687, -1.4350,  1.1158,  0.4346, -0.4915, -0.1916,  1.4139, -0.4590,\n         -0.5869,  1.6688,  0.8819,  0.8542, -0.0366, -0.6968,  0.1041,  0.8881,\n          0.7592, -0.5573,  0.9596],\n        [-0.1725, -1.5476,  1.5005,  1.4560,  0.9079, -1.2025,  0.1265,  0.1533,\n         -0.2189, -1.3150,  1.6275,  0.3342,  1.4620, -0.3458, -0.2391,  0.5896,\n          1.7679,  1.1726, -0.6278, -0.1539, -0.6117, -0.0106,  0.7131,  2.0526,\n          1.2183,  1.6270, -1.3764],\n        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,\n          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,\n          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,\n          0.1524,  1.5829,  0.3142],\n        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,\n          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,\n          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,\n          0.1524,  1.5829,  0.3142],\n        [ 0.2948,  0.0746, -0.4187,  0.4092, -0.6537,  1.1562,  0.6917, -1.2596,\n         -0.1424, -0.5520, -1.1731, -0.4088, -0.6465, -0.2629, -0.3580,  0.8126,\n         -1.7589,  1.7377, -0.5665,  1.9188, -0.6135, -1.2176,  0.0166,  0.1594,\n         -0.8806,  0.6167, -0.9173]])","content_type":"text/plain"}}},"children":[],"key":"bXmzfO2EIA"}],"key":"F83298MAbi"}],"key":"LC8K68YQH7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Predictably, we get ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qU9fVCOIy7"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IISrD40OGX"},{"type":"text","value":" arrays (one per input/character) of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yf8nAqYARU"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"olIMpmp9gp"},{"type":"text","value":" outputs (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b5WKhIkQYw"},{"type":"inlineMath","value":"[5 \\times 27] \\cdot [27 \\times 27] \\rightarrow [5 \\times 27]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>5</mn><mo>×</mo><mn>27</mn><mo stretchy=\"false\">]</mo><mo>⋅</mo><mo stretchy=\"false\">[</mo><mn>27</mn><mo>×</mo><mn>27</mn><mo stretchy=\"false\">]</mo><mo>→</mo><mo stretchy=\"false\">[</mo><mn>5</mn><mo>×</mo><mn>27</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[5 \\times 27] \\cdot [27 \\times 27] \\rightarrow [5 \\times 27]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">27</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">27</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">27</span><span class=\"mclose\">]</span></span></span></span>","key":"cIdfSjCvEh"},{"type":"text","value":"). Each output number represents each neuron’s firing rate of a specific input. For example, the following is the firing rate of the 13th neuron of the 3rd input:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U6va0LkAmI"}],"key":"Y3G4SksSxU"}],"key":"AX4TwfalZ5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc @ W)[3, 13]","key":"CSdaANoDEP"},{"type":"outputs","id":"7FYVf9VlSLgn0jYCr6nkh","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":38,"metadata":{},"data":{"text/plain":{"content":"tensor(-1.2633)","content_type":"text/plain"}}},"children":[],"key":"XetH5FgfDE"}],"key":"jjPqNTFZEo"}],"key":"WU69lpbVJJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What PyTorch allows is matrix multiplication that enables parallel dot products of many inputs in a batch with the weights of neurons of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QLkaY4PloS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ASmIQVZMRM"}],"key":"UesAmojGiT"},{"type":"text","value":". For example, this is how to multiply the inputs that represent the 3rd character with the weights of the 13th neuron:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ikT24DmmGJ"}],"key":"rjXOKo6CWm"}],"key":"TtPUnnysiB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xenc[3]","key":"h2vari4r0C"},{"type":"outputs","id":"ZCpPtUkz2PrqrJI8TTj4C","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.])","content_type":"text/plain"}}},"children":[],"key":"PKMUoOwlE7"}],"key":"yFTJuf5etq"}],"key":"MHpPyOTBmV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W[:, 13]","key":"XNmzhsTjSL"},{"type":"outputs","id":"-nkrrgI0PtyFMAdJPBTyg","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":40,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.1916, -0.2629, -1.1183,  0.9108,  0.7797, -0.3458, -1.2783, -0.7899,\n        -0.3221, -0.4800,  0.3307,  0.2826, -0.5372, -1.2633,  0.3663,  0.1210,\n         0.0446, -0.1690, -0.3741, -0.0798, -0.5883, -0.9373, -0.1367, -0.2475,\n        -0.4424, -2.0253, -0.1943])","content_type":"text/plain"}}},"children":[],"key":"uKOl1FUHYO"}],"key":"VgZ0ZWPaqK"}],"key":"jKyNYjANhC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc[3] * W[:, 13]).sum()","key":"fLhs4Y5Hzb"},{"type":"outputs","id":"cLWwW4qGjnHloILZNiWBs","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/plain":{"content":"tensor(-1.2633)","content_type":"text/plain"}}},"children":[],"key":"s95Rv2XxJC"}],"key":"dchnr14FbT"}],"key":"pK5QBhILfx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc @ W)[3, 13]  # same as above","key":"CKYFefEVgP"},{"type":"outputs","id":"kgvANCe2dzQILj5Olip1G","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":42,"metadata":{},"data":{"text/plain":{"content":"tensor(-1.2633)","content_type":"text/plain"}}},"children":[],"key":"ezEjhcV32O"}],"key":"QAAdaniA1I"}],"key":"uUforoedkZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Ok, so what did is we fed our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lt4HlmgQdZ"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R84ANO0AZW"},{"type":"text","value":"-dimensional inputs into the first layer of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u74a8N6BbQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SvfmFoYLHA"}],"key":"OulmhF9VUW"},{"type":"text","value":" that has 27 neurons. These neurons perform ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OFOZLPnPqh"},{"type":"inlineCode","value":"W * x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rQXR2To4eb"},{"type":"text","value":". They don’t have a bias and they don’t have a non-linearity like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BGeTizPm7P"},{"type":"inlineCode","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JiCUGKaEa9"},{"type":"text","value":". We are going to leave our network as is: a 1-layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fc9hp0OhMm"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hGCU1yBFWL"}],"key":"ABRftRCNmz"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mj5Qqw406R"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"im1hlH8dD5"}],"key":"mBIxKApPx2"},{"type":"text","value":". That’s it. Basically, the dumbest, smallest, simplest ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XtLGGYhPso"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lQ2BnelACS"}],"key":"OpJ2CM5VVs"},{"type":"text","value":". Remember, what we trying to produce is a probability distribution for a next character in a sequence. And there’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f2FPA8T4OI"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ri1KspX6SG"},{"type":"text","value":" of them. But we have to come up with exact semantics as to how we are going to interpret these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ENvrDf3xyC"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DHfM2suSU6"},{"type":"text","value":" numbers that these neurons take on. Intuitively, as we can see in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dpsCy6wMY3"},{"type":"inlineCode","value":"xenc @ W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iEWP5NYfsE"},{"type":"text","value":" output, some of these outputs numbers are positive and some negative. That’s because they come out of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zBcrWfYz7k"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QrQzSz3hmb"}],"key":"DoNtIdaaa8"},{"type":"text","value":" layer with weights are initialized from the normal ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uh86D5K8CL"},{"type":"inlineMath","value":"[-1, 1]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-1, 1]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span></span></span></span>","key":"rlDNkzWHOi"},{"type":"text","value":" distribution. But, what we want however is something like a bigram count table that we previously produced, where each row told us the counts which we then normalized to get the probabilities. So, we want something similar to come out of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qWYEtLjVBE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jrhxzVkRZr"}],"key":"vPkpNwTKt6"},{"type":"text","value":". But, what we have right now, are some negative and positive numbers. Now, we therefore want these numbers to represent the probabilities for the next character with their unique characteristics. For example, probabilities are positive numbers and they sum to 1. Also, they obviously have to be probabilities. They can’t be counts because counts are positive integers; not a great output from a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hrctJLZVVF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QJeG9MsFhy"}],"key":"TBpAIOrbVj"},{"type":"text","value":". Instead, what the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z9GRuDiPrk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DWclVZ6JXA"}],"key":"DME5AQKQMN"},{"type":"text","value":" is going to output and how we are going to interpret these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SjlWpnoBU2"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iIDRTvPf32"},{"type":"text","value":" output numbers is as log counts. One way to accomplish this is by exponentiating each output number so that the result is always positive. Specifically, exponentiating a  negative number yields a result that is a positive value ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Unmpo4jdKX"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"less","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p5UCajtq7Q"}],"key":"WhbaQ0IRDq"},{"type":"text","value":" than ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XDvhiL1vGz"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yt19BIzDtv"},{"type":"text","value":". Whereas, exponentiating a positive number yields a result whose value is between greater than ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OCnbMSHbnI"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KM9La2UJjC"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BsWBWdI1L8"},{"type":"inlineMath","value":"\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord\">∞</span></span></span></span>","key":"oftP23Z88v"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oJVSF5paAk"}],"key":"E8H2Lb5dTG"}],"key":"mkuYXDbmUv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc @ W).exp()","key":"DDEp9gOgiW"},{"type":"outputs","id":"dtUDgIKVexIJ3CyO9ynGN","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":43,"metadata":{},"data":{"text/plain":{"content":"tensor([[2.7125, 1.3265, 3.1128, 1.4619, 0.7557, 1.0695, 0.3168, 8.4932, 0.7644,\n         0.2381, 3.0519, 1.5444, 0.6117, 0.8256, 4.1119, 0.6319, 0.5561, 5.3059,\n         2.4156, 2.3495, 0.9641, 0.4982, 1.1098, 2.4304, 2.1365, 0.5727, 2.6108],\n        [0.8415, 0.2128, 4.4841, 4.2888, 2.4792, 0.3004, 1.1348, 1.1657, 0.8034,\n         0.2685, 5.0910, 1.3968, 4.3146, 0.7077, 0.7873, 1.8032, 5.8586, 3.2305,\n         0.5338, 0.8574, 0.5424, 0.9895, 2.0402, 7.7883, 3.3814, 5.0888, 0.2525],\n        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,\n         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,\n         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],\n        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,\n         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,\n         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],\n        [1.3429, 1.0774, 0.6579, 1.5056, 0.5201, 3.1778, 1.9972, 0.2838, 0.8673,\n         0.5758, 0.3094, 0.6644, 0.5239, 0.7688, 0.6990, 2.2538, 0.1722, 5.6841,\n         0.5675, 6.8131, 0.5415, 0.2959, 1.0168, 1.1728, 0.4145, 1.8528, 0.3996]])","content_type":"text/plain"}}},"children":[],"key":"ia1Yf2VFJH"}],"key":"ExMw6kUCbL"}],"key":"fSYRTUCsRy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Such exponentiation is a great way to make the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AwkEMb0V2y"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zd6Q456FUM"}],"key":"PrY3Dzxcoy"},{"type":"text","value":" predict counts. Which are positive numbers that can take on various values depending on the setting of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XrHTWDEttB"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tY6B4TlrHW"},{"type":"text","value":". Let’s break it down more:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Xiw4rwaF95"}],"key":"fK0BayckrV"}],"key":"c7kaS2AMXZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits = xenc @ W  # log-counts\ncounts = logits.exp()  # equivalent to the N bigram counts array\nprobs = counts / counts.sum(1, keepdims=True)\nprobs","key":"GLJGsE5L4G"},{"type":"outputs","id":"S7Jz6R1YzvMF03eBJeByi","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":44,"metadata":{},"data":{"text/plain":{"content":"tensor([[0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,\n         0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,\n         0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502],\n        [0.0139, 0.0035, 0.0739, 0.0707, 0.0409, 0.0050, 0.0187, 0.0192, 0.0132,\n         0.0044, 0.0840, 0.0230, 0.0711, 0.0117, 0.0130, 0.0297, 0.0966, 0.0533,\n         0.0088, 0.0141, 0.0089, 0.0163, 0.0336, 0.1284, 0.0558, 0.0839, 0.0042],\n        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,\n         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,\n         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],\n        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,\n         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,\n         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],\n        [0.0371, 0.0298, 0.0182, 0.0416, 0.0144, 0.0879, 0.0552, 0.0078, 0.0240,\n         0.0159, 0.0086, 0.0184, 0.0145, 0.0213, 0.0193, 0.0623, 0.0048, 0.1572,\n         0.0157, 0.1884, 0.0150, 0.0082, 0.0281, 0.0324, 0.0115, 0.0512, 0.0111]])","content_type":"text/plain"}}},"children":[],"key":"StNUp3mN4S"}],"key":"luqpY1wlCv"}],"key":"ahhrRRXSbm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Therefore, we have a way to get the probabilities, where each row sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GfqrAQ2lzq"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sLJHgfX5ck"},{"type":"text","value":" (since they are normalized), e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ty2BMxDw5X"}],"key":"qATpQEDBLR"}],"key":"ZxsJQyCZnI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs[0].sum().item()","key":"SrbjRfEJxC"},{"type":"outputs","id":"VeJCTQT93NsH5eFLZvGIl","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":45,"metadata":{},"data":{"text/plain":{"content":"1.0","content_type":"text/plain"}}},"children":[],"key":"KQL9IKd5Ig"}],"key":"Re35YYFGEa"}],"key":"L0dsX1j5Rj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs.shape","key":"JK0Ku32Rmw"},{"type":"outputs","id":"I4-JjRQVVZHzLRagMUiUQ","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":46,"metadata":{},"data":{"text/plain":{"content":"torch.Size([5, 27])","content_type":"text/plain"}}},"children":[],"key":"bC0Lu2mQgU"}],"key":"vP1xnaP8rB"}],"key":"gW3eobXI4V"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What we have achieved is that for every one of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r6n31Hz3uL"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dQHfFgjByx"},{"type":"text","value":" examples, we now have a row that came out of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BWAEdWesbg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N3KbnAObb6"}],"key":"aWs5tlVYpj"},{"type":"text","value":". And because of the transformations here, we made sure that this output of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V4OCb4QE1L"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x0tPUC6IpI"}],"key":"Afkp06IBF2"},{"type":"text","value":" can be interpreted as probabilities. In other words, what we have done is that we took inputs, applied differentiable operations on them (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xab05BAPt5"},{"type":"inlineCode","value":"@","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"clgkvDAs8e"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p9Q8fhMIQ0"},{"type":"inlineCode","value":"exp()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GbtbKyWDxn"},{"type":"text","value":") that we can ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NXLwnc89qg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ypWmtH5eTP"}],"key":"sGfHfBlPdD"},{"type":"text","value":" through and we are getting out probability distributions. Take the first input character that was fed in as an example:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yzsXmdUACv"}],"key":"PkzW1eXJqc"}],"key":"U6XroBKBQe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xenc[0]","key":"h6Gt39PZHW"},{"type":"outputs","id":"OSwGC8nWtNA8AatyRd_Fb","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":47,"metadata":{},"data":{"text/plain":{"content":"tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.])","content_type":"text/plain"}}},"children":[],"key":"gjlnV3MeFd"}],"key":"ETHyhlBPMW"}],"key":"X9dnJdzxDh"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"that corresponds to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vJDUwqqLOH"},{"type":"inlineCode","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xNCnL0DYLp"},{"type":"text","value":" symbol from the name:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WuQ9wNKuHK"}],"key":"GmKuSROzLk"}],"key":"dkUvp1iS6a"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words[0]","key":"eNvzq1F4Uv"},{"type":"outputs","id":"ZNSYqsusmGEg9BfEGX5uN","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":48,"metadata":{},"data":{"text/plain":{"content":"'emma'","content_type":"text/plain"}}},"children":[],"key":"mp73U6SAEm"}],"key":"DE0L2R5ya3"}],"key":"YRUCh85jBm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The way we fed this character into the neural network is that we first got its index, then we one-hot encoded it, then it went into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tws2sjtW5L"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IUDRGywx18"}],"key":"H5a2tUFYrj"},{"type":"text","value":" and out came this distribution of probabilities:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iRUdamSGl3"}],"key":"jusD0rvIoI"}],"key":"UAYEk7vpIR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs[0]","key":"LafiIXiZHx"},{"type":"outputs","id":"uB4c6axVZlswhi7LGzH8Y","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":49,"metadata":{},"data":{"text/plain":{"content":"tensor([0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,\n        0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,\n        0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502])","content_type":"text/plain"}}},"children":[],"key":"tlguPRqqjw"}],"key":"ve5vU4D4gV"}],"key":"pn85bI7cYh"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"with a shape of:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qUyYPchvQj"}],"key":"cdatOeGr7J"}],"key":"SoKJRvkn0U"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs[0].shape","key":"Il8Lu7O8Qy"},{"type":"outputs","id":"YfLnyzt90W7j0aZXNsAln","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":50,"metadata":{},"data":{"text/plain":{"content":"torch.Size([27])","content_type":"text/plain"}}},"children":[],"key":"BbKOV7XXqa"}],"key":"UuVa2F051M"}],"key":"cpoFflkDBR"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RQkOkro742"},{"type":"text","value":" numbers. We interpret these numbers of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vpNdX1pQxc"},{"type":"inlineCode","value":"probs[0]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qtCnbtRnrS"},{"type":"text","value":" as the probability or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FpbSdDyXrI"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"how likely it is","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RWMXKWslGW"}],"key":"qibPdLcgBM"},{"type":"text","value":" for each of the corresponding characters to come next. As we train the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OG9VAMUJ0u"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p9vHPxzYME"}],"key":"t6Ina3FrBs"},{"type":"text","value":" by tuning the weights ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GTe0kQ8FLb"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ihXi6QqWX5"},{"type":"text","value":", we are of course going to be getting different probabilities out for every character that you input. So, the question is: can we tune ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tn24PvwBhx"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ds6WzAS28Z"},{"type":"text","value":" such that the probabilities coming out are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"odk5atKzhN"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"pretty good","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P2YPXgIuQm"}],"key":"UyqfQ32Uuu"},{"type":"text","value":"? The way we measure ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KgMuas4j8N"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"pretty good","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gbpoXJY05c"}],"key":"yfbiDzpl3L"},{"type":"text","value":" is by the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wicFdQgs2j"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wCBYOO4Fu5"}],"key":"B7yUIsIJAo"},{"type":"text","value":" function. Below you can see what have done in a simple summary:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tz1DeDjqnM"}],"key":"lSpgRxvvfF"}],"key":"aIcuQNFnQY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# SUMMARY ------------------------------>>>>\nxs  # inputs","key":"lARVA55cqB"},{"type":"outputs","id":"ZY8m2PB-pj0LFnKHIgvdi","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":51,"metadata":{},"data":{"text/plain":{"content":"tensor([ 0,  5, 13, 13,  1])","content_type":"text/plain"}}},"children":[],"key":"PF5z9mHjNm"}],"key":"GQX96RppU5"}],"key":"UbfqtHf9ny"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ys  # targets","key":"t4FPyN3p6E"},{"type":"outputs","id":"Fc5wsD02-c2Vdhu5ucVj8","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":52,"metadata":{},"data":{"text/plain":{"content":"tensor([ 5, 13, 13,  1,  0])","content_type":"text/plain"}}},"children":[],"key":"q4hQXT6HJb"}],"key":"Tn4T6L4WbX"}],"key":"LdhwE8gIgi"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Both ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HyyVmPREnc"},{"type":"inlineCode","value":"xs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V0sthTNhnm"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uCF6E9cz4U"},{"type":"inlineCode","value":"ys","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PQUHNKIRVC"},{"type":"text","value":" constitute the dataset. They are integers representing characters of a sequence/word.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lvmdcU3y1a"}],"key":"EoaLyeImAu"}],"key":"hzyK1mkyu5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Use a generator for reproducability and randomly initialize 27 neurons' weights. Each neuron receives 27 inputs.\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn((27, 27), generator=g)  # 27 incoming weights for 27 neurons\n# Encode the inputs into one-hot representations\nxenc = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n# Pass encoded inputs through first layer to get logits\nlogits = xenc @ W  # predict log-counts\n# Exponentiate the logits to get fake counts\ncounts = logits.exp()  # counts, equivalent to N\n# Normalize these counts to get probabilities\nprobs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n# NOTE: the 2 lines above constitute what is called a 'softmax'\nprobs.shape","key":"IkQZrWfyaw"},{"type":"outputs","id":"QRv_sFreVPJgS5m2HBpwG","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":53,"metadata":{},"data":{"text/plain":{"content":"torch.Size([5, 27])","content_type":"text/plain"}}},"children":[],"key":"Nm9rjEsx8e"}],"key":"pJtvJfcVmP"}],"key":"BQOMcKsqeT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Softmax is a very-often-used ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lRRT2J89lF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wW6AavHlUX"}],"key":"dAZqi6mbMn"},{"type":"text","value":" function in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JilW4prUpN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n6zpBoExBE"}],"key":"EHTeS0TrzC"},{"type":"text","value":"s. It takes in logits, exponentiates them, then divides and normalizes. It’s a way of taking outputs of a linear layer that might be positive or negative and it outputs numbers that are only positive and always sum to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rn21XNmJiY"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qppIgydTiZ"},{"type":"text","value":", adhering to the properties of probability distributions. It can be viewed as a normalization function if you want to think of it that way.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JkxpGDMPFw"}],"key":"XYfYEmq9gX"}],"key":"eG9WgjbMk5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\ndisplay(Image(filename='softmax.jpeg'))","key":"PMK6yRerEf"},{"type":"outputs","id":"UaNQIOIURnSS-b3VoSjZm","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"16dc2c955b6f9f81a6cf3b8b72fe56f3","path":"/build/16dc2c955b6f9f81a6cf3b8b72fe56f3.jpeg"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"ekMQK6gijy"}],"key":"I1cDdygFDN"}],"key":"O4nXFhxwVo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, since every operation in the forward pass is differentiable, we can ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SVrFraI4qj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BIrkh56nHF"}],"key":"xgFsizYzvS"},{"type":"text","value":" through. Below, we iterate over every input character and describe what is going on:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kUrMW4oI4D"}],"key":"sk1w25bAhf"}],"key":"burC0QhZqn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"nlls = torch.zeros(5)\nfor i in range(5):\n    # i-th bigram:\n    x = xs[i].item()  # input character index\n    y = ys[i].item()  # label character index\n    print(\"--------\")\n    print(f\"bigram example {i+1}: {itoc[x]}{itoc[y]} (indexes {x},{y})\")\n    print(\"input to the nn:\", x)\n    print(\"output probabilities from the nn:\", probs[i])\n    print(\"label (actual next character):\", y)\n    p = probs[i, y]\n    print(\"probability assigned by the nn to the correct next character:\", p.item())\n    logp = torch.log(p)\n    print(\"log likelihood:\", logp.item())\n    nll = -logp\n    print(\"negative log likelihood:\", nll.item())\n    nlls[i] = nll\nloss = nlls.mean()\nprint(\"=========\")\nprint(\"average negative log likelihood, i.e. loss =\", loss.item())","key":"ByVzvuu4iR"},{"type":"outputs","id":"y8ef1qKkx09jm4d5KKThp","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"--------\nbigram example 1: .e (indexes 0,5)\ninput to the nn: 0\noutput probabilities from the nn: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\nlabel (actual next character): 5\nprobability assigned by the nn to the correct next character: 0.01228625513613224\nlog likelihood: -4.399273872375488\nnegative log likelihood: 4.399273872375488\n--------\nbigram example 2: em (indexes 5,13)\ninput to the nn: 5\noutput probabilities from the nn: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\nlabel (actual next character): 13\nprobability assigned by the nn to the correct next character: 0.018050700426101685\nlog likelihood: -4.014570713043213\nnegative log likelihood: 4.014570713043213\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the nn: 13\noutput probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 13\nprobability assigned by the nn to the correct next character: 0.026691533625125885\nlog likelihood: -3.623408794403076\nnegative log likelihood: 3.623408794403076\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the nn: 13\noutput probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 1\nprobability assigned by the nn to the correct next character: 0.07367686182260513\nlog likelihood: -2.6080665588378906\nnegative log likelihood: 2.6080665588378906\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the nn: 1\noutput probabilities from the nn: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\nlabel (actual next character): 0\nprobability assigned by the nn to the correct next character: 0.014977526850998402\nlog likelihood: -4.201204299926758\nnegative log likelihood: 4.201204299926758\n=========\naverage negative log likelihood, i.e. loss = 3.7693049907684326\n"},"children":[],"key":"DNysiNRJGG"}],"key":"szJL3Yma7U"}],"key":"E0P0eBVwJw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, the probabilities assigned by the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jnMTTLcACy"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HTUZDQA08N"}],"key":"P3I9aL7pVX"},{"type":"text","value":" to the correct next character are bad (pretty low). See for example the probability predicted by the network of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oqqvlMKEdY"},{"type":"inlineCode","value":"m","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dRCHwCF6vK"},{"type":"text","value":" following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"epzU7lTHnW"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qTRAdkLUgz"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dDPAHeUrC0"},{"type":"inlineCode","value":"em","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IO1AtYvRbk"},{"type":"text","value":" example): the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g2gdHkytIX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XVtUOSEFr4"}],"key":"ZvJI7b2YCC"},{"type":"text","value":" value is very high (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tKEf7Q4QOl"},{"type":"text","value":"4.0145","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wVaxQQdrfc"},{"type":"text","value":"). And in general, for the whole word, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"axjg7CcESL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HsaYepbdiU"}],"key":"HANoEHB54L"},{"type":"text","value":" (the average ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MiczOv7Qru"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YPVUZErmFC"}],"key":"v0LfkhIePG"},{"type":"text","value":") is high! This means that this is not a favorable setting of weights and we can do better. One easy way to do better is to reinitialize ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L7spS1Lsea"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dJJ6qqTZ39"},{"type":"text","value":" using a different seed for example and pray to god that the loss is smaller or repeat until it is. But that is what amateurs do. We are professionals or, at least, we want to be! And what professionals do is they start with random weights, like we did, and then they optimize those weights in order to minimize the loss. We do so by some gradient-based optimization (e.g. gradient descent) which entails first doing backprop in order to compute the gradients of that weight ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vOvbVjgJSH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"w.r.t.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DlRA1MWLVo"}],"key":"hWoY48HoYT"},{"type":"text","value":" to those weights and then changing the weights by some such gradient amount in order to optimize them and minimize the loss. As we did with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yhYZdeXqly"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TZ5TgHkJC5"}],"key":"wCAX90fSTK"},{"type":"text","value":", we will write an optimization loop for doing the backward pass. But instead of mean-squared error, we are using the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UO2Et9cUsY"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XzU2nLeMgc"}],"key":"ZPQLOZombq"},{"type":"text","value":" as a loss function, since we are dealing with a classification task and not a regression one.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vJQ2GlTeJK"}],"key":"rKL2SYTeeD"}],"key":"d6F8PtDJLO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nW = torch.randn(\n    (27, 27), generator=g, requires_grad=True\n)  # 27 incoming weights for 27 neurons\n\n\ndef forward_pass(regularize=False):\n    num = xs.nelement()\n    xenc = F.one_hot(\n        xs, num_classes=27\n    ).float()  # input to the network: one-hot encoding\n    logits = xenc @ W  # predict log-counts\n    counts = logits.exp()  # counts, equivalent to N\n    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n    loss = -probs[torch.arange(num), ys].log().mean()\n    return W, loss\n\n\nW, loss = forward_pass()\n# backward pass\nW.grad = None  # set to zero\nloss.backward()","key":"KDDprrcmxw"},{"type":"outputs","id":"WMdop2p6vQGK2w4HaZD9g","children":[],"key":"wlzRCsojF3"}],"key":"oKPdgjHCGD"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, something magical happened when ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hLhKy59bs5"},{"type":"inlineCode","value":"backward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ebjiYcHVfR"},{"type":"text","value":" ran. Like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"neffYhRUfG"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WdmZ3bouQn"}],"key":"DUjFXvD8ZS"},{"type":"text","value":", PyTorch, during the forward pass, keeps track of all the operations under the hood and builds a full computational graph. So, it knows all the dependencies and all the mathematical operations of everything. Therefore, calling ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tJW4PtMhLP"},{"type":"inlineCode","value":"backward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P6zE6y7qCA"},{"type":"text","value":" on the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zxzhP9Qbuj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pOPLHDQADu"}],"key":"Ts2DKkFzqg"},{"type":"text","value":" fills in the gradients of all the intermediate nodes, all the way back to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hhlfxBdbCC"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hM8RYWIsTh"},{"type":"text","value":" value nodes. Take a look:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D96KOb4T4F"}],"key":"bvMQDIkB3M"}],"key":"vxAmxH5Aqv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W.grad","key":"ew52hQAY8F"},{"type":"outputs","id":"UQh4RJK1VRMbEK46espAa","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":57,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n          0.0024,  0.0307,  0.0292],\n        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n          0.0131,  0.0101,  0.0018],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n          0.0024,  0.0004,  0.0094],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n          0.0482,  0.0187,  0.0051],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000]])","content_type":"text/plain"}}},"children":[],"key":"pzgPKwFgVv"}],"key":"kJtwuPVEKB"}],"key":"O6NLh9u8TK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And obviously:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LFAMDENA5p"}],"key":"byrZTCsjs3"}],"key":"aUtEpu7I7s"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert W.shape == W.grad.shape","key":"XlFmOpk6XR"},{"type":"outputs","id":"esqbz5sAkIngbLn8y2FL6","children":[],"key":"CRObfo2cel"}],"key":"fwjmuS4i4W"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What a gradient value is telling us, e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k0H9a2FYlJ"}],"key":"WBaH8Qka9Y"}],"key":"fpvkvfSAU6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W.grad[1][4].item()","key":"UIihgBFRbH"},{"type":"outputs","id":"CzcBuTuXG4KzbDUJQDqDf","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":59,"metadata":{},"data":{"text/plain":{"content":"0.012119228951632977","content_type":"text/plain"}}},"children":[],"key":"nsT8TTqkXE"}],"key":"EccujxsXLx"}],"key":"IiWOlX2892"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"is that nudging the specific corresponding weight by a small ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k0JWnhkd1d"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pYj9FnjJd1"},{"type":"text","value":" value, would nudge the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KrDGqZ4Gxf"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"roXATPYfsA"}],"key":"tm994BH4uz"},{"type":"text","value":" by that gradient amount. Since we want to decrease the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cOC1oIjll5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iDUncr0U4f"}],"key":"r3V7Ua7Jpc"},{"type":"text","value":", we simply need to change the weights by a small negative fraction of the gradients in order to move them in the direction that locally most steeply decreases the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OYqR9LeUqu"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eOvuxEIaXw"}],"key":"iQguc2abov"},{"type":"text","value":" value:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c0ffbniKBx"}],"key":"cZzYN8w4GL"}],"key":"RLWDOBo575"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W.data += -0.1 * W.grad","key":"SdafoBfmLk"},{"type":"outputs","id":"eK-vSzOLxEl6EiHO-3-N_","children":[],"key":"eDw4ieof39"}],"key":"CL99yz1A3U"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We just did a single gradient descent optimization step, which means that if we re-calculate the loss, it will be lower:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Uil2tYyOFo"}],"key":"jZwidkJemk"}],"key":"gxQXt4mXIk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W, loss = forward_pass()\nloss.item()","key":"fYZEclPKU0"},{"type":"outputs","id":"L47G2FeeGgX1q1_obWX69","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":61,"metadata":{},"data":{"text/plain":{"content":"3.7492127418518066","content_type":"text/plain"}}},"children":[],"key":"d7hOLjHVkh"}],"key":"L5BWociyFw"}],"key":"fgZlRdR5ew"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Tada! All we have to do now is put everything together and stick the single step into a loop so that we can do multi-step gradient descent optimization. This time, for all the words in our dataset, not just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MCOdkvuTev"},{"type":"inlineCode","value":"emma","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pOeQsUblHL"},{"type":"text","value":"!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ICzqwUaGIn"}],"key":"GFPtOq1YHW"}],"key":"vKM76SWNu4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# create the dataset\nxs, ys = [], []\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        xs.append(ctoi[ch1])\n        ys.append(ctoi[ch2])\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint(\"number of examples (bigrams): \", num)\n# initialize the 'network'\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn((27, 27), generator=g, requires_grad=True)","key":"kA5MNULsWV"},{"type":"outputs","id":"0md73N1Ac0m0SvZLBZdGQ","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"number of examples (bigrams):  228146\n"},"children":[],"key":"ATpKpOkON2"}],"key":"BnG5G7hUOZ"}],"key":"gLTpKFK6NS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# gradient descent\nfor k in range(100):\n    W, loss = forward_pass()\n    print(loss.item())\n    # backward pass\n    W.grad = None  # set to zero the gradient\n    loss.backward()\n    # update\n    W.data += -50 * W.grad","key":"jDg0qIcs76"},{"type":"outputs","id":"g3tPsiIn0To7IjY21roEX","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"3.758953809738159\n3.371098756790161\n3.1540417671203613\n3.020373821258545\n2.9277119636535645\n2.860402822494507\n2.8097293376922607\n2.7701027393341064\n2.7380733489990234\n2.711496591567993\n2.6890034675598145\n2.6696884632110596\n2.6529300212860107\n2.638277292251587\n2.6253881454467773\n2.6139907836914062\n2.603863477706909\n2.5948219299316406\n2.586712121963501\n2.57940411567688\n2.572789192199707\n2.5667762756347656\n2.5612881183624268\n2.5562589168548584\n2.551633596420288\n2.547365665435791\n2.5434155464172363\n2.539748430252075\n2.5363364219665527\n2.5331544876098633\n2.5301806926727295\n2.5273969173431396\n2.5247862339019775\n2.522334575653076\n2.520029067993164\n2.517857789993286\n2.515810966491699\n2.513878345489502\n2.512052059173584\n2.510324001312256\n2.5086867809295654\n2.5071346759796143\n2.5056610107421875\n2.5042612552642822\n2.502929210662842\n2.5016613006591797\n2.5004522800445557\n2.4992990493774414\n2.498197317123413\n2.497144937515259\n2.496137857437134\n2.495173692703247\n2.4942495822906494\n2.493363380432129\n2.4925124645233154\n2.4916954040527344\n2.4909095764160156\n2.4901540279388428\n2.4894261360168457\n2.488725185394287\n2.4880495071411133\n2.4873974323272705\n2.4867680072784424\n2.4861605167388916\n2.4855728149414062\n2.4850049018859863\n2.484455108642578\n2.4839231967926025\n2.483408212661743\n2.4829084873199463\n2.482424020767212\n2.481955051422119\n2.481499195098877\n2.4810571670532227\n2.4806275367736816\n2.480210304260254\n2.479804754257202\n2.479410171508789\n2.4790265560150146\n2.4786536693573\n2.478290557861328\n2.4779367446899414\n2.477592706680298\n2.477257251739502\n2.4769301414489746\n2.476611852645874\n2.4763011932373047\n2.4759981632232666\n2.4757025241851807\n2.475414276123047\n2.475132703781128\n2.474858045578003\n2.4745893478393555\n2.474327802658081\n2.474071741104126\n2.4738216400146484\n2.4735770225524902\n2.4733383655548096\n2.47310471534729\n2.47287654876709\n"},"children":[],"key":"zI8mDfJ758"}],"key":"DHmQkvMHVe"}],"key":"NJRQW2pg5I"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome! What we least expect is that our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VrdSEL0Slr"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jjG9Wjzxu9"}],"key":"PHsdV0rYSG"},{"type":"text","value":", by using such gradient-based optimization, becomes as small as the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MVb42iz1y5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zYmJWaK8GJ"}],"key":"HGGe1ejliZ"},{"type":"text","value":" we got by our more primitive bigram-count-matrix way that we previously employed for optimizing. So, basically, before, we achieved roughly the same ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hDW02KYtKh"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gw5co9s2FJ"}],"key":"oOsmEsW0Jm"},{"type":"text","value":" just by counting, whereas now we used gradient descent. It just happens that the explicit, counting approach nicely optimizes the model without the need for any gradient-based optimization because the setup for bigram language models is so straightforward and simple that we can afford to just directly estimate the probabilities and keep them in a table. However, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fzxwhYKmOp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kTcUbuaVbe"}],"key":"VmXkFdLiLP"},{"type":"text","value":" approach is much more flexible and scalable! And we have actually gained a lot. What we can do from hereon is expand and complexify our approach. Meaning, that instead of just taking a single character and predicting the next one in an extremely simple ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PVwsn79qz3"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MHk8yD1yEu"}],"key":"IKDNRycQlp"},{"type":"text","value":", as we have done so far, we will be taking multiple previous characters and we will be feeding them into increasingly more complex ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iMC1FlRf4Z"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dFdXYxVtrT"}],"key":"QuBAzbc2GB"},{"type":"text","value":"s. But, fundamentally, we will still be just calculating logits that will be going through exactly the same transformation by passing them through a softmax and doing the same gradient-based optimization process we just did. But before we do that, remember the smoothing we did by adding fake counts to our bigram count matrix? Turns out, we can do equivalent smoothing in our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZVDWNzOxdG"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N5MEmYSQsx"}],"key":"th8EBgwKKe"},{"type":"text","value":" too! In particular, just incentivizing the weights to be zero for example leads to the probabilities being uniform, which is a form of smoothing. Such incentivization can be accomplished through regularization. It involves just adding a term like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fcdZunDFOb"}],"key":"xxUIdAjxGu"}],"key":"JvtdKTN5l4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(W**2).mean()","key":"N9S0TQqbec"},{"type":"outputs","id":"3PxzCoKqMN7sUcKTRPN5f","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":64,"metadata":{},"data":{"text/plain":{"content":"tensor(1.6880, grad_fn=<MeanBackward0>)","content_type":"text/plain"}}},"children":[],"key":"LLaQI6IFHJ"}],"key":"hYSexKxEc0"}],"key":"BuhqLrY3JC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h9GncnrvbC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WbwLoXrfGx"}],"key":"zb0YJvuo4Y"},{"type":"text","value":" as such:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NJc4POlSbd"}],"key":"XcFdsd9xEB"},{"type":"code","lang":"python","value":"loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"key":"W7pVXOZYec"}],"key":"S3pmfTiWh0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"where ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TqSorWLrs0"},{"type":"inlineCode","value":"0.01","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I3MFJVwdH8"},{"type":"text","value":" represents the strength of the regulatization term. Optimizing with this term included in the loss would smoothen the model. Yay! Lastly, let’s sample from our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r8FbE0XRwo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qui9eZ3T0z"}],"key":"oSwvwBB4eY"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YYaOXLsImT"}],"key":"fmeiFEKiRd"}],"key":"s7n3YkDqQh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nfor i in range(20):\n    out = []\n    ix = 0\n    while True:\n        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n        logits = xenc @ W  # predict log-counts\n        counts = logits.exp()  # counts, equivalent to N\n        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n        # sample from probabilities distribution\n        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[ix])\n        if ix == 0:\n            break\n    print(''.join(out))","key":"CWSbEhnQjQ"},{"type":"outputs","id":"tl_96beGgdxsRkfBKP00s","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"junide.\njanasah.\np.\ncfay.\na.\nnn.\nkohin.\ntolian.\njuwe.\nkilanaauranilevias.\ndedainrwieta.\nssonielylarte.\nfaveumerifontume.\nphynslenaruani.\ncore.\nyaenon.\nka.\njabi.\nwerimikimaynin.\nanaasn.\n"},"children":[],"key":"cSwj7q0FEI"}],"key":"UoLeJy3CjG"}],"key":"QXZyxLDcWu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We are getting kind of the same results as we previously did with our counting method! Not unpredictable at all, since our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DnjN3BLmx3"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zMNtOeC6BH"}],"key":"aVD6ABOwGK"},{"type":"text","value":" values are close enough. If we trained our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qk9dB5aXN4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uKqAqHyw1Q"}],"key":"KBfgHM8X8D"},{"type":"text","value":" more and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NomYgwhf4v"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y8K85O6SD0"}],"key":"f5Sikn1jas"},{"type":"text","value":" values became the same, it would means that the two models are identical. Meaning that given the same inputs, they would spit out the same outputs.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ruSa03Bdhp"}],"key":"wU2Vy2lGXf"}],"key":"keRkboxDjZ"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Summary","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i1Dnv9sPiw"}],"identifier":"summary","label":"Summary","html_id":"summary","implicit":true,"key":"kzgMufooP8"}],"key":"vqZM0IuPNg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"All in all, we have actually covered lots of ground. To sum up, we introduced the bigram character language model, we saw how we can train the model, how we can sample from the model and how we can evaluate the quality of the model using the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nOkYMESrrd"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Owxjed3xDV"}],"key":"w2LnaK4d6J"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kW1ts71o3n"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a6F7gXRKh1"}],"key":"Szx9LFjXxn"},{"type":"text","value":". We actually trained the model in two completely different ways that actually give or can give (with adequate training) the same result. In the first way, we just counted up the frequency of all the bigrams and normalized. Whereas, in the second way, we used the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tt3YvAZIOy"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yO9HxGZzi1"}],"key":"QeQQC5PnOk"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kTGaLUSIqE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E8hxbtkEjH"}],"key":"hLNTWCqJVs"},{"type":"text","value":" as a guide to optimizing the counts matrix or the counts array, so that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sI6q09w3kg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L7Uo5NXLUp"}],"key":"GIYeIytAZv"},{"type":"text","value":" is minimized in a gradient-based framework. Despite our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vvilpn4ABS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u5hHzwjEcI"}],"key":"Yb0mYW89Cg"},{"type":"text","value":" being super simple (single linear layer), it is the more flexible approach.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dkXOVJYxMK"}],"key":"GNaB6TJYmO"}],"key":"LfAipqzIBw"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Outro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rt96lk4oGK"}],"identifier":"outro","label":"Outro","html_id":"outro","implicit":true,"key":"m8iTc7oo6v"}],"key":"s9oxNprGeX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In the follow-up lessons, we are going to complexify by taking more and more of these characters and we are going to be feeding them into a new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bBdRaylwo0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DFQIGdRGSo"}],"key":"iDdmhJH3ht"},{"type":"text","value":" that does more exciting stuff. Buckle up!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z2x8mDNIUi"}],"key":"GeNdZwQBWz"}],"key":"GOh4DbRJOS"}],"key":"oS0Rsrkjs5"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"1. micrograd: implementing an autograd engine","url":"/micrograduate/micrograd","group":"microgra∇uate"},"next":{"title":"3. makemore (part 2): mlp","url":"/micrograduate/makemore2","group":"microgra∇uate"}}},"domain":"http://localhost:3000"}