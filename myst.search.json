{"version":"1","records":[{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":""},"content":"\n\nâ­\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"","lvl2":"Preface"},"type":"lvl2","url":"/#preface","position":2},{"hierarchy":{"lvl1":"","lvl2":"Preface"},"content":"A self-contained course to learn the basics of neural networks: from backprop to GPT!","type":"content","url":"/#preface","position":3},{"hierarchy":{"lvl1":"","lvl3":"ðŸ“– Read ðŸŒ»","lvl2":"Preface"},"type":"lvl3","url":"/#id-read","position":4},{"hierarchy":{"lvl1":"","lvl3":"ðŸ“– Read ðŸŒ»","lvl2":"Preface"},"content":"Visit \n\nmicrograduate-book.github.io.","type":"content","url":"/#id-read","position":5},{"hierarchy":{"lvl1":"","lvl3":"âš¡ Run â˜„ï¸","lvl2":"Preface"},"type":"lvl3","url":"/#id-run","position":6},{"hierarchy":{"lvl1":"","lvl3":"âš¡ Run â˜„ï¸","lvl2":"Preface"},"content":"","type":"content","url":"/#id-run","position":7},{"hierarchy":{"lvl1":"","lvl4":"ðŸŒ Online","lvl3":"âš¡ Run â˜„ï¸","lvl2":"Preface"},"type":"lvl4","url":"/#id-online","position":8},{"hierarchy":{"lvl1":"","lvl4":"ðŸŒ Online","lvl3":"âš¡ Run â˜„ï¸","lvl2":"Preface"},"content":"#\n\nðŸ“”\n\nðŸ”—\n\n1\n\nmicrograd\n\n\n\n2\n\nmakemore1\n\n\n\n3\n\nmakemore2\n\n\n\n4\n\nmakemore3\n\n\n\n5\n\nmakemore4\n\n\n\n6\n\nmakemore5\n\n\n\n7\n\npicoGPT\n\n","type":"content","url":"/#id-online","position":9},{"hierarchy":{"lvl1":"","lvl4":"ðŸ¡ Locally","lvl3":"âš¡ Run â˜„ï¸","lvl2":"Preface"},"type":"lvl4","url":"/#id-locally","position":10},{"hierarchy":{"lvl1":"","lvl4":"ðŸ¡ Locally","lvl3":"âš¡ Run â˜„ï¸","lvl2":"Preface"},"content":"Clone repo:git clone https://github.com/ckaraneen/micrograduate.git\n\nCreate and activate env:cd micrograduate\nconda env create --file environment.yaml\nconda activate micrograduate-env\n\nInstall requirements:uv pip install -r requirements.txt\n\nRun notebooks in \n\nNotebook Server, \n\nVSCode/Cursor, etc.","type":"content","url":"/#id-locally","position":11},{"hierarchy":{"lvl1":"","lvl3":"âœ¨ Acknowledgements ðŸ™","lvl2":"Preface"},"type":"lvl3","url":"/#id-acknowledgements","position":12},{"hierarchy":{"lvl1":"","lvl3":"âœ¨ Acknowledgements ðŸ™","lvl2":"Preface"},"content":"micrograâˆ‡uate builds on the works of \n\nAndrej Karpathy (check out \n\nEureka Labs!) and \n\nJay Mody:\n\nNeural Networks: Zero to Hero \n\nlecture series and \n\nrepo\n\nmicrograd\n\nmakemore\n\nAn Intuition for Attention\n\nGPT in 60 lines of code\n\npicoGPT\n\nIt would not exist without their contributions. â¤ï¸","type":"content","url":"/#id-acknowledgements","position":13},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model"},"type":"lvl1","url":"/micrograduate/makemore1","position":0},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/makemore1","position":1},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/makemore1#intro","position":2},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Intro"},"content":"\n\nJust like \n\nmicrograd before it, here, step-by-step with everything spelled-out, we will build makemore: a bigram character-level language model. Weâ€™re going to build it out slowly and together! But what is makemore? As the name suggests, makemore makes more of things that you give it. \n\nnames.txt is an example dataset. Specifically, it is a very large list of different names. If you train makemore on this dataset, it will learn to make more of name-like things, basically more unique names! So, maybe if you have a baby and youâ€™re looking for a new, cool-sounding unique name, makemore might help you. Here are some examples of such names that the makemore will be able to generate:\n\ndontell\nkhylum\ncamatena\naeriline\nnajlah\nsherrith\nryel\nirmi\ntaislee\nmortaz\nakarli\nmaxfelynn\nbiolett\nzendy\nlaisa\nhalliliana\ngoralynn\nbrodynn\nromima\nchiyomin\nloghlyn\nmelichae\nmahmed\nirot\nhelicha\nbesdy\nebokun\nlucianno\n\ndontell, irot, zendy, and so on, you name it! So under the hood, makemore is a character-level language model. That means that itâ€™s treating every single line (i.e. name) of \n\nits training dataset as an example. And each example is treated as a sequence of individual characters. For instance, it treats the name reese as the sequence of characters: r, e, e, s, e. That is the level on which we are building out makemore. Basically, its purpose is this: given a character, it can predict the next character in the sequence based upon the names that it has seen so far. Now, weâ€™re actually going to implement a large number of character-level language models, following a few key innovations:\n\nBigram (one character predicts the next one with a lookup table of counts)\n\nMLP, following \n\nBengio et al. 2003\n\nCNN, following \n\nDeepMind WaveNet 2016 (in progress...)\n\nRNN, following \n\nMikolov et al. 2010\n\nLSTM, following \n\nGraves et al. 2014\n\nGRU, following \n\nKyunghyun Cho et al. 2014\n\nTransformer, following \n\nVaswani et al. 2017\n\nIn fact, the transformer we are going to build will be the equivalent of \n\nGPT-2. Kind of a big deal, since itâ€™s a modern network and by the end of this guide youâ€™ll actually understand how it works at the level of characters. Later on, we will probably spend some time on the word level, so we can generate documents of words, not just segments of characters. And then weâ€™re probably going to go into image and image-text networks such as \n\nDALL-E, \n\nStable Diffusion, and so on. But first, letâ€™s jump into character-level modeling.\n\n","type":"content","url":"/micrograduate/makemore1#intro","position":3},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Building a bigram language model"},"type":"lvl2","url":"/micrograduate/makemore1#building-a-bigram-language-model","position":4},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Building a bigram language model"},"content":"\n\nLetâ€™s start by reading all the names into a list:\n\nwords = open(\"names.txt\").read().splitlines()\n\n\n\nwords[:10]\n\n\n\nNow, we want to learn a bit more about this dataset.\n\nlen(words)\n\n\n\nlen(min(words, key=len))  # shortest\n\n\n\nlen(max(words, key=len))  # longest\n\n\n\nLetâ€™s think through our very first language model. A character-level language model is predicting the next character in the sequence given already some concrete sequence of characters before it. What we have to realize here is that every single word like isabella is actually quite a few examples packed in that single word. Because, letâ€™s think: what is a word telling us really? Itâ€™s saying that the character i is a very likely character to come first in the sequence that constitutes a name. The character s is likely to come after i, the character a is likely to come after is, the character b is likely to come after isa, and so on all the way to a following isabell. And then thereâ€™s one more important piece of information in here. And that is that after isabella, the word is very likely to end. So, time to build our first network: a bigram language model. In these, we are working with two characters at a time. So, we are only looking for one character we are given and we are trying to predict the next character in a sequence. For example, in the name charlotte, we ask: what characters are likely to follow r?  In the name sophia: we ask what characters are likely to follow p? And so on. This mean we are just modeling that local structure. Meaning, we only look at the previous character, even though there might be a lot of useful information before it. This is a very simple model, which is why itâ€™s a great place to start! We can learn about the statistics of which characters are likely to follow which other characters by counting. So by iterating over all names, we can count how often each consecutive pair (bigram) of characters appears.\n\nb = {}\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0) + 1\n\n\n\nNotice that we have also added the character . to signify the start and end of each word. And obviously, the variable b now holds the statistics of the entire dataset.\n\nsorted(b.items(), key=lambda tup: tup[1], reverse=True)\n\n\n\nAnd this is the sorted list of counts of the individual bigrams across all the words in the dataset! Now letâ€™s convert our current bigram-to-occurence-frequency map into a bigram counts array, where every row index represents the first character and every column index represents the second character of each bigram. Before doing so, we must first find a way to convert each character into a unique integer index:\n\nchars = [\".\"] + sorted(list(set(\"\".join(words))))\nctoi = {c: i for i, c in enumerate(chars)}\nprint(ctoi)\n\n\n\nNow that we have a character-to-index map, we may construct our bigram counts array N:\n\nimport torch\n\nnchars = len(chars)\nN = torch.zeros(nchars, nchars, dtype=torch.int32)\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        N[ctoi[ch1], ctoi[ch2]] += 1\n\n\n\nN\n\n\n\nDone! Of course, this looks like a mess. So letâ€™s visualize it better.\n\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\n\nitoc = {i: c for c, i in ctoi.items()}\nplt.figure(figsize=(16, 16))\nplt.imshow(N, cmap=\"Blues\")\nfor i in range(27):\n    for j in range(27):\n        chstr = itoc[i] + itoc[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\nplt.axis(\"off\")\n\n\n\n\n\nThe color-graded bigram counts array! Looks good. This array actually has all the necessary information for us to start sampling from this bigram character language model. Letâ€™s just start by sampling the start character (of course) of each name: the . character. The first row tells us how often each other character follows it. In other words, the first row tells us how often each character is the first character of a word:\n\nN[0]\n\n\n\nTo get the probability of each of character being the first:\n\np = N[0].float()\np = p / p.sum()\np\n\n\n\nEach value of this probability distribution corresponds simply to the probability of the corresponding character being the first character of a word. And of course it sums to 1:\n\nassert p.sum() == 1\n\n\n\nNow, weâ€™ll sample numbers according to this probability distribution using \n\ntorch.multinomial. And to do so deterministically we are going to use a generator. So, letâ€™s take a brief detour and test out how to sample. First we create a probability distribution:\n\nSEED = 2147483647\ng = torch.Generator().manual_seed(SEED)\nptest = torch.rand(3, generator=g)\nptest = ptest / ptest.sum()\nptest\n\n\n\nThen, we sample from this distribution:\n\ns = torch.multinomial(ptest, num_samples=100, replacement=True, generator=g)\ns\n\n\n\nSimple. Now, notice that it outputs the same tensor however many times you run the cells. Thatâ€™s because we have set a fixed seed and passed the generator object to the functions. Now, notice the output of torch.multinomial. What we expect is that around 60.64\\% of the numbers to be 0, 30.33\\% to be 1 and 9.03\\% to be 2:\n\nsbc = torch.bincount(s)\nfor i in [0, 1, 2]:\n    print(f\"Ratio of {i}: {sbc[i]/sbc.sum()}\")\n\n\n\nNot too far away from what we expected! But, if we increase the number of samples, we will get much closer to the probabilities of our distribution. Try it out! The more samples we take, the more the actual occurence ratios match the probabilities of the distribution the numbers were sampled from. Now, itâ€™s time to sample from our initial character probability distribution:\n\ng = torch.Generator().manual_seed(SEED)\nidx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nitoc[idx]\n\n\n\nWe are now ready to write out our name generator.\n\ng = torch.Generator().manual_seed(SEED)\nP = N.float()\nP = P / P.sum(\n    1, keepdim=True\n)  # sum over the column dimension and keep column dimension\nfor i in range(20):\n    out = []\n    idx = 0\n    while True:\n        p = P[idx]\n        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[idx])\n        if idx == 0:\n            break\n    print(\"\".join(out))\n\n\n\nIt works! It yields names. Well, kinda. Some look name-like enough but most are just terrible. Lol. This is a bigrams model for you! To recap, we trained a bigrams language model essentially just by counting how frequently any pairing of characters occurs and then normalizing so that we get a nice probability distribution. Really, the elements of array P are the parameters of our model that summarize the statistics of these bigrams. We train the model and iteratively sample the next character and feed it in each time and get the next character. But how do we evaluate our model? We can do so, by looking at the probability of each bigram.\n\nfor w in words[:3]:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = ctoi[ch1]\n        ix2 = ctoi[ch2]\n        prob = P[ix1, ix2]\n        print(f\"{ch1}{ch2}: {prob:.4f}\")\n\n\n\nHere we are looking at the probabilities that the model assigns to every bigram in the dataset. Just keep in mind that we have 27 characters, so if everything was equally likely we would expect all probabilities to be:\n\n1/27\n\n\n\nSince they are not and we have mostly higher probabilities, it means that our model has learned something useful. In an ideal case, we would expect the bigram probabilities to be near 1.0 (perfect prediction probability). Now, when you look at the literature of \n\nmaximum likelihood estimation, statistical modelling and so on, youâ€™ll see that whatâ€™s typically used here is something called the likelihood: the product of all the above probabilities. This gives us the probability of the entire dataset assigned by the model that you made. But, because the product of these probabilities is an unwieldly, very tiny number to work with (think 0.0478 \\times 0.0377 \\times 0.0253 \\times ...), for convenience, what people usually work with is not the likelihood, but the log-likelihood. The log, as you can see:\n\nimport numpy as np\n\nx = np.arange(0.01, 1.0, 0.01)\ny = np.log(x)\nplt.figure()\nplt.plot(x, y)\n\n\n\nis a monotonic transformation of the probability, where if you pass in probability 1.0 you get log-probability of 0, and as the probabilities you pass in decrease, the log-probability decreases all the way to -\\infty as the probability approaches 0. Therefore, letâ€™s also add the log probability in our loop to see what that looks like:\n\ndef test_model(iterable, print_probs=True, calc_ll=False, print_nll=False):\n    if print_nll:\n        calc_ll = True\n    log_likelihood = 0.0\n    n = 0\n    for w in iterable:\n        chs = [\".\"] + list(w) + [\".\"]\n        for ch1, ch2 in zip(chs, chs[1:]):\n            prob = P[ctoi[ch1], ctoi[ch2]]\n            logprob = torch.log(prob)\n            if calc_ll:\n                log_likelihood += logprob.item()\n                n += 1\n            if print_probs:\n                print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n    if calc_ll:\n        print(f\"{log_likelihood=}\")\n    if print_nll:\n        nll = -log_likelihood\n        print(f\"{nll=}\")\n        print(f\"loss={nll/n}\")\n    return log_likelihood\n\n\n_ = test_model(words[:3])\n\n\n\nAs you can see, for higher probabilities we get closer and closer to 0, but lower probabilities gives us a more negative number. And so to calculate the log-likelihood, we just sum up all the log probabilities:\n\nlog_likelihood = test_model(words[:3], calc_ll=True)\n\n\n\nNow, how high can log-likelihood get? As high as 0! So, when all the probabilities are 1.0, it will be 0. But the further away from 1.0 they are, the more negative the log-likehood will get. Now, we donâ€™t actually like this because we are looking to define here is a loss function, that has the semantics where high is bad and low is good, since we are trying to minimize it. Any ideas? Well, we actually just need to invert the log-likelihood, aka take the negative log-likelihood (nll):\n\nnll = -log_likelihood\nprint(f'{nll=}')\n\n\n\nnll is a very nice loss function because the lowest it can get is zero and the higher it is the worse off the predictions are that we are making. People also usually like to see the average of the nll instead of just the sum:\n\ntest_model(words[:3], print_probs=False, calc_ll=True, print_nll=True);\n\n\n\nOur loss function for the training set assigned by the model yields a loss of 2.424. The lower it is, the better off we are. The higher it is, the worse off we are. So, the job of training is produce a high-quality model, by finding the parameters that minimize the loss. In this case, ones that minimize the nll loss. To summarize, our goal is to maximize likelihood of the data w.r.t. model parameters (in our statistical modeling case these are the bigram probabilities), which is:\n\nequivalent to maximizing the log-likelihood (because the \\log function is monotonic)\n\nequivalent to minimizing the nll\n\nequivalent to minimizing the average nll\n\nThe lower the nll loss the better, since that would mean assigning high probabilities. Remember: \\log(a \\cdot b \\cdot c) = \\log(a) + \\log(b) + \\log(c). Also, keep in mind that here we store the probabilities in a table format. But in whatâ€™s coming up, these numbers will not be kept explicitly but they will be calculated by a nn and we will change its parameters to maximize the likelihood of these probabilities. Letâ€™s now test out our model with a random name:\n\ntest_model(iterable=['christosqj'], calc_ll=True, print_nll=True);\n\n\n\nAs you can see, the probability of the bigram sq is super low. Whereas the probability for qj, since it is never encountered in our training data (see our bigram count table!), is 0, which predictably yields a log-probability of -\\infty, which in turn causes the loss to be -\\infty. What this means is that this model is exactly 0 \\% likely to predict this name (infinite loss). If you look up the table you see that q is followed by j zero times. This kind of behavior people donâ€™t usually like too much, so there is a simple trick to alleviate it: model smoothing. It involves adding some fake counts to the bigram counts array so that never is there a bigram with 0 counts (and therefore 0 probability). This ensures that there are no zeros in our bigram counts matrix. E.g.\n\nP = (N + 1).float()\nP = P / P.sum(\n    1, keepdim=True\n)  # sum over the column dimension and keep column dimension\ntest_model(iterable=[\"christosqj\"], calc_ll=True, print_nll=True)\n\n\n\n\n\nNow, we avoid getting a loss of -\\infty. Cool! So weâ€™ve now trained a respectable bigram character-level language model. We trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions. We saw that we can also then use those parameters of this model to perform sampling of new words (sample new names according to these distributions) and evaluate the quality of this model which is summarized by a single number: the nll. And the lower this number is, the better the model is because it is giving high probabilities to the actual mixed characters of all the bigrams in our training set. Great! We basically, counted and then normalized those counts, which is sensible enough.\n\n","type":"content","url":"/micrograduate/makemore1#building-a-bigram-language-model","position":5},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Casting the model as a nn"},"type":"lvl2","url":"/micrograduate/makemore1#casting-the-model-as-a-nn","position":6},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Casting the model as a nn"},"content":"\n\nLetâ€™s now try a different approach by casting such a bigram language model into a nn framework to achieve the same goal. Our nn is still going to be a bigram character-level language model. It will receive a single character as an input that will pass through a bunch of weighted neurons and then output the probability distribution over the next character in the sequence. Itâ€™s going to make guesses about what character is going to follow the input character. In addition, weâ€™ll be able to evaluate any setting of the parameters of the nn, since we have a loss function. Basically, weâ€™re going to take a look at the probabilities distributions our model assigns for our next character and find the loss between those and the labels (which are the character that we expect to come next in the bigram). By doing so, we can use gradient-based optimization to tune the weights of our nn that give us the output probabilities. Letâ€™s begin this alternative approach by first constructing our dataset:\n\n# Create training dataset of bigrams (x, y)\nxs, ys = [], []\nfor w in words[:1]:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2)\n        xs.append(ctoi[ch1])\n        ys.append(ctoi[ch2])\n# Convert to pytorch tensor (https://pytorch.org/docs/stable/generated/torch.tensor.html)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n\n\n\nxs\n\n\n\nys\n\n\n\nNow, how do we pass each character into the nn? \n\nOne-hot encoding! With this encoding, each integer is encoded with bits.\n\nimport torch.nn.functional as F\n\nxenc = F.one_hot(xs, num_classes=27).float()\nxenc\n\n\n\nxenc.shape\n\n\n\nplt.figure()\nplt.imshow(xenc);\n\n\n\nLetâ€™s create our neuron:\n\nW = torch.randn((27, 1))\nxenc @ W\n\n\n\nOur neuron receives one character of size 27 and spits out 1 output value. However, as you can see, since PyTorch supports matrix multiplication, our neuron can receive 5 characters of size 27 in parallel and output each characterâ€™s output in a 5 \\times 1 matrix ([5 \\times 27] \\cdot [27 \\times 1] \\rightarrow [5 \\times 1]). Now, letâ€™s pass our 5 characters as inputs through 27 neurons instead of just 1 neuron:\n\nW = torch.randn((27, 27))\nxenc @ W\n\n\n\nPredictably, we get 5 arrays (one per input/character) of 27 outputs ([5 \\times 27] \\cdot [27 \\times 27] \\rightarrow [5 \\times 27]). Each output number represents each neuronâ€™s firing rate of a specific input. For example, the following is the firing rate of the 13th neuron of the 3rd input:\n\n(xenc @ W)[3, 13]\n\n\n\nWhat PyTorch allows is matrix multiplication that enables parallel dot products of many inputs in a batch with the weights of neurons of a nn. For example, this is how to multiply the inputs that represent the 3rd character with the weights of the 13th neuron:\n\nxenc[3]\n\n\n\nW[:, 13]\n\n\n\n(xenc[3] * W[:, 13]).sum()\n\n\n\n(xenc @ W)[3, 13]  # same as above\n\n\n\nOk, so what did is we fed our 27-dimensional inputs into the first layer of a nn that has 27 neurons. These neurons perform W * x. They donâ€™t have a bias and they donâ€™t have a non-linearity like tanh. We are going to leave our network as is: a 1-layer linear nn. Thatâ€™s it. Basically, the dumbest, smallest, simplest nn. Remember, what we trying to produce is a probability distribution for a next character in a sequence. And thereâ€™s 27 of them. But we have to come up with exact semantics as to how we are going to interpret these 27 numbers that these neurons take on. Intuitively, as we can see in the xenc @ W output, some of these outputs numbers are positive and some negative. Thatâ€™s because they come out of a nn layer with weights are initialized from the normal [-1, 1] distribution. But, what we want however is something like a bigram count table that we previously produced, where each row told us the counts which we then normalized to get the probabilities. So, we want something similar to come out of our nn. But, what we have right now, are some negative and positive numbers. Now, we therefore want these numbers to represent the probabilities for the next character with their unique characteristics. For example, probabilities are positive numbers and they sum to 1. Also, they obviously have to be probabilities. They canâ€™t be counts because counts are positive integers; not a great output from a nn. Instead, what the nn is going to output and how we are going to interpret these 27 output numbers is as log counts. One way to accomplish this is by exponentiating each output number so that the result is always positive. Specifically, exponentiating a  negative number yields a result that is a positive value less than 1. Whereas, exponentiating a positive number yields a result whose value is between greater than 1 and \\infty.\n\n(xenc @ W).exp()\n\n\n\nSuch exponentiation is a great way to make the nn predict counts. Which are positive numbers that can take on various values depending on the setting of W. Letâ€™s break it down more:\n\nlogits = xenc @ W  # log-counts\ncounts = logits.exp()  # equivalent to the N bigram counts array\nprobs = counts / counts.sum(1, keepdims=True)\nprobs\n\n\n\nTherefore, we have a way to get the probabilities, where each row sums to 1 (since they are normalized), e.g.\n\nprobs[0].sum().item()\n\n\n\nprobs.shape\n\n\n\nWhat we have achieved is that for every one of our 5 examples, we now have a row that came out of our nn. And because of the transformations here, we made sure that this output of the nn can be interpreted as probabilities. In other words, what we have done is that we took inputs, applied differentiable operations on them (e.g. @, exp()) that we can backprop through and we are getting out probability distributions. Take the first input character that was fed in as an example:\n\nxenc[0]\n\n\n\nthat corresponds to the . symbol from the name:\n\nwords[0]\n\n\n\nThe way we fed this character into the neural network is that we first got its index, then we one-hot encoded it, then it went into the nn and out came this distribution of probabilities:\n\nprobs[0]\n\n\n\nwith a shape of:\n\nprobs[0].shape\n\n\n\n27 numbers. We interpret these numbers of probs[0] as the probability or how likely it is for each of the corresponding characters to come next. As we train the nn by tuning the weights W, we are of course going to be getting different probabilities out for every character that you input. So, the question is: can we tune W such that the probabilities coming out are pretty good? The way we measure pretty good is by the loss function. Below you can see what have done in a simple summary:\n\n# SUMMARY ------------------------------>>>>\nxs  # inputs\n\n\n\nys  # targets\n\n\n\nBoth xs and ys constitute the dataset. They are integers representing characters of a sequence/word.\n\n# Use a generator for reproducability and randomly initialize 27 neurons' weights. Each neuron receives 27 inputs.\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn((27, 27), generator=g)  # 27 incoming weights for 27 neurons\n# Encode the inputs into one-hot representations\nxenc = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n# Pass encoded inputs through first layer to get logits\nlogits = xenc @ W  # predict log-counts\n# Exponentiate the logits to get fake counts\ncounts = logits.exp()  # counts, equivalent to N\n# Normalize these counts to get probabilities\nprobs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n# NOTE: the 2 lines above constitute what is called a 'softmax'\nprobs.shape\n\n\n\nSoftmax is a very-often-used loss function in nns. It takes in logits, exponentiates them, then divides and normalizes. Itâ€™s a way of taking outputs of a linear layer that might be positive or negative and it outputs numbers that are only positive and always sum to 1, adhering to the properties of probability distributions. It can be viewed as a normalization function if you want to think of it that way.\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='softmax.jpeg'))\n\n\n\nNow, since every operation in the forward pass is differentiable, we can backprop through. Below, we iterate over every input character and describe what is going on:\n\nnlls = torch.zeros(5)\nfor i in range(5):\n    # i-th bigram:\n    x = xs[i].item()  # input character index\n    y = ys[i].item()  # label character index\n    print(\"--------\")\n    print(f\"bigram example {i+1}: {itoc[x]}{itoc[y]} (indexes {x},{y})\")\n    print(\"input to the nn:\", x)\n    print(\"output probabilities from the nn:\", probs[i])\n    print(\"label (actual next character):\", y)\n    p = probs[i, y]\n    print(\"probability assigned by the nn to the correct next character:\", p.item())\n    logp = torch.log(p)\n    print(\"log likelihood:\", logp.item())\n    nll = -logp\n    print(\"negative log likelihood:\", nll.item())\n    nlls[i] = nll\nloss = nlls.mean()\nprint(\"=========\")\nprint(\"average negative log likelihood, i.e. loss =\", loss.item())\n\n\n\nAs you can see, the probabilities assigned by the nn to the correct next character are bad (pretty low). See for example the probability predicted by the network of m following e (em example): the nll value is very high (e.g. 4.0145). And in general, for the whole word, the loss (the average nll) is high! This means that this is not a favorable setting of weights and we can do better. One easy way to do better is to reinitialize W using a different seed for example and pray to god that the loss is smaller or repeat until it is. But that is what amateurs do. We are professionals or, at least, we want to be! And what professionals do is they start with random weights, like we did, and then they optimize those weights in order to minimize the loss. We do so by some gradient-based optimization (e.g. gradient descent) which entails first doing backprop in order to compute the gradients of that weight w.r.t. to those weights and then changing the weights by some such gradient amount in order to optimize them and minimize the loss. As we did with micrograd, we will write an optimization loop for doing the backward pass. But instead of mean-squared error, we are using the nll as a loss function, since we are dealing with a classification task and not a regression one.\n\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn(\n    (27, 27), generator=g, requires_grad=True\n)  # 27 incoming weights for 27 neurons\n\n\ndef forward_pass(regularize=False):\n    num = xs.nelement()\n    xenc = F.one_hot(\n        xs, num_classes=27\n    ).float()  # input to the network: one-hot encoding\n    logits = xenc @ W  # predict log-counts\n    counts = logits.exp()  # counts, equivalent to N\n    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n    loss = -probs[torch.arange(num), ys].log().mean()\n    return W, loss\n\n\nW, loss = forward_pass()\n# backward pass\nW.grad = None  # set to zero\nloss.backward()\n\n\n\nNow, something magical happened when backward ran. Like micrograd, PyTorch, during the forward pass, keeps track of all the operations under the hood and builds a full computational graph. So, it knows all the dependencies and all the mathematical operations of everything. Therefore, calling backward on the loss fills in the gradients of all the intermediate nodes, all the way back to the W value nodes. Take a look:\n\nW.grad\n\n\n\nAnd obviously:\n\nassert W.shape == W.grad.shape\n\n\n\nWhat a gradient value is telling us, e.g.\n\nW.grad[1][4].item()\n\n\n\nis that nudging the specific corresponding weight by a small h value, would nudge the loss by that gradient amount. Since we want to decrease the loss, we simply need to change the weights by a small negative fraction of the gradients in order to move them in the direction that locally most steeply decreases the loss value:\n\nW.data += -0.1 * W.grad\n\n\n\nWe just did a single gradient descent optimization step, which means that if we re-calculate the loss, it will be lower:\n\nW, loss = forward_pass()\nloss.item()\n\n\n\nTada! All we have to do now is put everything together and stick the single step into a loop so that we can do multi-step gradient descent optimization. This time, for all the words in our dataset, not just emma!\n\n# create the dataset\nxs, ys = [], []\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        xs.append(ctoi[ch1])\n        ys.append(ctoi[ch2])\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint(\"number of examples (bigrams): \", num)\n# initialize the 'network'\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\n\n\n# gradient descent\nfor k in range(100):\n    W, loss = forward_pass()\n    print(loss.item())\n    # backward pass\n    W.grad = None  # set to zero the gradient\n    loss.backward()\n    # update\n    W.data += -50 * W.grad\n\n\n\nAwesome! What we least expect is that our loss, by using such gradient-based optimization, becomes as small as the loss we got by our more primitive bigram-count-matrix way that we previously employed for optimizing. So, basically, before, we achieved roughly the same loss just by counting, whereas now we used gradient descent. It just happens that the explicit, counting approach nicely optimizes the model without the need for any gradient-based optimization because the setup for bigram language models is so straightforward and simple that we can afford to just directly estimate the probabilities and keep them in a table. However, the nn approach is much more flexible and scalable! And we have actually gained a lot. What we can do from hereon is expand and complexify our approach. Meaning, that instead of just taking a single character and predicting the next one in an extremely simple nn, as we have done so far, we will be taking multiple previous characters and we will be feeding them into increasingly more complex nns. But, fundamentally, we will still be just calculating logits that will be going through exactly the same transformation by passing them through a softmax and doing the same gradient-based optimization process we just did. But before we do that, remember the smoothing we did by adding fake counts to our bigram count matrix? Turns out, we can do equivalent smoothing in our nn too! In particular, just incentivizing the weights to be zero for example leads to the probabilities being uniform, which is a form of smoothing. Such incentivization can be accomplished through regularization. It involves just adding a term like this:\n\n(W**2).mean()\n\n\n\nto the loss as such:loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n\nwhere 0.01 represents the strength of the regulatization term. Optimizing with this term included in the loss would smoothen the model. Yay! Lastly, letâ€™s sample from our nn:\n\ng = torch.Generator().manual_seed(SEED)\nfor i in range(20):\n    out = []\n    ix = 0\n    while True:\n        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n        logits = xenc @ W  # predict log-counts\n        counts = logits.exp()  # counts, equivalent to N\n        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n        # sample from probabilities distribution\n        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[ix])\n        if ix == 0:\n            break\n    print(''.join(out))\n\n\n\nWe are getting kind of the same results as we previously did with our counting method! Not unpredictable at all, since our loss values are close enough. If we trained our nn more and the loss values became the same, it would means that the two models are identical. Meaning that given the same inputs, they would spit out the same outputs.\n\n","type":"content","url":"/micrograduate/makemore1#casting-the-model-as-a-nn","position":7},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Summary"},"type":"lvl2","url":"/micrograduate/makemore1#summary","position":8},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Summary"},"content":"\n\nAll in all, we have actually covered lots of ground. To sum up, we introduced the bigram character language model, we saw how we can train the model, how we can sample from the model and how we can evaluate the quality of the model using the nll loss. We actually trained the model in two completely different ways that actually give or can give (with adequate training) the same result. In the first way, we just counted up the frequency of all the bigrams and normalized. Whereas, in the second way, we used the nll loss as a guide to optimizing the counts matrix or the counts array, so that the loss is minimized in a gradient-based framework. Despite our nn being super simple (single linear layer), it is the more flexible approach.\n\n","type":"content","url":"/micrograduate/makemore1#summary","position":9},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Outro"},"type":"lvl2","url":"/micrograduate/makemore1#outro","position":10},{"hierarchy":{"lvl1":"2. makemore (part 1): implementing a bigram character-level language model","lvl2":"Outro"},"content":"\n\nIn the follow-up lessons, we are going to complexify by taking more and more of these characters and we are going to be feeding them into a new nn that does more exciting stuff. Buckle up!","type":"content","url":"/micrograduate/makemore1#outro","position":11},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp"},"type":"lvl1","url":"/micrograduate/makemore2","position":0},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/makemore2","position":1},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/makemore2#intro","position":2},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Intro"},"content":"\n\nTime to make more out of makemore! In the last lesson, we implemented the bigram language model, both using counts and a super simple, 1-linear-layer nn. How we approach training is that we looked only at the single previous character and we predicted a distribution for the character coming next in the sequence. We did that by taking counts and normalizing them into probabilities so that each row in the count matrix sums to 1. This method is great if you only have one character of previous context. The problem with that model though is that predictions are not very good. Another problem, if we are to take more context into account, is that the counts in the matrix grow exponentially as we increase the length of the context. For just 1 character of context we have 27 rows, each representing the next possible character. For 2 characters, the number of rows would grow to 27 \\cdot 27 = 729. Whereas for 3 characters, it would explode to 27 \\cdot 27 \\cdot 27 = 19683, and so on. This solution simply doesnâ€™t scale well and explodes. That is why we are going to move on and instead implement an mlp model to predict the next character in a sequence.\n\n","type":"content","url":"/micrograduate/makemore2#intro","position":3},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Building a mlp language model"},"type":"lvl2","url":"/micrograduate/makemore2#building-a-mlp-language-model","position":4},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Building a mlp language model"},"content":"\n\nThe modeling approach we are going to adopt follows \n\nBengio et al. 2003, an important paper that we are going to implement. Although they implement a word-level language model, we are going to stick to our character-level language model, but follow the same approach. The authors propose associating each and every word (out of e.g. 17000) with a feature vector (e.g. of 30 dimensions). In other words, every word is a point that is embedded into a 30-dimensional space. You can think of it this way. We have 17000 point-vectors in a 30-dimensional space. As you can imagine, that is very crowded, thatâ€™s lots of points for a very small space. Now, in the beginning, these words are initialized completely randomly: they are spread out at random. But, then we are going to tune these embeddings of these words using backprop. So during the course of training of this nn, these point-vectors are going to basically be moved around in this space. And you might imagine that, for example, words that have very similar meanings or that are indeed synonyms of each other might end up in a very similar part of the vector space, and, conversely, words with very different meanings will go somewhere else in that space. Now, their modeling approach otherwise is identical to what ours has been so far. They are using a mlp nn to predict the next word, given the previous words and to train the nn they are maximizing the log-likehood of the training data, just like we did. Here, is their example of this intuition: suppose the exact phrase a dog was running in a has never occured and at test time we want our model to complete the sentence by predicting the word that might follow it (e.g. room). Because the model has never encountered this exact phrase in the training set, it is out of distribution, as we say. Meaning, you donâ€™t have fundamentally any reason to suspect what might come next. However, the approach we are following allows you to get around such suspicion. Maybe we havenâ€™t seen the exact phrase, but maybe we have seen similar phrases like: the dog was running in a and maybe your nn has learned that a and the are frequently interchangeble with each other. So maybe our model took the embeddings for a and the and it actually put them nearby eachother in the vector space. Thus, you can transfer knowledge through such an embedding and generalize in that way. Similarly, it can do the same with other similar words such that a phrase such as The cat is walking in the bedroom can help us generalize to a diserable or at least valid sentence like a dog was running in a room by merit of the magic of feature vector similarity after training! To put it more simply, manipulating the embedding space allows us to transfer knowledge, predict and generalize to novel scenarios even when fed inputs like the sequence of words mentioned that we have not trained on. If you scroll down the paper, you will see the following diagram:\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='bengio2003nn.jpeg'))\n\n\n\nThis is the nn where we are taking e.g. three previous words and we are trying to predict the fourth word in a sequence, where w_{t-3}, w_{t-2}, w_{t-1} are the indeces of each incoming word. Since there are 17000 possible words, these indeces are integers between 0-16999. Thereâ€™s also a lookup table C, a matrix that has 17000 rows (one for each word embedding) and 30 columns (one for each feature vector/embedding dimension). Every index basically plucks out a row of this embedding matrix so that each index is converted to the 30-dimensional embedding vector corresponding to that word. Therefore, each word index corresponds to 30 neuron activations exiting the first layer: w_{t-3} \\rightarrow C(w_{t-3}), w_{t-2} \\rightarrow C(w_{t-2}), w_{t-1} \\rightarrow C(w_{t-1}). Thus, the first layer contains 90 neurons in total. Notice how the C matrix is shared, which means that we are indexing the same matrix over and over. Next up is the hidden layer of this nn whose size is a hyperparameter, meaning that it is up to the choice of the designer how wide, aka how many neurons, it is going to have. For example it could have 100 or any other number that endows the nn with the best performance, after evaluation. This hidden layer is fully connected to the input layer of 90 neurons, meaning each neuron is connected to each one of this layerâ€™s neurons. Then thereâ€™s a \\tanh non-linearity, and then thereâ€™s an output layer. And because of course we want the nn to give us the next word, the output layer has 17000 neurons that are also fully connected to the previous (hidden) layerâ€™s neurons. So, thereâ€™s a lot of parameters, as there are a lot of words, so most computation happens in the output layer. Each of this layerâ€™s 17000 logits is passed through a softmax function, meaning they are all exponentiated and then everything is normalized to sum to 1, so that we have a nice probability distribution P(w_{t}=i\\ |\\ context) for the next word w_{t} in the sequence. During training of course, we have the label or target index: the index of the next word in the sequence which we use to pluck out the probability of that word from that distribution. The point of training is to maximize the probability of that word w.r.t. the nn parameters, meaning the weights and biases of the output layer, of the hidden layer and of the embedding lookup table C. All of these parameters are optimized using backprop. Ignore the green dashed arrows in the diagram, they represent a variation of the nn we are not going to explore in this lesson. So, what we  described is the setup. Now, letâ€™s implement it!\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\n\n\n\n# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nwords[:8]\n\n\n\nlen(words)\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {c: i + 1 for i, c in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: c for c, i in ctoi.items()}\nprint(itoc)\n\n\n\nAs you can see, we are reading 32033 words into a list and we are creating character-to/from-index mappings. From here, the first thing we want to do is compile the dataset for the nn:\n\n# context length: how many characters do we take to predict the next one?\nblock_size = 3\n\n\ndef build_dataset(words, verbose=False):\n    x, y = [], []\n    for w in words:\n        if verbose:\n            print(w)\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            if verbose:\n                print(\"\".join(itoc[i] for i in context), \"--->\", itoc[ix])\n            context = context[1:] + [ix]  # crop and append\n            x.append(context)\n            y.append(ix)\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(f\"{x.shape=}, {y.shape=}\")\n    print(f\"{x.dtype=}, {y.dtype=}\")\n    return x, y\n\n\nx, y = build_dataset(words[:5], verbose=True)\n\n\n\nWe first define the block_size which is how many characters we need to predict the next one. In the example I just described, we used 3 words to predict the next one. Here, we also use a block size of 3 and do the same thing, but remember, instead of words we expect characters as inputs and predictions. After defining the block size, we construct x: a feature list of word index triplets (e.g. [[ 0,  0,  5], [ 0,  5, 13], ...]) that represent the context inputs, and y: a list of corresponding target word indeces (e.g. [ 5, 13, ...]). In the printout above, you can see for each wordâ€™s context character triplet, the corresponding target character. E.g. for an input of  ... the target character is e, for ..e itâ€™s m, and so on! Change the block_size and see the print out for yourself. Notice how we are using dots as padding. After building the dataset, inputs and targets look like this:\n\nx, y\n\n\n\nGiven these, letâ€™s write a nn that takes the x and predicts y. First, letâ€™s build the embedding lookup table C. In the paper, they have 17000 words and embed them in spaces as low-dimensional as 30, so they cram 17000 into a 30-dimensional space. In our case, we have only 27 possible characters, so letâ€™s cram them into as small as -letâ€™s say- a 2-dimensional space:\n\nSEED = 2147483647\ng = torch.Generator().manual_seed(SEED)\nC = torch.randn((27, 2), generator=g)\nC\n\n\n\nEach of our 27 characters will have a 2-dimensional embedding. Therefore, our table C will have 27 rows (one for each character) and 2 columns (number of dimensions per character embedding). Before we embed all the integers inside input x using this lookup table C, letâ€™s first embed a single, individual character, letâ€™s say, 5, so we get a sense of how this works. One way to do it is to simply index the table using the character index:\n\nC[5]\n\n\n\nAnother way, as we saw in the previous lesson, is to one-hot encode the character:\n\nohv = F.one_hot(torch.tensor(5), num_classes=27)\nprint(ohv)\nprint(ohv.shape)\n\n\n\nWith this way we get a one-hot encoded representation whose 5-th element is 1 and all the rest are 0. Now, notice how, just as we previously alluded to in the previous lesson, if we take this one-hot vector and we multiply it by C:\n\nohv_matmul_C = ohv.float() @ C\nohv_matmul_C\n\n\n\nas you can see, they are identical:\n\nassert C[5].equal(ohv_matmul_C)\n\n\n\nMultiplying a one-hot encoded vector and an appropriate matrix acts like indexing that matrix with the index of the vector that points to element 1! And so, we actually arrive at the same result. This is interesting since it points out how the first layer of this nn (see diagram above) can be thought of as a set of neurons whose weight matrix is C, when the inputs (the integer character indeces) are one-hot encoded. Note aside, in order to embed a character, we are just going to simply index the table as itâ€™s much faster:\n\nC[5]\n\n\n\nwhich is easy for a single character. But what if we want to index more simultaneously? Thatâ€™s also easy:\n\nC[[5, 6, 7]]\n\n\n\nCool! You can actually index a PyTorch tensor with a list or another tensor. Therefore, to easily get all character embeddings, we can simply do:\n\nemb = C[x]\nprint(emb)\nprint(emb.shape)\n\n\n\nNotice the shape: [<number of character input sets>, <input size>, <number of character embedding dimensions>]. Indexing as following, we can assert that both ways of representation are valid:\n\nassert emb[13, 2].equal(C[x[13, 2]])\n\n\n\nLong story short, PyTorch indexing is awesome and tensors such as embedding tables can be indexed by other tensors, e.g. inputs. One last thing, as far as the first layer is concerned. Since each embedding of our 3 inputs has 2 dimensions, the output dimension of our first layer is basically 3 \\cdot 2 = 6. Usually, a nn layer is described by a pair of input and output dimensions. The input dimension of our first, embeddings layer is 32 (<number of character inputs>). To get the output dimension we have to concatenate the following <inputs size> and <number of character embedding dimensions> tensor dimensions into one dimension:\n\nlast_dims = emb.shape[1:]  # get dimensions after the first one\nlast_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\nemb_proper = emb.view(-1, last_dims_product)\nemb_proper.shape  # tada!\n\n\n\nWe have prepared the dimensionality of the first layer. Now, letâ€™s implement the hidden, second layer:\n\nl0out = emb_proper.shape[-1]\nl1in = l0out\nl1out = 100  # neurons of hidden layer\nw1 = torch.randn(l1in, l1out, generator=g)\nb1 = torch.randn(l1out, generator=g)\nprint(w1.shape)\nprint(b1.shape)\n\n\n\nh = torch.tanh(emb_proper @ w1 + b1)\nh\n\n\n\nh.shape\n\n\n\nDone! And now, to create the output layer:\n\nl2in = l1out\nl2out = 27  # number of characters\nw2 = torch.randn(l2in, l2out, generator=g)\nb2 = torch.randn(l2out, generator=g)\nprint(w2.shape)\nprint(b2.shape)\n\n\n\nlogits = h @ w2 + b2\nlogits.shape\n\n\n\nExactly as we saw in the previous lesson, we want to take these logits and we want to first exponentiate them to get our fake counts. Then, we want to normalize them to get the probabilities of how likely it is for each character to come next:\n\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nprob.shape\n\n\n\nRemember, we also have the the target values y, the actual characters that come next that we would like our nn to be able to predict:\n\ny\n\n\n\nSo, what we would like to do now is index into the rows of prob and for each row to pluck out the probability given to the correct character:\n\nprob[range(len(y)), y]\n\n\n\nThis gives the current probabilities for these specific correct, target characters that come next after each character sequence, given the current nn configuration (weights and biases). Currently these probabilities are pretty bad and most characters are pretty unlikely to occur next. Of course, we havenâ€™t trained the nn yet. So, we want to train it so that each probability approximates 1. As we saw previously, to do so, we have to define the loss and then minimize it:\n\nloss = -prob[range(len(y)), y].log().mean()\nloss\n\n\n\nPretty big loss! Haha. Now we will minimize it so our nn able to predict the next character in each sequence correctly. To do so, we have to optimize the parameters. Letâ€™s define a function that defines them and collects them all into a list just so we have easy access:\n\nimport random\n\n# context length: how many characters do we take to predict the next one?\nblock_size = 3\n\n\n# build the dataset\ndef build_dataset(words):\n    X, Y = [], []\n    for w in words:\n        # print(w)\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            X.append(context)\n            Y.append(ix)\n            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n            context = context[1:] + [ix]  # crop and append\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\n\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\n\ndef define_nn(l1out=100, embsize=2):\n    global C, w1, b1, w2, b2\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((27, embsize), generator=g)\n    l1in = embsize * block_size\n    # l1out: neurons of hidden layer\n    w1 = torch.randn(l1in, l1out, generator=g)\n    b1 = torch.randn(l1out, generator=g)\n    l2in = l1out\n    l2out = 27  # neurons of output layer, number of characters\n    w2 = torch.randn(l2in, l2out, generator=g)\n    b2 = torch.randn(l2out, generator=g)\n    parameters = [C, w1, b1, w2, b2]\n    return parameters\n\n\nparameters = define_nn()\nsum(p.nelement() for p in parameters)\n\n\n\n\n\nTo recap the forward pass:\n\nemb = C[x]  # [32, 3, 2]\nlast_dims = emb.shape[1:]  # get dimensions after the first one\nlast_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\nemb_proper = emb.view(-1, last_dims_product)  # [32, 6]\nh = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]\nlogits = h @ w2 + b2  # [32, 27]\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nloss = -prob[range(len(y)), y].log().mean()\nloss\n\n\n\nA better and more efficient way to calculate the loss from logits and targets is through the \n\ncross entropy loss function:\n\nF.cross_entropy(logits, y)\n\n\n\nLetâ€™s tidy up the forward pass:\n\ndef forward_pass(x, y):\n    emb = C[x]  # [32, 3, 2]\n    last_dims = emb.shape[1:]  # get dimensions after the first one\n    last_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\n    emb_proper = emb.view(-1, last_dims_product)  # [32, 6]\n    h = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]\n    logits = h @ w2 + b2  # [32, 27]\n    loss = F.cross_entropy(logits, y)\n    return loss\n\n\n\n","type":"content","url":"/micrograduate/makemore2#building-a-mlp-language-model","position":5},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Training the model"},"type":"lvl2","url":"/micrograduate/makemore2#training-the-model","position":6},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Training the model"},"content":"\n\nAnd now, letâ€™s train our nn:\n\ndef train(x, y, epochs=10):\n    parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for _ in range(epochs):\n        loss = forward_pass(x, y)\n        print(loss.item())\n        for p in parameters:\n            p.grad = None\n        loss.backward()\n        for p in parameters:\n            p.data += -0.1 * p.grad\n\n\n\ntrain(x, y)\n\n\n\nThe loss keeps decreasing, which means that the training process is working! Now, since we are only training using a dataset of 5 words, and since our parameters are many more than the samples we are training on, our nn is probably overfitting. What we have to do now, is train on the whole dataset.\n\nx_all, y_all = build_dataset(words)\n\n\n\ntrain(x_all, y_all)\n\n\n\nSame, the loss for all input samples also keeps decreasing. But, youâ€™ll notice that training takes longer now. This is happening because we are doing a lot of work, forward and backward passing on 228146 examples. Thatâ€™s way too much work! In practice, what people usually do in such cases is they train on minibatches of the whole dataset. So, what we want to do, is we want to randomly select some portion of the dataset, and thatâ€™s a minibatch! And then, only forward, backward and update on that minibatch, likewise iterate and train on those minibatches. A simple way to implement minibatching is to set a batch size, e.g.:\n\nbatchsize = 32\n\n\n\nand then to randomly select batchsize number of indeces referencing the subset of input data to be used for minibatch training. To get the indeces you can do something like this:\n\nbatchix = torch.randint(0, x_all.shape[0], (batchsize,))\nprint(batchix)\n\n\n\nThen, to actually get a minibatch per epoch, just create a new, random set of indeces and index the samples and targets from the dataset before each forward pass. Like this:\n\ndef train(x, y, lr=0.1, epochs=10, print_all_losses=True):\n    parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for _ in range(epochs):\n        batchix = torch.randint(0, x.shape[0], (batchsize,))\n        bx, by = x[batchix], y[batchix]\n        loss = forward_pass(bx, by)\n        if print_all_losses:\n            print(loss.item())\n        for p in parameters:\n            p.grad = None\n        loss.backward()\n        for p in parameters:\n            p.data += -lr * p.grad\n    if not print_all_losses:\n        print(loss.item())\n    return loss.item()\n\n\n\nNow, if we train using minibatches...\n\ntrain(x_all, y_all)\n\n\n\n\n\ntraining is much much faster, almost instant! However, since we are dealing with minibatches, the quality of our gradient is lower, so the direction is not as reliable. Itâ€™s not the actual exact gradient direction, but the gradient direction is good enough even though itâ€™s being estimated for only batchsize (e.g. 32) examples. In general, it is better to have an approximate gradient and just make more steps than it is to compute the exact gradient and take fewer steps. And that is why in practice, minibatching works quite well.\n\n","type":"content","url":"/micrograduate/makemore2#training-the-model","position":7},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Finding a good learning rate"},"type":"lvl2","url":"/micrograduate/makemore2#finding-a-good-learning-rate","position":8},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Finding a good learning rate"},"content":"\n\nNow, one issue that has popped up as you may have noticed, is that during minibatch training the loss seems to fluctuate. For some epochs it decreases, but then it increases again, and vice versa. That question that arises from this observation is this: are we stepping too slow or too fast? Meaning, are we updating the parameters by a fraction of their gradients that is too small or too large? Such magnitude is determined by the step size, aka the learning rate. Therefore, the overarching question is: how do you determine this learning rate? How do we gain confidence that we are stepping with the right speed? Letâ€™s see one way to determine the learning rate. We basically want to find a reasonable search range, if you will. What people usually do is they pick different learning rate values until they find a satisfactory one. Letâ€™s try to find one that is better. We see for example if it is very small:\n\ntrain(x_all, y_all, lr=0.0001, epochs=100)\n\n\n\n\n\nThe loss barely decreases. So this value is too low. Letâ€™s try something bigger, e.g.\n\ntrain(x_all, y_all, lr=0.001, epochs=100)\n\n\n\n\n\nThis is ok, but still not good enough. The loss value decreases but not steadily and fluctuates a lot. For an even bigger value:\n\ntrain(x_all, y_all, lr=1, epochs=100)\n\n\n\n\n\nBetter! How about:\n\ntrain(x_all, y_all, lr=10, epochs=100)\n\n\n\n\n\nHaha, no thanks. So, we know that a satisfactory learning rate lies between 0.001 and 1. To find it, we can lay these numbers out, exponentially separated:\n\nstep = 1000\nlre = torch.linspace(-3, 0, step)\nlrs = 10**lre\nlrs\n\n\n\nthen, we plot the loss for each learning rate:\n\nparameters = define_nn()\nlrei = []\nlossi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(1000):\n    batchix = torch.randint(0, x_all.shape[0], (batchsize,))\n    bx, by = x_all[batchix], y_all[batchix]\n    loss = forward_pass(bx, by)\n    # print(loss.item())\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    lr = lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n    lrei.append(lre[i])\n    lossi.append(loss.item())\nplt.figure()\nplt.plot(lrei, lossi)\n\n\n\nHere, we see the loss dropping as the exponent of the learning rate starts to increase, then, after a learning rate of around 0.5, the loss starts to increase. A good rule of thumb is to pick a learning rate whose at a point around which the loss is the lowest and most stable, before any increasing tendency. Letâ€™s pick the learning rate whose exponent corresponds to the lowest loss:\n\nlr = 10**lrei[lossi.index(min(lossi))]\nlr\n\n\n\nNow we have some confidence that this is a fairly good learning rate. Now letâ€™s train for many epochs using this new learning rate!\n\n_ = train(x_all, y_all, lr=lr, epochs=10000, print_all_losses=False)\n\n\n\nNice. We got a much smaller loss after training. We have dramatically improved on the bigram language model, using this simple nn of only 3481 parameters. Now, thereâ€™s something we have to be careful with. Although our loss is the lowest so far, it is not exactly true to say that we now have a better model. The reason is that this is actually a very small model. Even though these kind of models can get much larger by adding more and more parameters, e.g. with 10000, 100000 or a million parameters, as the capacity of the nn grows, it becomes more and more capable of overfitting your training set. What that means is that the loss on the training set (the data that you are training on), will become very very low. As low as 0. But in such a case, all that the model is doing is memorizing your training set exactly, verbatim. So, if you were to take this model and itâ€™s working very well, but you try to sample from it, you will only get examples, exactly as they are in the training set. You wonâ€™t get any new data. In addition to that, if you try to evaluate the loss on some withheld names or other input data (e.g. words), you will actually see that the loss of those will be very high. And so it is in practice basically not a very good model, since it doesnâ€™t generalize. So, it is standard in the field to split up the dataset into 3 splits, as we call them: the training split (roughly 80\\% of data), the dev/validation split (10\\%) and the test split (10\\%).\n\n# training split, dev/validation split, test split\n# 80%, 10%, 10%\n\n\n\nNow, the training split is used to optimize the parameters of the model (for training). The validation split is typically used for development and tuning over all the hyperparameters of the model, such as the learning rate, layer width, embedding size, regularization parameters, and other settings, in order to choose a combination that works best on this split. The test split is used to evaluate the performance of the model at the end (after training). So, we are only evaluating the loss on the test split very very sparingly and very few times. Because, every single time you evaluate your test loss and you learn something from it, you are basically trying to also train on the test split. So, you are only allowed to evaluate the loss on the test dataset very few times, otherwise you risk overfitting to it as well, as you experiment on your model. Now, letâ€™s actually split our dataset into training, validation and test datasets. Then, we are going to train on the training dataset and only evaluate on the test dataset very very sparingly. Here we go:\n\nimport random\n\nrandom.seed(42)\nrandom.shuffle(words)\nlenwords = len(words)\nn1 = int(0.8 * lenwords)\nn2 = int(0.9 * lenwords)\nprint(f\"{lenwords=}\")\nprint(f\"{n1} words in training set\")\nprint(f\"{n2 - n1} words in validation set\")\nprint(f\"{lenwords - n2} words in test\")\n\n\n\nxtrain, ytrain = build_dataset(words[:n1])\nxval, yval = build_dataset(words[n1:n2])\nxtest, ytest = build_dataset(words[n2:])\n\n\n\nWe now have the 3 split sets. Great! Letâ€™s now re-define our parameters train, anew, on the training dataset:\n\nloss_train = train(xtrain, ytrain, lr=lr, epochs=30000, print_all_losses=False)\n\n\n\nAwesome. Our nn has been trained and the final loss is actually surprisingly good. Letâ€™s now evaluate the loss of the validation set (remember, this data was not in the training set on which it was trained):\n\nloss_val = forward_pass(xval, yval)\nloss_val\n\n\n\nNot too bad! Now, as you can see, our loss_train and loss_val are pretty close. In fact, they are roughly equal. This means that we are not overfitting, but underfitting. It seems that this model is not powerful enough so as not to be purely memorizing the data. Basically, our nn is very tiny. But, we can expect to make performance improvements by scaling up the size of this nn. The easiest way to do this is to redefine our nn with more neurons in the hidden layer:\n\nparameters = define_nn(l1out=300)\n\n\n\nThen, letâ€™s re-train and visualize the loss curve:\n\nlossi = []\nstepi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(30000):\n    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))\n    bx, by = xtrain[batchix], ytrain[batchix]\n    loss = forward_pass(bx, by)\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    for p in parameters:\n        p.data += -lr * p.grad\n    stepi.append(i)\n    lossi.append(loss.log10().item())\nplt.figure()\nplt.plot(stepi, lossi)\n\n\n\nAs you can see, it is a bit noisy, but that is just because of the minibatches!\n\nloss_train = forward_pass(xtrain, ytrain)\nloss_val = forward_pass(xval, yval)\nprint(loss_train)\nprint(loss_val)\n\n\n\nAwesome, the training loss is actually lower than before, whereas the validation loss is pretty much the same. So, increasing the size of the hidden layer gave us some benefit. Letâ€™s experiment more to see if we can get even lower losses by increasing the embedding layer. First though, letâ€™s visualize the character embeddings:\n\n# visualize dimensions 0 and 1 of the embedding matrix C for all characters\nplt.figure(figsize=(8, 8))\nplt.scatter(C[:, 0].data, C[:, 1].data, s=200)\nfor i in range(C.shape[0]):\n    plt.text(\n        C[i, 0].item(),\n        C[i, 1].item(),\n        itoc[i],\n        ha=\"center\",\n        va=\"center\",\n        color=\"white\",\n    )\nplt.grid(\"minor\")\n\n\n\nThe network has basically learned to separate out the characters and cluster them a little bit. For example, it has learned some characters are usually found more closer together than others. Letâ€™s try to improve our model loss by choosing a greater embeddings layer size of 10 and by increasing the number of epochs to 200000, also weâ€™ll decay the learning rate after 100000 epochs:\n\nparameters = define_nn(l1out=200, embsize=10)\n\n\n\nlossi = []\nstepi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(200000):\n    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))\n    bx, by = xtrain[batchix], ytrain[batchix]\n    loss = forward_pass(bx, by)\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    lr = 0.1 if i < 100000 else 0.01\n    for p in parameters:\n        p.data += -lr * p.grad\n    stepi.append(i)\n    lossi.append(loss.log10().item())\nplt.figure()\nplt.plot(stepi, lossi);\n\n\n\nloss_train = forward_pass(xtrain, ytrain)\nloss_val = forward_pass(xval, yval)\nprint(loss_train)\nprint(loss_val)\n\n\n\nWow, we did it! Both train and validation losses are lower now. Can we go lower? Play around and find out! Now, before we end this lesson, letâ€™s sample from our model:\n\n# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\nfor _ in range(20):    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        emb = C[torch.tensor([context])] # (1,block_size,d)\n        h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n        logits = h @ w2 + b2\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join(itoc[i] for i in out))\n\n\n\n","type":"content","url":"/micrograduate/makemore2#finding-a-good-learning-rate","position":9},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Outro"},"type":"lvl2","url":"/micrograduate/makemore2#outro","position":10},{"hierarchy":{"lvl1":"3. makemore (part 2): mlp","lvl2":"Outro"},"content":"\n\nAs you can see, our model now is pretty decent and able to produce suprisingly name-like text, which is what we wanted all along! Next up, we will explore mlp internals and other such magic.","type":"content","url":"/micrograduate/makemore2#outro","position":11},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm"},"type":"lvl1","url":"/micrograduate/makemore3","position":0},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/makemore3","position":1},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/makemore3#intro","position":2},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Intro"},"content":"\n\nHere, we will continue our implementation of makemore. In the previous lesson, we implemented an character-level language model using a mlp along the lines of \n\nBengio et al. 2003. The model took as inputs a few past characters and predicted the next character in the sequence. What we would like to do is move on to more complex and larger nns, like\n\nRNN, following \n\nMikolov et al. 2010\n\nLSTM, following \n\nGraves et al. 2014\n\nGRU, following \n\nKyunghyun Cho et al. 2014\n\nCNN, following \n\nOord et al., 2016\n\nTransformer, following \n\nVaswani et al. 2017\n\nBut before we do so, letâ€™s stick around at the level of the mlp for a little longer in order to develop an intuitive understanding of the activations during training, and especially the gradients flowing backwards: how they behave and how they look like. This is important for understanding the history of the development of newer architectures. Because, RNNs, as weâ€™ll see, for example, although they are very expressive, are universal function approximators and can in principle implement all algorithms, we will see that they are not that easily optimizable with the first-order gradient-based techniques that we have available to us and that we use all the time. The key to understanding why they are not easily optimizable, is to understand the activations and the gradients and how they behave during training. What weâ€™ll also see is that a lot of variants since RNNs, have tried to improve upon this situation. And so, thatâ€™s the path that we have to take.\n\n","type":"content","url":"/micrograduate/makemore3#intro","position":3},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Rebuilding mlp"},"type":"lvl2","url":"/micrograduate/makemore3#rebuilding-mlp","position":4},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Rebuilding mlp"},"content":"\n\nSo, letâ€™s get started by first building on the code from the previous lesson.\n\nimport random\nrandom.seed(42)\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\nSEED = 2147483647\n\n\n\n# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nwords[:8]\n\n\n\nlen(words)\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {s: i + 1 for i, s in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: s for s, i in ctoi.items()}\nvocab_size = len(itoc)\nprint(itoc)\nprint(vocab_size)\n\n\n\nblock_size = 3\n\n\ndef build_dataset(words):\n    x, y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            x.append(context)\n            y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(x.shape, y.shape)\n    return x, y\n\n\n\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\nxtrain, ytrain = build_dataset(words[:n1])\nxval, yval = build_dataset(words[n1:n2])\nxtest, ytest = build_dataset(words[n2:])\n\n\n\ndef define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, b1, w2, b2\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    b1 = torch.randn(n_hidden, generator=g) * b1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    parameters = [C, w1, b1, w2, b2]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return parameters\n\n\n\ndef forward(x, y):\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss\n\n\n\ndef backward(parameters, loss):\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n\n\ndef update(parameters, lr):\n    for p in parameters:\n        p.data += -lr * p.grad\n\n\n\ndef train(x, y, initial_lr=0.1, maxsteps=200000, batchsize=32, redefine_params=False):\n    global parameters\n    lossi = []\n    if redefine_params:\n        parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for i in range(maxsteps):\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        hpreact, h, logits, loss = forward(xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 100000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n    return hpreact, h, logits, lossi\n\n\n\n@torch.no_grad()  # this decorator disables gradient tracking\ndef print_loss(x, y, prefix=\"\"):\n    _, _, _, loss = forward(x, y)\n    print(f\"{prefix} {loss}\")\n    return loss\n\n\n\nparameters = define_nn()\n_, _, _, lossi = train(xtrain, ytrain)\nprint_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\");\n\n\n\nplt.figure()\nplt.plot(lossi)\n\n\n\ndef sample_from_model():\n    # sample from the model\n    g = torch.Generator().manual_seed(SEED + 10)\n    for _ in range(20):\n        out = []\n        context = [0] * block_size  # initialize with all ...\n        while True:\n            emb = C[torch.tensor([context])]  # (1,block_size,d)\n            h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n            logits = h @ w2 + b2\n            probs = F.softmax(logits, dim=1)\n            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n            context = context[1:] + [ix]\n            out.append(ix)\n            if ix == 0:\n                break\n        print(\"\".join(itoc[i] for i in out))\n\n\n\nsample_from_model()\n\n\n\nSo thatâ€™s our starting point. Awesome!\n\n","type":"content","url":"/micrograduate/makemore3#rebuilding-mlp","position":5},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Dealing with bad weights"},"type":"lvl2","url":"/micrograduate/makemore3#dealing-with-bad-weights","position":6},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Dealing with bad weights"},"content":"\n\nNow, the first thing to scrutinize is the initialization. An experienced person would tell you that our network is very improperly configured at initialization and there are multiple things wrong with it. Letâ€™s start with the first one. If you notice the loss at iteration 0/200000, it is rather high. This rapidly comes down to 2 or so in the following training iterations. But you can tell that initialization is all messed up just by an initial loss that is way too high. In the training of nns, it is almost always the case that youâ€™ll have a rough idea of what loss to expect at initialization. And that just depends on the loss function and the problem setup. In our case, we expect a number much lower than what we get. Letâ€™s calculate it together. Basically, thereâ€™s 27 characters that can come next for any one training example. At initialization, we have no reason to believe that any characters to be much more likely than others. So, weâ€™d expect that the probability distribution that comes out initially is a uniform distribution, assigning about-equal probability to all the 27 characters. This means that what weâ€™d like the ideal probability we should record for any character coming next to be:\n\nideal_p = torch.tensor(1.0 / 27)\nideal_p.item()\n\n\n\nAnd then the loss we would expect is the negative log probability:\n\nexpected_loss = -torch.log(ideal_p)\nexpected_loss.item()\n\n\n\nSo whatâ€™s happening right now is that at initialization the network is creating probability distributions that are all messed up. Some characters are very confident and some characters are very not-confident. Basically, the network is very confidently wrong and thatâ€™s what makes it record a very high loss. For simplicity, letâ€™s see a smaller, 4-dimensional example of the issue, by assuming we only have 4 characters.\n\ndef logits_4d(logits=torch.tensor([0.0, 0.0, 0.0, 0.0]), index=0):\n    probs = torch.softmax(logits, dim=0)\n    loss = -probs[index].log()\n    return probs, loss.item()\n\n\n\nlogits_4d()\n\n\n\nSuppose we have logits that come out of an nn that are all 0. Then, when we calculate the softmax of these logits and get probabilities that are a diffused distribution that sums to 1 and is exactly uniform. Whereas, the loss we get is the loss we would expect for a 4-dimensional example with a uniform probability distribution. And so it doesnâ€™t matter whether the index is 0, 1, 2 or 3. Weâ€™ll see of course that as we start to manipulate these logits, the loss changes. For example:\n\nlogits_4d(logits=torch.tensor([0.0, 0.0, 5.0, 0.0]), index=2)\n\n\n\nYields a very low loss since we are assigning the correct probability at initialization to the correct (3rd) label. Much more likely it is that some other dimension will have a high logit, e.g.\n\nlogits_4d(logits=torch.tensor([0.0, 5.0, 0.0, 0.0]), index=2)\n\n\n\nand then what happens is we start to record a much higher loss. So, what of course can happens is that the logits might take on extreme values and come out like this:\n\nlogits_4d(logits=torch.tensor([-3.0, 5.0, 0.0, 2.0]), index=2)\n\n\n\nwhich also leads to a very high loss. For example, if logits are be relatively close to 0, the loss is not too big. For example:\n\nrandn_logits = torch.randn(4)\nprint(randn_logits)\nlogits_4d(logits=randn_logits, index=2)\n\n\n\n\n\nHowever, if they are larger, itâ€™s very unlikely that you are going to be guessing the correct bucket and so youâ€™d be confidently wrong and usually record a very high loss:\n\nbig_randn_logits = torch.randn(4) * 10\nprint(big_randn_logits)\nlogits_4d(logits=big_randn_logits, index=3)\n\n\n\n\n\nFor even more extreme logits, you might get extreme loss values:\n\nhuge_randn_logits = torch.randn(4) * 100\nprint(huge_randn_logits)\nlogits_4d(logits=huge_randn_logits, index=1)\n\n\n\n\n\nBasically, such logits are not good and we want the logits to be roughly 0 when the network is initialized. In fact, the logits donâ€™t need to be zero, they just have to be equal, e.g.:\n\nlogits_4d(logits=torch.tensor([3., 3., 3., 3.]), index=2)\n\n\n\nBecause of the normalization inside softmax, this will actually come out ok. But, for symmetry, we donâ€™t want it to be any arbitrary positive or negative number, just zero. So letâ€™s now concretely see where things go wrong in our initial example. First, letâ€™s reinitialize our network:\n\nparameters = define_nn()\n\n\n\nThen letâ€™s train it only for one step:\n\n_, _, logits, _ = train(xtrain, ytrain, maxsteps=1)\n\n\n\nIf we print the logits, weâ€™ll see that they take on quite extreme values:\n\nlogits[0]\n\n\n\nwhich is what is creating the fake confidence and why the loss is so high. Letâ€™s now try to scale down the values of the some of our parameters (e.g. w2) and retrain for a step:\n\nparameters = define_nn(w2_factor=0.1, b2_factor=0.0)\n_ = train(xtrain, ytrain, maxsteps=1)\n\n\n\nAha! The loss is lower, which makes sense. Letâ€™s try a smaller factor:\n\nparameters = define_nn(w2_factor=0.01, b2_factor=0.0)\n_ = train(xtrain, ytrain, maxsteps=1)\n\n\n\nThe loss decreases further. Alright, so weâ€™re getting closer and closer... So, you might ask, why not just initialize the weights to 0.0?\n\nparameters = define_nn(w2_factor=0.0, b2_factor=0.0)\n_ = train(xtrain, ytrain, maxsteps=1)\n\n\n\nBesides, it does yield an acceptable initial loss value. Well, you donâ€™t want to be setting the parameters of a nn exactly to 0. You usually want it to be small numbers instead of exactly 0. Letâ€™s see soon where things might go wrong if we set the initial parameters to 0. For now, letâ€™s just consider the 0.01 factor, which yields a small-enough initial loss:\n\nparameters = define_nn(w2_factor=0.01, b2_factor=0.0)\n_, _, logits, _ = train(xtrain, ytrain, maxsteps=1)\n\n\n\nThe logits are now coming out as closer to 0:\n\nlogits[0]\n\n\n\nCool. So, letâ€™s now train the network completely, and see what losses we get.\n\nparameters = define_nn(w2_factor=0.01, b2_factor=0.0)\n_, _, _, lossi = train(xtrain, ytrain)\n\n\n\nplt.figure()\nplt.plot(lossi)\n\n\n\nprint_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\")\n\n\n\n\n\nThe loss gets smaller after the first step. Now, notice that our loss plot does not have the previous loss plotâ€™s hockey stick appearance. The reason is that that shape came from the optimization process basically squashing down the weights to a much smaller range than the initial one. But, now since weâ€™ve already initialized the weights with small values, no such significant shrinking takes places, and thus no big loss drop happens between the first couple training steps. Therefore, we are not getting any easy gains, as we previously did in the beginning, but only just the hard gains from training. One important point to keep in mind is that the training and validation losses are now a bit better, since training now goes on for a bit longer, since the first epochs are no longer spent for squashing the parameters.\n\n","type":"content","url":"/micrograduate/makemore3#dealing-with-bad-weights","position":7},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Dealing with dead neurons"},"type":"lvl2","url":"/micrograduate/makemore3#dealing-with-dead-neurons","position":8},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Dealing with dead neurons"},"content":"\n\nNow, time to deal with a second problem. Although our loss after initializing with smaller weights is low:\n\nparameters = define_nn(w2_factor=0.01, b2_factor=0.0)\nhpreact, h, logits, _ = train(xtrain, ytrain, maxsteps=1)\n\n\n\nthe activation variable contains many 1.0 and -1.0 values:\n\nh\n\n\n\nNow, h is the result of the tanh activation function which is basically a squashing function that maps values within the [-1.0, 1.0] range. To get an idea of the distribution of the values of h, letâ€™s look at its histogram.\n\nplt.figure()\nplt.hist(h.view(-1).tolist(), 50);\n\n\n\nWe clearly see that most of the values of h are either -1.0 or 1.0. So, this tanh is very very active. We can look at why that is by plotting the pre-activations that feed into the tanh:\n\nplt.figure()\nplt.hist(hpreact.view(-1).tolist(), 50);\n\n\n\nAnd we can see that the distribution of the preactivations is very very broad, with numbers between -20 and around 20. That is why in the tanh output values of h, everything is being squashed and capped to be in the [-1.0, 1.0] range, with many extreme -1.0 and 1.0 values. If you are new to nns, you might not see this as an issue. But if youâ€™re well-versed in the dark arts of backprop and have an intuitive sense of how these gradients flow through a nn, you are looking at how the tanh values are distributed and you are sweating! Either case, letâ€™s see why this is an issue. First and foremost, we have to keep in mind that during backprop, we do a backward pass by starting at the loss and flowing through the network backwards. In particular, we get to a point where we backprop through the tanh function. If you scroll up to the forward() function, youâ€™ll see that the layer we first backprop through is the hidden nn layer (with parameters w2, b2), with n_hidden number of neurons, that implements an element-wise tanh non-linearity. Now, letâ€™s look at what happens in tanh in the backward pass. We can actually go back to our very first \n\nmicrograd implementation, in the first notebook and see how we implement tanh. This is how the gradient of tanh is calculated: (1 - t^2) \\cdot \\dfrac{\\partial L}{\\partial out}.  If the value of t, the output of tanh is 0, then the tanh neuron is basically inactive and the gradient of the previous layer just passes through. Whereas, if t is -1 or +1, then the gradient becomes 0. This means that if most of the h values (outputs of tanh) are close to the flat -1 and +1 regions of the tanh output value range, then the gradients that are flowing through the network are getting destroyed at this layer: an unwanted side-effect. Letâ€™s further investigate the amount of h activation values that are concentrated at the flat regions:\n\nplt.figure(figsize=(20, 10))\nplt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\");\n\n\n\nWhat we see in this data display are each one of the 200 neurons (columns) per each of the 32 examples/batches (rows). A white pixel represents a neuron whose output is in the flat tanh region: either -1 or +1. Whereas, a black pixel represents a neuron whose output is in-between those flat region values. In other words, the white neurons are all the maximum-valued neurons that block the flow of gradients during backprop. Of course, we would be in grave trouble if for all of these 200 neurons in each column (across all batches) were white. Because in that case we would have what we call a dead neuron. This would be a case wherein the initialization of weights and biases is such that no single example (batch) ever activates a neuron in the active region of the tanh value range, in between the flat, saturated regions. Since our display does not contain any column of all-whites, for each neuron of our nn, there are least one or a couple of neurons that activate in the active region, meaning that some gradients will flow through and each neuron will learn. Nevertheless, cases of dead neurons are possible and the way this manifests (e.g. for tanh neurons) is that no matter what inputs you plug in from your dataset, these dead neurons only fire either completely +1 or completely -1 and then these neurons will just not learn, because all the gradients will be zeroed out. These scenarios are not only true for tanh, but for many other non-linearities that people use in nns.\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='activations.png'))\n\n\n\nFor example, the sigmoid activation function will have the exact same issues, as it is a similar squashing function. Now, consider ReLU, which has a completely flat region for negative input values. So, if you have a ReLU neuron, it is a pass-through if it is positive and if the pre-activation value is negative, it will just shut it off, giving an output value of 0. Therefore, if a neuron with a ReLU non-linearity never activates, so for any inputs you feed it from the dataset it never turns on and remains always in its flat region, then this ReLU neuron is considered a dead neuron: its weights and bias will never receive a gradient and will thus never learn, simply because the neuron never activated. And this can sometimes happen at initialization, because the weights and biases just make it so that by chance some neurons are forever dead. But it can also happen during optimization. If you have too high of a learning rate for example, sometimes you have these neurons that get too much of a gradient and get knocked out of the data manifold, resulting in no example ever activating such a neuron. Consequently, one danger with large gradient is knocking off neurons and making them forever dead. Other non-linearities such as leaky ReLU will not suffer from this issue as much, because of the lack of flat tails, as theyâ€™ll almost always yield gradients. But, to return to our tanh issue, the problem is that our tanh pre-activation hpreact values are too far away from 0, thus yielding a flat region activation distribution that is too saturated at the tanh flat regions, which leads to a suppression of learning for many neurons. How do we fix this? One easy way is to decrease the initial value of the w1 and b1 parameters:\n\nparameters = define_nn(w1_factor=0.2, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0)\nhpreact, h, logits, _ = train(xtrain, ytrain, maxsteps=1)\nplt.figure(figsize=(20, 10))\nplt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\");\n\n\n\n\n\nNow, our activations are not as saturated above 0.99 as they were before, with only a few white neurons. What is more, the activations are now more evenly distributed and the range of pre-activations is now significantly narrower:\n\nplt.figure()\nplt.hist(h.view(-1).tolist(), 50)\nplt.figure()\nplt.hist(hpreact.view(-1).tolist(), 50);\n\n\n\n\n\nSince distributions look nicer now, perhaps this is what our initialization should be. Letâ€™s now train a new network with this initialization setting and print the losses:\n\nparameters = define_nn(w1_factor=0.2, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0)\n_ = train(xtrain, ytrain)\n\n\n\nprint_loss(xtrain, ytrain, prefix='train')\nprint_loss(xval, yval, prefix='val');\n\n\n\nThe validation loss continues to improve! This exercise clarifies the effect of good initialization on performance and emphasizes being aware of nn internals like activations and gradients. Now, weâ€™re working with a very small network which is basically just a 1-hidden layer mlp. Because the network is so shallow, the optimization problem is quite easy and very forgiving. So, even though our initialization in the beginning was terrible, the network still learned eventually. It just got a bit of a worse result. This is not the case in general though. Once we actually start working with much deeper networks that have say 50 layers, things can get much more complicated and these problems stack up, and it is often not surprising to get into a place where a network is basically not training at all, if your initialization is bad enough. Generally, the deeper and more complex a network is, the less forgiving it is to some of the aforementioned errors. But what has worked so far with our simple example is great! However, we have come up with a bunch of magic weight and bias factors (e.g. w1_factor). How did we come up with these? And how are we supposed to set these if we have a large nn with lots and lots of layers? As you might assume, no one sets these by hand. And thereâ€™s rather principled ways of setting these scales that Iâ€™d like to introduce to you now.\n\n","type":"content","url":"/micrograduate/makemore3#dealing-with-dead-neurons","position":9},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Learning to set the factors"},"type":"lvl2","url":"/micrograduate/makemore3#learning-to-set-the-factors","position":10},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Learning to set the factors"},"content":"\n\nLetâ€™s start with a short snippet, just to begin to motivate this discussion by defining an input tensor of many multi-dimensional examples and a weight tensor of a hidden layer, both drawn from a Gaussian distribution. Weâ€™ll calculate the mean and standard deviation of these inputs and the corresponding pre-activations:\n\ndef plot_x_y_distributions(n_inputs=10, weight_factor=1.0):\n    x = torch.randn(1000, n_inputs)  # many examples of inputs\n    w = torch.randn(n_inputs, 200) * weight_factor  # weights of the hidden layer\n    y = x @ w  # pre-activations\n    print(x.mean(), x.std())\n    print(y.mean(), y.std())\n    plt.figure(figsize=(20, 5))\n    plt.subplot(121)\n    plt.hist(x.view(-1).tolist(), 50, density=True)\n    plt.subplot(122)\n    plt.hist(y.view(-1).tolist(), 50, density=True)\n\n\n\nplot_x_y_distributions()\n\n\n\n\n\nIf you notice, the std of the pre-activations y has increased compared to that of x, as can also be seen by the widening of the Gaussian. The left Gaussian has basically undergone a stretching operation, resulting in the expanded right plot. We donâ€™t want that. We want most of our nn to have relatively similar activations with a relatively uniform Gaussian throughout the nn. So the question becomes, how do we scale these weight vectors (e.g. w) to preserve the Gaussian distribution of the inputs (e.g. left)? Letâ€™s do some experiments. If we scale w by a large number, e.g. 5:\n\nplot_x_y_distributions(weight_factor=5)\n\n\n\n\n\nthis Gaussian grows and grows in std, with the outputs in the x-axis taking on more and more extreme values (right plot). But if we scale the weights down, e.g. by 0.2:\n\nplot_x_y_distributions(weight_factor=0.2)\n\n\n\n\n\nthen, conversely, the Gaussian gets smaller and smaller and itâ€™s shrinking. Notice the std of y now being smaller than that of x. The question then becomes: what is the appropriate factor to exactly preserve the std of the inputs? And it turns out that the correct answer, mathematically, (when you work out the variance of the x @ w multiplication), is that you are supposed to divide by the square root of the fan-in. Meaning, the square root of the number of inputs. Therefore if the number of inputs is 10 then the appropriate factor for preserving the Gaussian distribution of the inputs is 10^{-1/2}.\n\nplot_x_y_distributions(n_inputs=10, weight_factor=10**-0.5)\n\n\n\n\n\nNow we see that the std remains roughly the same! Now, unsuprisingly, a number of papers have looked into how to best initialize nns and in the case of mlps, we can have these fairly deep networks that have these nonlinearities in between layers and we want to make sure that the activations are well-behaved and they donâ€™t expand to infinity or shrink all the way to zero. And the question is, how do we initialize the weights so that they take on reasonable values throughout the network. In \n\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, they study convolutional nns (CNNs) and ReLU and PReLU non-linearities. But the analysis is very similar to the tanh non-linearity. As we saw previously, ReLU is somewhat of a squashing function where all the negative values are simply clamped to 0. Because with ReLUs half of the distribution is thrown away, they find in their analysis of the forward activations of the nn, that you have to compensate for that with a gain. They find that to initialize their weights they have to do it with a zero-mean Gaussian whose std is \\sqrt{2/n_l}. We just did the same, multiplying our weights by \\sqrt{1/10} (the 2 has to do with the ReLU activation function they use). They also study the backward propagation case, finding that the backward pass is also approximately initialized up to a constant factor c_2/d_L that has to do with the number of hidden neurons in early and late layer. Now, this \n\nKaiming initialization is also implemented in pytorch and it is probably the most common way of initializing nns now. This PyTorch method takes a mode and nonlinearity argument among others, with the latter determining the gain factor (e.g. \\sqrt{2}). Why do we need a gain? For example, ReLU is a contractive transformation that squashes the output distribution by taking any negative value and clamping it to zero. tanh also squashes in some way, as it will squeeze values at the tails of its range. Therefore, in order to fight the squeezing-in of these activation functions, we have to boost the weights a little bit in order to counteract this effect and re-normalize everything back to standard unit deviation. So thatâ€™s why thereâ€™s a little bit of a gain that comes out. Now weâ€™re actually intentionally skipping through this section quickly. The reason for that is the following. Around 2015, when this paper was written, you had to actually be extremely careful with the activations and the gradients, their ranges, their histograms, the precise setting of gains and the scrutinizing of the non-linearities and so on... So, everything was very finicky and very fragile and everything had to be very properly arranged in order to train a nn. But there are a number of modern innovations that made everything significantly more stable and well-behaved. And it has become less important to initialize these networks â€œexactly rightâ€. Some of those innovations are for example: residual connections (which we will cover in the next notebooks), a number of normalization layers (e.g. batch normalization, layer normalization, group normalization) and of course much better optimizers: not just stochastic gradient descent (the simple optimizer we have been using), but slightly more complex optimizers such as RMSProp and especially Adam. All of these modern innovations make it less important for you to precisely calibrate the initialization of the nn. So, what do people do in practice? They usually initialize their weights with Kaiming-normally, like we did. Now notice how the following multiplier ends up being the std of Gaussian distribution:\n\nmultiplier = 0.2\n(torch.randn(10000) * multiplier).std().item()\n\n\n\nBut, according to the \n\nkaiming PyTorch docs, we want an std of \\frac{gain}{\\sqrt{fan\\_in}}. Therefore, for a tanh nonlinearity:\n\nn_embd = 10\nkaiming_w1_factor = (5 / 3) / ((n_embd * block_size) ** 0.5)\n\n\n\nNow letâ€™s re-initialize and re-train our nn with this initilization:\n\nparameters = define_nn(\n    w1_factor=kaiming_w1_factor, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0\n)\nhpreact, _, _, _ = train(xtrain, ytrain)\n\n\n\nprint_loss(xtrain, ytrain, prefix='train')\nprint_loss(xval, yval, prefix='val');\n\n\n\nOf course, our loss is quite similar to before. The difference now though is that we donâ€™t need to inspect histograms and introduce arbitrary factors. On the contrary, we now have a semi-principled way to initialize our weights that is also scalable to much larger networks which we can use as a guide. However, this precise weight initialization is not as important as we might think nowadays, due to some modern innovations.\n\n","type":"content","url":"/micrograduate/makemore3#learning-to-set-the-factors","position":11},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Batchnorm"},"type":"lvl2","url":"/micrograduate/makemore3#batchnorm","position":12},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Batchnorm"},"content":"\n\nLetâ€™s now introduce one of them. Batch Normalization (batchnorm) came out in 2015 from a team at Google in \n\nan extremely impactful paper, making it possible to train deep neural networks quite reliably. It basically just worked. Hereâ€™s what batchnorm does and how itâ€™s implemented. Like we mentioned before, we donâ€™t want the pre-activations (e.g. hpreact) to tanh to be too small, nor too large because then the outputs will turn out either close to 0 or saturated. Instead, we want the pre-activations to be roughly Gaussian (with a zero mean and a std of 1), at least at initialization. So, the insight from the batchnorm paper is: ok, we have these hidden pre-activation states/values that weâ€™d like to be Gaussian, then why not take them and just normalize them to be Gaussian? Haha. I know, it sounds kinda crazy but you can just do that, because standardizing hidden states so that they become Gaussian is a perfectly differentiable operation. And so the gist of batchnorm is that if we want unit Gaussian hidden states in our network, then we can just normalize them to be so. Letâ€™s see how that works. If you scroll up to our definition of the forward function we can see the pre-activations hpreact before they are fed into the tanh function. Now, the idea, remember, is we are trying to make these roughly Gaussian. If the values are too small, the tanh is kind of inactive, whereas if they are very large, the tanh becomes very saturated and the gradients donâ€™t flow. So, letâ€™s learn how to standardize hpreact to be roughly Gaussian.\n\nhpreact.shape\n\n\n\nhpmean = hpreact.mean(0, keepdim=True)\nhpmean.shape\n\n\n\nhpstd = hpreact.std(0, keepdim=True)\nhpstd.shape\n\n\n\nAfter calculating the mean and std across batches of hpreact, which in the paper are referred to the â€œmini-batch meanâ€ and â€œmini-batch varianceâ€, respectively, next, following along the paper, we are going to normalize or standardize the inputs (e.g. hpreact) by subtracting the mean and dividing by the std. Basically:\n\nhpreact = (hpreact - hpmean) / hpstd\n\n\n\nWhat normalization does is that now every single neuron and its firing rate will be exactly unit Gaussian for each batch (which is why itâ€™s called batchnorm). Now, we could in principle train using normalization. But we would not achieve a very good result. And the reason for that is that we want the pre-activations to be roughly Gaussian, but only at initialization. But we donâ€™t want these to be forced to be Gaussian always. Weâ€™d like to allow the nn to move the distributions around, such as making them more diffuse, more sharp, perhaps to make some tanh neurons to be more trigger-happy or less trigger-happy. So weâ€™d like this distribution to move around and weâ€™d like the backprop to tell us how that distribution should move around. So in addition to standardization of any point in the network, we have to also introduce this additional component mentioned in the paper described as â€œscale and shiftâ€. Basically, what we want to be doing is multiplying the normalized values by a gain \\gamma and then addding a bias \\beta to get a final output of each layer. Letâ€™s define them:\n\nbngain = torch.ones(1, hpreact.shape[1])\nbnbias = torch.zeros(1, hpreact.shape[1])\n\n\n\nso that:\n\nhpreact = bngain * (hpreact - hpmean) / hpstd + bnbias\n\n\n\nBecause the gain is initialized to 1 and the bias to 0, at initialization, each neuronâ€™s firing values in this batch will be exactly unit Gaussian and weâ€™ll have nice numbers, regardless of what the distribution of the incoming (e.g. hpreact) tensors are. That is roughly what we want, at least during initialization. And during optimization, weâ€™ll be able to backprop and change the gain and the bias, so the network is given the full ability to do with this whatever it wants internally. In order to train these, we have to make sure to include these in the parameters of the nn. To do so, and by effect facilitate backprop, letâ€™s update our define_nn, forward functions accordingly:\n\ndef define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, b1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    b1 = torch.randn(n_hidden, generator=g) * b1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    bngain = torch.ones((1, n_hidden))\n    bnbias = torch.zeros((1, n_hidden))\n    parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return parameters\n\n\n\ndef forward(x, y):\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    hpreact = (\n        bngain\n        * (hpreact - hpreact.mean(0, keepdim=True))\n        / hpreact.std(0, keepdim=True)\n        + bnbias\n    )\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss\n\n\n\nAnd now, letâ€™s initialize our new nn and train!\n\nparameters = define_nn(\n    w1_factor=kaiming_w1_factor, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0\n)\n_ = train(xtrain, ytrain)\nprint_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\");\n\n\n\nWe get a loss that is comparable to our previous results. Hereâ€™s a rough summary of our losses (re-running this notebook might yield slightly different values, but you get the point):\n\n# loss log\n\n# original:\ntrain 2.127638339996338\nval 2.171938180923462\n\n# fix softmax confidently wrong\ntrain 2.0707266330718994\nval 2.1337196826934814\n\n# fix tanh layer saturated at init\ntrain 2.0373899936676025\nval 2.1040639877319336\n\n# use semi-principled kaiming initialization instead of hacky way:\ntrain 2.038806438446045\nval 2.108304977416992\n\n# add a batchnorm layer\ntrain 2.0688135623931885\nval 2.10699462890625\n\nHowever, we should not actually be expecting an improvement in this case. And thatâ€™s because we are dealing with a very simple nn that has just a single hidden layer. In fact, in this very simple case of just one hidden layer, we were actually able to calculate what the scale of the weights should be to have the activations have a roughly Gaussian shape. So, batchnorm is not doing much here. But you might imagine that once you have a much deeper nn, that has lots of different types of operations and thereâ€™s also, for example, residual connections (which weâ€™ll cover) and so on, it will become very very difficult to tune the scales of the weight matrices such that all the activations throughout the nn are roughly Gaussian: at scale, an intractable approach. Therefore, compared to that, it is much much easier to sprinkle batchnorm layers throughout the nn.  In particular, itâ€™s common to look at every single linear layer like this one hpreact = embcat @ w1 + b1 (multiply by a weight matrix and add a bias), or for example convolutions that also perform matrix multiplication, (just in a more â€œstructuredâ€ format) and append a batchnorm layer right after it to control the scale of these activations at every point in the nn. So, weâ€™d be adding such normalization layers throughout the nn to control the scale of these activation (again, throughout the nn) without requiring us to do perfect mathematics in order to manually control individual activation distributions for any â€œbuilding blockâ€ (e.g. layer) we might want to introduce into our nn. So, batchnorm significantly stabilizes training and thatâ€™s why these layers are quite popular. Beware though, the stability offered by batchnorm often comes at a terrible cost. If you think about it, what is happening at a batchnorm layer (e.g. hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias) is something strange and terrible. Before introducing such a layer, it used to be the case that a single example was fed into the nn and then we calculated its activations and its logits in a deterministic manner in which a specific example yields specific logits. Then, for reasons of efficiency of learning, we started to use batches of examples. Those batches of examples were processed independently, but this was just an efficient thing. But now suddenly, with batchnorm, because of the normalization through the batch, we are mathematically coupling these examples in the forward pass and the backward pass of the nn. So with batchnorm, the hidden states (e.g. hpreact) and the output states (e.g. logits), are not just a function of the inputs of a specific example, but theyâ€™re also a function of all the other examples that happen to come for a ride in that batch. Damn! So whatâ€™s happening is, if you see for example the activations h = torch.tanh(hpreact), for every different example/batch, the activations are going to actually change slightly, depending on what other examples there are in the batch. Thus depending on what examples there are, h is going to jitter if you sample from many examples, since the statistics of the mean and std are going to be impacted. So, youâ€™ll get a jitter for the h and for the logits values. And youâ€™d think that this would be a bug or something undesirable, but in a very strange way, this actually turns out to be good in nn training as a side effect. The reason for that is that you can think of barchnorm as some kind of regularizer. Because what is happening is the you have your input and your h and because of the other examples the value of h is jittering a bit. What that does is that is effectively padding-out any one of these input examples and itâ€™s introducing a little bit of entropy. And because of the padding-out, the jittering effect is actually kind of like a form of data augmentation, making it harder for the nn to overfit for these concrete specific examples. So, by introducing all this noise, it actually pads out the examples and it regularizes the nn. And that is the reason why, deceivingly, as a second-order effect, this is acts like a regularizer, making it harder for the us as a community to remove or do without batchnorm. Because, basically, no one likes this property that the examples in a batch are coupled mathematically in the forward pass and it leads to all kinds of strange results, bugs and so on. Therefore, people do not like these side effects so many have advocated for deprecating the use of batchnorm and move to other normalization techniques that do not couple the examples of a batch. Examples are layer normalization, instance normalization, group normalization, and so on. But basically, long story short, batchnorm is the first kind of normalization layer to be introduced, it worked extremely well, it happened to have this regularizing effect, it stabilized training and people have been trying to remove it and move unto the other normalization techniques. But itâ€™s been hard, because it just works quite well. And some of the reason it works quite well is because of this regularizing effect and because it is quite effective at controlling the activations and their distributions. So, thatâ€™s the brief story of batchnorm. But letâ€™s see one of the other weird outcomes of this coupling. Basically, once weâ€™ve trained a nn, weâ€™d like to deploy it in some kind of setting and weâ€™d like to feed in a single individual example and get a prediction out from our nn. But how can we do that when our nn now with batchnorm in a forward pass estimates the statistics of the mean and the std of a batch and not a single example? The nn expects batches as an input now. So how do we feed in a single example and get sensible results out? The proposal in the batchnorm paper is the following. What we would like to do is implement a step after training that calculates and sets the batchnorm mean and std a single time over the training dataset. Basically, calibrate the batchnorm statistics at the end of training as such. We are going to get the training dataset and get the pre-activations for every single training example, and then one single time estimate the mean and std over the entire training set: two fixed numbers.\n\n@torch.no_grad()  # disable gradient calculation\ndef infer_mean_and_std_over_trainset():\n    # pass the entire training set through\n    emb = C[xtrain]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # measure the mean/std over the entire training set\n    bnmean_xtrain = hpreact.mean(dim=0, keepdim=True)\n    bnstd_xtrain = hpreact.std(dim=0, keepdim=True)\n    return bnmean_xtrain, bnstd_xtrain\n\n\n\nbnmean_xtrain, bnstd_xtrain = infer_mean_and_std_over_trainset()\n\n\n\nAnd so after calculating these values, at test time we are going to clamp them to the batchnorm calculation. To do so, letâ€™s extend the forward and print_loss functions as such:\n\ndef forward(x, y, bnmean=None, bnstd=None):\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:\n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss\n\n\n\n@torch.no_grad()  # this decorator disables gradient tracking\ndef print_loss(x, y, prefix=\"\", bnmean=None, bnstd=None):\n    _, _, _, loss = forward(x, y, bnmean=bnmean, bnstd=bnstd)\n    print(f\"{prefix} {loss}\")\n    return loss\n\n\n\nNow, if we do an inference with these mean and std values, instead of the batch-specific ones:\n\nprint_loss(xtrain, ytrain, bnmean=bnmean_xtrain, prefix='train')\nprint_loss(xval, yval, bnstd=bnstd_xtrain, prefix='val');\n\n\n\nThe losses we get may be more or less the same as our last losses before, but the benefit we have gained is that we can now forward a single example, because now the mean and std are fixed tensors. That said, because everyone is lazy, nobody wants to estimate the mean and std as a second stage after nn training. So, the batchnorm paper also introduced one more idea: that we can estimate these mean and std values in a running manner during the nn training phase. Letâ€™s see what that would look like. First, weâ€™ll define running value variables in the definition of our nn. Then weâ€™ll modify the train function and calculate the running values:\n\ndef define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, b1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    b1 = torch.randn(n_hidden, generator=g) * b1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    bngain = torch.ones((1, n_hidden))\n    bnbias = torch.zeros((1, n_hidden))\n    bnmean_running = torch.ones((1, n_hidden))\n    bnstd_running = torch.zeros((1, n_hidden))\n    parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return bnmean_running, bnstd_running, parameters\n\n\n\ndef forward(x, y, bnmean=None, bnstd=None):\n    global bnmean_running, bnstd_running\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:\n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    with torch.no_grad():  # disable gradient calculation\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss\n\n\n\ndef train(x, y, initial_lr=0.1, maxsteps=200000, batchsize=32, redefine_params=False):\n    global parameters\n    lossi = []\n    if redefine_params:\n        parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for i in range(maxsteps):\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        hpreact, h, logits, loss = forward(xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 100000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n    return hpreact, h, logits, lossi\n\n\n\nNow if we train, we will be calculating the running values of bnmean and bnstd without requiring a second step after training. This also happens when using PyTorch batchnorm layers: running values are calculated and then are used during inference. Now, letâ€™s re-define our nn and train it.\n\nbnmean_running, bnstd_running, parameters = define_nn(\n    w1_factor=kaiming_w1_factor, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0\n)\n_ = train(xtrain, ytrain)\n\n\n\nbnmean_xtrain, bnstd_xtrain = infer_mean_and_std_over_trainset()\nprint_loss(xtrain, ytrain, bnmean=bnmean_xtrain, prefix=\"train\")\nprint_loss(xval, yval, bnstd=bnstd_xtrain, prefix=\"val\");\n\n\n\nIf we calculate the mean over the whole training set and compare it with the running mean, we notice they are quite similar:\n\ntorch.set_printoptions(sci_mode=False)\nbnmean_xtrain\n\n\n\nbnmean_running\n\n\n\nSimilarly:\n\nbnstd_xtrain\n\n\n\nbnstd_running\n\n\n\nTherefore, we can easily infer the loss using the running values:\n\nprint_loss(xtrain, ytrain, bnmean=bnmean_running, prefix=\"train\")\nprint_loss(xval, yval, bnstd=bnstd_running, prefix=\"val\");\n\n\n\nAnd the resulting losses are basically identical. So, calculating running mean and std values eliminates the need for calculating them in a second step after training. Ok, so we are almost done with batchnorm. There are two more notes to make. First, is that we skipped the discussion of what the \\epsilon term is that is added to the normalization stepâ€™s denominator square root. It is usually a small, fixed number (e.g. 1e-05) by default. What this number does is that it prevents a division by 0 in the case that the variance over the batch is exactly 0. We could add it in our example and feel free to, but we are just going to skip it since a 0 variance is very very unlikely in our very simple example. Second note is that we are being wasteful with b1 in forward(). There, we are first adding b1 to embcat @ w1  to calculate hpreact, but then, within the batchnorm layer, we are normalizing by subtracting the pre-activation mean (that contains b1), which basically subtracts b1 out, rendering it redundant:\n\n    ...\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:        \n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    ...\n\nSince it is being subtracted out, as a parameter it is neither contributing to the nn training or inference nor is it being optimized. If we inspect itâ€™s gradient attribute, it is zero:\n\nprint(b1.grad)\ntorch.testing.assert_close(b1.grad, torch.zeros(b1.shape))\n\n\n\nTherefore, whenever using batchnorm, then if you have any layers with weights before it, like a linear layer or a convolutional layer or something like that, you are better off disabling the bias parameter for that layer, since we have the batchnorm bias (e.g. bnbias in our case) which compensates for it. To sum up this point: batchnorm has its own bias and thus thereâ€™s no need to have a bias in the layer before it, because that bias is going to be subtracted out anyway. So thatâ€™s the other small detail to be careful of sometimes. Of course, keeping a unnecessary bias in a layer is not going to do anything catastrophic but it is not going to be doing anything and is just wasteful, so it is better to remove it. Therefore, letâ€™s deprecate b1, the first layer bias, from our network and add some nice comments:\n\ndef define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    # batchnorm layer parameters\n    bngain = torch.ones((1, n_hidden))\n    bnbias = torch.zeros((1, n_hidden))\n    bnmean_running = torch.ones((1, n_hidden))\n    bnstd_running = torch.zeros((1, n_hidden))\n    parameters = [C, w1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return bnmean_running, bnstd_running, parameters\n\n\n\ndef forward(x, y, bnmean=None, bnstd=None):\n    global bnmean_running, bnstd_running\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    # linear layer\n    hpreact = embcat @ w1  # hidden layer pre-activation\n    # batchnorm layer\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:\n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    with torch.no_grad():  # disable gradient calculation\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd\n    # non-linearity\n    h = torch.tanh(hpreact)  # hidden layer\n    logits = h @ w2 + b2  # output layer\n    loss = F.cross_entropy(logits, y)  # loss function\n    return bnmean, bnstd, hpreact, h, logits, loss\n\n\n\nFor a final sum up: we use batchnorm to control the statistics of activations in a nn. It is common to sprinkle batchnorm layers across the nn and usually we will place it after layers that have multiplications (linear, convolutional, etc.). Internally, batchnorm has parameters for the gain (e.g. bngain) and the bias (e.g. bnbias). And these are trained using backprop. It also has two buffers. These are the running mean and the running mean of the std, which are not trained using backprop but which are updated during, and finally calculated after, training. So, what batchnorm does is it calculates the batch mean and std of the activations that are feeding into batchnorm layer, then itâ€™s centering that batch to be unit Gaussian and then itâ€™s offsetting and scaling it by the learned bias (e.g. bnbias) and gain (e.g. bngain). And then, on top of that, itâ€™s keeping track of the mean and std of the inputs, which are then used during inference. In addition, this allows us to forward individual examples during test time. So, thatâ€™s the batchnorm layer, which is a fairly complicated layer, but this is a simple example of what itâ€™s doing internally. Now, we are going to go through a real example.\n\n","type":"content","url":"/micrograduate/makemore3#batchnorm","position":13},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"ResNet"},"type":"lvl2","url":"/micrograduate/makemore3#resnet","position":14},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"ResNet"},"content":"\n\nresidual nns ( resnets) are common types of nns used for image classification. Although we havenâ€™t yet nor will we be covering or explaining all the pieces of these networks in detail, it is still worth noting that an image basically feeds into a resnet, and there are many many layers with repeating structure all the way to the output layer the gives us the predictions (e.g. what is inside the input image).\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"resnet.png\"))\n\n\n\nresnets (top network in the above image) are a repeating structure made up of blocks that are sequentially stacked-up. In PyTorch, each such block is defined as a \n\nBottleneck object:\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\nAlthough we havenâ€™t covered all the components of the above pytorch snippet (e.g. CNNs), letâ€™s point out some small pieces of it. The constructor, __init__, basically initializes the nn, similarly to our define_nn function. And, similarly to our forward function, the Bottleneck.forward method specifies how the nn acts for a given input x. Now, if you initialize a bunch of Bottleneck blocks and stack them up serially, you get a resnet. Notice what is happening here. We have convolutional layers (e.g. conv1x1, conv3x3). These are the same thing as a linear layer, except that convolutional layers are used for images and so they have spatial structure. What this means is that the linear multiplication and bias offset (e.g. logits = h @ w2 + b2) are done on overlapping patches, or parts, of the input, instead on the full input (since the images have spatial structure). Otherwise though, convolutional layers basically do an wx + b type of operation. Then, we have a norm layer (e.g. bn1), which is initialized to be a 2D batchnorm layer (BatchNorm2d). And then, there is a relu non-linearity. We have used tanh so far, but these are both common non-linearities that can be used relatively interchangeably. But for very deep networks, ReLU typically and empirically works a bit better. And in the Bottleneck.forward method, youâ€™ll notice the following pattern: conv layer -> batchnorm layer -> relu, repeated three times. This however is basically almost exactly the same pattern employed in our forward function: linear layer -> batchnorm layer -> tanh. And thatâ€™s the motif that you would be stacking up when you would be creating these deep nns that are called resnets. Now, if you dig deeper into the \n\nPyTorch resnet implementation, youâ€™ll notice that in the functions that return a convolutional layer, e.g. conv1x1:\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n","type":"content","url":"/micrograduate/makemore3#resnet","position":15},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Interim summary"},"type":"lvl2","url":"/micrograduate/makemore3#interim-summary","position":16},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Interim summary"},"content":"\n\nthe bias is disabled (bias=False) for the exact same reason we deprecated the bias in our layer that precedes our batchnorm layer (like we said, keeping these parameters wouldnâ€™t hurt performance, but they are practically useless). So, because of the motif, the convolutional layers donâ€™t need a bias, as there is a bias in the following barchnorm layers to make up for them. Letâ€™s now also briefly descend into the definitions of similar pytorch layers and the parameters that they take. Instead of a convolutional layer, weâ€™re going to look at the \n\nlinear layer as implemented by PyTorch. As we discussed, convolutional layers are basically linear ones except on patches of the image. So a linear layer performs a wx + b or xA^T + b as described in the docs. And to initiliaze this Linear layer object, you need to know the fan-in (in_features) and the fan-out (out_features) in order to construct a weight matrix that has a shape [in\\_features \\times out\\_features]. In our case, the equivalent parameters for the first layer are: n_embd * block_size, n_hidden. Also there is an option to enable or disable the bias. Furthermore, if you see the Variables section of the docs, thereâ€™s a weight and a bias bulletpoint whose default initialization is described. So by default, PyTorch initializes the weights by taking the fan-in and then calculating the square root of k = \\frac{1}{in\\_features}. And then,  the weights are drawn from a U(-\\sqrt{k}, \\sqrt{k}) uniform distribution. Despite the lack of the tanh gain 5/3 that we are using, this is the same kaiming initialization as we have described throughout this lesson. The reason is that if you have a roughly Gaussian input, a kaiming initialization will ensure that out of this layer (e.g. Linear) you will have a roughly Gaussian output. Letâ€™s now look at the \n\nPyTorch BatchNorm1D batchnorm layer. It takes a num_features argument (e.g. n_hidden in our case) in order to initialize the gain, bias and running parameters, as well as an \\epsilon parameter that is used for the square root of the normalization denominator. There is also a momentum=0.1 parameter that is used in the calculation of the running mean and std values (our equivalent is 0.001, e.g. bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean) which you may want to change sometimes. Roughly speaking, if you have a very large batch size, typically what youâ€™ll see is that when you estimate the mean and std, for every single batch size, if itâ€™s large enough, youâ€™re going to get roughly the same result. And therefore, slightly higher momentum like the default 0.1. However, for a batch size as small as 32 (e.g. the one we use), the mean and std here might take on slightly different numbers, because thereâ€™s only 32 (instead of letâ€™s say 128) to estimate the mean and std. So in that case, the 0.001 in our example is more appropriate for convergence than the larger, potentially dangerous 0.1 that would cause more thrashing and higher inaccuracies in the calculations. Thereâ€™s also the affine boolean parameter, that determines whether the batchnorm layerâ€™s gain and bias parameters are learnable, and the track_running_stats boolean parameter. One reason you may want to skip running stats is because you may want to, for example, calculate them after training, as a second stage (e.g. through mean_and_std_over_trainset()). And so in that case, you wouldnâ€™t want the batchnorm layer to do all this extra compute that youâ€™re not gonna use. Finally, you can also specify the device that the batchnorm layer pass is going to happen on (either cpu or gpu) and what the datatype is going to be (half-precision, single-precision, double-precision, and so on). So that is more or less the batchnorm layer covered in the paper, as implemented by us and as quite-similarly provided in PyTorch. And thatâ€™s all we wanted to cover in this lecture: the importance of understanding the activations and the gradients and their statistics in nns. And this becomes increasingly important especially as you make your nns bigger, larger and deeper. We looked at the distributions at the output layer and we saw that if you have too confident mispredictions, because the activations are too messed up at the last layer, you can end up with these hockey stick losses. And if you fix this, you get a better loss at the end of training, because your training is not doing wasteful work. Then, we also saw that we need to control the activations as we donâ€™t want them to squash to zero or explode to the flat regions of the non-linearityâ€™s output range, because you can run into trouble (e.g. dead neurons). Basically, you want everything to be fairly homogeneous throughout the nn. You want roughly Gaussian activations throughout the nn. And then we pondered, if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the nn so that everything is as controlled as possible. By a bit of trial and error, we found some appropriate scaling factors that gave us the uniform activations that we seeked. Of course, we realize that scaling parameters like that is very very hard and practically unsustainable a method when the nn is much much deeper. So then we introduced the notion of the normalization layer that people use in practice: batchnorm, layer normalization, instance normalization, group normalization. And we introduced and covered the one that came out first. batchnorm is layer that you can sprinkle throughout your deep nn and the basic idea is that if you want roughly Gaussian activations, well then take your activations, find their mean and std and center your data. And you can do that because the centering operation is differentiable. On top of that, we had to add a lot of bells and whistles, giving us a sense of the complexity of batchnorm. Because, ok now weâ€™re centering the data: thatâ€™s great. But suddenly we need the gain and bias parameters and now those are trainable. And because we are coupling all the training examples, the questioning is how do you do the inference? To do the inference, we then realized that we have to estimate the mean and std once on the entire training set and then use those at inference. But then, no one likes to do that as a second stage after training. So calculate those values as running averages during training and estimate these in a running manner so that everything is a bit simpler. And again! That was the batchnorm layer. Last time, I promise. Lol. Although helpful, no one likes this layer! It causes a huge amount of bugs and intuitively thatâ€™s because itâ€™s coupling different examples (per batch) in the forward pass of a nn. And many have shot themselves in the foot with this layer, over and over again in their lifetimes. So, in order to avoid sufferring, try to avoid it as much as possible (e.g. by using other normalization alternatives). Nevertheless, batchnorm proven to be very influential when it came out in 2015 because that was the first time that you could train reliably much deeper nns. The reason for that is that this layer is very effective at controlling the statistics of the activations in a nn. Now, thatâ€™s all for now. In the next notebooks, we can start going fully into recurrent nns which are very very deep networks (due to unrolling during optimization). And that is where a lot of this analysis around the activation statistics and all these normalization layers will become very very important for good performance. So, weâ€™ll see that next time. Bye!\n\n","type":"content","url":"/micrograduate/makemore3#interim-summary","position":17},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"torchification"},"type":"lvl2","url":"/micrograduate/makemore3#torchification","position":18},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"torchification"},"content":"\n\nJust kidding! - As a bonus, before the next lesson, we will cover one more summary of everything we have presented in this lesson so far. But also, it would be very useful to â€œtorchifyâ€ our code a little bit so it looks much more like what you would encounter in PyTorch. We will structure our code into modules. Then we will construct our nn like we would in PyTorch and we will run our training loop to optimize it. Then, as one last thing we will visualize the activation statistics both in the forward pass and in the backward pass, before evaluating and sampling just like we have done before. Letâ€™s start. Similar to \n\ntorch.nn.Linear, we will implement our own linear layer. By default, we initialize the weights by drawing numbers from a Gaussian distribution and doing a kaiming initialization and we initialize the bias to zero. When calling this module, we do a forward pass and calculate x @ w + b, whereas calling a parameters method will return the weight and bias tensors of this layer.\n\nclass Linear:\n    def __init__(self, fan_in, fan_out, generator, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=generator) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\n\nSimilar to the \n\nPyTorch BatchNorm1D batchnorm layer, we will define our own. Apart from the parameters we discussed previously (dim, eps, momentum), we will also define the training attribute. When this boolean flag is enabled, the running mean values are calculated (training mode) and when it is disabled, they are not (testing mode). When calling this layer, we do a forward pass, wherein mean values are assigned and an output value is calculated (as described previously) and saved (in order to comfortably plot them later on) and finally we update the moving average buffers. Notice the torch.no_grad() context manager we use in order to make our code more efficient by bypassing unnecessary saving into a maintained computation graph (since we do not care about gradients for the buffer variables, there is no point in wasting memory for the allocation of gradient-related data). This context manager essentially signifies that we will not be calling backward on the variables inside it.\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (\n                    1 - self.momentum\n                ) * self.running_mean + self.momentum * xmean\n                self.running_var = (\n                    1 - self.momentum\n                ) * self.running_var + self.momentum * xvar\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\n\nLastly, in PyTorch fashion, we also calculate an equivalent \n\ntorch.nn.Tanh layer:\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nSo, by defining everything in layers it now becomes very easy to stack them up into a list and more intuitively â€œdefineâ€ any nn. Letâ€™s see how by updating our define_nn():\n\ndef define_nn(\n    n_embd=10,\n    hidden_dims=[100, 100, 100, 100, 100],\n    weight_gain=5 / 3,\n    batchnorm_enabled=False,\n    add_batchnorm_last_layer=False,\n    tanh_enabled=True,\n):\n    global C, g\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    input_size = n_embd * block_size\n    output_size = vocab_size\n    # define input layer\n    layers = [\n        Linear(input_size, hidden_dims[0], generator=g, bias=False),\n    ]\n    if batchnorm_enabled:\n        layers.append(BatchNorm1d(hidden_dims[0]))\n    if tanh_enabled:\n        layers.append(Tanh())\n    # define hidden layers\n    for i, n_hidden in enumerate(hidden_dims[:-1]):\n        layers.append(Linear(n_hidden, hidden_dims[i + 1], generator=g, bias=False))\n        if batchnorm_enabled:\n            layers.append(BatchNorm1d(n_hidden))\n        if tanh_enabled:\n            layers.append(Tanh())\n    # define output layer\n    layers.append(Linear(hidden_dims[-1], output_size, generator=g, bias=False))\n    if add_batchnorm_last_layer:\n        layers.append(BatchNorm1d(output_size))\n    # scale parameters\n    with torch.no_grad():\n        # last layer: make less confident\n        if add_batchnorm_last_layer:\n            layers[-1].gamma *= 0.1\n        else:\n            layers[-1].weight *= 0.1\n        # all other layers: apply gain\n        for layer in layers[:-1]:\n            if isinstance(layer, Linear):\n                layer.weight *= weight_gain\n    # collect parameters\n    parameters = [C] + [p for layer in layers for p in layer.parameters()]\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters\n\n\n\nNow letâ€™s update the our forward, backward and train functions:\n\ndef forward(layers, xb, yb):\n    emb = C[xb]  # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, yb)  # loss function\n    return loss\n\n\n\ndef backward(layers, parameters, loss, debug=False):\n    if debug:\n        for layer in layers:\n            layer.out.retain_grad()\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n\n\ndef train(\n    x,\n    y,\n    layers,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(layers, xb, yb)\n        backward(layers, parameters, loss, debug=(break_at_step is not None))\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        if break_at_step is not None and i >= break_at_step:\n            break  # AFTER_DEBUG: would take out obviously to run full optimization\n    return lossi\n\n\n\nNow, weâ€™ll define a new nn and train in debug mode, for only one step.\n\nlayers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\n\n\n\n","type":"content","url":"/micrograduate/makemore3#torchification","position":19},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Visualizing activations and gradients"},"type":"lvl2","url":"/micrograduate/makemore3#visualizing-activations-and-gradients","position":20},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Visualizing activations and gradients"},"content":"\n\nNow, since we defined an out attribute in our custom Tanh layer, after training we have saved the activations which we can visualize! Specifically, we will plot the histogram of each layerâ€™s tanh activations.\n\ndef visualize_layer_values(layers, grad=False, layer_cls=Tanh):\n    plt.figure(figsize=(20, 4))  # width and height of the plot\n    legends = []\n    for i, layer in enumerate(layers[:-1]):  # note: exclude the output layer\n        if isinstance(layer, layer_cls):\n            t = layer.out.grad if grad else layer.out\n            if grad:\n                print(\n                    \"layer %d (%10s): mean %+f, std %e\"\n                    % (i, layer.__class__.__name__, t.mean(), t.std())\n                )\n            else:\n                print(\n                    \"layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%\"\n                    % (\n                        i,\n                        layer.__class__.__name__,\n                        t.mean(),\n                        t.std(),\n                        (t.abs() > 0.97).float().mean() * 100,\n                    )\n                )\n            hy, hx = torch.histogram(t, density=True)\n            plt.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f\"layer {i} ({layer.__class__.__name__}\")\n    plt.legend(legends)\n    if grad:\n        plt.title(\"gradient distribution\")\n    else:\n        plt.title(\"activation distribution\")\n\n\n\nvisualize_layer_values(layers)\n\n\n\n\n\nThis histogram shows us how many values in these tensors take on any of the values on the x-axis. layer 1 is fairly saturated (~20%), with a significant amount of values being close to the saturation points at the tails (-1, +1) and the subsequent layers being more stable. And why the values are pretty stable is because the weight values of the Linear layer are boosted by a gain of 5/3. If we use a gain of 1 (aka no gain), letâ€™s see what happens:\n\nlayers, parameters = define_nn(weight_gain=1)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)\n\n\n\n\n\nNow, from the first to the last tanh layer, the std shrinks and the saturation goes to 0. What this means is that the activations are being shrunk to 0. The reason for that is that when you just have a sandwich of linear layer, tanh layer pairs, these tanh layers act as squashing functions that take a distribution and slightly squeeze it towards zero. Therefore, some gain is necessary in order to keep expanding the distributions and by doing so to fight the squashing phenomenon. So, if the gain is close to 1, the activations will then come towards 0, but if it is something too big (such as 3), then, on the contrary, the saturations end up way too large:\n\nlayers, parameters = define_nn(weight_gain=3)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)\n\n\n\n\n\nSo, 5/3 (the default value) is a good setting for a sandwich of linear layers with tanh activations. And it roughly stabilizes the std at a reasonable value (~5%), which is a pretty good number and this is a good setting of the gain in this context:\n\nlayers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)\n\n\n\n\n\nSimilarly, we can do the exact same thing with the gradients. So, here we will run the exact same loop by using the exact same function, but instead of the layer outputs we will now visualize the gradients (.grad):\n\nlayers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, grad=True)\n\n\n\n\n\nHere, you will see that the gradient distribution is fairly reasonable. And in particular, what we are looking for is that all of these layers (layer 1, 2, etc.) in this â€œsandwichâ€ have roughly the same gradient. Things are not shrinking or exploding. So, letâ€™s train and set the gain as way too small, 0.5 and see what happens to the activations and the gradients:\n\nlayers, parameters = define_nn(weight_gain=0.5)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\n\n\n\n\n\n\n\nFirst of all, now, the activations are shrinking to zero but also the gradients are doing something weird: they start off very narrow, around 0.0 (see layer 1), but then in layers that follow, they are expanding out (layer 3, 5, etc.). If we now use a too-high of a gain, e.g. 3.0, like we did before:\n\nlayers, parameters = define_nn(weight_gain=3.0)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, grad=True)\n\n\n\n\n\nthen we see that for the gradients there is some asymmetry going on where, as you go into deeper and deeper layers, the activations are also changing. Therefore, we have to very carefully set the grains to get nice activations in both the forward and backard passes. Now, before we move on to batchnorm, letâ€™s see what happens with the activations when we remove the Tanh units and thus only a giant linear sandwich remains as our nn:\n\nlayers, parameters = define_nn(tanh_enabled=False)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, layer_cls=Linear)\n\n\n\n\n\n\n\nWhat we are seeing is that the activations started out on the blue (layer 1) and by layer 4 they have become very diffuse, so what is happening to the activations is that they are expanding.\n\nvisualize_layer_values(layers, grad=True, layer_cls=Linear)\n\n\n\n\n\nConversely, the gradients follow the opposite pattern, as you go down deeper in the layers. So basically you have an asymmetry in the nn. And you might imagine that if you have very deep nns, say like 50 layers or something like that, the above pattern is not a good place to be! Thatâ€™s why before the batchnorm technique, the grain was incredibly tricky to set. See what happens, for a very small gain:\n\nlayers, parameters = define_nn(tanh_enabled=False, weight_gain=0.5)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, layer_cls=Linear)\nvisualize_layer_values(layers, grad=True, layer_cls=Linear)\n\n\n\n\n\n\n\nBasically, the reverse occurs: activations shrink and gradients diffuse, as we go deeper in the layers. Therefore, certainly these patterns are not what we would want and in this case the correct setting of the gain is exactly 1.0, just as we are doing at initialization:\n\nlayers, parameters = define_nn(tanh_enabled=False, weight_gain=1.0)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, layer_cls=Linear)\nvisualize_layer_values(layers, grad=True, layer_cls=Linear)\n\n\n\n\n\n\n\nNow we see that the statistics for the forward and backward passes are well behaved! And so the reason we are demonstrating these phenomena is to highlight how getting nns to train before these normalization layers and before the use of advanced optimizers like Adam (which we still have to cover) and residual connections and so on, training nns basically looked like this:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"pencil_balancing.jpeg\"))\n\n\n\nHaha, like a total balancing act. You have to make sure that everything is precisely orchestrated and have to care about the activations and the gradients and the statistics and then maybe you can train something. But, it was basically impossible to train very deep networks, and this was fundamentally the reason for that. You would have to be very very careful with your initialization. The other point to make here is the question: why do we need tanh layers at all? Why do we include them and then have to worry about the gain? The reason for that of course is that if you just have a stack of linear layers, then certainly we are very easily getting nice activations and so on but this is just a massive linear sandwich. And it turns out that it collapses to a single linear layer in terms of its representation power. So, if you were to plot the output as a function of the input, in that case, you are just getting a linear function. No matter how many linear layers you stack up, you still end up with just a linear transformation: all the sets of wx + b just collapse into a large WX + B with a slightly different weight and bias matrix. Interestingly though, even though in that case, the forward pass collapses to just a linear layer, because of backprop and the dynamics of the backward pass, the optimization is really not identical. You actually end up with all kinds of interesting dynamics in the backward pass because of the way the chain rule is calculating it. And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layer, in both cases, those are just a linear transformation in the forward pass, but the training dynamics would be different. And there are actually in fact entire papers that analyze infinitely layered linear layers, etc. and so as you can imagine thereâ€™s a lot of things too that you can play with there. Basically, the Tanh non-linearities allow us to turn this sandwich from just a linear transformation into a nn that can in priciple approximate any arbitrary function.\n\n","type":"content","url":"/micrograduate/makemore3#visualizing-activations-and-gradients","position":21},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Further visualization"},"type":"lvl2","url":"/micrograduate/makemore3#further-visualization","position":22},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Further visualization"},"content":"\n\nNow, weâ€™ll define a nn with batchnorm layers between the linear and tanh layers and we will look at another kind of visualization that is very important to consider when training nns:\n\ndef visualize_weight_gradients(parameters):\n    plt.figure(figsize=(20, 4))  # width and height of the plot\n    legends = []\n    for i, p in enumerate(parameters):\n        t = p.grad\n        if p.ndim == 2:\n            print(\n                \"weight %10s | mean %+f | std %e | grad:data ratio %e\"\n                % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std())\n            )\n            hy, hx = torch.histogram(t, density=True)\n            plt.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f\"{i} {tuple(p.shape)}\")\n    plt.legend(legends)\n    plt.title(\"weights gradient distribution\")\n\n\n\nlayers, parameters = define_nn(batchnorm_enabled=True)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_weight_gradients(parameters)\n\n\n\n\n\nSo, ultimately what we are doing during training is that we are updating the parameters of the nn. So, we care about the parameters, their values and their gradients. Therefore, in visualize_weight_gradients what we are doing is we are iterating over all the available parameters and then we are only considering the 2-dimensional ones (by checking if p.ndim == 2), which are basically the weights of these linear layers. We are skipping the biases, the gammas and the betas in the batchnorm layer just for simplicity, because what is happening with the weights is instructive by itself. Here, we have printed the mean, std and gradient-to-data ratio, which is helpful for getting a sense of the scale of the gradient compared to the scale of the actual values. This is important because we are going to be taking a step update that is the learning rate times the gradient onto the data. And so if the gradient has too large of a magnitude (if the numbers in that tensor are too large) compared to the data (the numbers in the data tensor), then you are in trouble. But in our case, our grad-to-data ratios are low numbers (e.g. 1.209762e-03) and the grad values are 100 to 1000 times smaller than the data values of these weight parameters. Notably, this is not true about the last layer (16, pink) which is a bit of a troublemaker in the way that it is currently arranged. Because you can see that this layer takes on values that are much larger than some of the other layerâ€™s values inside the nn. And so the std values are roughly \n\n10-3 throughout the layers, except for the last linear layer that has an std of roughly \n\n10-2. That is problematic, because in the simple stochastic gradient descent setup, you would be training the last layer about 10 times faster than you would be training the other layers at initialization. Now this actually fixes itself a little bit if you train for a bit longer. So, for example if we stop the training at step 1000 and plot the distributions:\n\nlayers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\n\n\n\n\n\nHere we see how in the forward pass the neurons are saturating just a bit (~21% for layer 1, ~11% for layers 2+).\n\nvisualize_layer_values(layers, grad=True)\n\n\n\n\n\nAnd if we also look at the backward pass, the stds are more or less equal and there is no shrinking to 0 or exploding to \\infty.\n\nvisualize_weight_gradients(parameters)\n\n\n\n\n\nAnd last but not least, you can see here, in the weight gradients, things are also stabilizing a little bit. So the tails of the last layer (6, pink) are being drawn to 0 during the optimization. But this is certainly a little bit troubling. Especially if you are using a very simple update rule like stochastic gradient descent, instead of a modern optimizer like Adam. Now, letâ€™s look at another plot that is very useful to look at when training nns. First of all, letâ€™s agree that the grad-to-data ratio is actually not that informative because what matters at the end instead is actually the update-to-data ratio. Because that is the amount by which we will actually change the data in these tensors. So, now letâ€™s update the train function by introducing a new update-to-data ratio list (ud) that we are going to be building up for every single training iteration in order to keep track of this ratio:\n\ndef train(\n    x,\n    y,\n    layers,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    ud = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(layers, xb, yb)\n        backward(layers, parameters, loss, debug=(break_at_step is not None))\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        with torch.no_grad():\n            ud.append(\n                [\n                    ((lr * p.grad).std() / p.data.std()).log10().item()\n                    for p in parameters\n                ]\n            )\n        if break_at_step is not None and i >= break_at_step:\n            break  # AFTER_DEBUG: would take out obviously to run full optimization\n    return lossi, ud\n\n\n\nNow, letâ€™s initialize a new nn and train for 1000 iterations:\n\nlayers, parameters = define_nn()\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\n\n\n\nAnd look at the activations, the gradients and the weight gradients:\n\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\n\n\n\n\n\n\n\n\n\nbut also one more plot we will now introduce:\n\ndef visualize_update_ratios(parameters, ud):\n    plt.figure(figsize=(20, 4))\n    legends = []\n    for i, p in enumerate(parameters):\n        if p.ndim == 2:\n            plt.plot([ud[j][i] for j in range(len(ud))])\n            legends.append(\"param %d\" % i)\n    plt.plot(\n        [0, len(ud)], [-3, -3], \"k\"\n    )  # these ratios should be ~1e-3, indicate on plot\n    plt.legend(legends)\n\n\n\nvisualize_update_ratios(parameters, ud)\n\n\n\nSo, when we plot the ud ratios, you can see that they evolve over time. During initialization they take on certain values and these updates sort of like start stabilizing during training. But youâ€™ll also notice we have plotted a straight black line. This is an approximate value that is a rough guide for what the ratios should roughly be, which in this case is roughly ~1e-3. That basically means that there are some certain values in the data tensor and the updates to those values at every single iteration are no more than roughly 1000th of the actual magnitude in those tensors. If instead of roughly ~1e-3, the desired ratio value are much larger (e.g. ~1e-1 or a log value of -1 in this plot), then the data values are updating a lot, meaning that they are undergoing a lot of change. This is the case for the ud ratio values of the last layer, layer 6. The reason why this layer is an outlier, is because this layer was artificially shrunk down to keep the softmax unconfident: see the define_nn function where we specifically do: layers[-1].weight *= 0.1. This artificially made inside that last layer tensor way too low and that is why we are temporarily getting a very high ud ratio. But as you can see, that ratio does decrease and then stabilizes over time, once that weight starts to learn. In general, itâ€™s helpful to look at the evolution of this ud ratio and as a rule of thumb to make sure that the values are not too much above roughly ~1e-3 (-3 on this log plot). If itâ€™s below ~1e-3, usually this means that the parameters are not training fast enough. So, if our learning rate was very low, letâ€™s say 0.001, this plot will typically reveal it:\n\nlayers, parameters = define_nn()\nlossi, ud = train(\n    xtrain, ytrain, layers, parameters, break_at_step=1000, initial_lr=0.001\n)\nvisualize_update_ratios(parameters, ud)\n\n\n\n\n\nSo you see how all of these updates are way too small. The size of the update is ~1e-5 times smaller than the size of the data tensor values. And this is essentially a symptop of training way too slow. So this is another way to sometimes set the learning rate and to get a sense of what that learning rate should be after inspecting the ud ratio evolution. If anything, the default learning rate of 0.1:\n\nlayers, parameters = define_nn()\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_update_ratios(parameters, ud)\n\n\n\n\n\nis a little bit on the higher side. Because you see that weâ€™re above the black line of 1e-3 a little bit, but everything is somewhat stabilizing. So this looks like a pretty decent setting of learning rates. But this is something to look at in general. And when something is miscalibrated you will quickly realize it. So for example, everything looks pretty well behaved, right? But, just as a comparison, when things are not properly calibrated, what does that look like? For example, letâ€™s simulate the scenario were we initialize the weights of the Linear layers from a Gaussian distribution without the fan_in normalization (torch.randn((fan_in, fan_out), generator=generator) and not torch.randn((fan_in, fan_out), generator=generator) / fan_in**0.5). An easy way to do this without having to re-define the Linear class and re-write stuff is to simply call a function after defining our nn that multiplies each layerâ€™s weight tensor with a fan_in**0.5 in order to revert the effect of division by the same number (that happened during initialization):\n\ndef revert_fan_in_normalization(layers):\n    for layer in layers:\n        if isinstance(layer, Linear):\n            fan_in = layer.weight.shape[0]\n            layer.weight.requires_grad = False\n            # revert division by fan_in**0.5 and simulate initialization\n            # of weight by sampling from plain Gaussian distribution:\n            layer.weight *= fan_in**0.5\n            layer.weight.requires_grad = True\n\n\n\nlayers, parameters = define_nn()\nrevert_fan_in_normalization(layers)\n\n\n\nNow, how do we notice in this case that something is off? Well, after training, the activations plot should tell you â€œwoaw! your neurons are way too saturatedâ€:\n\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\n\n\n\n\n\nAlso, the gradients and weight gradients are going to be all messed up:\n\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\n\n\n\n\n\n\n\nAnd if we look at the update-to-data ratios, they are also quite messed up and all over the places:\n\nvisualize_update_ratios(parameters, ud)\n\n\n\nYikes! As you can see, there is a lot of discrepancy in how fast these layers are learning and some of them are learning way too fast. And so 1e-1, 1e-1.5, etc. are very large numbers in terms of this ratio. Again, we should be somewhere around ~1e-3 and not much more above that. So, this is how miscalibrations of your nns are going to manifest. And therefore such plots are a good way of bringing those miscalibrations to your attention, so you can address them. Okay so so far we have seen that when we have such a linear tanh sandwich as the one we have constructed, we can actually precisely calibrate the gains and make the activations, the gradients and the parameters and the updates all look pretty decent. But it definitely does feel like trying to balance a pencil on your finger and thatâ€™s because the gain has to be very precisely calibrated. So now letâ€™s introduce batchnorm layers into the magical sandwich and letâ€™s see how that helps fix the problem, by placing them in-between our linear and tanh layers (note: placing them after the tanh layers would also yield similar results). Luckily, we have already implemented an option for that and all we have to do is to enable the batchnorm option. But now we will also add a batchnorm after the last layer too using the corresponding option:\n\nlayers, parameters = define_nn(batchnorm_enabled=True, add_batchnorm_last_layer=True)\n\n\n\nNow, letâ€™s train and look at the distributions:\n\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\n\n\n\n\n\n\n\n\n\nwhich of course look very good. And they are necessarily going to look good because now before every single Tanh layer thereâ€™s a batchnorm happening. This yields a saturation of ~2\\% and roughly equal std across all layers and everything looks very homogeneous in the activations distribution, with the gradient and weight gradient distributions also looking great. Also, the updates:\n\nvisualize_update_ratios(parameters, ud)\n\n\n\nalso look pretty reasonable, with all parameters training at around roughly the same rate. Now, what we have gained is that we can now be slightly less brittle with respect to the gain values of the weights. Meaning, that if for example we make the gain be 0.2 (much lower than the default tanh gain of 5/3) and then train and print the same distributions:\n\nlayers, parameters = define_nn(batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=0.2)\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\n\n\n\n\n\n\n\n\n\nThey will all look pretty ok and unaffected! However, if we plot the updates:\n\nvisualize_update_ratios(parameters, ud)\n\n\n\nwe see that these do in fact change. And so even though the forward and backward pass to a very large extent look okay because of the backward pass of the batchnorm and how the specifically the scale of the incoming activations interacts in the batchnorm and its backward pass, the decrease of the gain is actually changing the scale of the updates on these parameters. So, the gradients on these weights are affected. So, we still donâ€™t get a completely free pass to pass any arbitrary weight gain, but everything else is significantly more robust in terms of the forward and backward passes and the weight gradients. Itâ€™s just that you may in such a case need to retune the learning rate if you are changing sufficiently the scale of the activations that are coming into the batchnorm layers. To verify this, we can see how making the gain to a greater value like 5.0:\n\nlayers, parameters = define_nn(batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=5.0)\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\nvisualize_update_ratios(parameters, ud)\n\n\n\n\n\n\n\n\n\n\n\nnow causes the updates to come out lower, as a result. Finally letâ€™s now remove the weight gain by setting it to 1.0 notice that with batchnorm enabled we can now also skip the fan_in normalization at initialization. So, like we did before, if we define our nn by sampling the initial weights from a plain Gaussian:\n\nlayers, parameters = define_nn(\n    batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=1.0\n)\nrevert_fan_in_normalization(layers)\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\nvisualize_update_ratios(parameters, ud)\n\n\n\n\n\n\n\n\n\n\n\neverything looks pretty much ok. But from the update plot you can see that everything looks below 1e-3, so we would have to bump up the learning rate in order to make sure that we are training more properly. Intuitively, we would probably need to 10x the learning rate from 0.1 (default) to 1.0. Letâ€™s try it out:\n\nlayers, parameters = define_nn(\n    batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=1.0\n)\nrevert_fan_in_normalization(layers)\nlossi, ud = train(\n    xtrain, ytrain, layers, parameters, break_at_step=1000, initial_lr=1.0\n)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\nvisualize_update_ratios(parameters, ud)\n\n\n\n\n\n\n\n\n\n\n\neverything again looks good and voilÃ ! Now our updates are more reasonable. So, long story short, with barchnorm, we are now significantly more robust to the gain of these linear layers, whether or not we have to apply the fan_in normalization, with the caveat (in terms of the former) that we do have to worry about the update scales and making sure that the learning rate is properly calibrated here. So, the forward and backward pass statistics are all looking significantly more behaved, except for the scales of the updates that should be taken into consideration.\n\n","type":"content","url":"/micrograduate/makemore3#further-visualization","position":23},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Summary"},"type":"lvl2","url":"/micrograduate/makemore3#summary","position":24},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Summary"},"content":"\n\nOk, so now letâ€™s summarize (again, lol). There are three things this section was intended to teach:\n\nintroducing you to batchnorm, which is one of the first modern innovations that helped stabilize very deep nns and their training\n\nPyTorch-ifying some of our code by wrapping it up into layer modules (Linear, BatchNorm1D, Tanh, etc.) that can be stacked up into nn like lego building blocks. Since these synonymous layers exist as objects in the torch.nn API, the way we have constructed it, we could easily replace each one of our custom modules (Linear with nn.Linear and so on) and everything would probably work just fine.\n\npresent you with the diagnostic tools that you would use to understand whether your nn is in a good state dynamically. This means looking at histograms of the forward pass activations and backward pass gradients. And then also the weights that are going to be activated as part of stochastic gradient descent by looking at their means, stds and also the gradient-to-data ratios or even better, the update-to-data ratios. And we saw that what people usually do is look at the evolution of these update-to-data ratios, instead of single step snapshots frozen in time, and make sure everything looks fine. In particular, we highlighted that around 1e-3 (-3 on the log scale) is a good rough heuristic of what you want this ratio to be and if itâ€™s way too high, then probably the learning rate is a little too big. Whereas, if itâ€™s too small, then the learning rate is probably too small. So, these are the things that you might want to play with when you want to get your nn to work very well.\n\nNow, there are certain things we did not try to achieve in this lesson. As an example, we did not try to beat the performace from our previous lessons. If we do actually try to, by using batchnorm layers (by using the learning rate finding mechanism described in the previous lesson), we would end up with results that are very very similar to the ones that we obtained before. And in that case, that would be because our performance now is not bottlenecked by the optimization, which is what batchnorm is helping with. But, the performance in actually most likely bottlenecked by the context length we are choosing as our context. Currently we are taking in 3 characters in order to predict the 4th one. To go beyond that, we would need to look at more powerful architectures, like RNNs and Transformers, in order to further push the log probabilities that weâ€™re achieving on this dataset. Also, we did not give a full explanation of all of these activations and the gradients (e.g. from the backward pass or the weights). Maybe you found those parts slightly unintuitive and maybe youâ€™re slightly confused about: okay, if I change the gain, how come that we need a different learning rate? And the reason we didnâ€™t go into full detail to make such questions clearer is because weâ€™d have to actually look at the backward pass of all these different layers and get an intuitive understanding of how that works, and so we did not go into that in this lesson. The purpose really was just to introduce you to the diagnostic tools and what they look like. But of course thereâ€™s still a lot of work remaining on the intuitive level to understand the initialization, the backward pass and how all of these interact.\n\n","type":"content","url":"/micrograduate/makemore3#summary","position":25},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Outro"},"type":"lvl2","url":"/micrograduate/makemore3#outro","position":26},{"hierarchy":{"lvl1":"4. makemore (part 3): activations & gradients, batchnorm","lvl2":"Outro"},"content":"\n\nWe certainly havenâ€™t solved initialization, nor have we solved backprop, or anything of that sorts. These are still very much an active area of research with lots of people trying to figure out what the best way is to initialize these networks, what is the best update rule to use and so on. So none of all this is solved and we donâ€™t really have all the answers to all these cases but at least we are making progress and at least we have some tools to tell us whether or not things are on the right track, for now. So, all in all, we have made progress in this lesson and I hope you enjoyed it. See you in the next lesson!","type":"content","url":"/micrograduate/makemore3#outro","position":27},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja"},"type":"lvl1","url":"/micrograduate/makemore4","position":0},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/makemore4","position":1},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/makemore4#intro","position":2},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Intro"},"content":"\n\nHi everyone. So today we are once again continuing our implementation of makemore!\n\nBigram (one character predicts the next one with a lookup table of counts)\n\nMLP, following \n\nBengio et al. 2003\n\nRNN, following \n\nMikolov et al. 2010\n\nLSTM, following \n\nGraves et al. 2014\n\nGRU, following \n\nKyunghyun Cho et al. 2014\n\nCNN, following \n\nOord et al., 2016\n\nTransformer, following \n\nVaswani et al. 2017\n\nNow so far, from the above list, weâ€™ve come up to mlps and our nn has looked like this:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"bengio2003nn.jpeg\"))\n\n\n\nand we have been implementing this over the last few lessons. Now, Iâ€™m sure everyone is very excited to go into recurrent neural networks (rnns) and all of their variants and how they work and their diagrams look cool:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"rnns.png\"))\n\n\n\nand such models are very exciting and interesting, and weâ€™re going to get a better result. But unfortunately, I think we have to remain here for one more lecture. And the reason for that is weâ€™ve already trained this mlp and we are getting pretty good loss and we have developed a pretty decent understanding of the architecture and how it works. But in train(), we are using loss.backward(), a line of code that we should take an issue with if we want to have a holistic and intuitive understanding of how gradients are actually being calculated and what exactly is going on here. That is, we have been using PyTorchâ€™s autograd and using it to calculate all of our gradients along the way. Here, what we will do is remove the use of loss.backward(), and we will write our backward pass manually, at the level of tensors.\n\n","type":"content","url":"/micrograduate/makemore4#intro","position":3},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Backprop back in the day"},"type":"lvl2","url":"/micrograduate/makemore4#backprop-back-in-the-day","position":4},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Backprop back in the day"},"content":"\n\nThis will prove to be a very useful exercise for the following reasons. Andrej has actually an entire blogpost on this topic: \n\nYes you should understand backprop, where he basically characterizes backprop as a leaky abstraction. And what I mean by that is that backprop doesnâ€™t just make your nns just work magically. Itâ€™s not the case that you can just stack up arbitrary lego blocks of differentiable functions and just cross your fingers, do backprop and everything is great. Things donâ€™t just work automatically. It is a leaky abstraction in the sense that you can shoot yourself in the foot if you do not understand its internals. It may magically not work or not work optimally. And you will need to understand how it works under the hood if youâ€™re hoping to debug an issue with it and if you are hoping to address it in your nn. So \n\nthe blog post goes into some of those examples. So for example, weâ€™ve already covered some of them already. For example, the flat tails of functions such as the sigmoid or tanh and how you do not want to saturate them too much because your gradients will die:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"sigmoid_derivative.jpeg\"))\n\n\n\nThereâ€™s the case of dead neurons, which weâ€™ve already covered as well:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"relu_derivative.jpeg\"))\n\n\n\nAlso, the case of exploding or vanishing gradients in the case of rnns, which we are about to cover:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"rnngradients.jpeg\"))\n\n\n\nAnd then also you will often come across some examples in the wild:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"dqnbadclipping.jpeg\"))\n\n\n\nThis is a snippet that Andrej found in a random code base on the internet where they actually have like a very subtle but pretty-major bug in their implementation. And the bug points at the fact that the author of this code does not actually understand backprop. So what theyâ€™re trying to do here is theyâ€™re trying to clip the loss at a certain maximum value. But actually what theyâ€™re trying to do is theyâ€™re trying to clip the gradients to have a maximum value instead of trying to clip the loss at a maximum value. And indirectly, theyâ€™re basically causing some of the outliers to be actually ignored. Because, when you clip the loss of an outlier, you are setting its gradient to 0. And so have a look through this and read through it. But thereâ€™s basically a bunch of subtle issues that youâ€™re going to avoid if you actually know what youâ€™re doing. And thatâ€™s why I donâ€™t think itâ€™s the case that because PyTorch or other frameworks offer autograd, it is okay for us to ignore how it works. Now, weâ€™ve actually already covered autograd and we wrote \n\nmicrograd. But \n\nmicrograd was an autograd engine only at the level of individual scalars. So the atoms were single individual numbers which I donâ€™t think is enough. And Iâ€™d like us to basically think about backprop at the level of tensors as well. And so in a summary, I think itâ€™s a good exercise. I think it is very, very valuable. Youâ€™re going to become better at debugging nns and making sure that you understand what youâ€™re doing. It is going to make everything fully explicit. So youâ€™re not going to be nervous about what is hidden away from you. Basically, weâ€™re going to emerge stronger! So letâ€™s get into it. A bit of a fun historical note here is that today, manually writing your backward pass by hand is not recommended and no one does it, except for the purposes of exercise and education. But about around 10+ years ago in deep learning, this was fairly standard and in fact pervasive.\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"backwardmemelol.png\"))\n\n\n\nSo at the time, everyone (including Andrej himself) used to write their backward pass manually, by hand. Now, itâ€™s our turn too! Now of course everyone just calls loss.backward(). But, weâ€™ve lost something. I want to give you a few examples of this. So,  hereâ€™s a 2006 paper from Geoffrey Hinton and Ruslan Salakhutdinov in Science that was influential at the time: \n\nReducing the Dimensionality of Data with Neural Networks. And this was training some architectures called restricted Boltzmann machines (rbms). And basically, itâ€™s an autoencoder trained here:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"rbms.jpeg\"))\n\n\n\nAnd this is from roughly 2010, when Andrej had written a MATLAB library for training (rbms) called \n\nmatrbm. And this was at the time when Python was not used for deep learning as pervasively as it is now. It was all MATLAB, which was this scientific computing package that everyone would use:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"matlab.png\"))\n\n\n\nSo we would write MATLAB, which is barely a programming language as well. But it had a very convenient tensor class. And it was this computing environment and you would run here. It would all run on the CPU, of course. But you would have very nice plots to go with it and a built-in debugger. And it was pretty decent. Now, the code in the 2010 \n\nmatrbm package that written for fitting rbms to a large extent is recognizable. Letâ€™s open up the \n\nrbmFit.m source code file and see if we can get a rough idea of what is going on. Well first Andrej creates the data and the x, y batches:\n\n...\n%Create targets: 1-of-k encodings for each discrete label\nu= unique(y);\ntargets= zeros(N, nclasses);\nfor i=1:length(u)\n    targets(y==u(i),i)=1;\nend\n\n%Create batches\nnumbatches= ceil(N/batchsize);\ngroups= repmat(1:numbatches, 1, batchsize);\ngroups= groups(1:N);\ngroups = groups(randperm(N));\nfor i=1:numbatches\n    batchdata{i}= X(groups==i,:);\n    batchtargets{i}= targets(groups==i,:);\nend\n...\n\nHeâ€™s initializing the nn so that itâ€™s got weights as and biases just like weâ€™re used to:\n\n...\n%fit RBM\nnumcases=N;\nnumdims=d;\nnumclasses= length(u);\nW = 0.1*randn(numdims,numhid);\nc = zeros(1,numdims);\nb = zeros(1,numhid);\nWc = 0.1*randn(numclasses,numhid);\ncc = zeros(1,numclasses);\nph = zeros(numcases,numhid);\nnh = zeros(numcases,numhid);\nphstates = zeros(numcases,numhid);\nnhstates = zeros(numcases,numhid);\nnegdata = zeros(numcases,numdims);\nnegdatastates = zeros(numcases,numdims);\nWinc  = zeros(numdims,numhid);\nbinc = zeros(1,numhid);\ncinc = zeros(1,numdims);\nWcinc = zeros(numclasses,numhid);\nccinc = zeros(1,numclasses);\nWavg = W;\nbavg = b;\ncavg = c;\nWcavg = Wc;\nccavg = cc;\nt = 1;\nerrors=zeros(1,maxepoch);\n...\n\nAnd then this is the training loop where we actually do the forward pass:\n\n...\nfor epoch = 1:maxepoch\n    \n\terrsum=0;\n    if (anneal)\n        penalty= oldpenalty - 0.9*epoch/maxepoch*oldpenalty;\n    end\n    \n    for batch = 1:numbatches\n\t\t[numcases numdims]=size(batchdata{batch});\n\t\tdata = batchdata{batch};\n\t\tclasses = batchtargets{batch};\n        \n        %go up\n        ph = logistic(data*W + classes*Wc + repmat(b,numcases,1));\n\t\tphstates = ph > rand(numcases,numhid);\n        if (isequal(method,'SML'))\n            if (epoch == 1 && batch == 1)\n                nhstates = phstates;\n            end\n        elseif (isequal(method,'CD'))\n            nhstates = phstates;\n        end\n...\n\nAnd then here, at this time, they didnâ€™t even necessarily use back propagation to train nns. So this, in particular, implements a lot of the training that weâ€™re doing. It implements contrastive divergence, which estimates a gradient:\n\n...\n        %go down\n\t\tnegdata = logistic(nhstates*W' + repmat(c,numcases,1));\n\t\tnegdatastates = negdata > rand(numcases,numdims);\n\t\tnegclasses = softmaxPmtk(nhstates*Wc' + repmat(cc,numcases,1));\n\t\tnegclassesstates = softmax_sample(negclasses);\n\t\t\n        %go up one more time\n\t\tnh = logistic(negdatastates*W + negclassesstates*Wc + ... \n            repmat(b,numcases,1));\n\t\tnhstates = nh > rand(numcases,numhid);\n...\n\nAnd then here, we take that gradient and use it for a parameter update along the lines that weâ€™re used to:\n\n...\n        %update weights and biases\n        dW = (data'*ph - negdatastates'*nh);\n        dc = sum(data) - sum(negdatastates);\n        db = sum(ph) - sum(nh);\n        dWc = (classes'*ph - negclassesstates'*nh);\n        dcc = sum(classes) - sum(negclassesstates);\n\t\tWinc = momentum*Winc + eta*(dW/numcases - penalty*W);\n\t\tbinc = momentum*binc + eta*(db/numcases);\n\t\tcinc = momentum*cinc + eta*(dc/numcases);\n\t\tWcinc = momentum*Wcinc + eta*(dWc/numcases - penalty*Wc);\n\t\tccinc = momentum*ccinc + eta*(dcc/numcases);\n\t\tW = W + Winc;\n\t\tb = b + binc;\n\t\tc = c + cinc;\n\t\tWc = Wc + Wcinc;\n\t\tcc = cc + ccinc;\n\t\t\n        if (epoch > avgstart)\n            %apply averaging\n\t\t\tWavg = Wavg - (1/t)*(Wavg - W);\n\t\t\tcavg = cavg - (1/t)*(cavg - c);\n\t\t\tbavg = bavg - (1/t)*(bavg - b);\n\t\t\tWcavg = Wcavg - (1/t)*(Wcavg - Wc);\n\t\t\tccavg = ccavg - (1/t)*(ccavg - cc);\n\t\t\tt = t+1;\n\t\telse\n\t\t\tWavg = W;\n\t\t\tbavg = b;\n\t\t\tcavg = c;\n\t\t\tWcavg = Wc;\n\t\t\tccavg = cc;\n        end\n        \n        %accumulate reconstruction error\n        err= sum(sum( (data-negdata).^2 ));\n        errsum = err + errsum;\n    end\n...\n\nBut you can see that basically people are meddling with these gradients directly and inline all by themselves. It wasnâ€™t that common to use an autograd engine. Hereâ€™s one more example from Andrejâ€™s paper from 2014 called \n\nDeep Fragment Embeddings for Bidirectional Image Sentence Mapping. Here, what he was doing is he was aligning images and text:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"karpathy2014fig2.jpeg\"))\n\n\n\nAnd so itâ€™s kind of like a clip if youâ€™re familiar with it, but instead of working at the level of entire images and entire sentences, he was working on the level of individual objects and little pieces of sentences and he was embedding them calculating a clip-like loss: the cost. Back in 2014, it was standard to implement not just the cost, but also the backward pass manually. Take a look at the DeFragCost function from \n\nthe paperâ€™s source code:\n\nfunction [cost_struct, grad, df_CNN] = DeFragCost(theta,decodeInfo,params, oWe,imgFeats,depTrees, rcnn_model)\n% returns cost, gradient, and gradient wrt image vectors which\n% can be forwarded to the CNN for finetuning outside of this\n\ndomil = getparam(params, 'domil', false);\nuseglobal = getparam(params, 'useglobal', true);\nuselocal = getparam(params, 'uselocal', true);\ngmargin = params.gmargin;\nlmargin = params.lmargin;\ngscale = params.gscale;\nlscale = params.lscale;\nthrglobalscore = params.thrglobalscore;\nsmoothnum = params.smoothnum;\nmaxaccum = params.maxaccum;\n\nfinetuneCNN = false;\ndf_CNN = 0;\n\n% unpack parameters\n[Wi2s, Wsem] = stack2param(theta, decodeInfo);\ncost = 0;\n\nN = length(depTrees); % number of samples\n\n% forward prop all image fragments and arrange them into a single large matrix\nimgVecsCell = cell(1, N);\nimgVecICell = cell(1, N);\nif finetuneCNN\n    % forward RCNN\n    imgVecs = andrej_forwardRCNN(imgFeats, rcnn_model, params);\n    for i=1:N\n        imgVecICell{i} = ones(size(imgFeats{i}.codes,1), 1)*i;\n    end\nelse\n    for i=1:N        \n        imgVecsCell{i} = imgFeats{i}.codes;\n    end\n    imgVecs = cat(1, imgVecsCell{:});\n    for i=1:N\n        imgVecICell{i} = ones(size(imgVecsCell{i},1), 1)*i;\n    end\nend\nimgVecI = cat(1, imgVecICell{:});\nallImgVecs = Wi2s * imgVecs'; % the mapped vectors are now columns\nNi = size(allImgVecs,2);\n\n% forward prop all sentences and arrange them\nsentVecsCell = cell(1, N);\nsentTriplesCell = cell(1, N);\nsentVecICell = cell(1, N);\nfor i = 1:N\n    [z, ts] = ForwardSent(depTrees{i},params,oWe,Wsem);\n    sentVecsCell{i} = z;\n    sentTriplesCell{i} = ts;\n    sentVecICell{i} = ones(size(z,2), 1)*i;\nend\nsentVecI = cat(1, sentVecICell{:});\nallSentVecs = cat(2, sentVecsCell{:});\nNs = size(allSentVecs, 2);\n\n% compute fragment scores\ndots = allImgVecs' * allSentVecs;\n\n% compute local objective\nif uselocal\n\n    MEQ = bsxfun(@eq, imgVecI, sentVecI'); % indicator array for what should be high and low\n    Y = -ones(size(MEQ));\n    Y(MEQ) = 1;\n\n    if domil\n\n        % miSVM formulation: we are minimizing over Y in the objective,\n        % what follows is a heuristic for it mentioned in miSVM paper.\n        fpos = dots .* MEQ - 9999  * (~MEQ); % simplifies things\n        Ypos = sign(fpos);\n\n        ixbad = find(~any(Ypos==1,1));\n        if ~isempty(ixbad)\n            [~, fmaxi] = max(fpos(:,ixbad), [], 1);\n            Ypos = Ypos + sparse(fmaxi, ixbad, 2, Ni, Ns); % flip from -1 to 1: add 2\n        end\n\n        Y(MEQ) = Ypos(MEQ); % augment Y in positive bags\n    end\n\n    % weighted fragment alignment objective\n    marg = max(0, lmargin - Y .* dots); % compute margins\n    W = zeros(Ni, Ns);\n    for i=1:Ns\n        ypos = Y(:,i)==1;\n        yneg = Y(:,i)==-1;\n        W(ypos, i) = 1/sum(ypos);\n        W(yneg, i) = 1/(sum(yneg));\n    end\n    wmarg = W .* marg;\n    lcost = lscale * sum(wmarg(:));\n    cost = cost + lcost;\nend\n\n% compute global objective\nif useglobal\n    \n    % forward scores in all regions\n    SG = zeros(N,N);\n    SGN = zeros(N,N); % the number of values (for mean)\n    accumsis = cell(N,N);\n    for i=1:N\n        for j=1:N\n            d = dots(imgVecI == i, sentVecI == j);\n            if thrglobalscore, d(d<0) = 0; end\n            if maxaccum\n                [sv, si] = max(d, [], 1); % score will be max (i.e. we're finding support of each fragment in image)\n                accumsis{i,j} = si; % remember switches for backprop\n                s = sum(sv);\n            else\n                s = sum(d(:)); % score is sum\n            end\n            nnorm = size(d,2); % number of sent fragments\n            nnorm = nnorm + smoothnum;\n            s = s/nnorm;\n            SG(i,j) = s;\n            SGN(i,j) = nnorm;\n        end\n    end\n    \n    % compute the cost\n    gcost = 0;\n    cdiffs = zeros(N,N);\n    rdiffs = zeros(N,N);\n    for i=1:N\n        % i is the pivot. It should have higher score than col and row\n        \n        % col term\n        cdiff = max(0, SG(:,i) - SG(i,i) + gmargin);\n        cdiff(i) = 0; % nvm score with self\n        cdiffs(:, i) = cdiff; % useful in backprop\n        \n        % row term\n        rdiff = max(0, SG(i,:) - SG(i,i) + gmargin);\n        rdiff(i) = 0;\n        rdiffs(i, :) = rdiff; % useful in backprop\n        \n        gcost = gcost + sum(cdiff) + sum(rdiff);\n    end\n    \n    gcost = gscale * gcost;\n    cost = cost + gcost;\nend\n\nltop = zeros(Ni, Ns);\n\nif uselocal\n    % backprop local objective\n    ltop = ltop - lscale * (marg > 0) .* Y .* W;\nend\n\nif useglobal\n    % backprop global objective\n    \n    % backprop margin\n    dsg = zeros(N,N);\n    for i=1:N\n        cd = cdiffs(:,i);\n        rd = rdiffs(i,:);\n        \n        % col term backprop\n        dsg(i,i) = dsg(i,i) - sum(cd > 0);\n        dsg(:,i) = dsg(:,i) + (cd > 0);\n        \n        % row term backprop\n        dsg(i,i) = dsg(i,i) - sum(rd > 0);\n        dsg(i,:) = dsg(i,:) + (rd > 0);\n    end\n    \n    % backprop into scores\n    ltopg = zeros(size(ltop));\n    for i=1:N\n        for j=1:N\n            \n            % backprop through the accumulation function\n            if maxaccum\n                % route the gradient along in each column. bit messy...\n                gradroute = dsg(i,j) / SGN(i,j);\n                mji = find(sentVecI == j);\n                mii = find(imgVecI == i);\n                accumsi = accumsis{i,j};\n                for q=1:length(mji)\n                    miy = mii(accumsi(q));\n                    mix = mji(q);\n                    if thrglobalscore\n                        if dots(miy,mix) > 0\n                            ltopg(miy, mix) = gradroute;\n                        end\n                    else\n                        ltopg(miy, mix) = gradroute;\n                    end\n                end\n            else\n                d = dots(imgVecI == i, sentVecI == j);\n                dd = ones(size(d)) * dsg(i,j) / SGN(i,j);\n                if thrglobalscore\n                    dd(d<0) = 0;\n                end\n                ltopg(imgVecI == i, sentVecI == j) = dd;\n            end\n        end\n    end\n    ltop = ltop + gscale * ltopg;\nend\n\n% backprop into fragment vectors\nallDeltasImg = allSentVecs * ltop';\nallDeltasSent = allImgVecs * ltop;\n\n% backprop image mapping\ndf_Wi2s = allDeltasImg * imgVecs;\n\nif finetuneCNN\n    % derivative wrt CNN data so that we can pass on gradient to RCNN\n    df_CNN = allDeltasImg' * Wi2s;\nend\n\n% backprop sentence mapping\ndf_Wsem = BackwardSents(depTrees,params,oWe,Wsem,sentVecsCell,allDeltasSent);\n\ncost_struct = struct();\ncost_struct.raw_cost = cost;\ncost_struct.reg_cost = params.regC/2 * sum(theta.^2);\ncost_struct.cost = cost_struct.raw_cost + cost_struct.reg_cost;\n\n%[grad,~] = param2stack(df_Wi2s, df_Wsem);\ngrad = [df_Wi2s(:); df_Wsem(:);]; % for speed hardcode param2stack\ngrad = grad + params.regC * theta; % regularization term gradient\nend\n\nAround 2014 it was standard to implement not just the cost but also the backward pass manually. So there he calculates the image embeddings sentence embeddings, sentence embeddings, the scores and the loss function. And after the loss function is calculated, he backwards through the nn and he appends a regularization term. So everything was done by hand manually, and you would just write out the backward pass. And then you would use a gradient checker to make sure that your numerical estimate of the gradient agrees with the one you calculated during backprop. This was very standard for a long time. Today, of course, it is standard to use an autograd engine. But it was definitely useful, and I think people sort of understood how these nns work on a very intuitive level. Therefore, itâ€™s still a good exercise.\n\n","type":"content","url":"/micrograduate/makemore4#backprop-back-in-the-day","position":5},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Manually implementing a backward pass"},"type":"lvl2","url":"/micrograduate/makemore4#manually-implementing-a-backward-pass","position":6},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Manually implementing a backward pass"},"content":"\n\nOkay, so just as a reminder that weâ€™re going to keep everything the same from our previous lecture. So weâ€™re still going to have a two-layer multi-layer perceptron with a batchnorm layer. So the forward pass will be basically identical to this lecture. But here, weâ€™re going to get rid of loss.backward(). And instead, weâ€™re going to write the backward pass manually. Apart from that, weâ€™re going to keep everything the same. So the forward pass will be basically identical to the previous lesson, etc. We are simply just going to write the backward pass manually. Hereâ€™s the starter code for this lecture:\n\nimport random\nrandom.seed(42)\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\nSEED = 2147483647\n\n\n\n# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {s: i + 1 for i, s in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: s for s, i in ctoi.items()}\nvocab_size = len(itoc)\nprint(itoc)\nprint(vocab_size)\n\n\n\nblock_size = 3\n\n\ndef build_dataset(words):\n    x, y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            x.append(context)\n            y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(x.shape, y.shape)\n    return x, y\n\n\n\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\nxtrain, ytrain = build_dataset(words[:n1])  # 80%\nxval, yval = build_dataset(words[n1:n2])  # 10%\nxtest, ytest = build_dataset(words[n2:])  # 10%\n\n\n\nNow, here, weâ€™ll introduce a utility function that weâ€™re going to use later to compare the gradients. So in particular, we are going to have the gradients that we estimate manually ourselves. And weâ€™re going to have gradients that PyTorch calculates. And weâ€™re going to be checking for correctness, assuming, of course, that PyTorch is correct.\n\n# utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(\n        f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\"\n    )\n\n\n\nThen, here, we have an initialization function that we are quite used to, where we create all the parameters:\n\ndef define_nn(\n    n_hidden=64,\n    n_embd=10,\n    w1_factor=5 / 3,\n    b1_factor=0.1,\n    w2_factor=0.1,\n    b2_factor=0.1,\n    bngain_factor=0.1,\n    bnbias_factor=0.1,\n):\n    global g, C, w1, b1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    input_size = n_embd * block_size\n    output_size = vocab_size\n    # Layer 1\n    w1 = (\n        torch.randn((input_size, n_hidden), generator=g)\n        * w1_factor\n        / ((input_size) ** 0.5)\n    )\n    b1 = (\n        torch.randn(n_hidden, generator=g) * b1_factor\n    )  # using b1 just for fun, it's useless because of batchnorm layer\n    # Layer 2\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    # BatchNorm parameters\n    bngain = torch.ones((1, n_hidden)) * bngain_factor + 1.0\n    bnbias = torch.zeros((1, n_hidden)) * bnbias_factor\n    # Note: We are initializating many of these parameters in non-standard ways\n    # because sometimes initializating with e.g. all zeros could mask an incorrect\n    # implementation of the backward pass.\n    parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return parameters\n\n\n\nNow, you will note that we changed the initialization a little bit to be small numbers. So normally you would set the biases to be all 0. Here, Iâ€™m setting them to be small random numbers. And Iâ€™m doing this because if your variables are all 0, or initialized to exactly 0, sometimes what can happen is that can mask an incorrect implementation of a gradient. Because when everything is 0, it sort of like simplifies and gives you a much simpler expression of the gradient than you would otherwise get. And so by making it small numbers, we are trying to unmask those potential errors in these calculations. You might also notice that we are using b1 in the first layer. We are using a bias despite batchnorm right afterwards. So this would typically not be what youâ€™d do as we talked about the fact that you donâ€™t need a bias. But we are doing this here just for fun, because we are going to have a gradient w.r.t. it and we can check that we are still calculating it correctly even though this bias is spurious.\n\nbatch_size = 32\n\n\ndef construct_minibatch():\n    ix = torch.randint(0, xtrain.shape[0], (batch_size,), generator=g)\n    xb, yb = xtrain[ix], ytrain[ix]\n    return xb, yb\n\n\n\nSo this function calculates a single batch. Then we will define a forward pass function:\n\ndef forward(xb, yb):\n    emb = C[xb]  # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n    # Linear layer 1\n    hprebn = embcat @ w1 + b1  # hidden layer pre-activation\n    # BatchNorm layer\n    bnmeani = 1 / batch_size * hprebn.sum(0, keepdim=True)\n    bndiff = hprebn - bnmeani\n    bndiff2 = bndiff**2\n    bnvar = (\n        1 / (batch_size - 1) * (bndiff2).sum(0, keepdim=True)\n    )  # note: Bessel's correction (dividing by n-1, not n)\n    bnvar_inv = (bnvar + 1e-5) ** -0.5\n    bnraw = bndiff * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # Non-linearity\n    h = torch.tanh(hpreact)\n    # Linear layer 2\n    logits = h @ w2 + b2  # output layer\n    # cross entropy loss (same as F.cross_entropy(logits, Yb))\n    logit_maxes = logits.max(1, keepdim=True).values\n    norm_logits = logits - logit_maxes  # subtract max for numerical stability\n    counts = norm_logits.exp()\n    counts_sum = counts.sum(1, keepdims=True)\n    counts_sum_inv = counts_sum**-1\n    probs = counts * counts_sum_inv\n    logprobs = probs.log()\n    globals().update(locals())  # make variables defined so far globally-accessible\n    loss = -logprobs[range(batch_size), yb].mean()\n    intermediate_values = [\n        logprobs,\n        probs,\n        counts,\n        counts_sum,\n        counts_sum_inv,\n        norm_logits,\n        logit_maxes,\n        logits,\n        h,\n        hpreact,\n        bnraw,\n        bnvar_inv,\n        bnvar,\n        bndiff2,\n        bndiff,\n        hprebn,\n        bnmeani,\n        embcat,\n        emb,\n    ]\n    return intermediate_values, loss\n\n\n\ndef backward(parameters, fp_intermed_values, loss):\n    # PyTorch backward pass\n    for p in parameters:\n        p.grad = None\n    for t in fp_intermed_values:\n        t.retain_grad()\n    loss.backward()\n\n\n\nThis forward() is now significantly expanded compared to what we are used to. The reason our forward pass is longer is for two reasons. Number one, before we just had an f.cross_entropy(), but here we are bringing back an explicit implementation of the loss function. And number two, we have broken up the implementation into manageable chunks, so we have a lot more intermediate tensors along the way in the forward pass. And thatâ€™s because we are about to go backwards and calculate the gradients in this backprop, from the bottom (loss) to the top (..., emb, C, xb). So weâ€™re going to go upwards and just like we have for example the logprobs in this forward pass, in the backward pass we are going to have a dlogprobs which is going to store the derivative of the loss w.r.t. the logprobs tensor (\\dfrac{\\partial **loss**}{\\partial logprobs}). And so in our manual implementation of backprop, we are going to be prepending d to every one of these tensors and calculating it along the way of this backprop. As an example, we have  a bnraw here, so weâ€™re going to be calculating a dbnraw (\\dfrac{\\partial **loss**}{\\partial bnraw}). Also, in the backward() function, before the loss.backward() call, we are calling t.retain_grad() and thus telling PyTorch that we want to retain the graph of all of these intermediate values. This is because:\n\nin exercise 1 (coming up) we are going to calculate the backward pass, so weâ€™re going to calculate all these d variables and use the cmp function weâ€™ve introduced above to check our correctness w.r.t. what PyTorch is telling us. This is going to be exercise 1, where we will backprop through this entire graph (starting from the loss and going upward through the intermediate values). Then,\n\nin exercise 2 we will fully break up the loss and backprop through it manually in all the little atomic pieces that make it up. But then weâ€™re going to collapse the loss into a single cross entropy call and instead weâ€™re going to analytically derive using math and paper and pencil the gradient of the loss w.r.t. the logits and instead of backproping through all of its little chunks, one at a time, weâ€™re just going to analytically derive what that gradient is and weâ€™re going to implement that, which is much more efficient as weâ€™ll see in a bit. Next,\n\nfor exercise 3 weâ€™re going to do the exact same thing for batchnorm. Weâ€™re going to use pen and paper and calculus to derive the gradient through the batchnorm layer. So weâ€™re going to calculate the backward pass through the batchnorm layer in a much more efficient expression, instead of backprop through all of its little pieces independently. And then,\n\nin exercise 4 weâ€™re going to put it all together and this is the full code of training this two layer mlp and weâ€™re going to basically insert our manual backprop and weâ€™re going to take out loss.backward() and you will basically see that we can get all the same results using fully our own code and the only thing weâ€™re going to be using from PyTorch is torch.tensor to make the calculations efficient. But otherwise, you will understand fully what it means to forward and backward through the nn and train it and I think thatâ€™ll be awesome so letâ€™s get to it.\n\nOkay, letâ€™s now define our nn and do one forward and one backward pass on it:\n\nparameters = define_nn()\nxb, yb = construct_minibatch()\nfp_intermed_values, loss = forward(xb, yb)\nbackward(parameters, fp_intermed_values, loss)\n\n\n\n","type":"content","url":"/micrograduate/makemore4#manually-implementing-a-backward-pass","position":7},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 1: implementing the backward pass"},"type":"lvl2","url":"/micrograduate/makemore4#exercise-1-implementing-the-backward-pass","position":8},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 1: implementing the backward pass"},"content":"\n\nNow, exercise 1: implementing the backward pass:\n\nWeâ€™ll start with dlogprobs, which basically means we are going to start with calculating the gradient of the loss w.r.t. all the elements of logprobs tensor: \\dfrac{\\partial \\textbf{loss}}{\\partial \\textbf{logprobs}}. So, dlogprobs will have the same shape as logprobs:\n\nlogprobs.shape\n\n\n\nNow, how does logprobs influence the loss? Like this: -logprobs[range(batch_size), yb].mean(). Just as a reminder, yb is only just an array of the correct indeces:\n\nyb\n\n\n\nAnd so by doing logprobs[range(batch_size), yb], what we are essentially doing is for each row i of the logprobs tensor (of size batch_size), we are plucking out the element at column index yb[i]. For example, from row 0 we would be getting the element from column yb[0] == 1, from row 1 we would be getting the element from column yb[1] == 12, and so on. Therefore, the elements inside the logprobs[range(batch_size), yb] could be alternatively calculated as such:logprobslist = [logprobs[i, yb[i]] for i in range(batch_size)]\n\nHere is the proof by assertion that logprobslist is the same as logprobs:\n\nlogprobslist = [logprobs[i, yb[i]].item() for i in range(batch_size)]\nassert logprobs[range(batch_size), yb].tolist() == logprobslist\n\n\n\nSo these elements get plucked out from logprobs and then the negative of their mean becomes the loss. It is always nice to work with simpler examples in order to understand the numerical form of derivatives. Whatâ€™s going on here is once weâ€™ve plucked out these examples, weâ€™re taking the mean and then the negative (-logprobs[range(batch_size), yb].mean()). So the loss basically, to write it in a simpler way, is the negative of the mean of e.g. 3 values:\n\nloss = -(a + b + c) / 3\nloss = -a/3 - b/3 - c/3\n\nThat would be how we achieve the mean of three numbers a, b, c (although we actually have 32 numbers here). But therefore the derivative of this toy loss w.r.t. a would basically be just:\n\ndloss/da = -1/3\n\nSo you can see that if we donâ€™t just have 3 numbers (a, b, c), but we have 32 numbers, then dloss by dk (where k is every one of those 32 numbers) is going to be: 1/32 or just \\frac{1}{batch\\_size} more generally. Now, what about the other elements inside logprobs? Remember, logprobs is a large array with a logprobs.shape of 32 \\times 27 (864 numbers in total) and only:\n\nlogprobs[range(batch_size), yb].shape\n\n\n\n32 of these numbers participate in the loss calculation. So, what about the derivative of all the other 832 numbers of the logprobs tensor, meaning the rest of the elements that do not get plucked out? Well, their gradient intuitively is 0. Simply because they do not participate in the loss calculation. They are left out. So, since most of these numbers inside the logprobs tensor does not feed into the loss, if we were to change these numbers, then the value of loss wouldnâ€™t change. Which is the equivalent way of saying that the derivative of the loss w.r.t. them is zero. They donâ€™t impact it. So hereâ€™s a way to implement this derivative then: we create a tensor of zeros exactly in the shape of logprobs and then fill in the derivative value -1/batch_size inside exactly these indeces: [range(batch_size), yb]. Therefore:\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(batch_size), yb] = -1.0 / batch_size\n\n\n\nThen this is the candidate derivative for dlogprobs. Letâ€™s call our assertion function to compare with the actual derivative computed by PyTorch and check whether our manually calculated derivative is correct:\n\ncmp(\"logprobs\", dlogprobs, logprobs)\n\n\n\nCool! So letâ€™s look at the definition of the cmp function:def cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n\nWhat it does is that it basically receives the calculated value provided by us, which is dt and then checks whether it is exactly equal to t.grad as calculated by PyTorch. This makes sure that all of the elements are exactly equal. After that, it calculates for approximate equality because of floating point issues using torch.allclose which has a little bit of a wiggle available because sometimes you can get very, very close. But if you use a slightly different calculation, because of floating point, you canâ€™t get very, very close. Generally, because of floating point arithmetic, you can get a slightly different result. So this second call checks for an approximately equal result. And then it finds the maximum, basically the value that has the highest difference, and what is the difference, and the absolute value difference between those two. At the end it prints whether we have an exact equality, an approximate equality, and what is the largest difference. In our case, we have an exact equality. And so therefore, of course, we also have an approximate equality, and the maximum difference is exactly zero. So basically, our dlogprobs is exactly equal to what PyTorch calculated to be logprobs.grad in its backpropagation. So far, weâ€™re doing pretty well. Okay, so letâ€™s now continue our manual backprop. We know that logprobs depends on probs through a .log() call. So a log is applied to all the elements of probs, element-wise. Now, if we want dprobs: \\dfrac{\\partial loss}{\\partial probs} = \\dfrac{\\partial loss}{\\partial logprobs}\\cdot\\dfrac{\\partial logprobs}{\\partial probs}, then, well letâ€™s \n\nask WolframAlpha what the derivative of log(x) is w.r.t. x: \\dfrac{d(log(x))}{dx} = \\dfrac{1}{x}. Therefore we write out the derivative and because we are doing backprop and thus applying the chain rule, we want to chain it to the previous result:\n\ndprobs = (1.0 / probs) * dlogprobs\ncmp(\"probs\", dprobs, probs)\n\n\n\nNow, notice what happens if your probs is very very close to 1.0, that means that the nn is predicting the character correctly, then dprobs = (1.0 / 1.0) * dlogprobs = dlogprobs and thus dlogprobs just gets passed through. But if probs are very low, then the fraction part that is multiplied by dlogprobs will be boosted instead of dlogprobs. So what this line is doing intuitively is it is boosting the the gradient of the examples that have a very low probability currently assigned. Next up is counts_sum_inv. So we want dcounts_sum_inv (\\dfrac{\\partial loss}{\\partial counts\\_sum\\_inv}). But letâ€™s just pause for a moment and explain what is happening until the calculation of counts_sum_inv, in case all this is a bit confusing:\n\n...\n# Linear layer 2\nlogits = h @ w2 + b2  # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1\nprobs = counts * counts_sum_inv\n...\n\nWe have the logits that come out of the nn. Then we are finding the maximum in each row that we then subtract from the logits for purposes of numerical stability. Then, because some of the logits may take on too large values we end up exponentiating (this is done just for safety, numerically). This exponentiation yields our counts. And then, we take the sum of these counts. Using this counts_sum, we normalize the counts so that all of the probs sum to 1. But basically, all thatâ€™s happening here is we got the logits, we want to exponentiate all of them, and we want to normalize the counts to create our probabilities, itâ€™s just that we have defined this set of intermediate calculations across multiple lines. Ok, so what should be dcounts_sum_inv? Now, we actually have to be careful here because we have to scrutinize and be careful with the shapes. So, counts.shape and counts_sum_inv.shape are different:\n\nprint(counts.shape, counts_sum_inv.shape)\nassert counts.shape != counts_sum_inv.shape\n\n\n\nSo during the multiplication probs = counts * counts_sum_inv, there is an implicit broadcasting that PyTorch does. Because what it needs to do is take one column of 32 \\times 1 that is counts_sum_inv and replicate it horizontally 27 times in order to align these two tensors so it can do an element-wise multiplication. Using a toy example, this is what this looks like:\n\n# c = a * b, but with tensors:\n# a[3x3] * b[3,1] --->\n# a11*b1 a12*b1 a13*b1\n# a21*b2 a22*b2 a23*b2\n# a31*b3 a32*b3 a33*b3\n# c[3x3]\n\n\n\nIn this example, in order for a[3\\times3] to multiply b[3\\times1], we have to replicate 3 times the vertical column [b1, b2, b3]^T horizontally. And then do the actual multiplication. And this is exactly what PyTorch does in this situation. Meaning, in order to do c = a \\cdot b (c[3\\times3] = a[3\\times3] \\cdot b[3\\times1]) it:\n\nfirst replicates b across the column dimension (1): b \\rightarrow b\\_replicated (b[3\\times1] \\rightarrow b[3\\times3])\n\nthen it does the multiplication c = a \\cdot b\\_replicated (c[3\\times3] = a[3\\times3] \\cdot b[3\\times3])\n\nTherefore, if we wanted to find \\dfrac{\\partial c}{\\partial a}, then that would equal be a\\_replicated (and not a). What the replication operation means in terms of the computational graph is that a in this case is being fed to multiple outputs. Thus, as we discussed in micrograd (lesson 1), during the backward pass, the correct thing to do in this case is to sum all the gradients that arrive at any one node  (i.e. a). So if a node is used multiple times, the gradients for all of its uses sum during backprop. This means that since e.g. b1 is used multiple times in each column of c, therefore the right thing to do here would be to sum horizontally across all the columns:da = a_replicated = a.sum(1, keepdim=True)\n\nEquivalently, probs = counts * counts_sum_inv, is broken up into to operations as well:\n\nfirst, the replication: counts\\_sum\\_inv \\rightarrow counts\\_sum\\_inv\\_replicated (counts\\_sum\\_inv[32\\times1] \\rightarrow counts\\_sum\\_inv[32\\times27])\n\nthen, the multiplication: probs = counts \\cdot counts\\_sum\\_inv\\_replicated (probs[32\\times27] = counts[32\\times27] \\cdot counts\\_sum\\_inv[32\\times27])\n\nAnd therefore dcounts_sum_inv (\\dfrac{\\partial loss}{\\partial counts\\_sum\\_inv} = \\dfrac{\\partial loss}{\\partial probs} \\dfrac{\\partial probs}{\\partial counts\\_sum\\_inv}):\n\ndcounts_sum_inv = (counts * dprobs).sum(\n    1, keepdim=True\n)  # multiply by `dprobs` because of chain rule and keepdim=True to keep the column dimension\nassert dcounts_sum_inv.shape == counts_sum_inv.shape  # shapes are equals\ncmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n\n\n\nAgain, this derivative is exactly correct. Letâ€™s also backprop into counts. Similarly, dcounts is:\n\ndcounts = counts_sum_inv * dprobs\nassert dcounts.shape == counts.shape\n\n\n\nwith no additional summation required here since there is a broadcasting going on that will yield a derivative with a shape that is equivalent to the shape of the counts tensor. But we canâ€™t call cmp and check the correctness of dcounts just yet. The reason is that besides being used to calculate probs, it is also being used in another branch to calculate counts_sum_inv. So, even though we have calculated the first contribution of it: \\dfrac{\\partial loss}{\\partial probs}\\cdot\\dfrac{\\partial probs}{\\partial counts}, we still have to calculate the second contribution of it: \\dfrac{\\partial loss}{\\partial counts\\_sum}\\cdot\\dfrac{\\partial counts\\_sum}{\\partial counts}. To continue with this branch, we now want to calculate dcounts_sum: \\dfrac{\\partial loss}{\\partial counts\\_sum} = \\dfrac{\\partial counts\\_sum\\_inv}{\\partial counts\\_sum}\\cdot\\dfrac{\\partial loss}{\\partial counts\\_sum\\_inv}. With the \n\nhelp of WolframAlpha:\n\ndcounts_sum = (-(counts_sum**-2)) * dcounts_sum_inv\ncmp(\"counts_sum\", dcounts_sum, counts_sum)\n\n\n\nCool. Next, moving along, we want to backprop through the preceding operation: counts_sum = counts.sum(1, keepdims=True). This means we want to now calculate the additional dcounts component: \\dfrac{\\partial loss}{\\partial counts} = \\dfrac{\\partial loss}{\\partial counts\\_sum}\\cdot\\dfrac{\\partial counts \\_sum}{\\partial counts}. If we remember from micrograd, the sum operation basically acts just like a router, allowing the gradient (dcounts_sum, in our case) to distribute equally to all the input during the backward flow. In other words, to use a toy example:\n\n# b = sum(a), but with tensors:\n# b[3,1] = a[3x3].sum(1) --->\n# a11 a12 a13 ---> b1 (= a11 + a12 + a13)\n# a21 a22 a23 ---> b2 (= a21 + a22 + a23)\n# a31 a32 a33 ---> b3 (= a31 + a32 + a33)\n\n\n\nTherefore considering that a and b are multi-dimensional tensors: \\dfrac{\\partial b_{3\\times1}}{\\partial a_{3\\times3}} = \\dfrac{\\partial{\\sum a_{3\\times3}}}{\\partial a_{3\\times3}} = \\dfrac{\\partial \\textbf{1}_{3\\times3}^T a_{3\\times3}}{\\partial a_{3\\times3}} = \\textbf{1}_{3\\times3}. Therefore, similarly, \\dfrac{\\partial counts\\_sum}{\\partial counts} = \\textbf{1}, where the matrix \\textbf{1} has the shape of counts. Therefore, we can now add the calculated second component \\dfrac{\\partial counts\\_sum}{\\partial counts}\\dfrac{\\partial loss}{\\partial counts\\_sum} to dcounts. And since there is no extra component that makes up dcounts we can do:\n\ndcounts += torch.ones_like(counts) * dcounts_sum\n\n\n\nAnd now dcounts is equal to: \\dfrac{\\partial loss}{\\partial counts} = \\dfrac{\\partial loss}{\\partial probs}\\cdot\\dfrac{\\partial probs}{\\partial counts} + \\dfrac{\\partial loss}{\\partial counts\\_sum}\\cdot\\dfrac{\\partial counts\\_sum}{\\partial counts}. And now, we can check for correctness:\n\ncmp(\"counts\", dcounts, counts)\n\n\n\nAwesome! The match is exact, which of course means that once again we have correctly calculated the gradient of yet-another node, and our smooth manual backproping journey continues! So hopefully we should be getting a hang of this now. Now, dnorm_logits is easy:\n\ndnorm_logits = counts * dcounts\ncmp(\"norm_logits\", dnorm_logits, norm_logits)\n\n\n\nsince \\dfrac{\\partial loss}{\\partial norm\\_logits} = \\dfrac{\\partial counts}{\\partial norm\\_logits}\\cdot\\dfrac{\\partial loss}{\\partial counts} = \\dfrac{\\partial e^{norm\\_logits}}{\\partial norm\\_logits}\\cdot\\dfrac{\\partial loss}{\\partial counts} = e^{norm\\_logits}\\cdot\\dfrac{\\partial loss}{\\partial counts} = counts\\cdot\\dfrac{\\partial loss}{\\partial counts}. Letâ€™s continue. Next up is norm_logits = logits - logit_maxes. So now we care about finding dlogits and dlogit_maxes. We have to be careful here again as the shapes are not the same:\n\nnorm_logits.shape, logits.shape, logit_maxes.shape\n\n\n\nThis means that when the subtraction happens there is going to be an implicit broadcasting happening:\n\n# c = a - b, but with tensors: a[3x3], b[3x1], c[3x3]\n# c11 c12 c13 = a11 a12 a13   b1\n# c21 c22 c23 = a21 a22 a23 - b2\n# c31 c32 c33 = a31 a32 a33   b3\n\n# so e.g. c32 = a32 - b3\n\n\n\nFor calculating dlogits we care about finding the (first) component \\dfrac{\\partial norm\\_logits}{\\partial logits}\\dfrac{\\partial loss}{\\partial norm\\_logits} = \\dfrac{\\partial loss}{\\partial norm\\_logits}. And therefore:\n\ndlogits = dnorm_logits.clone()\n\n\n\nNow, like for dcounts previously, we cannot yet check for the correctness of dlogits because there is still a second component that should be added to the first one to make up the complete dlogits gradient \\dfrac{\\partial loss}{\\partial logits}. Before we calculate that letâ€™s first find dlogit_maxes: \\dfrac{\\partial loss}{\\partial logit\\_maxes} = \\dfrac{\\partial norm\\_logits}{\\partial logit\\_maxes}\\dfrac{\\partial loss}{\\partial norm\\_logits} = -\\dfrac{\\partial loss}{\\partial norm\\_logits}, which essentially means: dlogit_maxes = -dnorm_logits. Now remember, because we are dealing with tensors and logit_maxes has a shape of 32\\times1, that means that -dnorm_logits, whose shape is 32\\times27, must be summed along the column axis in order for dlogits_maxes to become a 32\\times1 tensor (the same shape as logit_maxes). Therefore:\n\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ncmp(\"logit_maxes\", dlogit_maxes, logit_maxes)\n\n\n\nNext up is: logit_maxes = logits.max(1, keepdim=True).values. Now, this is the line out of which we will find the second derivative component of dlogits: \\dfrac{\\partial logit\\_maxes}{\\partial logits}\\cdot\\dfrac{\\partial loss}{\\partial logit\\_maxes}. But what is \\dfrac{\\partial logit\\_maxes}{\\partial logits} lossin this case? Before we answer this question, letâ€™s pause here briefly and look at these logit_maxes, and especially their gradients. Weâ€™ve talked previously in the previous lesson that the only reason weâ€™re doing this is for the numerical stability of the softmax that we are implementing here. And we talked about how if you take these logits for any one of these examples, so one row of this logits tensor, if you add or subtract any value equally to all the elements, then the value of the probs will be unchanged. Youâ€™re not changing the softmax. The only thing that this is doing is itâ€™s making sure that the subsequent exp() operation doesnâ€™t overflow. And the reason weâ€™re using a max is because then we are guaranteed that each row of logits, the highest number, is 0. Basically, that has repercussions. If it is the case that changing logit_maxes does not change the probs, and therefore does not change the loss, then the gradient on logit_maxes should be 0. Are they?\n\ndlogit_maxes\n\n\n\nNot exactly. But regardless, these are very very tiny floating point numbers that are close to 0. And so this is telling us that the values of logit_maxes are not impacting the loss, as they should not. It feels kind of weird to backprop through this branch, honestly, because if you have any implementation of f.cross_entropy in PyTorch, and you block together all of these elements, and youâ€™re not doing backprop piece by piece, then you would probably assume that the derivative through here is exactly 0. So you would be sort of skipping this branch. Because itâ€™s only done for numerical stability. But itâ€™s interesting to see that even if you break up everything into the full atoms, and you still do the computation as youâ€™d like w.r.t. numerical stability, the correct thing happens. And you still get very, very small gradients here. Basically reflecting the fact that the values of these do not matter w.r.t. the final loss. Okay, so letâ€™s now continue backprop through this line here. Weâ€™ve just calculated the logit_maxes, and now we want to backprop into logits through this second branch. Here we took the max along all the rows and then looked at its values logits.max(1, keepdim=True).values. In PyTorch, max returns both the max values but also the indices of those values:\n\nlogits.max(1, keepdim=True)\n\n\n\nAlthough we just keep the values, in the backward pass, it is extremely useful to know about where those maximum values occured. And these indices will help us with the backprop. Again, we ask, what is \\dfrac{\\partial logit\\_maxes}{\\partial logits} in this case? We have the logits whose shape is:\n\nlogits.shape\n\n\n\nand in each row we find the maximum value. And then that value gets plucked out into logit_maxes. And so intuitively, the derivative we are looking for (that is flowing through) 1 (for the appropriate entry that was plucked out) times dlogit_maxes. So if you think about it, what we want here is we need to take the dlogit_maxes tensor and scatter it into the correct positions in logits, meaning the indices of the max values. One way to do this is create a tensor with 1s at the indices we care about and 0s elsewhere and multiply this tensor by dlogit_maxes.\n\nones_maxes_indeces = F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])\nplt.imshow(ones_maxes_indeces)\nplt.show();\n\n\n\nThis is basically what we want. And array of 1s at one position (the index where the maxes came from) in each row of our logits tensor. Therefore, letâ€™s now add the second derivative component to dlogits and check for correctness.\n\ndlogits += ones_maxes_indeces * dlogit_maxes\ncmp(\"logits\", dlogits, logits)\n\n\n\nNext up: logits = h @ w2 + b2. Letâ€™s look at the shapes of all the intermediate tensors:\n\nlogits.shape, h.shape, w2.shape, b2.shape\n\n\n\nNow letâ€™s see the shape of h @ w2:\n\n(h @ w2).shape\n\n\n\nThis tells us that since the shape of b2 is 27, the h @ w2 + b2 will first broadcast the b2 tensor into a shape of 1\\times27 and then it will be replicated vertically along the columns (across the 0 dimension) in order to give rise to a b2_replicated with a shape of 32\\times27. But the question now is how do we backprop from logits to the hidden states h, the weight matrix w2 and the bias matrix b2? Now you might think that we need to go to some matrix calculus textbook and look up the derivative for matrix multiplication. But actually first principles suffice here to derive this yourself on a piece of paper. Specifically, what works great in these types of situations is to find a specific small example that you then fully write out and then in the process of analyzing how that individual small example works out, you will understand a broader pattern and youâ€™ll be able to generalize and write out the full general formula for how derivatives flow in an expression like this one. So letâ€™s try that out:\n\n$d = a \\cdot b + c\\\\begin{bmatrix}\nd_{11} & d_{12} \\\\\nd_{21} & d_{22}\n\\end{bmatrix}\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\\begin{bmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{bmatrix}\\begin{bmatrix}\nc_{1} & c_{2}\n\\end{bmatrix}\n\nd_{11} = a_{11}b_{11} + a_{12}b_{21} + c_1 \\\nd_{12} = a_{11}b_{12} + a_{12}b_{22} + c_2 \\\nd_{21} = a_{21}b_{11} + a_{22}b_{21} + c_1 \\\nd_{22} = a_{21}b_{12} + a_{22}b_{22} + c_2\n$\n\nwhere\n\\begin{bmatrix}\nc_{1} & c_{2}\n\\end{bmatrix}\ngets implicitly replicated vertically into a\n\\begin{bmatrix}\nc_{1} & c_{2} \\\\\nc_{1} & c_{2}\n\\end{bmatrix}\nin order for the addition to be a valid one. This is something that PyTorch also does. So as we know dlogits, by analogy, assume we also know dd. And since we are looking for dh, dw2 and db2, in this simple example, by analogy, we are looking for da, db, dc. Letâ€™s write them out by hand:\n\n$\n\\dfrac{\\partial L}{\\partial a} =\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial a_{11}} & \\dfrac{\\partial L}{\\partial a_{12}} \\\\\n\\dfrac{\\partial L}{\\partial a_{21}} & \\dfrac{\\partial L}{\\partial a_{22}}\n\\end{bmatrix}\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial d_{11}}b_{11} + \\dfrac{\\partial L}{\\partial d_{12}}b_{12} & \\dfrac{\\partial L}{\\partial d_{11}}b_{21} + \\dfrac{\\partial L}{\\partial d_{12}}b_{22} \\\\\n\\dfrac{\\partial L}{\\partial d_{21}}b_{11} + \\dfrac{\\partial L}{\\partial d_{22}}b_{12} & \\dfrac{\\partial L}{\\partial d_{21}}b_{21} + \\dfrac{\\partial L}{\\partial d_{22}}b_{12}\n\\end{bmatrix}\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial d_{11}} & \\dfrac{\\partial L}{\\partial d_{12}} \\\\\n\\dfrac{\\partial L}{\\partial d_{21}} & \\dfrac{\\partial L}{\\partial d_{22}}\n\\end{bmatrix}\\begin{bmatrix}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{bmatrix}\n\n\\dfrac{\\partial L}{\\partial d} \\cdot b^T\n$\n\n$\\dfrac{\\partial L}{\\partial b} =\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial b_{11}} & \\dfrac{\\partial L}{\\partial b_{12}} \\\\\n\\dfrac{\\partial L}{\\partial b_{21}} & \\dfrac{\\partial L}{\\partial b_{22}}\n\\end{bmatrix}\\begin{bmatrix}\na_{11}\\dfrac{\\partial L}{\\partial d_{11}} + a_{21}\\dfrac{\\partial L}{\\partial d_{21}} & a_{11}\\dfrac{\\partial L}{\\partial d_{12}} + a_{21}\\dfrac{\\partial L}{\\partial d_{22}} \\\\\na_{12}\\dfrac{\\partial L}{\\partial d_{11}} + a_{22}\\dfrac{\\partial L}{\\partial d_{21}} & a_{12}\\dfrac{\\partial L}{\\partial d_{12}} + a_{22}\\dfrac{\\partial L}{\\partial d_{22}}\n\\end{bmatrix}\\begin{bmatrix}\na_{11} & a_{21} \\\\\na_{12} & a_{22}\n\\end{bmatrix}\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial d_{11}} & \\dfrac{\\partial L}{\\partial d_{12}} \\\\\n\\dfrac{\\partial L}{\\partial d_{21}} & \\dfrac{\\partial L}{\\partial d_{22}}\n\\end{bmatrix}\n\na^T \\cdot \\dfrac{\\partial L}{\\partial d}$\n\n$\\dfrac{\\partial L}{\\partial c} =\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial c_{1}} & \\dfrac{\\partial L}{\\partial c_{2}}\n\\end{bmatrix}\\begin{bmatrix}\n\\dfrac{\\partial L}{\\partial c_{1}} & \\dfrac{\\partial L}{\\partial c_{2}} \\\\\n\\end{bmatrix}\\begin{bmatrix}\n1\\dfrac{\\partial L}{\\partial d_{11}} + 1\\dfrac{\\partial L}{\\partial d_{21}} & 1\\dfrac{\\partial L}{\\partial d_{12}} + 1\\dfrac{\\partial L}{\\partial d_{22}}\n\\end{bmatrix}\n\n(\\dfrac{\\partial L}{\\partial d}).sum(0)$\n\nAfter calculating these derivatives, long story short, the backward pass of a matrix multiply is a matrix multiply. Therefore, just like we had d = a * b + c (scalar case), so does d = a @ b + c (matrix case) lead us to something very very similar but now with a matrix multiplication instead of a scalar multiplication. In both cases: da = dd @ b.T and db = a.T @ dd we have matrix multiplication with both dd and da or db terms involved, whereas dc is simply a sum: dc = dd.sum(0). Now hereâ€™s a dirty little secret: you donâ€™t need to remember the formulas we just derived for backpropagating through matrix multiplication. You can backprop through these expressions just fine. And the reason this works is because the dimensions have to work out. Letâ€™s see an example. Consider h @ w2 + b2. What is dh? The shape of dh must be the same as the shape of h:\n\nh.shape\n\n\n\nAnd then the other piece of information we now know is that dh must be some kind of matrix multiplication of dlogits with w2. Letâ€™s see the shapes:\n\ndlogits.shape, h.shape, w2.shape, b2.shape\n\n\n\nSo since dlogits is 32\\times27, w2 is 64\\times27 and h is 32\\times64 there is only a single way to make the shape work out in this case. Namely, the only way to achieve dh with size 32\\times64 (the size of h) is to matrix multiply dlogits with the transpose of w2 (in order to make the dimensions work out):\n\ndh = dlogits @ w2.T\n\n\n\nAnd it is the only way to make these to matrix multiply those two pieces to make the shapes work out. And that turns out to be the correct formula, since, by analogy, as we saw in our simple example: da = dd @ b.T. So thereâ€™s no real need to remember these formulas. Similarly, knowing that each parameter derivative must end up with the target shape of the parameter:\n\ndw2 = h.T @ dlogits  # 64x32 @ 32x27 -> 64x27\ndb2 = dlogits.sum(0, keepdim=True)  # 32x27 -> 27\n\n\n\nSo thatâ€™s the intuitive, hacky way of findings matrix parameter derivatives. Letâ€™s check if we got dh, dw2 and db2 correct:\n\ncmp(\"h\", dh, h)\ncmp(\"w2\", dw2, w2)\ncmp(\"b2\", db2, b2)\n\n\n\nExactly! Nice. We have backproped through a linear layer. Woohoo! Next up for h = torch.tanh(hpreact), we already have dh and we now need to backpropagate through tanh into hpreact, which means we are looking for dhpreact. If you remember, we have already done something similar in \n\nmicrograd and we remember that \n\ntanh has a very simple derivative:\n\ndhpreact = (1.0 - h**2) * dh\ncmp(\"hpreact\", dhpreact, hpreact)\n\n\n\nHere, we notice that there only an approximate (and not an exact) equality between the the PyTorch gradient values and our own. This can be attributed to tiny floating point imprecision or perhaps a bug that has to do with specific library versions. Since the maxdiff is negligible, we can ignore the slight difference and safely assume that they are indeed exactly equal and make the assignment (in order to avoid any potential accumulation of slight differences and further exact inequalities):\n\ndhpreact = hpreact.grad\ncmp(\"hpreact\", dhpreact, hpreact)\n\n\n\nOkay, next up we have dhpreact and now we want to backprop through the bngain, the bnraw and the bnbias (hpreact = bngain * bnraw + bnbias). So here these are the batchnorm parameters: bngain and bnbias  that take the bnraw that is exact unit Gaussian and they scale it and shift it. Here, we have a multiplication (*), but itâ€™s worth noting that this multiply is very very different from the matrix multiply @ (e.g. in the logits = h @ w2 + b2 we saw previously). matrix multiply means dot products between rows and columns of the involved matrices (e.g. h and w2). Whereas, * is an element wise multiply so things are quite a bit simpler. Yet, still, we do have to be careful with some of the broadcasting happening in this line of code though: hpreact = bngain * bnraw + bnbias.\n\nhpreact.shape, bngain.shape, bnraw.shape, bnbias.shape\n\n\n\nYou see how bngain and bnbias are 1\\times64 but hpreact and bnraw are 32\\times64. We have to be careful with that and make sure that all the shapes work out fine and that the broadcasting is correctly backpropagated. Letâ€™s start with calculating dbngain. Using the chain rule:\n\ndbngain = bnraw * dhpreact\n\n\n\nBut this is incorrect. We have to be careful, since bngain is of size 1\\times64, which is the size dbngain has to have. However, bnraw * dhpreact has a size of 32x64. So the correct thing to do in this case of course is to sum across the examples/rows dimension while being careful to keep the number of dimensions the same:\n\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n\n\n\nAnd now the dimensions are correct! Similarly:\n\ndbnraw = bngain * dhpreact  # replication occurs automatically, no summing required\ndbnbias = dhpreact.sum(0, keepdim=True)  # sum to reduce the batch size dimension\n\n\n\nLetâ€™s check for correctness:\n\ncmp(\"bngain\", dbngain, bngain)\ncmp(\"bnraw\", dbnraw, bnraw)\ncmp(\"bnbias\", dbnbias, bnbias)\n\n\n\nAwesome! Now we get to the batchnorm layer. We see how here bngain and bnbias are parameters, so the backpropagation ends. But bnraw = bndiff * bnvar_inv is the output of the standardization:bnmeani = 1 / batch_size * hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(batch_size - 1) * (bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\n\nwith an equivalence to the \n\nbatchnorm paper assignments:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"batchnorm_algorithm1.png\"))\n\n\n\nSo, next up, on the line bnraw = bndiff * bnvar_inv we have to backpropagate into bndiff and bnvar_inv, having already calculated dbnraw. Letâ€™s see the shapes:\n\nbnraw.shape, bndiff.shape, bnvar_inv.shape\n\n\n\nWe can infer that there is broadcasting of bnvar_inv happening here that we have to be careful with. Other than that, this is just an element-wise multiplication. By now we should be pretty comfortable with that. Therefore:\n\ndbndiff = bnvar_inv * dbnraw\n# we sum since dbnvar_inv must have the same shape as `bnvar_inv`\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n\n\n\nAre these correct? Letâ€™s see.\n\ncmp(\"bnvar_inv\", dbnvar_inv, bnvar_inv)\ncmp(\"bndiff\", dbndiff, bndiff)\n\n\n\ndbnvar_inv is correct. But, oh no! dbndiff isnâ€™t... Well, this is actually expected, because we are not yet done with bndiff, since it not only contributes to bnraw but also indirectly to bnvar_inv (through bndiff2 and so on). So basically, bndiff branches out into two branches of which we have only backpropagated through one of them. So we have to continue our backprop until we get to the second branch of bndiff, where we can calculate and add the second partial derivative component to dbndiff using a += (like we have done before for previous variables). Letâ€™s do so! Next up is: bnvar_inv = (bnvar + 1e-5)**-0.5. \n\nThis is just:\n\ndbnvar = (-0.5 * ((bnvar + 1e-5) ** -1.5) * 1) * dbnvar_inv\ncmp(\"bnvar\", dbnvar, bnvar)\n\n\n\nwhich is the correct result! Now before we move on here, letâ€™s talk a bit about Besselâ€™s correction from: bnvar = 1/(batch_size - 1) * (bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n). Youâ€™ll notice that dividing by n-1 instead of n is a departure from the paper (where m == n == batch_size). So it turns out that there are two ways of estimating variance of an array. One is the biased estimate, which is 1/n and the other one is the unbiased estimate which is 1/n-1. Now confusingly in the paper itâ€™s not very clearly described and also itâ€™s a detail that kind of matters. They essentially use the biased version at training time but later when they are talking about the inference, they are mentioning that when they do the inference they are using the unbiased estimate (which is 1/n-1 version) to calibrate the running mean and the running variance basically. And so they actually introduce a train test mismatch where in training they use the biased version and in test time they use the unbiased version. This of course is very confusing. You can read more about the Besselâ€™s correction: \n\nhttps://â€‹mathcenterâ€‹.oxfordâ€‹.emoryâ€‹.eduâ€‹/siteâ€‹/math117â€‹/besselCorrection/ and why dividing by n-1 gives you a better estimate of the variance in the case where you have population sizes or samples from a population that are very small. And that is indeed the case for us because we are dealing with mini-matches and these mini-matches are a small sample of a larger population which is the entire training set. And it turns out that if you just estimate it using 1/n that actually almost always underestimates the variance and it is a biased estimator and it is advised that you use the unbiased version and divide by n-1. The documentation of \n\ntorch.var and \n\ntorch.nn.BatchNorm1d are less confusing. So, long story short, since PyTorch uses Besselâ€™s correction, we will too. Ok, so letâ€™s now backprop through the next line: bnvar = 1/(batch_size - 1) * (bndiff2).sum(0, keepdim=True). As you might have realized until now, it is good practice to scrutinize the shapes first.\n\nbnvar.shape, bndiff2.shape\n\n\n\nTherefore the sum operation in this line is squashing the first dimension 32 into 1. This hints to us that will be some kind of replication or broadcasting in the backward pass. And maybe youâ€™re noticing a pattern here: everytime you have a sum in the forward pass, that turns into a replication or broadcasting in the backward pass along the same dimension. And conversely, when we have a replication or broadcasting in the forward pass, that indicates a variable reuse and so in the backward pass that turns into a sum over the exact same dimension. And so we are noticing a duality, where these two operations are kind of like the opposites of each other in the forward and the backward pass. Now, usually once we have understood the shapes, the next thing to look at is a toy example in order to roughly understand how the variable dependencies go in the mathematical formula. In the line we are interested in we have an array (bndiff2) on which we are summing vertically over the columns ((bndiff2).sum(0, keepdim=True)) and that we are scaling (by 1/(batch_size - 1)). So if we have a 2\\times2 matrix a that we then sum over the columns and then scale, we would get:\n\n# a11 a12\n# a21 a22\n# -> \n# b1 b2, where:\n# b1 = 1/(n-1)*(a11 + a21)\n# b2 = 1/(n-1)*(a12 + a22)\n\n\n\nLooking at this simple example, what we want basically is we want to backpro the derivative of b, db1 and db2 into all the elements of a. And so itâ€™s clear that, by just differentiating in your head, the local derivatives of b are simply (1/(n-1))*1 for each one of these elements of a. Basically, each local derivative will flow through each column of a and be scaled by 1/(n-1). Therefore, intuitively:\n\n# basically a large array of ones the size of bndiff2 times dbnvar (chain rule), scaled\ndbndiff2 = (1 / (batch_size - 1)) * torch.ones_like(bndiff2) * dbnvar\n\n\n\nNotice here how we are multiplying a scaled ones array of shape 32x64 with dbnvar, an array of 1\\times64. Basically we are just letting PyTorch do the replication, so that we end up with a dbndiff2 array of 32\\times64 (same size as bndiff2). And indeed we see that this derivative is correct:\n\ncmp(\"bndiff2\", dbndiff2, bndiff2)\n\n\n\nNext up, letâ€™s differentiate here: bndiff2 = bndiff**2 into bndiff. This is a simple one, but donâ€™t forget the addition assignment, as this will now be adding the second partial derivative component to the first one we already calculated before. So this completes the backpropagation through the second branch of bndiff:\n\ndbndiff += (2 * bndiff) * dbndiff2\ncmp(\"bndiff\", dbndiff, bndiff)\n\n\n\nAs you can see now, the derivative of bndiff is now exact and correct. Thatâ€™s comforting! Next up: bndiff = hprebn - bnmeani. Letâ€™s check the shapes:\n\nbndiff.shape, hprebn.shape, bnmeani.shape\n\n\n\nTherefore, the minus sign here (-) is actually doing broadcasting. Letâ€™s be careful about that as this hints us to the duality we previously mentioned. A broadcasting in the forward pass means variable reuse and therefore translates to a sum operation in the backward pass. Letâ€™s find the derivatives:\n\ndhprebn = 1 * dbndiff\ndbnmeani = (-1 * dbndiff).sum(0, keepdim=True)\ncmp(\"hprebn\", dhprebn, hprebn)\ncmp(\"bnmeani\", dbnmeani, bnmeani)\n\n\n\nAnd dhprebn is wrong... Damn! Haha, well, donâ€™t sweat it. This is supposed to be wrong! Can you guess why? Like happened for bndiff, this is also not the only branch we have to backpropagate to for calculating dhprebn, since bnmeani also depends on hprebn. Therefore, there will be a second derivative component coming from this second branch. So we are not done yet. This we will find now since the next line is: bnmeani = 1 / batch_size * hprebn.sum(0, keepdim=True). Now here again we have to be careful since there is a sum along dimension 0 so this will turn into broadcasting in the backward pass. So, similarly to the bnvar line and the dbndiff2 calculation:\n\ndhprebn += (1.0 / batch_size) * torch.ones_like(hprebn) * dbnmeani\ncmp(\"hprebn\", dhprebn, hprebn)\n\n\n\nAwesome! So that completes the backprop of the batchnorm layer. Next up we will backpropagate through the linear layer: hprebn = embcat @ w1 + b1. Like we have already mentioned, backpropagating through linear layers is pretty easy. So letâ€™s inspect the shapes and begin.\n\nhprebn.shape, embcat.shape, w1.shape, b1.shape\n\n\n\nNow we find dembcat to be 32x30. We need to take w1 of size 30x64 (the tensor that is matrix-multiplied by embcat) and matrix-multiply it with dhprebn (think chain rule). Therefore, we need to do:\n\ndembcat = dhprebn @ w1.T\ncmp(\"embcat\", dembcat, embcat)\n\n\n\nAnd similarly for the remaining variables:\n\ndw1 = embcat.T @ dhprebn\ndb1 = (1 * dhprebn).sum(0)\ncmp(\"w1\", dw1, w1)\ncmp(\"b1\", db1, b1)\n\n\n\nCool, everything is correct! Moving onto embcat = emb.view(emb.shape[0], -1):\n\nembcat.shape, emb.shape\n\n\n\nAs you can see, the view operation squashes the last two dimensions of the emb tensor (3, 10) into one dimension (30). So here we are dealing with a concatenation of dimensions in the forward pass, therefore we want to undo that in the backward pass. We can do this again using the view operation:\n\ndemb = dembcat.view(emb.shape)\ncmp(\"emb\", demb, emb)\n\n\n\nSimple, right? Now the only operation that is left to backprop into is the initial indexing operation: emb = C[xb]. Letâ€™s look at the shapes:\n\nemb.shape, C.shape, xb.shape\n\n\n\nSo what is happening in this indexing operation? xb (32 examples, 3 character indeces per example) is being used to index the lookup table C (27 possible character indeces, 10 embedding dimensions per character index) to yield emb (32 examples, 3 character indeces per example, 10 embedding dimensions per character index). In other words, by indexing C using xb we are using each integer in xb to specify which row of C we want to pluck out. This indexing operation yields an embeddings tensor emb of 32 examples (same as the indexing tensor xb), 3 plucked-out character rows per example (from C), each of which contains 10 embedding dimesions. So now, for each one of these plucked-out rows we have their gradients demb (arranged in a 32\\times3\\times10 tensor). All we have to do now is to route this gradients backwards through this indexing operation. So first we need to find the specific rows of C that every one of these 10-dimensional embeddings comes from. And then, into the corresponding dC row indices we need to deposit the demb gradients. Basically we need to undo the indexing. And of course, if any of these rows of C was used multiple times, then we have to remember that the gradients that arrive there have to add (accumulate through an addition operation). The PyTorch operation that will help us do this is \n\nindex_add_.\n\n# Define `dC` tensor (same size as `C`)\ndC = torch.zeros_like(C)\n# Plucked-out `C` row indices tensor\nindices = xb.flatten()  # shape: 32 * 3 = 96, range of index values: 0-26\n# `demb` derivative values tensor\nvalues = demb.view(-1, 10)  # shape: (32 * 3)x10 = 96x10\n# Add `demb` values to specific indices\ndC.index_add_(0, indices, values)\ncmp(\"C\", dC, C)\n\n\n\nMake sure to take some time to understand what we just did. Hereâ€™s a bare-minimum example (produced by \n\nChatGPT) of what the index_add_ operation does exactly, in case youâ€™re confused:\n\nbase_tensor = torch.zeros(5)  # tensor to add values into\nprint(\"Base tensor before index_add_ operation:\", base_tensor)\nindices = torch.tensor([0, 1, 3])  # indices where we want to add values\nvalues = torch.tensor([1.0, 2.0, 3.0])  # values to add at those indices\nbase_tensor.index_add_(0, indices, values)  # add values at specified indices\nprint(\"Base tensor after index_add_ operation:\", base_tensor)\n\n\n\nWhat remains now is... Well, nothing. Yey :D! We have backpropagated through this entire beast. This was our first exercise. Letâ€™s clearly write out all the gradients we manually calculated, along with their correctness calls:\n\n# Exercise 1: backprop through the whole thing manually,\n# backpropagating through exactly all of the variables\n# as they are defined in the forward pass above, one by one\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(batch_size), yb] = -1.0 / batch_size\ndprobs = (1.0 / probs) * dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-(counts_sum**-2)) * dcounts_sum_inv\ndcounts += torch.ones_like(counts) * dcounts_sum\ndnorm_logits = counts * dcounts\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\ndh = dlogits @ w2.T\ndw2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\n# FIXME: PyTorch version-specific precision issue\ndhpreact = hpreact.grad  # correct: dhpreact = (1.0 - h**2) * dh\ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\ndbndiff2 = (1.0 / (batch_size - 1)) * torch.ones_like(bndiff2) * dbnvar\ndbndiff += (2 * bndiff) * dbndiff2\ndhprebn = dbndiff.clone()\ndbnmeani = (-dbndiff).sum(0)\ndhprebn += 1.0 / batch_size * (torch.ones_like(hprebn) * dbnmeani)\ndembcat = dhprebn @ w1.T\ndw1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = dembcat.view(emb.shape)\ndC = torch.zeros_like(C)\ndC.index_add_(0, xb.flatten(), demb.view(-1, 10))\n\ncmp(\"logprobs\", dlogprobs, logprobs)\ncmp(\"probs\", dprobs, probs)\ncmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\ncmp(\"counts_sum\", dcounts_sum, counts_sum)\ncmp(\"counts\", dcounts, counts)\ncmp(\"norm_logits\", dnorm_logits, norm_logits)\ncmp(\"logit_maxes\", dlogit_maxes, logit_maxes)\ncmp(\"logits\", dlogits, logits)\ncmp(\"h\", dh, h)\ncmp(\"w2\", dw2, w2)\ncmp(\"b2\", db2, b2)\ncmp(\"hpreact\", dhpreact, hpreact)\ncmp(\"bngain\", dbngain, bngain)\ncmp(\"bnbias\", dbnbias, bnbias)\ncmp(\"bnraw\", dbnraw, bnraw)\ncmp(\"bnvar_inv\", dbnvar_inv, bnvar_inv)\ncmp(\"bnvar\", dbnvar, bnvar)\ncmp(\"bndiff2\", dbndiff2, bndiff2)\ncmp(\"bndiff\", dbndiff, bndiff)\ncmp(\"bnmeani\", dbnmeani, bnmeani)\ncmp(\"hprebn\", dhprebn, hprebn)\ncmp(\"embcat\", dembcat, embcat)\ncmp(\"w1\", dw1, w1)\ncmp(\"b1\", db1, b1)\ncmp(\"emb\", demb, emb)\ncmp(\"C\", dC, C)\n\n\n\n","type":"content","url":"/micrograduate/makemore4#exercise-1-implementing-the-backward-pass","position":9},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 2: backprop through cross_entropy but all in one go"},"type":"lvl2","url":"/micrograduate/makemore4#exercise-2-backprop-through-cross-entropy-but-all-in-one-go","position":10},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 2: backprop through cross_entropy but all in one go"},"content":"\n\nNow we come to exercise 2. It basically turns out that in this first exercise we were doing way too much work. We were backpropagating way too much and it was all good practice and so on but itâ€™s not what you would do in practice.\nThe reason for that is, for example at some point we separated out a loss calculation over multiple lines and broke it up all to its smallest atomic pieces and we backpropagated through all of those individually. But it turns out that if you just look at the mathematical expression for the loss then actually you can do the differentiation on pen and paper and a lot of terms cancel and simplify and the mathematical expression you end up with is significantly shorter and easier to implement than backpropagating through all the little pieces of everything youâ€™ve done. So before we had this complicated forward pass going from logits to the loss:\n\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(batch_size), yb].mean()\n\nBut in PyTorch everything can just be glued together into a single call at that cross entropy. You just pass in logits and the labels and you get the exact same loss as we verify here:\n\n# Exercise 2: backprop through cross_entropy but all in one go\n# to complete this challenge look at the mathematical expression of the loss,\n# take the derivative, simplify the expression, and just write it out\n\nloss_fast = F.cross_entropy(logits, yb)\nprint(loss_fast.item(), \"diff:\", (loss_fast - loss).item())\n\n\n\nSo our previous loss and the fast loss coming from the chunk of operations as a single mathematical expression is much faster than the whole forward pass we did previously. Itâ€™s also much much faster in the backward pass. And the reason for that is if you just look at the mathematical form of F.cross_entropy(logits, yb) and differentiate again you will end up with a very small and short expression. So thatâ€™s what we want to do here. In a single operation or in a single go or like very quickly, we want to go directly into dlogits and we need to implement dlogits as a function of logits and yb. But it will be significantly shorter that going through all these intermediate operations to get to it:\n\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(batch_size), yb] = -1.0 / batch_size\ndprobs = (1.0 / probs) * dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\ndcounts += torch.ones_like(counts) * dcounts_sum\ndnorm_logits = counts * dcounts\ndlogits = dnorm_logits.clone()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n\nBasically all of this above work can be skipped in a much much simpler mathematical expression that we will now implement. Letâ€™s give it a shot. The question we are asking is: what is the mathematical expression of the loss? Once, we have the answer we can differentiate w.r.t the logits. Hereâ€™s whatâ€™s happening: inputs go through the nn which yields logits. If these are passed through a softmax, we get the probabilities:\n\nx \\rightarrow nn \\rightarrow logits \\rightarrow softmax \\rightarrow probs\n\nThen, we are using the indentity y of the correct next character to pluck out a row of probabilities probs[y] and then take the negative log to get the negative log probability: -log(probs[y]). And then we average up all the negative log probabilities to get our loss.\n\nprobs[y] \\rightarrow -log(probs[y]) \\rightarrow average\\ \\textbf{loss}\n\nBasically, given a logit vector l, a softmax/probability vector P and a target label y:\n\nloss = -\\log \\mathbf{P}_y = -\\log \\left(\\frac{e^{l_y}}{\\sum_j e^{l_j}}\\right)\n\nAnd we are basically interested in:\n\n\\nabla_{l_i} loss = \\dfrac{\\partial loss}{\\partial l_i} = \\dfrac{\\partial}{\\partial l_i} \\left[-\\log \\left(\\dfrac{e^{l_y}}{\\sum_j e^{l_j}}\\right)\\right]\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\dfrac{\\partial}{\\partial l_i} \\left(\\dfrac{e^{l_y}}{\\sum_j e^{l_j}}\\right) (using \\dfrac{d\\log x}{dx} = \\dfrac{1}{x})\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\dfrac{\\partial}{\\partial l_i} \\left(e^{l_y} \\dfrac{1}{\\sum_j e^{l_j}}\\right)\n\nif i \\neq y:\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\left(\\dfrac{\\partial e^{l_y}}{\\partial l_i} \\cdot \\dfrac{1}{\\sum_j e^{l_j}} +e^{l_y} \\cdot \\dfrac{\\partial}{\\partial l_i} (\\dfrac{1}{\\sum_j e^{l_j}}) \\right)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\left(0 \\cdot \\dfrac{1}{\\sum_j e^{l_j}} +e^{l_y} \\cdot \\dfrac{\\partial}{\\partial l_i} (\\dfrac{1}{\\sum_j e^{l_j}}) \\right)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\left(-\\dfrac{e^{l_y} e^{l_i}}{(\\sum_j e^{l_j})^2} \\right)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{e^{l_i}}{\\sum_j e^{l_j}}\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = P_i\n\nif i = y:\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\left[\\dfrac{\\partial e^{l_y}}{\\partial l_i} \\cdot \\dfrac{1}{\\sum_j e^{l_j}} +e^{l_y} \\cdot \\dfrac{\\partial}{\\partial l_i} (\\dfrac{1}{\\sum_j e^{l_j}}) \\right]\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\sum_j e^{l_j}}{e^{l_y}} \\left[\\dfrac{e^{l_y}}{\\sum_j e^{l_j}} - \\dfrac{e^{l_y} e^{l_i}}{(\\sum_j e^{l_j})^2} \\right]\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -1 + \\dfrac{e^{l_i}}{\\sum_j e^{l_j}}\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = P_i - 1\n\nSweet! By separating the casing of whether the ith index of the logits is equal or not equal to the label y, and using basic rules of calculus such as the product rule and the power rule, we have derived the mathematical expression for the loss analytically. What we found is that for the first case we get P_i and for the other case P_i - 1. And that is the form that the gradient \\nabla_{l_i} loss takes analytically. Now itâ€™s time to implement it. Keep in mind, that we derived the analytical loss only for a single example, but here we will be working with batches of examples. Thus the loss for a batch will be the average loss over all the examples, meaning the sum of all the individual losses per example, divided by the number of examples. And so we have to backpropagate through that operation as well and be careful with it:\n\n# backward pass\ndlogits = F.softmax(logits, 1)  # apply softmax along the rows\ndlogits[\n    range(batch_size), yb\n] -= 1  # at the correct/target positions (where i == y) subtract 1\ndlogits /= batch_size  # divide by batch_size since the loss is also (during averaging across examples)\ncmp(\"logits\", dlogits, logits)  # we can only get approximate correctness ~6e-9\n\n\n\nAs you can see, we get an approximately-correct but not an exactly-correct answer, with a slight deviation of ~6e-9. This is due to floating point wonkiness, but this is basically the correct answer, approximately. Now, before we move on to the next exercise, letâ€™s pause for a little bit in order to gain an intuitive sense of what dlogits is, because it has a beautiful and surprisingly simple explanation honestly.\n\nplt.imshow(dlogits.detach(), cmap=\"gray\")\nplt.show();\n\n\n\nHere, we are visualizing the dlogits tensor that contains 32 examples and 27 characters per example. What is dlogits intuitively? dlogits is the softmax/probabilities matrix of the forward pass! The black squares are the positions of the correct indices where we subtracted a 1. So what does this mean? Letâ€™s look at the values of the first row:\n\nF.softmax(logits, 1)[0]\n\n\n\nSo these are the probabilities of the first row. Whereas:\n\ndlogits[0] * batch_size\n\n\n\ngives us the derivatives. Which is exactly equal to the probabilities, except that the value at the position of the correct index which has the probability value p - 1! Also if we sum dlogits of this row, we find out they sum to 0 (or at least very close to it):\n\ndlogits[0].sum()\n\n\n\nWe can think of the gradients of each row here as a force that pulls down on the probabilities of the incorrect characters and pulls up the probability at the correct (black dotted) index. And thatâ€™s whatâ€™s basically happening in each row. And the amount of push and pull is exactly equalized because the sum is 0. So the amount to which we pull down in the probabilities and the amount that we push up on the probability of the correct character is equal. So the repulsion and the attraction are equal. So you can think of the nn now as a massive pulley system, or something like that, in which at the level of dlogits weâ€™re pulling down the probabilities of incorrect and pulling up the probability of the correct ones. And in this complicated pulley system, we think of it as sort of like this tension translating to this complicating pulley mechanism and then eventually we get a tug on the weights and the biases. And basically in each update we just kind of like tug in the direction that weâ€™d like for each of these elements and the parameters are slowly given in to the tug and thatâ€™s what training the nn kind of looks like on a high level. And so the forces of push and pull in these gradients (e.g. see the values of dlogits[0] * batch_size) are actually very intuitive here weâ€™re pushing and pulling on the correct answer and the amount of force that weâ€™re applying is actually proportional to the probabilities that came out in the forward pass. And so for example if our probabilities came out exactly correct, so they would have had zero everywhere except for one at the correct position then the dlogits would be all a row of zeros for that example: there would be no push and pull. So the amount to which your prediction is incorrect is exactly the amount by which youâ€™re going to get a pull or a push in that dimension. So if you have for example a very confidently mispredicted element, then whatâ€™s going to happen is that element is going to be pulled down very heavily and the correct answer is going to be pulled up to the same amount and the other characters are not going to be influenced too much. So the amount to which you mispredict is then proportional to the strength of the pull and thatâ€™s happening independently in all the dimensions of this tensor and itâ€™s sort of very intuitive and very easy to think through and thatâ€™s basically the magic of the cross entropy loss and what itâ€™s doing dynamically in the backward pass of the nn.\n\n","type":"content","url":"/micrograduate/makemore4#exercise-2-backprop-through-cross-entropy-but-all-in-one-go","position":11},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 3: backprop through batchnorm but all in one go"},"type":"lvl2","url":"/micrograduate/makemore4#exercise-3-backprop-through-batchnorm-but-all-in-one-go","position":12},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 3: backprop through batchnorm but all in one go"},"content":"\n\nWe now get to exercise number three, which is a very fun exercise depending on your definition of fun. We are going to do for batchnorm exactly what we did for cross entropy loss in exercise 2. That is that is we are going to consider it as a glued single mathematical expression and backprop through it in a very efficient manner because we are going to derive a much simpler formula for the backward pass of batchnorm and weâ€™re going to do that using pen and paper. So previously weâ€™ve broken up batchnorm into all of the little intermediate pieces and all the atomic operations inside it and then we backpropagated through it one by one. Now, we just have a single forward pass and itâ€™s all glued together:\n\n# Exercise 3: backprop through batchnorm but all in one go\n# to complete this challenge look at the mathematical expression of the output of batchnorm,\n# take the derivative w.r.t. its input, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n# bndiff2 = bndiff**2\n# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n# bnvar_inv = (bnvar + 1e-5)**-0.5\n# bnraw = bndiff * bnvar_inv\n# hpreact = bngain * bnraw + bnbias\n\n# now:\nhpreact_fast = (\n    bngain\n    * (hprebn - hprebn.mean(0, keepdim=True))\n    / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5)\n    + bnbias\n)\nprint(\"max diff:\", (hpreact_fast - hpreact).abs().max())\n\n\n\nAnd we see that we get the exact same result as before: an almost-zero difference. Now for the backward pass, weâ€™d like to also implement a single formula basically for backpropagating through this entire operation that is the batchnorm. So in the forward pass previously we took hprebn, the hidden states of the pre batch normalization and created hpreact which is the hidden states just before the activation. In the batchnorm paper:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"batchnorm_algorithm1.png\"))\n\n\n\nhprebn is x and hpreact is y. So in the backward pass what weâ€™d like to do now is we have dhpreact and weâ€™d like to produce dhprebn and weâ€™d like to do that in a very efficient manner. So thatâ€™s the name of the game. Calculate dhprebn given dhpreact and for the purposes of this exercise, weâ€™re going to ignore gamma and beta and their derivatives, because they take on a very simple form in a very similar way to what we did up above. So letâ€™s calculate dhprebn (\\dfrac{\\partial L}{\\partial x_i}) given dhpreact (\\dfrac{\\partial L}{\\partial y_i}) (i.e. backprop through the batchnorm). According to Algorithm 1 above (from the paper):       ____Î¼______(3)_______\n      â•±     â•²               â”‚     \n    (4)      â•²              â”‚\n    â•±         â•²             â†“  \n  [x]---â†’ÏƒÂ²---(2)---â†’[xÌ‚]---(1)-----â†’[y]\n    â•²                       â†‘       â•±â”‚\n     â•²______________________â”‚      Î³ Î²        \n\nthe bracketed variables represent vectors and the numbering represents the order in which we are going to backprop, going from (1) (right) all the way to (4) (left). Therefore, given \\dfrac{\\partial L}{\\partial y_i} we want to calculate \\dfrac{\\partial L}{\\partial x_i}:\n\n1. \\dfrac{\\partial L}{\\partial \\hat{x}_i} = \\gamma \\dfrac{\\partial L}{\\partial y_i}, easy enough :)\n\n2. \\dfrac{\\partial L}{\\partial \\sigma^2} = \\sum_i \\dfrac{\\partial L}{\\partial \\hat{x}_i} \\dfrac{\\partial \\hat{x}_i}{\\partial \\sigma^2}, we sum the product of the local and global derivative because \\sigma^2 feeds into each \\hat{x}_i of the \\hat{x} vector (so basically there are m (batch_size, e.g. 32) number of arrows feeding from \\hat{x} back into \\sigma^2)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\gamma \\sum_i \\dfrac{\\partial L}{\\partial y_i} \\dfrac{\\partial}{\\partial \\sigma^2} \\left[ ( x_i - \\mu ) (\\sigma^2 + \\epsilon)^{-1/2} \\right]\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\dfrac{\\gamma}{2} \\sum_i \\dfrac{\\partial L}{\\partial y_i} ( x_i - \\mu ) (\\sigma^2 + \\epsilon)^{-3/2}\n\n3. \\dfrac{\\partial L}{\\partial \\mu} = \\sum_i \\dfrac{\\partial L}{\\partial \\hat{x}_i} \\dfrac{\\partial \\hat{x}_i}{\\partial \\mu} + \\dfrac{\\partial L}{\\partial \\sigma^2} \\dfrac{\\partial \\sigma^2}{\\partial \\mu}, here we also add a second component because as you can see above there are two branches emerging from \\mu, one into \\sigma^2 and one into \\hat{x}. To keep it simple, letâ€™s solve each partial derivative separately:\n\n\\dfrac{\\partial \\hat{x}_i}{\\partial \\mu} = \\dfrac{\\partial}{\\partial \\mu} \\left[ ( x_i - \\mu )(\\sigma^2 + \\epsilon)^{-1/2} \\right] = -(\\sigma^2 + \\epsilon)^{-1/2}\n\n\\dfrac{\\partial \\sigma^2}{\\partial \\mu} = \\dfrac{\\partial}{\\partial \\mu}\\left[\\dfrac{1}{m - 1}\\sum_{i=1}^{m} (x_i - \\mu)^2 \\right], notice \\dfrac{1}{m - 1} which is Besselâ€™s correction\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{-2}{m - 1}\\sum_{i=1}^{m} (x_i - \\mu)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{-2}{m - 1}(\\sum_{i=1}^{m}x_i - \\sum_{i=1}^{m}\\mu)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\dfrac{-2}{m - 1}(m\\mu - m\\mu)\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = 0, WHOA :)!\n\nTherefore: \\dfrac{\\partial L}{\\partial \\mu} = \\sum_i \\dfrac{\\partial L}{\\partial \\hat{x}_i} [-(\\sigma^2 + \\epsilon)^{-1/2}] + \\dfrac{\\partial L}{\\partial \\sigma^2} \\cdot 0\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -\\gamma\\sum_i\\dfrac{\\partial L}{\\partial y_i} (\\sigma^2 + \\epsilon)^{-1/2}\n\n4. \\dfrac{\\partial L}{\\partial x_i} = \\dfrac{\\partial L}{\\partial \\hat{x}_i}\\dfrac{\\partial \\hat{x}_i}{\\partial x_i} + \\dfrac{\\partial L}{\\partial \\mu}\\dfrac{\\partial \\mu}{\\partial x_i} + \\dfrac{\\partial L}{\\partial \\sigma^2}\\dfrac{\\partial \\sigma^2}{\\partial x_i}, three components (partial derivative products), one for every path exiting reaching x from L (one through \\hat{x}, one through \\mu and one through \\sigma^2). Similarly, letâ€™s solve each partial derivative that we do not yet know separately:\n\n\\dfrac{\\partial \\hat{x}_i}{\\partial x_i} = \\dfrac{\\partial}{\\partial x_i} \\left[ ( x_i - \\mu )(\\sigma^2 + \\epsilon)^{-1/2} \\right] = (\\sigma^2 + \\epsilon)^{-1/2}\n\n\\dfrac{\\partial \\mu}{\\partial x_i} = \\dfrac{\\partial}{\\partial x_i}\\left(\\dfrac{1}{m}\\sum_j x_j\\right) = \\dfrac{1}{m}\n\n\\dfrac{\\partial \\sigma^2}{\\partial x_i} = \\dfrac{\\partial }{\\partial x_i}\\left[\\dfrac{1}{m - 1}\\sum_j (x_j - \\mu)^2 \\right] = \\dfrac{2}{m - 1} (x_i - \\mu)\n\nNow that we have all the individual derivatives, letâ€™s find the three derivative products, one by one:\n\n\\dfrac{\\partial L}{\\partial \\hat{x}_i}\\dfrac{\\partial \\hat{x}_i}{\\partial x_i} = (\\sigma^2 + \\epsilon)^{-1/2} \\gamma \\dfrac{\\partial L}{\\partial y_i}\n\n\\dfrac{\\partial L}{\\partial \\mu}\\dfrac{\\partial \\mu}{\\partial x_i} = -\\gamma\\sum_j\\dfrac{\\partial L}{\\partial y_j} (\\sigma^2 + \\epsilon)^{-1/2} \\dfrac{1}{m}\n\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -(\\sigma^2 + \\epsilon)^{-1/2}\\dfrac{\\gamma}{m}\\sum_j\\dfrac{\\partial L}{\\partial y_j}\n\n\\dfrac{\\partial L}{\\partial \\sigma^2}\\dfrac{\\partial \\sigma^2}{\\partial x_i} = \\left[-\\dfrac{\\gamma}{2} \\sum_j \\dfrac{\\partial L}{\\partial y_j} ( x_j - \\mu ) (\\sigma^2 + \\epsilon)^{-3/2}\\right] \\left[\\dfrac{2}{m - 1} (x_i - \\mu)\\right]\n\n \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = (\\sigma^2 + \\epsilon)^{-1/2}\\left[-\\gamma \\sum_j \\dfrac{\\partial L}{\\partial y_j} ( x_j - \\mu ) (\\sigma^2 + \\epsilon)^{-1/2}\\right] \\left[\\dfrac{1}{m - 1} (x_i - \\mu)(\\sigma^2 + \\epsilon)^{-1/2}\\right]\n\n \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = (\\sigma^2 + \\epsilon)^{-1/2}\\left[-\\gamma \\sum_j \\dfrac{\\partial L}{\\partial y_j} \\dfrac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\right] \\left[\\dfrac{1}{m - 1} \\dfrac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\right]\n\n \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = (\\sigma^2 + \\epsilon)^{-1/2}\\left[-\\gamma \\sum_j \\dfrac{\\partial L}{\\partial y_j} \\hat{x}_j \\right] \\left[\\dfrac{1}{m - 1} \\hat{x}_i\\right]\n\nAnd finally, letâ€™s add up all the product terms to get the final expression for\n\n\\dfrac{\\partial L}{\\partial x_i} = (\\sigma^2 + \\epsilon)^{-1/2} \\gamma \\dfrac{\\partial L}{\\partial y_i} - (\\sigma^2 + \\epsilon)^{-1/2}\\dfrac{\\gamma}{m}\\sum_j\\dfrac{\\partial L}{\\partial y_j} + (\\sigma^2 + \\epsilon)^{-1/2}\\left(-\\gamma \\sum_j \\dfrac{\\partial L}{\\partial y_j} \\hat{x}_j \\right) \\left(\\dfrac{1}{m - 1} \\hat{x}_i\\right)\n\n \\ \\ \\ \\ \\ \\ \\ =  \\dfrac{\\gamma}{m}(\\sigma^2 + \\epsilon)^{-1/2} \\left( m\\dfrac{\\partial L}{\\partial y_i} - \\sum_j\\dfrac{\\partial L}{\\partial y_j} - \\dfrac{m}{m - 1} \\hat{x}_i \\sum_j \\dfrac{\\partial L}{\\partial y_j} \\hat{x}_j \\right)\n\nAnd what we end up with at the end is a fairly simple mathematical expression over here that we cannot simplify further. But basically youâ€™ll notice that it only uses the stuff we have (\\dfrac{\\partial L}{\\partial y_i}, \\hat{x}_i) and it gives us the thing we need (\\dfrac{\\partial L}{\\partial x_i}). Letâ€™s now implement this final result:\n\ndhprebn = (\n    bngain\n    / batch_size\n    * bnvar_inv\n    * (\n        batch_size * dhpreact\n        - dhpreact.sum(0)\n        - batch_size / (batch_size - 1) * bnraw * (dhpreact * bnraw).sum(0)\n    )\n)\ncmp(\"hprebn\", dhprebn, hprebn)  # we can only get approximate correctness ~9e-10\n\n\n\nhprebn.shape, dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(\n    0\n).shape\n\n\n\nRemember the mathematical expression we analytically calculated applies only to one neuron, whereas the programmatical dhprebn expression applies to all the batch_size number of neurons we have, in parallel. This is non-trivial, so take your time to see how it is written out with the  broadcasting happening appropriately, so that the shapes work out.\n\n","type":"content","url":"/micrograduate/makemore4#exercise-3-backprop-through-batchnorm-but-all-in-one-go","position":13},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 4: training the mlp with our own manual backward pass"},"type":"lvl2","url":"/micrograduate/makemore4#exercise-4-training-the-mlp-with-our-own-manual-backward-pass","position":14},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Exercise 4: training the mlp with our own manual backward pass"},"content":"\n\nFor the final exercise 4, we want to put it all together. Meaning, we want to train the mlp with our own manual backward pass. So first, letâ€™s define a new, simpler forward pass function, a manual backward pass function, a training function and a function to print the loss:\n\ndef forward(xb, yb):\n    emb = C[xb]  # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n    # Linear layer\n    hprebn = embcat @ w1 + b1  # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmean = hprebn.mean(0, keepdim=True)\n    bnvar = hprebn.var(0, keepdim=True)\n    bnvar_inv = (bnvar + 1e-5) ** -0.5\n    bnraw = (hprebn - bnmean) * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact)  # hidden layer\n    logits = h @ w2 + b2  # output layer\n    loss = F.cross_entropy(logits, yb)  # loss function\n    return emb, embcat, bnvar_inv, bnraw, h, hpreact, logits, loss\n\n\n\ndef manual_backward(\n    parameters,\n    logits,\n    batch_size,\n    xb,\n    yb,\n    h,\n    hpreact,\n    bnraw,\n    bngain,\n    bnvar_inv,\n    embcat,\n    emb,\n):\n    dlogits = F.softmax(logits, 1)\n    dlogits[range(batch_size), yb] -= 1\n    dlogits /= batch_size\n    # 2nd layer backprop\n    dh = dlogits @ w2.T\n    dw2 = h.T @ dlogits\n    db2 = dlogits.sum(0)\n    # tanh\n    dhpreact = (1.0 - h**2) * dh\n    # batchnorm backprop\n    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n    dbnbias = dhpreact.sum(0, keepdim=True)\n    dhprebn = (\n        bngain\n        / batch_size\n        * bnvar_inv\n        * (\n            batch_size * dhpreact\n            - dhpreact.sum(0)\n            - batch_size / (batch_size - 1) * bnraw * (dhpreact * bnraw).sum(0)\n        )\n    )\n    # 1st layer\n    dembcat = dhprebn @ w1.T\n    dw1 = embcat.T @ dhprebn\n    db1 = dhprebn.sum(0)\n    # embedding\n    demb = dembcat.view(emb.shape)\n    dC = torch.zeros_like(C)\n    dC.index_add_(0, xb.flatten(), demb.view(-1, 10))\n    grads = [dC, dw1, db1, dw2, db2, dbngain, dbnbias]\n    return grads\n\n\n\ndef train(\n    x,\n    y,\n    parameters,\n    maxsteps=200000,\n    batchsize=32,\n    initial_lr=0.1,\n    run_torch_backward=False,\n    break_at_step=None,\n):\n    lossi = []\n    # use this context manager to enable or disable gradient tracking\n    with torch.set_grad_enabled(run_torch_backward):\n        # kick off optimization\n        for i in range(maxsteps):\n            xb, yb = construct_minibatch()\n            emb, embcat, bnvar_inv, bnraw, h, hpreact, logits, loss = forward(xb, yb)\n            for p in parameters:\n                p.grad = None\n            if run_torch_backward:\n                for t in [emb, embcat, bnvar_inv, bnraw, h, hpreact, logits]:\n                    t.retain_grad()\n                loss.backward()\n            grads = manual_backward(\n                parameters=parameters,\n                batch_size=batch_size,\n                xb=xb,\n                yb=yb,\n                logits=logits,\n                h=h,\n                hpreact=hpreact,\n                bnraw=bnraw,\n                bngain=bngain,\n                bnvar_inv=bnvar_inv,\n                embcat=embcat,\n                emb=emb,\n            )\n            # update\n            lr = (\n                initial_lr if i < 100000 else initial_lr / 10\n            )  # step learning rate decay\n            for p, grad in zip(parameters, grads):\n                p.data += -lr * (p.grad if run_torch_backward else grad)\n            # track stats\n            if i % 10000 == 0:  # print every once in a while\n                print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n            lossi.append(loss.log10().item())\n            if break_at_step is not None and i >= break_at_step:\n                break\n    return grads, lossi\n\n\n\n@torch.no_grad()  # this decorator disables gradient tracking\ndef print_loss(x, y, prefix=\"\"):\n    loss = forward(x, y)[-1]\n    print(f\"{prefix} {loss}\")\n    return loss\n\n\n\nCool. Now, we will simply train our nn using the backprop expressions we calculated manually (manual_backward()) instead of having PyTorch do it for us (i.e. loss.backward()). Basically, we have plucked out all the operations that PyTorch would calculate and we have instead implemented them manually. Of course, our manual results are identical to the automated ones, as we have verified already. So, letâ€™s train and do both classic PyTorch backpropagation and our manual propagation and break the training loop at 100 epochs and check whether gradients match the PyTorch ones:\n\n# Exercise 4: putting it all together!\n# Train the MLP neural net with your own backward pass\nparameters = define_nn(n_hidden=200, n_embd=10)\ngrads, lossi = train(\n    xtrain, ytrain, parameters, run_torch_backward=True, break_at_step=100\n)\n# check gradients against PyTorch\nfor p, g in zip(parameters, grads):\n    cmp(str(tuple(p.shape)), g, p)\n\n\n\nSo here we see that our gradients are mostly, but not exactly, equal. Most are approximately equal with only tiny differences of the e-9 magnitude. Which is totally fine. Can you figure out why we donâ€™t get exact correctness for all the gradients? If we now redefine our nn and train without breaking and only do our manual backprop (feels amazing to realize we donâ€™t need the automatic backward pass anymore!):\n\nparameters = define_nn(n_hidden=200, n_embd=10)\ngrads, lossi = train(xtrain, ytrain, parameters)\n\n\n\nAfter training, we will calibrate the batchnorm parameters as we did not keep track of the running mean and variance in the training loop:\n\n# calibrate the batch norm at the end of training\nwith torch.no_grad():\n    # pass the training set through\n    emb = C[xtrain]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1\n    # measure the mean/std over the entire training set\n    bnmean = hpreact.mean(0, keepdim=True)\n    bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n\n\n\nAnd print both the training and validation losses:\n\nprint_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\");\n\n\n\nWe achieve a pretty good loss, very similar to what we achieved in our previous lecture! Now, all that remains is to sample from our model:\n\n# sample from the model\ng = torch.Generator().manual_seed(SEED + 10)\nfor _ in range(20):\n    out = []\n    context = [0] * block_size  # initialize with all ...\n    while True:\n        # ------------\n        # forward pass:\n        # Embedding\n        emb = C[torch.tensor([context])]  # (1,block_size,d)\n        embcat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n        hpreact = embcat @ w1 + b1\n        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n        h = torch.tanh(hpreact)  # (N, n_hidden)\n        logits = h @ w2 + b2  # (N, vocab_size)\n        # ------------\n        # Sample\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(\"\".join(itoc[i] for i in out))\n\n\n\nAnd we see the sort of name-like gibberish that we have been used to. Which of course means that the model works, as before ðŸ˜ƒ!\n\n","type":"content","url":"/micrograduate/makemore4#exercise-4-training-the-mlp-with-our-own-manual-backward-pass","position":15},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Summary"},"type":"lvl2","url":"/micrograduate/makemore4#summary","position":16},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Summary"},"content":"\n\nEverything is the same except that we didnâ€™t use loss.backward() or \n\ntorch.autograd and we estimated the gradients ourselves, by hand. And so hopefully youâ€™re looking at the the manual backward pass we did through this nn and youâ€™re thinking to yourself â€œactually thatâ€™s not too complicatedâ€. Each one of these layers is like three lines of code or something like that and most of it is fairly straightforward potentially with the notable exception of the batchnorm backward pass. And thatâ€™s everything we wanted to cover in this lecture. So hopefully you found this interesting and what is noteworthy about it honestly is that it gave us a very nice diversity of layers to backpropagate through. It should give us a pretty nice and comprehensive sense of how these backward passes are implemented and how they work and youâ€™d be able to derive them yourself. But of course in practice you probably donâ€™t want to: you want to use \n\ntorch.autograd instead. But hopefully now you have some intuition about how gradients flow backwards through the nn starting at the loss and how they flow through all the variables. And if you understood a good chunk of it and if you now have a sense of that, you should proudly count yourself as one of these buff doges on the left instead of the sorry little one on the right here:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"backwardmemelol.png\"))\n\n\n\n","type":"content","url":"/micrograduate/makemore4#summary","position":17},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Outro"},"type":"lvl2","url":"/micrograduate/makemore4#outro","position":18},{"hierarchy":{"lvl1":"5. makemore (part 4): becoming a backprop ninja","lvl2":"Outro"},"content":"\n\nNow in the next lecture weâ€™re actually going to go to LSTMs and all the other variants of RNNs and weâ€™re going to start to complexify the architecture and start to achieve better log likelihoods and what not. Now thereâ€™s something to look forward to! Time for new adventures! See you there.","type":"content","url":"/micrograduate/makemore4#outro","position":19},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet"},"type":"lvl1","url":"/micrograduate/makemore5","position":0},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/makemore5","position":1},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/makemore5#intro","position":2},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Intro"},"content":"\n\nHi, everyone! Today we are continuing our implementation of makemore, our favorite character-level language model. Now, over the last few lectures, weâ€™ve built up an architecture that is a mlp character-level language model. So we see that it receives 3 previous characters and tries to predict the 4th character in a sequence using one hidden layer of neurons with tanh nonlinearities:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"bengio2003nn.jpeg\"))\n\n\n\nSo what weâ€™d like to do now in this lecture is to complexify this architecture. In particular, we would like to take more characters in a sequence as an input, not just 3. In addition to that, we donâ€™t just want to feed them all into a single hidden layer, because that squashes too much information too quickly. Instead, we would like to make a deeper model that progressively fuses this information to make its guess about the next character in a sequence. Weâ€™re actually going to arrive at something that looks very much like \n\nWaveNet, a paper published by DeepMind in 2016. Which is a language model basically, but it tries to predict audio sequences instead of character-level sequences or word-level sequences:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig1.png\"))\n\n\n\nBut fundamentally, the modeling setup is identical. It is an autoregressive model and it tries to predict the next character in a sequence:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_eq1.png\"))\n\n\n\nAnd the architecture actually takes this interesting hierarchical sort of approach to predicting the next character in a sequence with this tree-like structure:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))\n\n\n\nAnd this is the architecture:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig4.png\"))\n\n\n\nAnd weâ€™re going to implement it in this lesson. So letâ€™s get started!\n\n","type":"content","url":"/micrograduate/makemore5#intro","position":3},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Starter code walkthrough"},"type":"lvl2","url":"/micrograduate/makemore5#starter-code-walkthrough","position":4},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Starter code walkthrough"},"content":"\n\nThe starter code for this part is very similar to where we ended up in makemore (part 3). So very briefly, we are doing imports:\n\nimport random\nrandom.seed(42)\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\nSEED = 2147483647\n\n\n\nWe are reading our data set of words:\n\n# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n\n\nAnd we are processing the dataset of words into lots and lots of individual examples:\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {s: i + 1 for i, s in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: s for s, i in ctoi.items()}\nvocab_size = len(itoc)\nprint(itoc)\nprint(vocab_size)\n\n\n\ndef build_dataset(words, block_size):\n    x, y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            x.append(context)\n            y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(x.shape, y.shape)\n    return x, y\n\n\n\ndef build_all_datasets(block_size):\n    random.shuffle(words)\n    n1 = int(0.8 * len(words))\n    n2 = int(0.9 * len(words))\n    xtrain_dataset = build_dataset(words[:n1], block_size)  # 80%\n    xval_dataset = build_dataset(words[n1:n2], block_size)  # 10%\n    xtest_dataset = build_dataset(words[n2:], block_size)  # 10%\n    return xtrain_dataset, xval_dataset, xtest_dataset\n\n\n\ndef print_next_character(xtrain, ytrain):\n    for x, y in zip(xtrain, ytrain):\n        print(\"\".join(itoc[ix.item()] for ix in x), \"-->\", itoc[y.item()])\n\n\n\nSpecifically many examples of...\n\nblock_size = (\n    3  # context length: how many characters do we take to predict the next one?\n)\n(xtrain, ytrain), (xval, yval), (xtest, ytest) = build_all_datasets(block_size)\n\n\n\n... block_size=3 characters and we are trying to predict the fourth one:\n\nprint_next_character(xtrain[:20], ytrain[:20])\n\n\n\nBasically, we are breaking down each of these word into little problems of â€œgiven 3 characters, predict the 4th oneâ€. So this is our data set and this is what weâ€™re trying to get the nn to do. Now in makemore (part 3), we started to develop our code around these following layer modules. Weâ€™re doing this because we want to think of these modules as lego building blocks that we can sort of stack up into nns and we can feed data between these layers and stack them up into sort of graphs. Now we also developed these layers to have APIs and signatures very similar to \n\nthose that are found in PyTorch. And so we have the Linear layer, the BatchNorm1d layer and the Tanh layer that we developed previously:\n\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (\n                    1 - self.momentum\n                ) * self.running_mean + self.momentum * xmean\n                self.running_var = (\n                    1 - self.momentum\n                ) * self.running_var + self.momentum * xvar\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nÎ‘nd Linear just does a matrix multiply in the forward pass of this module, BatchNorm1d of course is this crazy layer that we developed in the previous lecture. Whatâ€™s crazy about it is... well thereâ€™s many things. Number one, it has these running mean and variances that are trained outside of backprop. They are trained using exponential moving average inside this layer when we call the forward pass. In addition to that, thereâ€™s this self.training flag because the behavior of batchnorm is different during train time and evaluation time. And so suddenly we have to be very careful that batchnorm is in its correct state. That itâ€™s in the evaluation state or training state. So thatâ€™s something to now keep track of something that sometimes introduces bugs because you forget to put it into the right mode. And finally, we saw that batchnorm couples the statistics or the activations across the examples in the batch. So normally we thought of the batch as just an efficiency thing, but now we are coupling the computation across batch elements and itâ€™s done for the purposes of controlling the activation statistics as we saw in the previous video. So batchnorm is a very weird layer because you have to modulate the training and eval phase. Whatâ€™s more, you have to wait for the mean and the variance to settle and to actually reach a steady state and a state can become the source of many bugs, usually. And now letâ€™s define the appropriate functions:\n\n# seed rng for reproducability\ntorch.manual_seed(42)\n\n\n\nn_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 200  # the number of neurons in the hidden layer of the MLP\n\n\ndef define_nn(block_size, n_embd, n_hidden):\n    global C\n    C = torch.randn((vocab_size, n_embd))\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    layers = [\n        Linear(n_inputs, n_hidden, bias=False),\n        BatchNorm1d(n_hidden),\n        Tanh(),\n        Linear(n_hidden, n_outputs),\n    ]\n    # parameter init\n    with torch.no_grad():\n        layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = [C] + [p for l in layers for p in l.parameters()]\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters\n\n\n\ndef forward(layers, xb, yb):\n    emb = C[xb]  # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, yb)  # loss function\n    return loss\n\n\n\ndef backward(parameters, loss):\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n\n\ndef update(parameters, lr):\n    for p in parameters:\n        p.data += -lr * p.grad\n\n\n\ndef train(\n    x,\n    y,\n    layers,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(layers, xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        if break_at_step is not None and i >= break_at_step:\n            break\n    return lossi\n\n\n\ndef trigger_eval_mode(layers):\n    for l in layers:\n        l.training = False\n\n\n\n@torch.no_grad()\ndef infer_loss(layers, x, y, prefix=\"\"):\n    loss = forward(layers, x, y)\n    print(f\"{prefix} {loss}\")\n    return loss\n\n\n\ndef sample_from_model(block_size, layers):\n    for _ in range(20):\n        out = []\n        context = [0] * block_size  # initialize with all ...\n        while True:\n            # forward pass the neural net\n            emb = C[torch.tensor([context])]  # (1, block_size, n_embd)\n            x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n            for l in layers:\n                x = l(x)\n            logits = x\n            probs = F.softmax(logits, dim=1)\n            # sample from the distribution\n            ix = torch.multinomial(probs, num_samples=1).item()\n            # shift the context window and track the samples\n            context = context[1:] + [ix]\n            out.append(ix)\n            # if we sample the special '.' token, break\n            if ix == 0:\n                break\n        print(\"\".join(itoc[i] for i in out))  # decode and print the generated word\n\n\n\nThese should look somewhat familiar to you by now. Letâ€™s train!\n\nlayers, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, layers, parameters)\n\n\n\n\n\nplt.figure()\nplt.plot(lossi);\n\n\n\nThis loss function looks very crazy. We should probably fix this. And thatâ€™s because 32 batch elements are too few. And so you can get very lucky or unlucky in any one of these batches, and it creates a very thicc loss function. So weâ€™re gonna fix that soon. Now, before we evaluate the trained nn by inferring the training and validation loss, we need to remember because of the batchnorm layers to set all the layersâ€™ training flag to False:\n\ntrigger_eval_mode(layers)\ninfer_loss(layers, xtrain, ytrain, prefix=\"train\")\ninfer_loss(layers, xval, yval, prefix=\"val\");\n\n\n\nWe still have a ways to go, as far as the validation loss is concerned. But if we sample from our model, we see that we get relatively name-like results that do no exist in the training set:\n\nsample_from_model(block_size=block_size, layers=layers)\n\n\n\nBut we can improve our loss and improve our results even further. Weâ€™ll start by fixing that thicc loss plot!\n\n","type":"content","url":"/micrograduate/makemore5#starter-code-walkthrough","position":5},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Fixing the loss plot"},"type":"lvl2","url":"/micrograduate/makemore5#fixing-the-loss-plot","position":6},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Fixing the loss plot"},"content":"\n\nOne way to turn this thicc loss plot into a normal one is to only plot the mean. Remember, lossi is a very long list of floats that contains a loss for each training episode:\n\nlen(lossi), lossi[:5]\n\n\n\nLetâ€™s segment this very long list into a 2D tensor of rows, with each row containing 1000 loss values:\n\nt_loss = torch.tensor(lossi).view(-1, 1000)\nt_loss\n\n\n\nNow, if we take the mean of each row, we end up with a list of loss averages:\n\nmean_t_loss = t_loss.mean(1)\nmean_t_loss\n\n\n\nIf we plot this tensor list of mean losses, we should get a nicer loss plot:\n\nplt.figure()\nplt.plot(mean_t_loss);\n\n\n\nNow, the progress we make during training is much more clearly visible! Also, notice the learning rate decay, where the loss drops to a even lower minimum. This is the loss plot we are going to be using going forward.\n\n","type":"content","url":"/micrograduate/makemore5#fixing-the-loss-plot","position":7},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"torchifying the code: layers, containers, torch.nn"},"type":"lvl2","url":"/micrograduate/makemore5#torchifying-the-code-layers-containers-torch-nn","position":8},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"torchifying the code: layers, containers, torch.nn"},"content":"\n\nNow itâ€™s time to simplify our forward function a little bit. Notice how the embeddings and flattening operations are calculated outside of the layers:\n\ndef forward(layers, xb, yb):\n    emb = C[xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        ...\n\nTo start tidying things up, letâ€™s mirror \n\ntorch.nn.Embedding and \n\ntorch.nn.Flatten with our own incredibly simplified equivalent modules:\n\nclass Embedding:\n    def __init__(self, n_embd, embd_dim):\n        self.weight = torch.randn((n_embd, embd_dim))\n\n    def __call__(self, ix):\n        self.out = self.weight[ix]\n        return self.out\n\n    def parameters(self):\n        return [self.weight]\n\n\n\nclass Flatten:\n    def __call__(self, x):\n        self.out = x.view(x.shape[0], -1)\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nThese will simply be responsible for indexing and flattening. We can now simplify our forward pass by including the embedding and flattening operations as modules in the definition of the layers:\n\ndef define_nn(block_size, n_embd, n_hidden):\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    layers = [\n        Embedding(vocab_size, n_embd),\n        Flatten(),\n        Linear(n_inputs, n_hidden, bias=False),\n        BatchNorm1d(n_hidden),\n        Tanh(),\n        Linear(n_hidden, n_outputs),\n    ]\n    # parameter init\n    with torch.no_grad():\n        layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = [p for l in layers for p in l.parameters()]\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters\n\n\n\ndef forward(layers, xb, yb):\n    x = xb\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, yb)  # loss function\n    return loss\n\n\n\nAwesome. Now we can even further simplify our forward pass by replacing the list that contains our layers with our simplified implementation of the \n\ntorch.nn.Sequential container: this object contains layers and the functionality to iteratively pass data through them. Meaning that we now define a bunch of layers as a Sequential object (i.e. a model) through which we can pass input data (e.g. x), without the need to explicitly loop.\n\nclass Sequential:\n    def __init__(self, layers):\n        self.layers = layers\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        self.out = x\n        return self.out\n\n    def parameters(self):\n        # get parameters of all layers and stretch them out into one list\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\n\nLetâ€™s now further simplify our functions by replacing the layers list with model, a Sequential object:\n\ndef define_nn(block_size, n_embd, n_hidden):\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    model = Sequential(\n        [\n            Embedding(vocab_size, n_embd),\n            Flatten(),\n            Linear(n_inputs, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            Linear(n_hidden, n_outputs),\n        ]\n    )\n    # parameter init\n    with torch.no_grad():\n        model.layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = model.parameters()\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return model, parameters\n\n\n\ndef forward(model, xb, yb):\n    logits = model(xb)\n    loss = F.cross_entropy(logits, yb)  # loss function\n    return loss\n\n\n\ndef train(\n    x,\n    y,\n    model,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(model, xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        if break_at_step is not None and i >= break_at_step:\n            break\n    return lossi\n\n\n\ndef trigger_eval_mode(model):\n    for l in model.layers:\n        l.training = False\n\n\n\n@torch.no_grad()\ndef infer_loss(model, x, y, prefix=\"\"):\n    loss = forward(model, x, y)\n    print(f\"{prefix} {loss}\")\n    return loss\n\n\n\ndef sample_from_model(block_size, model):\n    for _ in range(20):\n        out = []\n        context = [0] * block_size  # initialize with all ...\n        while True:\n            # forward pass the neural net\n            logits = model(torch.tensor([context]))\n            probs = F.softmax(logits, dim=1)\n            # sample from the distribution\n            ix = torch.multinomial(probs, num_samples=1).item()\n            # shift the context window and track the samples\n            context = context[1:] + [ix]\n            out.append(ix)\n            # if we sample the special '.' token, break\n            if ix == 0:\n                break\n        print(\"\".join(itoc[i] for i in out))  # decode and print the generated word\n\n\n\nAnd letâ€™s verify that our new definitions work by re-training our nn:\n\nmodel, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)\n\n\n\nplt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));\n\n\n\ntrigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");\n\n\n\nsample_from_model(block_size, model)\n\n\n\nCool. Now itâ€™s time to decrease the loss even further by scaling up our model to make it bigger and deeper!\n\n","type":"content","url":"/micrograduate/makemore5#torchifying-the-code-layers-containers-torch-nn","position":9},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"WaveNet overview"},"type":"lvl2","url":"/micrograduate/makemore5#wavenet-overview","position":10},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"WaveNet overview"},"content":"\n\nCurrently, we are using this architecture here:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"bengio2003nn.jpeg\"))\n\n\n\nwhere we are taking in some number of characters, going into a single hidden layer, and then going to the prediction of the next character. The problem here is we donâ€™t have a naive way of making this bigger in a productive way. We could, of course, use our nn. We could use our layers, sort of like building block materials to introduce additional layers here and make the network deeper. But it is still the case that we are crushing all of the characters into a single layer all the way at the beginning. And even if we make this a layer bigger by adding neurons, itâ€™s still kind of like silly to squash all that information so fast in a single step. What weâ€™d like to do instead is weâ€™d like our network to look a lot more like this WaveNet case:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))\n\n\n\nSo you see in WaveNet, when we are trying to make the prediction for the next character in a sequence a function of the previous characters that feed in. But it is not the case that all of these different characters are just crushed to a single layer and then you have a sandwich. They are crushed slowly. So in particular, we take two characters and we fuse them into sort of like a bigram representation. And we do that for all these characters consecutively. And then we take the bigrams and we fuse those into four character level chunks. And then we fuse again. And so we do that in this tree-like hierarchical manner. So we fuse the information from the previous context gradually, as the network deepens. This is the kind of architecture that we want to implement. Now in the WaveNet case, this is a visualization of a stack of dilated causal convolution layers. And this makes it sound very scary, but actually the idea is quite simple. And the fact that itâ€™s a dilated causal convolution layer is really just an implementation detail to make everything fast. Weâ€™re going to see that later. But for now, letâ€™s just keep going. Weâ€™re going to keep the basic idea of it, which is this progressive fusion. So we want to make the network deeper, and at each level, we want to fuse only two consecutive elements. Two characters, then two bigrams, then two fourgrams, and so on. So letâ€™s implement this.\n\n","type":"content","url":"/micrograduate/makemore5#wavenet-overview","position":11},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Bumping the context size to 8"},"type":"lvl2","url":"/micrograduate/makemore5#bumping-the-context-size-to-8","position":12},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Bumping the context size to 8"},"content":"\n\nOkay, so first up, let me scroll to where we built the dataset, and letâ€™s change the block size from 3 to 8. So weâ€™re going to be taking 8 characters of context to predict the 9th character:\n\nblock_size = 8\n(xtrain, ytrain), (xval, yval), (xtest, ytest) = build_all_datasets(block_size)\n\n\n\nSo the dataset now looks like this:\n\nprint_next_character(xtrain[:20], ytrain[:20])\n\n\n\nThese 8 characters are going to be processed in the above tree-like structure. Letâ€™s find out how to implement this hierarchical scheme! But before doing that, letâ€™s train our simple fully-connected nn with this new dataset and see how well it performs:\n\nmodel, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)\n\n\n\nplt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));\n\n\n\ntrigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");\n\n\n\nInteresting! The loss has improved compared to the block_size = 3 case. Letâ€™s log our losses so far:\n\n# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n\nAlso, if we sample from the model, we can see the names improving qualitatively as well:\n\nsample_from_model(block_size, model)\n\n\n\nSo we could, of course, spend a lot of time here tuning things and scaling up our network further. But letâ€™s continue and letâ€™s implement the hierarchical model and treat this as just a rough baseline performance. Thereâ€™s a lot of optimization left on the table in terms of some of the hyperparameters that youâ€™re hopefully getting a sense of now.\n\n","type":"content","url":"/micrograduate/makemore5#bumping-the-context-size-to-8","position":13},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Implementing WaveNet"},"type":"lvl2","url":"/micrograduate/makemore5#implementing-wavenet","position":14},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Implementing WaveNet"},"content":"\n\nLetâ€™s now create a bit of a scratch space for us to just look at the forward pass of the  nn and inspect the shape of the tensors along the way of the forward pass:\n\n# let's look at a batch of just 4 examples\nix = torch.randint(0, xtrain.shape[0], (4,))\nxb, yb = xtrain[ix], ytrain[ix]\nlogits = model(xb)\nprint(xb.shape)\nxb\n\n\n\n\n\nHere we are just temporarily, for debugging purposes, creating a batch of just, say, 4 examples. So 4 random integers. Then, we are plucking out those rows from our training set. And then we are passing into the model the input xb. Now the shape of xb here, because we only have 4 examples. And 8 is the current block size. So xb contains 4 rows/examples of 8  characters each. And each integer tensor row of xb just contains the identities of those characters. Therefore, the first layer of our nn is the embedding layer. So passing xb, this integer tensor, through the Embedding layer creates an output:\n\nmodel.layers[0].out.shape  # output of Embedding layer\n\n\n\nSo our embedding table C has, for each character, a 10-dimensional vector (n_embd=10) that we are trying to learn. What the layer does here is it plucks out the embedding vector for each one of these integers (of xb and organizes it all in a 4\\times8\\times10 tensor. So all of these integers are translated into 10-dimensional vectors inside this 3-dimensional tensor now.\n\nmodel.layers[1].out.shape  # output of Flatten layer\n\n\n\nNow passing that through the Flatten layer, as you recall, what this does is it views this tensor as just a 4\\times80 tensor. And what that effectively does is that all these 10-dimensional embeddings for all these 8 characters just end up being stretched out into a long row. And that looks kind of like a concatenation operation, basically. So by viewing the tensor differently, we now have a 4\\times80. And inside this 80, itâ€™s all the 10-dimensional vectors just concatenated next to each other.\n\nmodel.layers[2].out.shape  # output of Linear layer\n\n\n\nAnd the linear layer, of course, takes 80 and creates 200 channels just via matrix multiplication. So far, so good. Now letâ€™s see something surprising. Letâ€™s look at the insides of the Linear layer and remind ourselves how it works:\n\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n...\n\nThe Linear layer here in a forward pass takes the input x, multiplies it with a weight and then optionally adds a bias. And the weight is 2D, as defined here, and the bias is 1D. So effectively, in terms of the shapes involved, whatâ€™s happening inside this Linear layer looks like this right now. And weâ€™re using random numbers here, but just to illustrate the shapes and what happens:\n\n(torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n\n\n\nBasically, a 4\\times80 comes into the Linear layer, gets multiplied by a 80\\times200 weight matrix inside, and then thereâ€™s a plus 200 bias. And the shape of the whole thing that comes out of the Linear layer is 4\\times200, as we see here. Notice, by the way, that the matrix multiplication here will create a 4x200 tensor, and then when adding 200 thereâ€™s a broadcasting happening here, but since 4x200 broadcasts with 200, everything works here. So now the surprising thing is how this works. Specifically, something you may not expect is that this input here, that is being matrix-multiplied, doesnâ€™t actually have to be 2D. This matrix multiply operator in PyTorch is quite powerful, and in fact, you can actually pass in higher dimensional arrays or tensors, and everything works fine. So for example, torch.randn(4, 80) could instead be torch.randn(4, 5, 80) (4\\times5\\times80) and the result in that case would become 4\\times5\\times200. You can add as many dimensions as you like to the left of the last dimension of the input tensor (here, dimension 80). And so effectively, whatâ€™s happening is that the matrix multiplication only works on a matrix multiplication on the last dimension, and the dimensions before it in the input tensor are left unchanged. So basically, these dimensions to the left of the last dimension are all treated as just a batch dimension. So we can have multiple batch dimensions (e.g. torch.randn(4, 5, 6, 7, 80)), and then in parallel over all those dimensions, we are doing the matrix multiplication only on the last dimension. So this is quite convenient, because we can use that in our nn now. Remember that we have these 8 characters coming in, e.g.# 1 2 3 4 5 6 7 8\n\nAnd we donâ€™t want to now flatten all of it out into a large 8-dimensional vector, because we donâ€™t want to matrix multiply 80 into a weight matrix multiply immediately. Instead, we want to group these like this:# (1 2) (3 4) (5 6) (7 8)\n\nSo every consecutive two elements should now basically be flattened and multiplied by a weight matrix. But the idea is that all of these four groups here, weâ€™d like to process in parallel. So itâ€™s kind of like a extra batch dimension that we can introduce. And then we can, in parallel, basically process all of these bigram groups in the four extra batch dimension of an individual example, and also over the actual batch dimension of the four examples. So letâ€™s see what this is all about and how that works. Right now, we take a 4\\times80 and multiply it by 80\\times200 in the linear layer. Effectively, what we want is instead of 8 characters (80 embedding numbers) coming in, we only want 2 characters (20 embedding numbers) to come in. Therefore, if we want that, we canâ€™t have a 4x80 feeding into the Linear layer, but instead 4 groups of 2 characters to be feeding in, like this:\n\n(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape\n\n\n\nTherefore, what we would want to do now is change the Flatten layer so that it doesnâ€™t output a 4x80 but a 4x4x20 where basically in each row tensor of xb:\n\nxb\n\n\n\nevery two consecutive characters (e.g. (0, 0), (10, 21), (12,  9), (5, 1)) are packed in on the very last dimension (i.e. 20). So that the first dimension (i.e. 4) is the first batch dimension and the second dimension (i.e. 4) is the second batch dimension. And this is where we want to get to:\n\n(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape\n\n\n\nNow we have to change our Flatten layer (so that it doesnâ€™t fully flatten out the examples, but creates a 4\\times4\\times20 instead of a 4\\times80) and our Linear layer (to expect 20 instead of 80). So letâ€™s see how this could be implemented. Basically, right now we have an input that is a 4\\times8\\times10 that feeds into the Flatten layer, and currently the Flatten layer just stretches it out:class Flatten:\n    def __call__(self, x):\n        self.out = x.view(x.shape[0], -1)\n        return self.out\n...\n\nthrough the view operation. Effectively what it does now is:\n\ne = torch.randn(\n    4, 8, 10\n)  # goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated\ne.view(4, -1).shape  # yields 4x80\n\n\n\nBut we want to just view the same tensor as a 4x4x20 instead, so:\n\ne.view(4, 4, -1).shape  # yields 4x4x20: this is what we want!\n\n\n\nEasy, right? Letâ€™s now rewrite Flatten, but since ours will now start to depart from \n\ntorch.nn.Flatten, weâ€™ll rename it to FlattenConsecutive just to make sure that our APIs are somewhat similar but not the same:\n\nclass FlattenConsecutive:\n    def __init__(self, n):\n        self.n = n\n\n    def __call__(self, x):\n        b, t, c = x.shape\n        x = x.view(b, t // self.n, c * self.n)\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        self.out = x\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nSo FlattenConsecutive takes in and flattens only some n consecutive elements and puts them into the last dimension. In __call__ we parse the 3 dimensions of the input x as b, c, t (e.g. 4, 8, 10) and then we view x as a b, t // n, c * n tensor (e.g. 4, 8/2, 10 \\cdot 2, a.k.a.: 4, 4, 20). Last but not least, we check whether the middle dimension of x (x.shape[1]) is 1 and if so, then we simply squeeze out that dimension (e.g. 4\\times1\\times10 would become 4\\times10). Letâ€™s now replace Flatten with our new FlattenConsecutive, while maintaining the same functionality:\n\ndef define_nn(block_size, n_embd, n_hidden):\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    model = Sequential(\n        [\n            Embedding(vocab_size, n_embd),\n            FlattenConsecutive(block_size),\n            Linear(n_inputs, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            Linear(n_hidden, n_outputs),\n        ]\n    )\n    # parameter init\n    with torch.no_grad():\n        model.layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = model.parameters()\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return model, parameters\n\n\n\nNow, letâ€™s define the model and verify that the shapes of the layer outputs are the same after feeding one batch of data into it:\n\nmodel, _ = define_nn(block_size, n_embd, n_hidden)\nprint(xb.shape)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))\n\n\n\nSo, we see the shapes as we expect them after every single layer in its output. Now, letâ€™s try to restructure it and do it hierarchically:\n\ndef define_nn(block_size, n_embd, n_hidden):\n    n_consec = 2\n    n_inputs = n_embd * n_consec\n    n_outputs = vocab_size\n    model = Sequential(\n        [\n            Embedding(vocab_size, n_embd),\n            FlattenConsecutive(n_consec),\n            Linear(n_inputs, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            FlattenConsecutive(n_consec),\n            Linear(n_hidden * n_consec, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            FlattenConsecutive(n_consec),\n            Linear(n_hidden * n_consec, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            Linear(n_hidden, n_outputs),\n        ]\n    )\n    # parameter init\n    with torch.no_grad():\n        model.layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = model.parameters()\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return model, parameters\n\n\n\nNow, letâ€™s inspect the numbers in between after a forward pass on a new nn:\n\nmodel, _ = define_nn(block_size, n_embd, n_hidden)\nprint(xb.shape)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))\n\n\n\nSo 4\\times8\\times20 was flattened consecutively into 4\\times4\\times20. Through the Linear layer, this was projected into 4\\times4\\times200. And then BatchNorm1d just works out of the box and so does Tanh, which is element-wise. Then we crushed it again. So we flattened consecutively once more and ended up with a 4\\times2\\times400 now. Then Linear brought it back down to 4\\times2\\times200, BatchNorm1d and Tanh didnâ€™t change the shape and for the last flattening,\nit squeezed out that dimension of 1, we end up with 4\\times400. And then Linear, BatchNorm1d, Tanh and the last Linear yield our logits that end up in the same shape as they were before. Now, we actually have a nice three-layer nn that basically corresponds to this WaveNet network:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))\n\n\n\nwith the only difference that we are using a blocksize of 8 instead of 16, as depicted above. Now with a new architecture, we just have to kind of figure out some good channel numbers (numbers of hidden units) to use here. If we decrease the number to:\n\nn_hidden = 68\n\n\n\nthen the total number of parameters comes out to 22000: exactly the same that we had before (when n_hidden=200). So we have the same amount of capacity with this nn in terms of the number of parameters. But the question is whether we are utilizing those parameters in a more efficient architecture.\n\n","type":"content","url":"/micrograduate/makemore5#implementing-wavenet","position":15},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Training the WaveNet: first pass"},"type":"lvl2","url":"/micrograduate/makemore5#training-the-wavenet-first-pass","position":16},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Training the WaveNet: first pass"},"content":"\n\nLetâ€™s train this WaveNet and see the results:\n\nmodel, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)\n\n\n\nplt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));\n\n\n\ntrigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");\n\n\n\n# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n# flat -> hierachical (22K params): train 1.937, val 2.026\n\nAs you can see, changing from the flat to hierachical model (while keeping the same number of parameters) is not giving us any noticeable significant benefit in terms of the loss.  That said, there are two things to point out. Number one, we didnâ€™t really â€œtortureâ€ the architecture here very much. And thereâ€™s a bunch of hyperparameter search that we could do in terms of how we allocate our budget of parameters to what layers. Number two, we still may have a bug inside the BatchNorm1d layer. So letâ€™s take a look at that because it runs, but doesnâ€™t do the right thing.\n\n","type":"content","url":"/micrograduate/makemore5#training-the-wavenet-first-pass","position":17},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Fixing the BatchNorm1d bug"},"type":"lvl2","url":"/micrograduate/makemore5#fixing-the-batchnorm1d-bug","position":18},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Fixing the BatchNorm1d bug"},"content":"\n\nIf we train for just one step and we print the layer output shapes:\n\n_ = train(xtrain, ytrain, model, parameters, break_at_step=1)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))\n\n\n\ncurrently, it looks like the BatchNorm is receiving an input that is 32\\times4\\times68, right? Letâ€™s take a look at the implementation of BatchNorm:class BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n  \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True) # batch mean\n            xvar = x.var(0, keepdim=True) # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n  \n    def parameters(self):\n        return [self.gamma, self.beta]\n\nIt assumed, in the way we wrote it and at the time, that the input x is 2D. So it was N \\times D, where N was the batch size. So thatâ€™s why we only reduced the mean and the variance over the 0th dimension. But now x will basically become 3D. So whatâ€™s happening inside the batchnorm layer right now? And how come itâ€™s working at all and not giving any errors? The reason for that is basically because everything broadcasts properly, but the batchnorm is not doing what we want it to do. So in particular, letâ€™s basically think through whatâ€™s happening inside the batchnorm. Letâ€™s look at whatâ€™s happening here in a simplified example:\n\ne = torch.randn(32, 4, 68)\nemean = e.mean(0, keepdim=True)  # 1, 4, 68\nevar = e.var(0, keepdim=True)  # 1, 4, 68\nehat = (e - emean) / torch.sqrt(evar + 1e-5)  # 32, 4, 68\nehat.shape\n\n\n\nSo weâ€™re receiving an input of 32\\times4\\times68. And then we are doing here x.mean(), but we have e instead of x. But weâ€™re doing the mean over 0 and thatâ€™s actually giving us 1\\times4\\times68. So weâ€™re doing the mean only over the very first dimension. And itâ€™s giving us a mean and a variance that still maintains the middle dimension in between (i.e. 4). So these means are only taken over 32 numbers in the first dimension. And then, when we perform the ehat assignment, everything broadcasts correctly still. But basically what ends up happening is when we also look at the running mean:\n\nmodel.layers[3].running_mean.shape\n\n\n\nthe shape of this running mean now is 1\\times4\\times68. Instead of it being just a size of dimension, because we have 68 channels, we expect to have 68 means and variances that weâ€™re maintaining. But actually, we have an array of 4\\times68. And so basically what this is telling us is this batchnorm is currently working in parallel over 4\\times68 instead of just 68 channels. So basically we are maintaining this. We are maintaining statistics for every one of these four positions individually and independently. And instead, what we want to do is we want to treat this middle 4 dimension as a batch dimension, just like the 0th dimension. So as far as the batchnorm is concerned, it doesnâ€™t want to average... We donâ€™t want to average over 32 numbers. But instead, we want to now average over 32\\times4 numbers for every single one of these 68 channels. Since \n\ntorch.mean allows us to reduce over multiple (and not just one) dimensions at the same time, weâ€™ll do just that and reduce over both the 0th and 1st dimensions:\n\ne = torch.randn(32, 4, 68)\nemean = e.mean((0, 1), keepdim=True)  # 1, 1, 68\nevar = e.var((0, 1), keepdim=True)  # 1, 1, 68\nehat = (e - emean) / torch.sqrt(evar + 1e-5)  # 32, 4, 68\nehat.shape\n\n\n\nAlthough the final shape of ehat remains the same, we see now that:\n\nemean.shape, evar.shape\n\n\n\ninstead of 1, 4, 68, since we reduced over both of the batch dimensions, it yields only 68 numbers total for each tensor, with a bunch of spurious leftover 1 dimensions remaining. Therefore, this is what should be happening with our running_mean and running_var tensors inside our BatchNorm1d implementation. So the change is pretty straightforward:\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            if x.ndim == 2:\n                dim = 0\n            elif x.ndim == 3:\n                dim = (0, 1)\n            xmean = x.mean(dim, keepdim=True)  # batch mean\n            xvar = x.var(dim, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (\n                    1 - self.momentum\n                ) * self.running_mean + self.momentum * xmean\n                self.running_var = (\n                    1 - self.momentum\n                ) * self.running_var + self.momentum * xvar\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\n\nBasically, now in __call__ we are checking the dimensionality of x and based on it we are determining the dim parameters to be passed to the mean and var functions. Now, to point out one more thing. Weâ€™re actually departing from the API of PyTorch here a little bit, because when you go read the documentation of \n\ntorch.nn.BatchNorm1d, it says:\n\nInput: (N,C) or (N,C,L), where N is the batch size, C is the number of features or channels, and L is the sequence length\n\nNotice, the input to this layer can either be N (batch size) \\times C (number of features or channels) or it actually does accept three-dimensional inputs, but it expects it to be N\\times C \\times L (sequence legth). So this is a problem because you see how C is nested here in the middle. And so when it gets 3D inputs, this batchnorm layer will reduce over 0 and 2 instead of 0 and 1. So basically, \n\ntorch.nn.BatchNorm1d layer assumes that C will always be the first dimension, whereas we assume here that C is the last dimension, and there are some number of batch dimensions beforehand. And so, it expects N\\times C or N\\times C\\times L, whereas we expect N\\times C or N\\times L\\times C. So just a small deviation from the PyTorch API. Now, after updating our BatchNorm1d, if we redefine our nn and run for one step:\n\nmodel, parameters = define_nn(block_size, n_embd, n_hidden)\n_ = train(xtrain, ytrain, model, parameters, break_at_step=1)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))\n\n\n\nthe shapes are of course the same as before, but now if we inspect the shape of the running_mean inside the batchnorm layer:\n\nmodel.layers[3].running_mean.shape\n\n\n\nSo correctly now we are only maintaining 68 means, for every one of our channels, and we are treating the 0th and the 1st dimension as batch dimensions, which is exactly what we want!\n\n","type":"content","url":"/micrograduate/makemore5#fixing-the-batchnorm1d-bug","position":19},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Re-training the WaveNet after bug fix"},"type":"lvl2","url":"/micrograduate/makemore5#re-training-the-wavenet-after-bug-fix","position":20},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Re-training the WaveNet after bug fix"},"content":"\n\nSo letâ€™s retrain the nn now, after the bug fix:\n\nmodel, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)\n\n\n\nplt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));\n\n\n\ntrigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");\n\n\n\nAnd we can see that we are getting a tiny improvement in our training and validation losses:\n\n# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n# flat -> hierachical (22K params): train 1.937, val 2.026\n# fix bug in batchnorm: train 1.911, val 2.019\n\nThe reason this improvement is to be expected is that now we have less numbers going into the estimates of the mean and variance which allows everything to be more stable and less wiggly.\n\n","type":"content","url":"/micrograduate/makemore5#re-training-the-wavenet-after-bug-fix","position":21},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Scaling up our WaveNet"},"type":"lvl2","url":"/micrograduate/makemore5#scaling-up-our-wavenet","position":22},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Scaling up our WaveNet"},"content":"\n\nAnd with this more general architecture in place, we are now set up to push the performance further by increasing the size of the network:\n\nn_embd = 24  # the dimensionality of the character embedding vectors\nn_hidden = 128  # the number of neurons in the hidden layer of the MLP\n\n\n\nAnd using the exact same architecture, we now have\n\nmodel, parameters = define_nn(block_size, n_embd, n_hidden)\n\n\n\n76579 parameters and the training takes a lot longer:\n\nlossi = train(xtrain, ytrain, model, parameters)\n\n\n\nbut we do get a nice curve:\n\nplt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));\n\n\n\nand we are now getting even better performance:\n\ntrigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");\n\n\n\nSo, to compare to previous performances:\n\n# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n# flat -> hierachical (22K params): train 1.937, val 2.026\n# fix bug in batchnorm: train 1.911, val 2.019\n# scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.768, val 1.990\n\nHowever because the experiments are starting to take longer to train, we are a little bit in the dark with respect to the correct setting of the hyperparameters here and the learning rates and so on. And so we are missing sort of like an experimental harness on which we could run a number of experiments and really tune this architecture very well.\n\n","type":"content","url":"/micrograduate/makemore5#scaling-up-our-wavenet","position":23},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"WaveNet but with dilated causal convolutions"},"type":"lvl2","url":"/micrograduate/makemore5#wavenet-but-with-dilated-causal-convolutions","position":24},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"WaveNet but with dilated causal convolutions"},"content":"\n\nSo letâ€™s conclude now with a few notes. We basically improved our performance noticeably from a val loss of 2.10 to 1.99. But this shouldnâ€™t be the focus as we are kind of in the dark. We have no experimental harness, we are just guessing and checking. And this whole thing is pretty terrible to be honest. We are just looking at the training loss, whereas we should be looking at the training and validation loss together. That said, we did implement the WaveNet architecture from the \n\npaper:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))\n\n\n\nBut we did not implement this specific forward pass of it:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig4.png\"))\n\n\n\nwhere you have a more complicated kind of gated linear layer with residual connections, skip connections and so on... So we did not implement this, but only the tree-like model. All things considered, letâ€™s briefly go over how what weâ€™ve done here relates to convolutional neural networks as used in the \n\nWaveNet paper. Basically the use of convolutions is strictly for efficiency. It doesnâ€™t actually change the model weâ€™ve implemented. So, here for example:\n\nprint_next_character(xtrain[46:54], ytrain[46:54])\n\n\n\nwe see the name analisa from our training set and it has 7 letters, so that is 8 rows which correspond to independent examples of that name. Now, we can forward any one of these rows independently:\n\n# forward a single example\nsingle_example = xtrain[[7]]  # index by [[7]] to get an extra batch dimension\nlogits = model(single_example)\nlogits.shape\n\n\n\nNow imagine that instead of just a single example, you would like to forward all of the 8 examples that make up the name into the network at the same time:\n\n# forward all of them\nlogits = torch.zeros(8, 27)\nfor i in range(7):\n    logits[i] = model(xtrain[[7 + i]])\nlogits.shape\n\n\n\nOf course, as weâ€™ve implemented this right now, this is 8 independent calls to our model. But what convolutions allow you to do is they allow you to â€œslideâ€ this model efficiently over the input sequence. And so this for loop we just wrote out can be done not â€œoutsideâ€, through iteration, in Python, but â€œinsideâ€ of kernels in \n\nCUDA. And so this for loop gets hidden into the convolution. So basically you can think of the convolution as a for loop applying a little linear filter over space of some input sequence. And in our case the space weâ€™re interested in is one dimensional. And we are interested in sliding these filter over the input data. This diagram is quite helpful for understanding actually:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))\n\n\n\nHere, you can see highlighted with black arrows a single tree of the calculation we just described. So depicted here, calculating a single orange node at the Output layer corresponds to us in our example forwarding a single example and getting out a single output. But what convolutions allow you to do is it allows you to take this black tree-like structure and kind of like slide it over the Input sequence (blue nodes) and calculate all of the orange outputs at the same time. In the above figure, this sliding action is represented by the dashed connections between the nodes. In our example:\n\nprint_next_character(xtrain[46:54], ytrain[46:54])\n\n\n\nthis sliding operation would correspond to calculating all the above 8 outputs at all the positions of the name (like we did in an explicit loop) at the same time. And the reason this is much more efficient is because the for loop is inside the CUDA kernels. That makes it efficient. Also, notice the node re-use in the diagram were for example in the first Hidden Layer each white node is the right child of the white node above it (in the second Hidden Layer), but also the left child of another white node (also in the second Hidden Layer). In the first Hidden Layer, each node and its value is used twice. Therefore, in our above example snippet, with our naive way we would have to recalculate the value that corresponds to such a node, whereas with such a convolutional nn we are allowed to reuse it. So, in the convolutional nn you can think of the linear layer as filters. And we take these filters and their linear filters and we slide them over the input sequence and we calculate the first layer, the second layer, the third layer and then the output layer of the sandwich and it is all done very efficiently using these convolutions.\n\n","type":"content","url":"/micrograduate/makemore5#wavenet-but-with-dilated-causal-convolutions","position":25},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Summary"},"type":"lvl2","url":"/micrograduate/makemore5#summary","position":26},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Summary"},"content":"\n\nAnother thing to take away from this lecture is having modeled our nn lego blocks: the module classes (Linear, etc.) after modules from \n\ntorch.nn. So it is now very easy to start using modules directly from PyTorch, from hereon. The next thing I hope you got a bit of a sense of is what the development process of building deep neural networks looks like. Which I think was relatively representative to some extent. So number one, we are spending a lot of time in the \n\ndocumentation page of PyTorch. And weâ€™re reading through all the layers, looking at documentations, what are the shapes of the inputs, what they can be, what does the layer do, and so on. Unfortunately, however, the PyTorch documentation is not very good, at least not at the time these lectures were implemented. The PyTorch developers spend a ton of time on hardcore engineering of all kinds of distributed primitives, etc. But no one is rigorously maintaining documentation. It will lie to you, it will be wrong, it will be incomplete, it will be unclear. So unfortunately, it is what it is and you just kind of have to do your best with what they give us. Also, thereâ€™s a ton of trying to make the shapes work. And thereâ€™s a lot of gymnastics around these multi-dimensional arrays. Are they 2D, 3D, 4D? What shapes do the layers take? Is it N\\times C\\times L or N\\times L\\times C? And youâ€™re permuting and viewing, and it just gets pretty messy. And so that brings me to number three. Itâ€™s often helpful to first prototype these layers and implementations in jupyter notebooks and make sure that all the shapes work out, initially making sure everything is correct. And then, once youâ€™re satisfied with the functionality, you can copy-paste the code into your actual code base or repository (e.g. in VSCode). So these are roughly only some notes on the development process of working with nns.\n\n","type":"content","url":"/micrograduate/makemore5#summary","position":27},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Outro"},"type":"lvl2","url":"/micrograduate/makemore5#outro","position":28},{"hierarchy":{"lvl1":"6. makemore (part 5): building a WaveNet","lvl2":"Outro"},"content":"\n\nLastly, this lecture unlocks a lot of potential further lectures because, number one, we have to convert our nn to actually use these dilated causal convolutional layers, so implementing the convnet. Number two, we potentially start to get into what this means, where are residual connections and skip connections and why are they useful. Number three, as we already mentioned, we donâ€™t have any experimental harness. So right now, we are just guessing and checking everything. This is not representative of typical deep learning workflows. You usually have to set up your evaluation harness. You have lots of arguments that your script can take. Youâ€™re more comfortably kicking off a lot of experiments. Youâ€™re looking at a lot of plots of training and validation losses, and youâ€™re looking at what is working and what is not working. And youâ€™re working on this like population level, and youâ€™re doing all these hyperparameter searches. So weâ€™ve done none of that so far. So how to set that up and how to make it good, I think is a whole other topic. And number four, we should probably cover RNNs, LSTMs, GRUs, and of course Transformers. So many places to go, and weâ€™ll cover that in the future. Thatâ€™s all for now. Bye! :)","type":"content","url":"/micrograduate/makemore5#outro","position":29},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine"},"type":"lvl1","url":"/micrograduate/micrograd","position":0},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/micrograd","position":1},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/micrograd#intro","position":2},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Intro"},"content":"\n\nWelcome! Letâ€™s begin by implementing micrograd: an autograd engine. autograd stands for automatic gradient computation/differentiation and it is a feature that facilitates the backpropagation algorithm (backprop), enabling efficient calculation the gradient of an error or loss function w.r.t. (w.r.t.) the weights and biases of a neural network (nn). This allows us to iteratively tune these parameters, minimize the loss and therefore improve the accuracy of the network. backprop is at the mathematical core of any modern deep neural network library, like PyTorch or jax. microgradâ€™s functionality is best illustrated by an example.\n\n","type":"content","url":"/micrograduate/micrograd#intro","position":3},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Sneak peek"},"type":"lvl2","url":"/micrograduate/micrograd#sneak-peek","position":4},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Sneak peek"},"content":"\n\nfrom micrograd.engine import Value\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f\"{g.data:.4f}, the outcome of this forward pass\")\ng.backward()\nprint(f\"{a.grad:.4f}, i.e. the numerical value of dg/da\")\nprint(f\"{b.grad:.4f}, i.e. the numerical value of dg/db\")\n\n\n\nYouâ€™ll see that micrograd allows us to build out mathematical expressions. Here, we have an expression that weâ€™re building out where you have two inputs a and b whose values have been wrapped into a Value object. In this example, we build a mathematical expression graph. a and b are transformed into c and d, and eventually into e, f and g through some intermediary operations such as + (addition), * (multiplication) and exponentiation to a constant power (e.g. b**3). Other operations include offsetting by one, negation, squashing at zero, squaring, division, etc. And so from these values micrograd will, in the background, build out a mathematical expression graph. It will know, for example, that c is also a value but also a result of the addition operation of two child value nodes a and b, to which it will maintain pointers. So, weâ€™ll basically know exactly how all this operations graph is laid out. Then, we can do a forward pass (a, b -> ... -> g), but also initiate the backward pass that constitutes backprop by calling backward at node g: g.backward(). What backprop is going to do is start at g and itâ€™s going to go backwards through that expression graph and itâ€™s going to recursively apply the chain rule from calculus. This allows it to evaluate the derivative of g w.r.t. all the internal nodes, like e, d and c (i.e. \\dfrac{\\partial g}{\\partial e}, \\dfrac{\\partial g}{\\partial d}, \\dfrac{\\partial g}{\\partial c}), but also w.r.t. the inputs a and b (i.e. \\dfrac{\\partial g}{\\partial a}, \\dfrac{\\partial g}{\\partial b}). Then we can actually query this derivative of g w.r.t., for example, a (\\dfrac{\\partial g}{\\partial a}) as such: a.grad, or the derivative of g w.r.t. b (\\dfrac{\\partial g}{\\partial b}) as such: b.grad. The values of these grad attributes tell us how a tiny change of the values of a and b is affecting g. If we slightly nudge a and make it slightly larger, a.grad == 138.8338 is telling us that g will grow and that the slope of that growth is going to be equal to that number. Similarly, the slope of the growth of g w.r.t. a slight positive nudge of b is going to be 645.5773. A nn is a function that takes in inputs and weights and returns an output: F(in, w) = out. micrograd will help you understand nns at such a fundamental level by disregarding non-essential elements (e.g. tensors) that you would otherwise encounter in practice, production, etc. micrograd is all you need in order to understand the basics. Everything else is just efficiency. Now, letâ€™s dive right in and start implementing!\n\nFirst, we must make sure that we develop a very good, intuitive understanding of what a derivative is and exactly what information it gives us.\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\n\n\n\ndef y(x):\n    return 3 * x**2 - 4 * x + 5\n\n\n\ny(3.0)\n\n\n\nxs = np.arange(-5, 5, 0.25)\nys = y(xs)\nplt.figure()\nplt.plot(xs, ys);\n\n\n\nAbove, we plotted a parabola. Now, to think through what the derivative of this function at any point x is letâ€™s look up the definition of \n\nwhat it means for a function to be differentiable. Understanding the derivative boils down to finding how a such a function f responds to a slight nudge of its input x by a small number h. Meaning, with what sensitivity does it respond? Does the value go up or down? Or, to be more precise, what is the slope at that point x? Is it positive or negative? This exact amount we are looking for is given to us by the limit equation that we can very simply approximate numerically with a small enough h value, as follows.\n\nh = 0.00000001\nx = 3.0  # try -3.0, 2/3, etc.\n(y(x + h) - y(x)) / h\n\n\n\nNow, with a more complex example, to start building an intuition about the derivative, letâ€™s find out the derivatives of d w.r.t. a, b and c (i.e. \\dfrac{\\partial d}{\\partial a}, \\dfrac{\\partial d}{\\partial b}, \\dfrac{\\partial d}{\\partial c}).\n\nh = 0.00000000000001\na = 2.0\nb = -3.0\nc = 10.0\nd1 = a * b + c\na += h  # try nudging `b` or `c` instead to see what happens\nd2 = a * b + c\nprint(f\"d1: {d1}\")\nprint(f\"d2: {d2}\")\nprint(f\"slope: {(d2 - d1)/h}\")\n\n\n\nWe now have some intuitive sense of what a derivative is telling us about a function. So now letâ€™s move unto nns, which are much more massive mathematical expressions. To easily do so, we must first build some infrastructure.\n\nclass Value:\n    def __init__(\n        self,\n        data,\n        prev=(),\n        op=\"\",\n        label=\"\",\n    ):\n        self.data = data\n        self.prev = set(prev)\n        self.op = op\n        self.label = label\n        self.grad = 0.0\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(data='{self.data}'\" + (\n            f\", label='{self.label}')\" if self.label else \")\"\n        )\n\n    def __add__(self, other):\n        return self.__class__(\n            data=(self.data + other.data),\n            prev=(self, other),\n            op=\"+\",\n        )\n\n    def __radd__(self, other):  # other + self\n        return self + other\n\n    def __mul__(self, other):\n        return self.__class__(\n            data=(self.data * other.data),\n            prev=(self, other),\n            op=\"*\",\n        )\n\n    def __rmul__(self, other):  # other * self\n        return self * other\n\n    def __neg__(self):  # -self\n        return self * -1\n\n    def __sub__(self, other):  # self - other\n        return self + (-other)\n\n    def tanh(self):\n        return self.__class__(\n            data=math.tanh(self.data),\n            prev=(self,),\n            op=\"tanh\",\n        )\n\n\na = Value(2.0, label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10.0, label=\"c\")\nf = Value(-2.0, label=\"f\")\n\n\ndef forward_pass(label=False):\n    d = a * b\n    e = d + c\n    L = e * f\n    if label:\n        d.label = \"d\"\n        e.label = \"e\"\n        L.label = \"L\"\n    return L\n\n\nL = forward_pass(label=True)\nL\n\n\n\nL.prev\n\n\n\nL.op\n\n\n\nThe Value object we defined acts as a value holder. It can also perform operations (+, etc.) with other Value objects to produce new Value instances. Each such instance holds:\n\nsome value data\n\nthe operation op that produced it (e.g. +)\n\nits children prev: a tuple of the Value instances that produced it\n\nits gradient grad which represents the derivative of some value (e.g. L)  (w.r.t. the value/instance itself)\n\nTo recap what we have done so far: we have built scalar-valued mathematical expression graphs using operations such as addition and multiplication. To see such a graph that each Value object represents, letâ€™s implement visualization function draw() and call it on a Value object, e.g. L.\n\nfrom graphviz import Digraph\n\n\ndef trace(root):\n    # builds a set of all nodes and edges in a graph\n    nodes, edges = set(), set()\n\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v.prev:\n                edges.add((child, v))\n                build(child)\n\n    build(root)\n    return nodes, edges\n\n\ndef draw(root):\n    dot = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})  # LR = left to right\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        # for any value in the graph, create a rectangular ('record') node for it\n        dot.node(\n            name=uid,\n            label=\"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad),\n            shape=\"record\",\n        )\n        if n.op:\n            # if this value is a result of some operation, create an op node for it\n            dot.node(name=uid + n.op, label=n.op)\n            # and connect this node to it\n            dot.edge(uid + n.op, uid)\n    for n1, n2 in edges:\n        # connect n1 to the op node of n2\n        dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n    return dot\n\n\n\ndraw(L)\n\n\n\nThe above graph depicts a forward pass. As a reminder, each grad attribute refers to how much each nudge of the value of a node (e.g. c) contributes to the output value L (i.e. \\dfrac{\\partial L}{\\partial c}). Letâ€™s now, fill in those gradients by doing backprop manually. Starting at L (the end of the graph), we ask: what is the derivative of L w.r.t. itself? Since f(L) = L, it is: \\frac{f(L + h) - f(L)}{h} = \\frac{L + h - L}{h}, ...\n\nh = 0.001\nL.grad = ((L.data + h) - L.data) / h\nL.grad\n\n\n\n...which of course equals 1! Now, moving backwards in the graph, what about the derivative of L w.r.t. e (\\dfrac{\\partial L}{\\partial e}), for example? Letâ€™s do the same and find out. Since L(e) = e \\cdot f \\Longrightarrow \\dfrac{\\partial L}{\\partial e} = \\frac{L(e + h) - L(e)}{h} = \\frac{(e + h) \\cdot f - e \\cdot f}{h} = \\frac{e \\cdot f + h \\cdot f - e \\cdot f}{h} = f. Similarly, \\dfrac{\\partial L}{\\partial f} = e. Therefore:\n\ne.grad = f.data\nf.grad = e.data\nprint(e.grad)\nprint(f.grad)\n\n\n\nNow, we will find \\dfrac{\\partial L}{\\partial c}, which basically constitutes the crux of backprop for this example and the most important node to understand. So pay attention!\nHow do we find the derivative of L w.r.t. c? Purely intuitively, just by knowing the sensitivity of L to e (\\dfrac{\\partial L}{\\partial e}) and how e is sensitive to c (\\dfrac{\\partial e}{\\partial c}), we should be able to somehow put this information together and get our result. Since we know the former, we just need to find the latter: e(c) = c + d and so \\dfrac{\\partial e}{\\partial c} = \\frac{e(c + h) - e(c)}{h} = \\frac{(c + h + d) - (c + d)}{h} = 1. And similarly: \\dfrac{\\partial e}{\\partial d} = 1. That â€œsomehowâ€ is the \n\nchain rule, according to which: \\dfrac{\\partial L}{\\partial c} = \\dfrac{\\partial L}{\\partial e} \\cdot \\dfrac{\\partial e}{\\partial c}. For us, this means:\n\nc.grad = e.grad * 1.0\nc.grad\n\n\n\nTo sum up:\n\nwe want \\dfrac{\\partial L}{\\partial c}\n\nwe know \\dfrac{\\partial L}{\\partial e}\n\nwe know \\dfrac{\\partial e}{\\partial c}\n\nwe apply the chain rule: \\dfrac{\\partial L}{\\partial c} = \\dfrac{\\partial L}{\\partial e} \\cdot \\dfrac{\\partial e}{\\partial c} = \\dfrac{\\partial L}{\\partial e} \\cdot 1 = \\dfrac{\\partial L}{\\partial e}\n\nsimilarly: \\dfrac{\\partial L}{\\partial d} = \\dfrac{\\partial L}{\\partial e} \\cdot \\dfrac{\\partial e}{\\partial d} = \\dfrac{\\partial L}{\\partial e} \\cdot 1 = \\dfrac{\\partial L}{\\partial e}\n\nEasy! Notice how \\dfrac{\\partial e}{\\partial c} = \\dfrac{\\partial e}{\\partial d} = 1. This equality verifies the fact that the addition operator (+) just acts as a router, merely distributing the derivative of the result (e.g. \\dfrac{\\partial L}{\\partial e}) backwards, through all the children nodes (e.g. c and d), unperturbed.\n\nTo complete this session of manual backprop, we are going to recurse this process and re-apply the chain rule all the way back to the input nodes, a and b. Thus, we are looking for \\dfrac{\\partial L}{\\partial a} and \\dfrac{\\partial L}{\\partial b}, meaning:\n\n\\dfrac{\\partial L}{\\partial a} = \\dfrac{\\partial L}{\\partial d} \\cdot \\dfrac{\\partial d}{\\partial a}\n\n\\dfrac{\\partial L}{\\partial b} = \\dfrac{\\partial L}{\\partial d} \\cdot \\dfrac{\\partial d}{\\partial b}\n\nWe already know that:\n\nf'dL/dd = dL/de = {e.grad}'\n\n\n\nBut, what about \\dfrac{\\partial d}{\\partial a} and \\dfrac{\\partial d}{\\partial b}? Letâ€™s derive them, the same way we did with \\dfrac{\\partial e}{\\partial c}:\n\n\\dfrac{\\partial d}{\\partial a} = \\frac{d(a + h) - d(a)}{h} = \\frac{(a + h) \\cdot b - a \\cdot b}{h} = b\n\n\\dfrac{\\partial d}{\\partial b} = a\n\nTherefore:\n\na.grad = e.grad * b.data\nb.grad = e.grad * a.data\nprint(a.grad)\nprint(b.grad)\n\n\n\nIf we re-draw our computational graph, we get:\n\ndraw(L)\n\n\n\nAnd thatâ€™s what backprop is! Just the recursive application of the chain rule backwards through the computational graph for calculating the gradient of a loss value (L) w.r.t. all other values that produced it (a, b, etc.). The purpose being, to be able to use the gradient values to nudge the loss value: optimization. This process goes as follows:\n\nto increase L, let each value follow the gradient in the positive direction\n\nto decrease L, let each value follow the gradient in the negative direction\n\nRunning the following example should increase the value of L:\n\nLold = L.data\na.data += 0.01 * a.grad\nb.data += 0.01 * b.grad\nc.data += 0.01 * c.grad\nd.data += 0.01 * d.grad\ne.data += 0.01 * e.grad\nf.data += 0.01 * f.grad\nL = forward_pass()\nprint(f\"old L: {Lold}\")\nprint(f\"new L: {L.data}\")\n\n\n\nYou just learned how to apply one optimization step! Optimization is fundamental when it comes to training nns, as you will see... Eventually we want to build up a level of understanding for training multilayer perceptrons (mlps), as they are called.\n\n","type":"content","url":"/micrograduate/micrograd#sneak-peek","position":5},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Multilayer perceptron example"},"type":"lvl2","url":"/micrograduate/micrograd#multilayer-perceptron-example","position":6},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Multilayer perceptron example"},"content":"\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"mlp.jpeg\"))\n\n\n\nTo gain some intuition necessary for training mlps, letâ€™s first explore an example of doing backprop through a simplified neuron.\n\ndisplay(Image(filename=\"neuron.jpeg\"))\n\n\n\nSuch a neuron:\n\nreceives input w_i x_i (output x_i of incoming neuron multiplied by a synaptic weights w_i)\n\nsummates those inputs \\displaystyle \\sum_i w_i x_i at the cell body\n\nadds a bias b\n\npasses the result through an activation function f\n\noutputs the value f\\bigg(\\displaystyle \\sum_i w_i x_i + b\\bigg)\n\nAn activation function f is usually represented by a squashing function such as \\tanh:\n\nplt.figure()\nplt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2)))\nplt.grid();\n\n\n\nLetâ€™s extend our Value class to incorporate this tanh activation function:\n\nclass Value(Value):\n    def tanh(self):\n        return self.__class__(\n            data=math.tanh(self.data),\n            prev=(self,),\n            op=\"tanh\",\n        )\n\n\n\nTo simulate the functionality of this neuron:\n\ndef activate_neuron():\n    global x1, x2, w1, w2, b, x1w1, x2w2, sum_, n, o\n    # inputs x1,x2\n    x1 = Value(2.0, label=\"x1\")\n    x2 = Value(0.0, label=\"x2\")\n    # weights w1,w2\n    w1 = Value(-3.0, label=\"w1\")\n    w2 = Value(1.0, label=\"w2\")\n    # bias of the neuron\n    b = Value(6.8813735870195432, label=\"b\")\n    # x1*w1 + x2*w2 + b\n    x1w1 = x1 * w1\n    x1w1.label = \"x1*w1\"\n    x2w2 = x2 * w2\n    x2w2.label = \"x2*w2\"\n    sum_ = x1w1 + x2w2\n    sum_.label = \"x1*w1 + x2*w2\"\n    n = sum_ + b\n    n.label = \"n\"\n    o = n.tanh()\n    o.label = \"o\"  # see the newly implemented Value.tanh method\n\n\nactivate_neuron()\n\n\n\ndraw(o)\n\n\n\nAs you can see, this graph represents a neuron that takes inputs and weights and produces a value n squashed by an activation function tanh that yields an output o. But, the grad attributes are empty. Letâ€™s fill them up by doing another manual backprop! Doing so requires finding out the derivatives of o w.r.t. the inputs. But of course in a typical nn setting what we really care about the most are the derivatives w.r.t. the weights w_i (e.g. \\dfrac{\\partial L}{\\partial w_i}) because those are the free parameters that are usually changed when improving performance.\n\no.grad = 1.0\no.grad\n\n\n\n\\dfrac{\\partial o}{\\partial n} = \\dfrac{\\partial \\tanh(n)}{\\partial n} = 1 - \\tanh(n)^2, according to the \n\nderivatives of hyperbolic functions. And therefore:\n\nn.grad = 1 - o.data**2\nn.grad\n\n\n\nAnd since we learnt from before that the + operation acts as a gradient router and we can see from our graph that n is recursively preceeded by two + operations, the children nodesâ€™ grad attribute will be the same value as n.grad:\n\nx2w2.grad = x1w1.grad = sum_.grad = b.grad = n.grad\nx2w2.grad\n\n\n\nAnd according to the chain rule, as previously described, the derivatives of the outputs w.r.t. to the inputs will be:\n\n\\dfrac{\\partial o}{\\partial x_1} = \\dfrac{\\partial o}{\\partial {(x_1 \\cdot w_1)}} \\cdot \\dfrac{\\partial {(x_1 \\cdot w_1)}}{\\partial x_1} = \\dfrac{\\partial o}{\\partial {(x_1 \\cdot w_1)}} \\cdot w_1\n\n\\dfrac{\\partial o}{\\partial w_1} = \\dfrac{\\partial o}{\\partial {(x_1 \\cdot w_1)}} \\cdot \\dfrac{\\partial {(x_1 \\cdot w_1)}}{\\partial w_1} = \\dfrac{\\partial o}{\\partial {(x_1 \\cdot w_1)}} \\cdot x_1\n\n\\dfrac{\\partial o}{\\partial x_2} = \\dfrac{\\partial o}{\\partial {(x_2 \\cdot w_2)}} \\cdot \\dfrac{\\partial {(x_2 \\cdot w_2)}}{\\partial x_2} = \\dfrac{\\partial o}{\\partial {(x_2 \\cdot w_2)}} \\cdot w_2\n\n\\dfrac{\\partial o}{\\partial w_2} = \\dfrac{\\partial o}{\\partial {(x_2 \\cdot w_2)}} \\cdot \\dfrac{\\partial {(x_2 \\cdot w_2)}}{\\partial w_2} = \\dfrac{\\partial o}{\\partial {(x_2 \\cdot w_2)}} \\cdot x_2\n\nThus:\n\nx1.grad = x1w1.grad * w1.data\nw1.grad = x1w1.grad * x1.data\nx2.grad = x2w2.grad * w2.data\nw2.grad = x2w2.grad * x2.data\nprint(x1.grad)\nprint(w1.grad)\nprint(x2.grad)\nprint(w2.grad)\n\n\n\ndraw(o)\n\n\n\nDone. Ok now, doing manual backprop is obviously ridiculous! So, we are gonna put an end to our suffering and see how we can do the backward pass automatically by codifying what we have learnt so far. Specifically, inside each operation of a Value instance, we will define a backward function that calculates the gradient:\n\nclass Value(Value):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._backward = lambda: None\n\n    def __add__(self, other):\n        other = other if isinstance(other, self.__class__) else self.__class__(other)\n        out = super().__add__(other)\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, self.__class__) else self.__class__(other)\n        out = super().__mul__(other)\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n        return out\n\n    def tanh(self):\n        out = super().tanh()\n\n        def _backward():\n            self.grad += (1 - out.data**2) * out.grad\n\n        out._backward = _backward\n        return out\n\n\n\nNotice how we accumulate each gradient (+=) instead of just assigning it (=). This guarantees that multiple backward calls on the same node are not overwritten, ensuring that the gradient is properly accumulated. This reflects the contributions from all paths leading to that node which respects the \n\nmultivariable case of the chain rule. Now, with our enhanced class, letâ€™s reactivate the neuron and calculate its new output:\n\nactivate_neuron()\n\n\n\nThen, letâ€™s do automatic backprop with the help of the backward functions we just defined. Calling backward on a value node will populate the grad attribute of the value nodes that produced it: essentially backprop.\n\no.grad = 1.0  # base case\no._backward()\nn._backward()\nb._backward()  # leaf node: nothing happens!\nsum_._backward()\nx2w2._backward()\nx1w1._backward()\nx2._backward()  # leaf node: nothing happens!\nw2._backward()  # leaf node: nothing happens!\nx1._backward()  # leaf node: nothing happens!\nw1._backward()  # leaf node: nothing happens!\n\n\n\ndraw(o)\n\n\n\nHurray! Awesome. We managed to ease the pain of manually calculating the gradients of each node. However, thereâ€™s a problem: calling backward on a node (e.g. x1w1) will only work if backward has already been called on its descendant node(s) (e.g. n). In our graph topology this means that backward must be called from the rightmost to the leftmost nodes. This can be done easily by laying out our graph using \n\ntopological sorting. Below, we re-do the forward pass and define a function that does so:\n\nactivate_neuron()\ntopo = []\nvisited = set()\n\n\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v.prev:\n            build_topo(child)\n        topo.append(v)\n\n\nbuild_topo(o)\ntopo\n\n\n\nBasically, it adds children nodes first before adding non-children nodes. For examples, the first node in the resulting topo list is a child node. Whereas, the non-child node o is last. In essence, what this sorting allows us to do is to traverse the graph and recursively call backward in a safe way, solving the aforementioned problem and thus facilitating automatic backprop with a single call. Something like this:\n\no.grad = 1.0\nfor node in reversed(topo):\n    node._backward()\n\n\n\ndraw(o)\n\n\n\nLetâ€™s incorporate this feature into our new Value class:\n\nclass Value(Value):\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v.prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n\n\nTo verify it works, we reactivate our neuron and produce a new output:\n\nactivate_neuron()\n\n\n\nand verify our one-call backprop works:\n\no.backward()\ndraw(o)\n\n\n\nVoilÃ ! backprop with a single backward call. Letâ€™s test it out one more time:\n\na = Value(-2.0, label=\"a\")\nb = Value(3.0, label=\"b\")\nc = a * b\nc.label = \"c\"\nd = a + b\nd.label = \"d\"\ne = d * c\ne.label = \"e\"\ne.backward()\ndraw(e)\n\n\n\nNice. That was easy... right? Next up, just to prove that this process works more generally, letâ€™s add a few more complex operations to our Value class. First, a power operation:\n\nclass Value(Value):\n    def __pow__(self, other):\n        assert isinstance(\n            other, (int, float)\n        ), \"only supporting int/float powers for now\"\n        out = self.__class__(\n            data=self.data**other,\n            prev=(self,),\n            op=f\"**{other}\",\n        )\n\n        def _backward():\n            self.grad += other * (self.data ** (other - 1)) * out.grad\n\n        out._backward = _backward\n        return out\n\n\n\na = Value(2.0)\nb = 3.0\nc = a**b\nc.grad = 1.0\nc._backward()\nassert a.grad == b * a.data ** (b - 1)\n\n\n\nAlso, a division operation:\n\nclass Value(Value):\n    def __truediv__(self, other):  # self / other\n        return self * other**-1\n\n\n\na = Value(2.0)\nb = Value(4.0)\nc = a / b\nassert c.data == (a * (1/b.data)).data == (a * b**-1).data\nc.grad = 1.0; c._backward()\nassert a.grad == 1 / b.data\n\n\n\nAnd last, but not least, an exponentiation operation:\n\nclass Value(Value):\n    def exp(self):\n        out = self.__class__(\n            data=math.exp(self.data),\n            prev=(self,),\n            op=\"exp\",\n        )\n\n        def _backward():\n            self.grad += out.data * out.grad\n\n        out._backward = _backward\n        return out\n\n\n\na = Value(2.0)\na_exp = a.exp()\na_exp.grad = 1.0; a_exp._backward()\nassert a.grad == a_exp.data\n\n\n\nUsing these newly defined operations, we can now explicitly define our own \n\ntanh operation, expressed as a composite operation, consisting of a combination of operations, as follows:\n\nclass Value(Value):\n    def tanh(self):\n        e = (2 * self).exp()\n        return (e - 1) / (e + 1)\n\n\n\na = Value(2.0)\na_tanh = a.tanh()\na_tanh.backward()\nassert math.isclose(a.grad, (1 - a_tanh.data**2))\n\n\n\nFurthermore, reactivating the neuron, now  with the new composite tanh, and doing backprop:\n\nactivate_neuron()\no.backward()\n\n\n\nyields...\n\ndraw(o)\n\n\n\n...the same gradients as before! In the end, what matters is being able to do a functional forward pass and backward pass (backprop) on the output of any of such operation we have defined, no matter how â€œcompositeâ€ it is. If you apply the chain rule properly, as demonstrated, the design of the function and itâ€™s complexity are totally up to you. As long as the backward processes are correct, properly, all is good.\n\nSo now letâ€™s do the exact same thing using a modern deep neural network library, like \n\nPyTorch, on which micrograd is roughly modeled.\n\nimport torch\n\n\n\nIn PyTorch, the equivalent of our Value object is the Tensor object. Tensors are just n-dimensional arrays of scalars. Examples:\n\ntorch.Tensor([2.0])  # 1x1 tensor\ntorch.Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # 2x3 tensor\n\n\n\nLike micrograd, PyTorch letâ€™s us construct the mathematical expression of a neuron function on which we can perform backprop!\n\nx1 = torch.tensor([2.0]).double().requires_grad_(True)\nx2 = torch.tensor([0.0]).double().requires_grad_(True)\nw1 = torch.tensor([-3.0]).double().requires_grad_(True)\nw2 = torch.tensor([1.0]).double().requires_grad_(True)\nb = torch.tensor([6.8813735870195432]).double().requires_grad_(True)\nn = x1 * w1 + x2 * w2 + b\no = torch.tanh(n)\nprint(o.data.item())\no.backward()\nprint(\"---\")\nprint(\"x2\", x2.grad.item())\nprint(\"w2\", w2.grad.item())\nprint(\"x1\", x1.grad.item())\nprint(\"w1\", w1.grad.item())\n\n\n\ntensor() makes a tensor, double() casts it to a double-precision data type (as Python also does, by default) and requires_grad_() enables gradient calculation for that tensor. In PyTorch, one has to explicitly enable it, as gradient calculation is disabled by default for efficiency reasons. Above, we just use single-element scalar tensors. But the whole point of PyTorch is to work with multi-dimensional tensors that can be acted upon in parallel by many different operations. Nevertheless, what we have built very much agrees with the API of PyTorch! Letâ€™s now construct the mathematical expression that constitutes a nn, piece by piece, specifically a 2-layer mlp. Weâ€™ll start out by writing up a neuron as weâ€™ve implemented it so far, but making it subscribe to the PyTorch API model in how it designs its own nn modules.\n\nimport random\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1, 1))\n\n\n\nw is an array of values that represents the weights of the nin input neurons and the bias b of this neuron. Now, we are gonna implement the forward pass call: the sum of the products of each w and input x and b (\\sum_i w_i x_i + b) passed through a non-linear activation function (e.g. tanh).\n\nclass Neuron(Neuron):\n    # w * x + b\n    def __call__(self, x):\n        preact = sum((w * x for w, x in zip(self.w, x)), self.b)\n        out = preact.tanh()\n        return out\n\n\n\nHereâ€™s an example of passing two inputs x through the neuron:\n\nx = [2.0, 3.0]\nnin = len(x)\nn = Neuron(nin)\nn(x)\n\n\n\nHaving done so, letâ€™s create a Layer object that represents a layer composed of many neurons as in typical nn fashion. It takes the number of input and output neurons (nin, nout) as arguments.\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n\n\nHereâ€™s an example of passing two inputs x through a layer of neurons:\n\nx = [2.0, 3.0]\nnin = len(x)\nnout = 3\nl = Layer(nin, nout)\nl(x)\n\n\n\nAs expected, after passing in nin input values, we get nout output values. Finally, letâ€™s implement an mlp, in which layers basically feed into each other sequentially. First, weâ€™ll define a constructor that creates a list of layers, each one initialized with the proper number of inputs and outputs, given an initial nin and nouts list. Then, a forward pass method that passes an initial input x through each of the layers, from first to last.\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for l in self.layers:\n            x = l(x)\n        return x\n\n\n\nHereâ€™s an example reconstructing the \n\nmlp example we saw previously. It has 3 input neurons, two hidden 4-neuron layers and a 1-neuron output layer, therefore:\n\nmlp = MLP(3, [4, 4, 1])\nx = [2.0, 3.0, -1.0]\nmlp_out = mlp(x)\nmlp_out\n\n\n\nAnd if we draw our single mlp output...\n\ndraw(mlp_out)\n\n\n\nWow, thatâ€™s a big graph! Now obviously, no one sane enough would dare differentiate these expressions with pen and paper. But with micrograd, youâ€™ll be able backprop all the way through this graph back into the leaf nodes, meaning the weights of our neurons (Neuron.w). To test it out, letâ€™s define our own inputs dataset (xs) to be fed into our mlp, as well as their corresponding target values ys. Basically, a simple binary classifier nn, since input corresponds to either 1.0 or -1.0.\n\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0],\n]\nys = [1.0, -1.0, -1.0, 1.0]  # desired targets\n\n\n\nEach one of the input vectors corresponds to each target value of y. E.g. [2.0, 3.0, -1.0] corresponds to target 1.0, [3.0, -1.0, 0.5] to -1.0, and so on. Now, ys are the desired values. Now, letâ€™s pass xs through our mlp and get the predicted values ypreds:\n\ndef forward_pass_mlp(mlp):\n    return [mlp(x) for x in xs]\n\n\nypreds = forward_pass_mlp(mlp)\nypreds\n\n\n\nAs you can see, currently, our mlp outputs values (ypreds) that are different from the desired target values we want (ys). So, how do we make ypreds equal, or at least as close as possible, to ys? Specifically, how do we tune the weights (aka, train our nn w.r.t. them) in order to better predict the desired targets? The trick used in deep learning that helps us achieve this, is to calculate a single number that somehow measures how well the nn is performing. This number is called the loss. Right now, ypreds is not close to ys, so we are not perfoming very well, meaning that the loss value is high. Our goal is to minimize the loss value and, by doing so, bring ypreds closer to ys. One way to calculate the loss value is through the mean squared error (MSE) function. This function will iterate over pairs of ys and ypreds and sum the squares of their differences. The squared difference of two values (e.g. a prediction and target value) discards any negative signs and gives us a positive quantification that represents how those values differ: their loss. If the loss is zero, the two values do not differ and are equal. Whereas if the loss is another, non-zero number, the two values are different and unequal. In general, the more two values differ, the greater their loss will be. The less they differ, the lower their loss will be. The final MSE loss of all such pairs will just be the sum of all the squared differences:\n\ndef calc_loss(ypreds, ys):\n    return sum([(ypreds - y) ** 2 for ypreds, y in zip(ypreds, ys)])\n\n\nloss = calc_loss(ypreds, ys)\n\n\n\nNow we want to minimize the loss, because if the loss is low, then every one of the predictions is equal to its target. The lowest the loss value can be is 0 (the ideal value). Whereas, the greater it is, the worse off the nn is predicting. So, how do we minimize the loss? First, backprop!\n\nloss.backward()\n\n\n\nAfter a backward call, something magical happens! All gradients of the loss w.r.t. the weights of our mlp get  calculated. To see this with our eyes, letâ€™s print out the grad attribute of any of the weights, e.g.:\n\nmlp.layers[0].neurons[0].w[0].grad\n\n\n\nWe see that because the gradient of this particular weight of this particular neuron of this particular layer of our nn is negative, we infer that its influence on the loss is also negative. Which means, that slightly increasing this particular weight, would make the loss value go down. Now if we draw the loss...\n\ndraw(loss)\n\n\n\nwe see that itâ€™s massive! The reason is that the loss depends on all the values produced by forward passing all the x values contained in xs into the mlp. Now, although the gradients of all the values in the graph have been calculated, we only care about the gradients of the parameters we want to change, namely the weights and biases. So, our next steps are to gather those parameters and tune them by nudging them based on their gradient information. To do so, we first extend the mlp component classes by implementing parameter getter methods:\n\nclass Neuron(Neuron):\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer(Layer):\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()]\n\n\nclass MLP(MLP):\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\n\nAfter getting our output and loss again (due to redefining the classes)...\n\nmlp = MLP(3, [4, 4, 1])\nypreds = forward_pass_mlp(mlp)\nloss = calc_loss(ypreds, ys)\nloss.backward()\n\n\n\nwe may inspect all the parameters...\n\nmlp_params = mlp.parameters()\nprint(mlp_params)\nprint(len(mlp_params))\n\n\n\nand change them in a manner that decreases the loss. Before we optimize all of them though, letâ€™s first optimize only one to see exactly how the process is done:\n\nmlp.layers[0].neurons[0].w[0].grad\n\n\n\nmlp.layers[0].neurons[0].w[0].data\n\n\n\nTo understand how to optimize the parameters, we need to understand the relationship between grad and loss through two distinct cases:\n\nA. A parameterâ€™s positive grad (> 0) value tells us that:\n\n(1) increasing that parameter would increase the loss\n\n(2) decreasing that parameter would decrease the loss\n\nB. A parameterâ€™s negative grad (< 0) value tells us that:\n\n(1) increasing that parameter would decrease the loss\n\n(2) decreasing that parameter would increase the loss\n\nSince, we only care about decreasing the loss (since we want to minimize it), we only care about cases A2 and B1. Therefore, we now know that decreasing the loss depends on changing the weight by an amount whose sign is the opposite of the grad value. Since the opposite of a value is just its negation, we describe the general optimization step required to change each parameter as follows:\n\nTo decrease the loss, change each parameter p in the direction that is opposite the direction of the the gradient grad of the loss w.r.t. p (\\dfrac{\\partial loss}{\\partial p}).\n\nThis can be achieved by subtracting the product of the gradient and a small step size \\alpha from the current parameter value:\n\np = p - \\alpha\\dfrac{\\partial loss}{\\partial p}, where Î± is a usually small positive number (e.g. 0.001) called the learning rate, which determines how big of a step size is to be taken during the optimization step. Since we descend the gradient, we call this process gradient descent. Letâ€™s now optimize one weight parameter with one step of gradient descent:\n\nypreds = forward_pass_mlp(mlp)\nloss = calc_loss(ypreds, ys)\n# zero out gradients (so they don't accummulate)\nfor p in mlp.parameters():\n    p.grad = 0.0\nloss.backward()\nmlp.layers[0].neurons[0].w[0].data += -0.001 * mlp.layers[0].neurons[0].w[0].grad\nypreds_after = forward_pass_mlp(mlp)\nloss_after = calc_loss(ypreds_after, ys)\nprint(f\"Loss before optimization step: {loss}\")\nprint(f\"Loss after optimization step: {loss_after}\")\n\n\n\nSee? Loss after the step is lower than before it. So, optimizing even one parameter with this procedure, decreases the loss of our mlp! But in order to decrease it even more, let alone minimize it, we must also optimize all of the parameters. How? Like this:\n\nypreds = forward_pass_mlp(mlp)\nloss = calc_loss(ypreds, ys)\n# zero gradients (so they don't accummulate)\nfor p in mlp.parameters():\n    p.grad = 0.0\nloss.backward()\nfor p in mlp.parameters():\n    p.data += -0.001 * p.grad\nypreds_after = [mlp(x) for x in xs]\nloss_after = sum([(ypreds - y) ** 2 for ypreds, y in zip(ypreds_after, ys)])\nprint(f\"Loss before optimization step: {loss}\")\nprint(f\"Loss after optimization step: {loss_after}\")\nprint([y.data for y in ypreds])\nprint([y.data for y in ypreds_after])\nprint(ys)\n\n\n\nSame thing happens: loss decreases! Most importantly, each time we re-run the optimization, the prediction values get even closer to the target values, which is our primary goal! In general, we must be careful with our step size. Too small of a step size will take many many steps to decrease the loss, whereas too big of a step size might be an overstep that causes an increase instead of a decrease of the loss. Sometimes, if the increase is too big, the loss value explodes. Finding a step size that is just right can be a subtle art sometimes. You can play around with the step size, re-running the cells each time, to see its effect. All in all, we have been able to derive a set of parameters (weights and biases):\n\nmlp.parameters()\n\n\n\nthat makes our network predict the target values. Basically, we have learned how to train a nn! But, letâ€™s make it a bit more respectable by implementing a training loop.\n\nfor k in range(20):\n    # forward pass\n    ypreds = forward_pass_mlp(mlp)\n    loss = calc_loss(ypreds, ys)\n    # zero gradients (so they don't accummulate)\n    for p in mlp.parameters():\n        p.grad = 0.0\n    # backward pass\n    loss.backward()\n    # update\n    for p in mlp.parameters():\n        p.data += -0.05 * p.grad\n    print(k, loss.data)\n\n\n\nAfter training, the prediction values should be significantly closer to target values:\n\nprint([y.data for y in ypreds])\nprint(ys)\n\n\n\n","type":"content","url":"/micrograduate/micrograd#multilayer-perceptron-example","position":7},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Training demo"},"type":"lvl2","url":"/micrograduate/micrograd#training-demo","position":8},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Training demo"},"content":"\n\nLast but not least, letâ€™s implement a small yet full demo of training an 2-layer nn (MLP) binary classifier. This is achieved by initializing a nn from micrograd.nn module, implementing a simple \n\nsvm â€œmax-marginâ€ binary classification loss and using SGD for optimization. Try and figure out what is happening here by reading the code!\n\nfrom sklearn.datasets import make_moons\n\n\nnp.random.seed(1337)\nrandom.seed(1337)\n\n# make up a dataset\nX_moons, y_moons = make_moons(n_samples=100, noise=0.1)\n\ny_moons = y_moons * 2 - 1  # make y be -1 or 1\n# visualize in 2D\nplt.figure(figsize=(5, 5))\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, s=20, cmap=\"jet\");\n\n\n\nfrom micrograd.nn import MLP\n\n# initialize a model\nmodel = MLP(2, [16, 16, 1])  # 2-layer neural network\nprint(model)\nprint(\"number of parameters\", len(model.parameters()))\n\n\n\nfrom micrograd.engine import Value\n\n\n# loss function\ndef loss(batch_size=None):\n\n    # inline DataLoader :)\n    if batch_size is None:\n        Xb, yb = X_moons, y_moons\n    else:\n        ri = np.random.permutation(X_moons.shape[0])[:batch_size]\n        Xb, yb = X_moons[ri], y_moons[ri]\n    inputs = [list(map(Value, xrow)) for xrow in Xb]\n\n    # forward the model to get scores\n    scores = list(map(model, inputs))\n\n    # svm \"max-margin\" loss\n    losses = [(1 + -yi * scorei).relu() for yi, scorei in zip(yb, scores)]\n    data_loss = sum(losses) * (1.0 / len(losses))\n    # L2 regularization\n    alpha = 1e-4\n    reg_loss = alpha * sum((p * p for p in model.parameters()))\n    total_loss = data_loss + reg_loss\n\n    # also get accuracy\n    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n    return total_loss, sum(accuracy) / len(accuracy)\n\n\ntotal_loss, acc = loss()\nprint(total_loss, acc)\n\n\n\n# optimization\nfor k in range(100):\n\n    # forward\n    total_loss, acc = loss()\n\n    # backward\n    model.zero_grad()\n    total_loss.backward()\n\n    # update (sgd)\n    learning_rate = 1.0 - 0.9 * k / 100\n    for p in model.parameters():\n        p.data -= learning_rate * p.grad\n\n    if k % 1 == 0:\n        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n\n\n\n# visualize decision boundary\n\nh = 0.25\nx_min, x_max = X_moons[:, 0].min() - 1, X_moons[:, 0].max() + 1\ny_min, y_max = X_moons[:, 1].min() - 1, X_moons[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nXmesh = np.c_[xx.ravel(), yy.ravel()]\ninputs = [list(map(Value, xrow)) for xrow in Xmesh]\nscores = list(map(model, inputs))\nZ = np.array([s.data > 0 for s in scores])\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max());\n\n\n\n","type":"content","url":"/micrograduate/micrograd#training-demo","position":9},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Summary"},"type":"lvl2","url":"/micrograduate/micrograd#summary","position":10},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Summary"},"content":"\n\nLetâ€™s now bring together and summarize what we have learned. nns are functions that take the data as well as the weights and biases (parameters) as inputs to produce outputs. The outputs, or predictions, together with target values are then passed through a loss function that yields an error called the loss value that represents their distance, in terms of how close or far apart they are. We train nns by using backprop to calculate the gradient of the loss w.r.t. the parameters. These gradients are used to then optimize those parameters using gradient descent in order to minimize the loss. The end goal is for the nn to perform a given task. Although relatively simple expressions, it turns out that nns can solve very complicated problems. Sometimes, if the problem is complex enough, and the nn big enough (> 100 billion parameters), fascinating proprties arise, as in the case of \n\nGPT, for example. This model, trained with large amounts of text from the internet can predict, given some words, the next words in a sequence.\n\n","type":"content","url":"/micrograduate/micrograd#summary","position":11},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Outro"},"type":"lvl2","url":"/micrograduate/micrograd#outro","position":12},{"hierarchy":{"lvl1":"1. micrograd: implementing an autograd engine","lvl2":"Outro"},"content":"\n\nThis brings us to the end of the micrograd tutorial. I hope you enjoyed this tutorial and found it helpful! Next up, itâ€™s time to make more!","type":"content","url":"/micrograduate/micrograd#outro","position":13},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch"},"type":"lvl1","url":"/micrograduate/picogpt","position":0},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch"},"content":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt\n\n\n\n","type":"content","url":"/micrograduate/picogpt","position":1},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Intro"},"type":"lvl2","url":"/micrograduate/picogpt#intro","position":2},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Intro"},"content":"\n\nIn this lesson, weâ€™ll implement a tiny GPT from scratch in just a few lines of NumPy. To keep things simple, weâ€™ll then load the trained GPT-2 model weights released by OpenAI into our implementation and generate some text. Sounds straightforward, right? Well, thatâ€™s because it is! Letâ€™s get started.\n\n","type":"content","url":"/micrograduate/picogpt#intro","position":3},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"What is a GPT?"},"type":"lvl2","url":"/micrograduate/picogpt#what-is-a-gpt","position":4},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"What is a GPT?"},"content":"\n\nGPT stands for Generative Pre-trained Transformer. Itâ€™s a type of nn architecture based on the \n\nTransformer. \n\nJay Alammarâ€™s How GPT3 Works is an excellent introduction to GPTs at a high level, but hereâ€™s the tl;dr:\n\nGenerative: A GPT generates text.\n\nPre-trained: A GPT is trained on lots of text from books, the internet, etc ...\n\nTransformer: A GPT is a decoder-only transformer nn.\n\nLarge Language Models (LLMs), like \n\nOpenAIâ€™s GPT-3, are just GPTs under the hood. What makes them special is they happen to be:\n\nvery big (billions of parameters) and\n\ntrained on lots of data (hundreds of gigabytes of text).\n\nFundamentally, a GPT generates text given a prompt. Even with this very simple API (input = text, output = text), a well-trained GPT can do some pretty awesome stuff like \n\nwrite your emails, \n\nsummarize a book, \n\ngive you instagram caption ideas, \n\nexplain black holes to a 5 year old, \n\ncode in SQL, and even \n\nwrite your will. So thatâ€™s a high-level overview of GPTs and their capabilities. Letâ€™s dig into some more specifics.\n\n","type":"content","url":"/micrograduate/picogpt#what-is-a-gpt","position":5},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Input / Output","lvl2":"What is a GPT?"},"type":"lvl3","url":"/micrograduate/picogpt#input-output","position":6},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Input / Output","lvl2":"What is a GPT?"},"content":"\n\nThe function signature for a GPT looks roughly like this:\n\ndef gpt(inputs: list[int]) -> list[list[float]]:\n    # inputs has shape [n_seq]\n    # output has shape [n_seq, n_vocab]\n    output = # beep boop neural network magic\n    return output\n\n","type":"content","url":"/micrograduate/picogpt#input-output","position":7},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Input","lvl3":"Input / Output","lvl2":"What is a GPT?"},"type":"lvl4","url":"/micrograduate/picogpt#input","position":8},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Input","lvl3":"Input / Output","lvl2":"What is a GPT?"},"content":"\n\nThe input is some text represented by a sequence of integers that map to tokens in the text:\n\n# integers represent tokens in our text, for example:\n# text   = \"not all heroes wear capes\":\n# tokens = \"not\"  \"all\" \"heroes\" \"wear\" \"capes\"\ninputs =   [1,     0,    2,      4,     6]\n\nTokens are sub-pieces of the text, which are produced using some kind of tokenizer. We can map tokens to integers using a vocabulary:\n\n# the index of a token in the vocab represents the integer id for that token\n# i.e. the integer id for \"heroes\" would be 2, since vocab[2] = \"heroes\"\nvocab = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n\n# a pretend tokenizer that tokenizes on whitespace\ntokenizer = WhitespaceTokenizer(vocab)\n\n# the encode() method converts a str -> list[int]\nids = tokenizer.encode(\"not all heroes wear\") # ids = [1, 0, 2, 4]\n\n# we can see what the actual tokens are via our vocab mapping\ntokens = [tokenizer.vocab[i] for i in ids] # tokens = [\"not\", \"all\", \"heroes\", \"wear\"]\n\n# the decode() method converts back a list[int] -> str\ntext = tokenizer.decode(ids) # text = \"not all heroes wear\"\n\nIn short:\n\nWe have a string.\n\nWe use a tokenizer to break it down into smaller pieces called tokens.\n\nWe use a vocabulary to map those tokens to integers.\n\nIn practice, we use more advanced methods of tokenization than simply splitting by whitespace, such as \n\nByte-Pair Encoding or \n\nWordPiece, but the principle is the same:\n\nThere is a vocab that maps string tokens to integer indices\n\nThere is an encode method that converts str -> list[int]\n\nThere is a decode method that converts list[int] -> str\n\nNote: For certain applications, the tokenizer doesnâ€™t require a decode method. For example, if you want to classify if a movie review is saying the movie was good or bad, you only need to be able to encode the text and do a forward pass of the model, there is no need for decode. For generating text however, decode is a requirement. â†©ï¸Ž\n\n","type":"content","url":"/micrograduate/picogpt#input","position":9},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Output","lvl3":"Input / Output","lvl2":"What is a GPT?"},"type":"lvl4","url":"/micrograduate/picogpt#output","position":10},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Output","lvl3":"Input / Output","lvl2":"What is a GPT?"},"content":"\n\nThe output is a 2D array, where output[i][j] is the modelâ€™s predicted probability that the token at vocab[j] is the next token inputs[i+1]. For example:\n\nvocab = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\ninputs = [1, 0, 2, 4] # \"not\" \"all\" \"heroes\" \"wear\"\noutput = gpt(inputs)\n#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n# output[0] =  [0.75    0.1     0.0       0.15    0.0   0.0    0.0  ]\n# given just \"not\", the model predicts the word \"all\" with the highest probability\n\n#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n# output[1] =  [0.0     0.0      0.8     0.1    0.0    0.0   0.1  ]\n# given the sequence [\"not\", \"all\"], the model predicts the word \"heroes\" with the highest probability\n\n#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n# output[-1] = [0.0     0.0     0.0     0.1     0.0    0.05  0.85  ]\n# given the whole sequence [\"not\", \"all\", \"heroes\", \"wear\"], the model predicts the word \"capes\" with the highest probability\n\nTo get a next token prediction for the whole sequence, we simply take the token with the highest probability in output[-1]:\n\nvocab = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\ninputs = [1, 0, 2, 4] # \"not\" \"all\" \"heroes\" \"wear\"\noutput = gpt(inputs)\nnext_token_id = np.argmax(output[-1]) # next_token_id = 6\nnext_token = vocab[next_token_id] # next_token = \"capes\"\n\nTaking the token with the highest probability as our prediction is known as \n\ngreedy decoding or greedy sampling. The task of predicting the next logical word in a sequence is called language modeling. As such, we can call a GPT a language model. Generating a single word is cool and all, but what about entire sentences, paragraphs, etc ...?\n\n","type":"content","url":"/micrograduate/picogpt#output","position":11},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Generating text","lvl2":"What is a GPT?"},"type":"lvl3","url":"/micrograduate/picogpt#generating-text","position":12},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Generating text","lvl2":"What is a GPT?"},"content":"\n\n","type":"content","url":"/micrograduate/picogpt#generating-text","position":13},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Autoregressive","lvl3":"Generating text","lvl2":"What is a GPT?"},"type":"lvl4","url":"/micrograduate/picogpt#autoregressive","position":14},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Autoregressive","lvl3":"Generating text","lvl2":"What is a GPT?"},"content":"\n\nWe can generate full sentences by iteratively getting the next token prediction from our model. At each iteration, we append the predicted token back into the input:\n\ndef generate(inputs, n_tokens_to_generate):\n    for _ in range(n_tokens_to_generate): # auto-regressive decode loop\n        output = gpt(inputs) # model forward pass\n        next_id = np.argmax(output[-1]) # greedy sampling\n        inputs.append(int(next_id)) # append prediction to input\n    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n\ninput_ids = [1, 0] # \"not\" \"all\"\noutput_ids = generate(input_ids, 3) # output_ids = [2, 4, 6]\noutput_tokens = [vocab[i] for i in output_ids] # \"heroes\" \"wear\" \"capes\"\n\nThis process of predicting a future value (regression), and adding it back into the input (auto), is why you might see a GPT described as autoregressive.\n\n","type":"content","url":"/micrograduate/picogpt#autoregressive","position":15},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Sampling","lvl3":"Generating text","lvl2":"What is a GPT?"},"type":"lvl4","url":"/micrograduate/picogpt#sampling","position":16},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Sampling","lvl3":"Generating text","lvl2":"What is a GPT?"},"content":"\n\nWe can introduce some stochasticity (randomness) to our generations by sampling from the probability distribution instead of being greedy:\n\ninputs = [1, 0, 2, 4] # \"not\" \"all\" \"heroes\" \"wear\"\noutput = gpt(inputs)\nnp.random.choice(np.arange(vocab_size), p=output[-1]) # capes\nnp.random.choice(np.arange(vocab_size), p=output[-1]) # hats\nnp.random.choice(np.arange(vocab_size), p=output[-1]) # capes\nnp.random.choice(np.arange(vocab_size), p=output[-1]) # capes\nnp.random.choice(np.arange(vocab_size), p=output[-1]) # pants\n\nThis allows us to generate different sentences given the same input. When combined with techniques like \n\ntop-k, \n\ntop-p, and \n\ntemperature, which modify the distribution prior to sampling, the quality of our outputs is greatly increased. These techniques also introduce some hyperparameters that we can play around with to get different generation behaviors (for example, increasing temperature makes our model take more risks and thus be more â€œcreativeâ€.\n\n","type":"content","url":"/micrograduate/picogpt#sampling","position":17},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Training","lvl2":"What is a GPT?"},"type":"lvl3","url":"/micrograduate/picogpt#training","position":18},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Training","lvl2":"What is a GPT?"},"content":"\n\nWe train a GPT like any other nn, using gradient descent w.r.t. some loss function. In the case of a GPT, we take the cross entropy loss over the language modeling task:\n\ndef lm_loss(inputs: list[int], params) -> float:\n    # the labels y are just the input shifted 1 to the left\n    #\n    # inputs = [not,     all,   heros,   wear,   capes]\n    #      x = [not,     all,   heroes,  wear]\n    #      y = [all,  heroes,     wear,  capes]\n    #\n    # of course, we don't have a label for inputs[-1], so we exclude it from x\n    #\n    # as such, for N inputs, we have N - 1 language modeling example pairs\n    x, y = inputs[:-1], inputs[1:] # both have shape [num_tokens_in_seq - 1]\n\n    # forward pass\n    # all the predicted next token probability distributions at each position\n    output = gpt(x, params) # has shape [num_tokens_in_seq - 1, num_tokens_in_vocab]\n\n    # cross entropy loss\n    # we take the average over all N-1 examples\n    loss = np.mean(-np.log(output[np.arange(len(output)), y]))\n\n    return loss\n\ndef train(texts: list[list[str]], params) -> float:\n    for text in texts:\n        inputs = tokenizer.encode(text)\n        loss = lm_loss(inputs, params)\n        gradients = compute_gradients_via_backpropagation(loss, params)\n        params = gradient_descent_update_step(gradients, params)\n    return params\n\nThis is a heavily simplified training setup, but it illustrates the point. Notice the addition of params to our gpt function signature (we left this out in the previous sections for simplicity). During each iteration of the training loop:\n\nWe compute the language modeling loss for the given input text example\n\nThe loss determines our gradients, which we compute via backprop\n\nWe use the gradients to update our model parameters such that the loss is minimized (gradient descent)\n\nNotice, we donâ€™t use explicitly labelled data. Instead, we are able to produce the input/label pairs from just the raw text itself. This is referred to as \n\nself-supervised learning. Self-supervision enables us to massively scale training data. Just get our hands on as much raw text as possible and throw it at the model. For example, GPT-3 was trained on 300 billion tokens of text from the internet and books:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"table_2.2_gpt3_paper.png\"))\n\n\n\nOf course, you need a sufficiently large model to be able to learn from all this data, which is why GPT-3 has 175 billion parameters and probably cost between \n\n$1m-10m in compute cost to train. This self-supervised training step is called pre-training, since we can reuse the â€œpre-trainedâ€ models weights to further train the model on downstream tasks, such as classifying if a tweet is toxic or not. Pre-trained models are also sometimes called foundation models. Training the model on downstream tasks is called fine-tuning, since the model weights have already been pre-trained to understand language, itâ€™s just being fine-tuned to the specific task at hand. The â€œpre-training on a general task + fine-tuning on a specific taskâ€ strategy is called \n\ntransfer learning.\n\nNote: Although, with the \n\nInstructGPT and \n\nChinchilla papers, weâ€™ve realized that we donâ€™t actually need to train models that big. An optimally trained and instruction fine-tuned GPT at 1.3B parameters can outperform GPT-3 at 175B parameters.\n\n","type":"content","url":"/micrograduate/picogpt#training","position":19},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Prompting","lvl2":"What is a GPT?"},"type":"lvl3","url":"/micrograduate/picogpt#prompting","position":20},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Prompting","lvl2":"What is a GPT?"},"content":"\n\nIn principle, the original \n\nGPT paper was only about the benefits of pre-training a transformer model for transfer learning. The paper showed that pre-training a 117M GPT achieved state-of-the-art performance on various NLP (natural language processing) tasks when fine-tuned on labelled datasets. It wasnâ€™t until the \n\nGPT-2 and \n\nGPT-3 papers that we realized a GPT model pre-trained on enough data with enough parameters was capable of performing any arbitrary task by itself, no fine-tuning needed. Just prompt the model, perform autoregressive language modeling, and voila, the model magically gives us an appropriate response. This is referred to as in-context learning, because the model is using just the context of the prompt to perform the task. In-context learning can be zero shot, one shot, or few shot:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"fig_2.1_gpt3_paper.png\"))\n\n\n\nGenerating text given a prompt is also sometimes referred to as conditional generation, since our model is generating some output conditioned on some input. GPTs are not limited to NLP tasks. You can condition the model on anything you want. For example, you can turn a GPT into a chatbot (i.e. \n\nChatGPT) by conditioning it on the conversation history. You can also further condition the chatbot to behave a certain way by prepending the prompt with some kind of description (i.e. â€œYou are a chatbot. Be polite, speak in full sentences, donâ€™t say harmful things, etc ...â€). Conditioning the model like this can even give your \n\nchatbot a persona. This is often referred to as a system prompt. However, this is not robust, you can still \n\nâ€œjailbreakâ€ the model and make it misbehave. With that out of the way, letâ€™s finally get to the actual implementation.\n\n","type":"content","url":"/micrograduate/picogpt#prompting","position":21},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Setup"},"type":"lvl2","url":"/micrograduate/picogpt#setup","position":22},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Setup"},"content":"\n\nLetâ€™s dive right into the GPT implementation. First though, letâ€™s define the necessary functions for downloading the model of our choice and the tokenizer files for  loading encoder, hparams, and params into our code:\n\n\"\"\"Byte pair encoding utilities.\n\nContains the code for OpenAI's BPE Tokenizer, taken straight from their gpt-2 repo: https://github.com/openai/gpt-2/blob/master/src/encoder.py.\n\"\"\"\n\nimport json\nimport os\nfrom functools import lru_cache\nimport regex as re\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1))\n        + list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1))\n        + list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n        self.encoder = encoder\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = errors  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(\n            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n        )\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(\n                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n            )\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\n            \"utf-8\", errors=self.errors\n        )\n        return text\n\n\ndef get_encoder(model_name, models_dir):\n    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n        encoder = json.load(f)\n    with open(\n        os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\"\n    ) as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n    return Encoder(encoder=encoder, bpe_merges=bpe_merges)\n\n\n\n\"\"\"GPT-2 utilities.\n\nContains the code to download and load the GPT-2 model weights, tokenizer, and hyperparameters.\n\"\"\"\n\nimport json\nimport os\nimport requests\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n\ndef download_gpt2_files(model_size, model_dir):\n    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n    for filename in [\n        \"checkpoint\",\n        \"encoder.json\",\n        \"hparams.json\",\n        \"model.ckpt.data-00000-of-00001\",\n        \"model.ckpt.index\",\n        \"model.ckpt.meta\",\n        \"vocab.bpe\",\n    ]:\n        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n        r.raise_for_status()\n\n        with open(os.path.join(model_dir, filename), \"wb\") as f:\n            file_size = int(r.headers[\"content-length\"])\n            chunk_size = 1000\n            with tqdm(\n                ncols=100,\n                desc=\"Fetching \" + filename,\n                total=file_size,\n                unit_scale=True,\n            ) as pbar:\n                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n                for chunk in r.iter_content(chunk_size=chunk_size):\n                    f.write(chunk)\n                    pbar.update(chunk_size)\n\n\ndef load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n    import re\n    import numpy as np\n\n    def set_in_nested_dict(d, keys, val):\n        if not keys:\n            return val\n        if keys[0] not in d:\n            d[keys[0]] = {}\n        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n        return d\n\n    init_vars = tf.train.list_variables(tf_ckpt_path)\n    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n    for name, _ in init_vars:\n        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n        name = name.removeprefix(\"model/\")\n        if name.startswith(\"h\"):\n            m = re.match(r\"h([0-9]+)/(.*)\", name)\n            n = int(m[1])\n            sub_name = m[2]\n            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n        else:\n            set_in_nested_dict(params, name.split(\"/\"), array)\n\n    return params\n\n\ndef load_encoder_hparams_and_params(model_size, models_dir):\n    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n\n    model_dir = os.path.join(models_dir, model_size)\n    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n    if not tf_ckpt_path:  # download files if necessary\n        os.makedirs(model_dir, exist_ok=True)\n        download_gpt2_files(model_size, model_dir)\n        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n\n    encoder = get_encoder(model_size, models_dir)\n    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n\n    return encoder, hparams, params\n\n\n\nCool. Having defined the necessary utilities, we will now define the prompt and generation functions:\n\nimport numpy as np\n\n\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n    pass  # TODO: implement this\n\n\ndef generate(inputs, params, n_head, n_tokens_to_generate):\n    for _ in tqdm(\n        range(n_tokens_to_generate), \"generating\"\n    ):  # auto-regressive decode loop\n        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n        next_id = np.argmax(logits[-1])  # greedy sampling\n        inputs.append(int(next_id))  # append prediction to input\n\n    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n\n\ndef prompt_gpt(\n    prompt: str,\n    n_tokens_to_generate: int = 40,\n    model_size: str = \"124M\",\n    models_dir: str = \"models\",\n):\n    # load encoder, hparams, and params from the released open-ai gpt-2 files\n    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n    # map numpy arrays to jax arrays if jax is installed (in case saved params contain numpy arrays)\n    if np.__name__ == \"jax.numpy\":\n        import jax.tree_util as jtu\n\n        params = jtu.tree_map(\n            lambda x: np.array(x) if isinstance(x, np.ndarray) else x, params\n        )\n    # encode the input string using the BPE tokenizer\n    input_ids = encoder.encode(prompt)\n    # make sure we are not surpassing the max sequence length of our model\n    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n    # generate output ids\n    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n    # decode the ids back into a string\n    output_text = encoder.decode(output_ids)\n    return output_text\n\n\n\nBreaking down each of the 4 sections:\n\nThe gpt2 function is the actual GPT code weâ€™ll be implementing. Youâ€™ll notice that the function signature includes some extra stuff in addition to inputs:\n\nwte, wpe, blocks, and ln_f are the parameters of our model.\n\nn_head is a hyperparameter that is needed during the forward pass.\n\nThe generate function is the autoregressive decoding algorithm we saw earlier. We use greedy sampling for simplicity. tqdm is a progress bar to help us visualize the decoding process as it generates tokens one at a time.\n\nThe prompt_gpt function handles:\n\nLoading the tokenizer (encoder), model weights (params), and hyperparameters (hparams)\n\nEncoding the input prompt into token IDs using the tokenizer\n\nCalling the generate function\n\nDecoding the output IDs into a string\n\n","type":"content","url":"/micrograduate/picogpt#setup","position":23},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Encoder","lvl2":"Setup"},"type":"lvl3","url":"/micrograduate/picogpt#encoder","position":24},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Encoder","lvl2":"Setup"},"content":"\n\nTake a closer look at the following call:encoder, hparams, params = load_encoder_hparams_and_params(\"124M\", \"models\")\n\nThis will download the necessary model and tokenizer files into models/124M and load encoder, hparams, and params into our code. Letâ€™s give it a try (may take some minutes, depending on your connection speed):\n\nencoder, hparams, params = load_encoder_hparams_and_params(\"124M\", \"models\")\n\n\n\nNow, letâ€™s encode a prompt and see the ids it returns. Letâ€™s then decode and verify that we get the same prompt back:\n\nprompt = \"Not all heroes wear capes.\"\nids = encoder.encode(prompt)\nprint(ids)\nprompt_decoded = encoder.decode(ids)\nprint(prompt_decoded)\nassert prompt == prompt_decoded\n\n\n\nWe do, indeed. Cool! Using the vocabulary of the tokenizer (stored in encoder.decoder), we can take a peek at what the actual tokens look like:\n\n[encoder.decoder[i] for i in ids]\n\n\n\nInteresting. Notice how sometimes our tokens are words (e.g. Not), sometimes they are words but with a space in front of them (e.g. Ä all, the \n\nÄ  represents a space), sometimes there are part of a word (e.g. capes is split into Ä cap and es), and sometimes they are punctuation (e.g. .). One nice thing about BPE is that it can encode any arbitrary string. If it encounters something that is not present in the vocabulary, it just breaks it down into substrings it does understand:\n\n[encoder.decoder[i] for i in encoder.encode(\"zjqfl\")]\n\n\n\nWe can also check the size of the vocabulary:\n\nlen(encoder.decoder)\n\n\n\nThe vocabulary, as well as the byte-pair merges which determines how strings are broken down, is obtained by training the tokenizer. When we load the tokenizer, weâ€™re loading the already trained vocab and byte-pair merges from some files, which were downloaded alongside the model files when we ran load_encoder_hparams_and_params. See the files models/124M/encoder.json (the vocabulary) and models/124M/vocab.bpe (byte-pair merges).\n\n","type":"content","url":"/micrograduate/picogpt#encoder","position":25},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Hyperparameters","lvl2":"Setup"},"type":"lvl3","url":"/micrograduate/picogpt#hyperparameters","position":26},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Hyperparameters","lvl2":"Setup"},"content":"\n\nhparams is a dictionary that contains the hyper-parameters of our model:\n\nhparams\n\n\n\nHereâ€™s what each key refers to:\n\nn_vocab: number of tokens in our vocabulary\n\nn_ctx: maximum possible sequence length of the input\n\nn_embd: embedding dimension (determines the â€œwidthâ€ of the network)\n\nn_head: number of attention heads (n_embd must be divisible by n_head)\n\nn_layer: number of layers (determines the â€œdepthâ€ of the network)\n\nWeâ€™ll use these symbols in our codeâ€™s comments to show the underlying shape of things. Weâ€™ll also use n_seq to denote the length of our input sequence (i.e. n_seq = len(inputs)).\n\n","type":"content","url":"/micrograduate/picogpt#hyperparameters","position":27},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Parameters","lvl2":"Setup"},"type":"lvl3","url":"/micrograduate/picogpt#parameters","position":28},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Parameters","lvl2":"Setup"},"content":"\n\nparams is a nested json dictionary that hold the trained weights of our model. The leaf nodes of the json are NumPy arrays. If we print params, replacing the arrays with their shapes, we get:\n\nimport numpy as np\n\n\ndef shape_tree(d):\n    if isinstance(d, np.ndarray):\n        return list(d.shape)\n    elif isinstance(d, list):\n        return [shape_tree(v) for v in d]\n    elif isinstance(d, dict):\n        return {k: shape_tree(v) for k, v in d.items()}\n    else:\n        ValueError(\"uh oh\")\n\n\nshape_tree(params)\n\n\n\nwhere each dictionary inside the 'blocks' list contains the weights information for each layer (n_layers total). These weights are loaded from the original OpenAI tensorflow checkpoint:\n\nimport tensorflow as tf\n\ntf_ckpt_path = tf.train.latest_checkpoint(\"models/124M\")\nfor name, _ in tf.train.list_variables(tf_ckpt_path):\n    arr = tf.train.load_variable(tf_ckpt_path, name).squeeze()\n    print(f\"{name}: {arr.shape}\")\n\n\n\n\n\nThe load_gpt2_params_from_tf_ckpt function converts the above tensorflow variables into our params dictionary. For reference, hereâ€™s the shapes of params but with the numbers replaced by the hparams they represent:\n\n{\n    \"wpe\": [n_ctx, n_embd],\n    \"wte\": [n_vocab, n_embd],\n    \"ln_f\": {\"b\": [n_embd], \"g\": [n_embd]},\n    \"blocks\": [\n        {\n            \"attn\": {\n                \"c_attn\": {\"b\": [3*n_embd], \"w\": [n_embd, 3*n_embd]},\n                \"c_proj\": {\"b\": [n_embd], \"w\": [n_embd, n_embd]},\n            },\n            \"ln_1\": {\"b\": [n_embd], \"g\": [n_embd]},\n            \"ln_2\": {\"b\": [n_embd], \"g\": [n_embd]},\n            \"mlp\": {\n                \"c_fc\": {\"b\": [4*n_embd], \"w\": [n_embd, 4*n_embd]},\n                \"c_proj\": {\"b\": [n_embd], \"w\": [4*n_embd, n_embd]},\n            },\n        },\n        ... # repeat for n_layers\n    ]\n}\n\nYouâ€™ll probably want to come back to reference this dictionary to check the shape of the weights as we implement our GPT. Weâ€™ll match the variable names in our code with the keys of this dictionary for consistency.\n\n","type":"content","url":"/micrograduate/picogpt#parameters","position":29},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Basic Layers"},"type":"lvl2","url":"/micrograduate/picogpt#basic-layers","position":30},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Basic Layers"},"content":"\n\nLast thing before we get into the actual GPT architecture itself, letâ€™s implement some of the more basic nn layers that are non-specific to GPTs.\n\n","type":"content","url":"/micrograduate/picogpt#basic-layers","position":31},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"GELU","lvl2":"Basic Layers"},"type":"lvl3","url":"/micrograduate/picogpt#gelu","position":32},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"GELU","lvl2":"Basic Layers"},"content":"\n\nThe non-linearity (activation function) of choice for GPT-2 is GELU (Gaussian Error Linear Units), an alternative for ReLU:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"fig_1_from_gelu_paper.png\"))\n\n\n\nIt is approximated by the following function:\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\n\n\nLike ReLU, GELU operates element-wise on the input:\n\ngelu(np.array([[1, 2], [-2, 0.5]]))\n\n\n\n","type":"content","url":"/micrograduate/picogpt#gelu","position":33},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Softmax","lvl2":"Basic Layers"},"type":"lvl3","url":"/micrograduate/picogpt#softmax","position":34},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Softmax","lvl2":"Basic Layers"},"content":"\n\nGood ole \n\nsoftmax:\n\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\n\nWe use the \n\nmax(x) trick for numerical stability. Softmax is used to a convert set of real numbers (between -\\infty and \\infty) to probabilities (between 0 and 1, with the numbers all summing to 1). We apply softmax over the last axis of the input.\n\nx = softmax(np.array([[2, 100], [-5, 0]]))\nx\n\n\n\nx.sum(axis=-1)\n\n\n\n","type":"content","url":"/micrograduate/picogpt#softmax","position":35},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Layer Normalization","lvl2":"Basic Layers"},"type":"lvl3","url":"/micrograduate/picogpt#layer-normalization","position":36},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Layer Normalization","lvl2":"Basic Layers"},"content":"\n\nLayer normalization standardizes values to have a mean of 0 and a variance of 1:\\text{LayerNorm}(x) = \\gamma\\cdot\\frac{x - \\mu}{\\sigma} + \\beta\n\nwhere \\mu is the mean of x, \\sigma^2 is the variance of x, and \\gamma and \\beta are learnable parameters.\n\ndef layer_norm(x, g, b, eps: float = 1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(\n        variance + eps\n    )  # normalize x to have mean=0 and var=1 over last axis\n    return g * x + b  # scale and offset with gamma/beta params\n\n\n\nLayer normalization ensures that the inputs for each layer are always within a consistent range, which is supposed to speed up and stabilize the training process. Like \n\nbatchnorm, the normalized output is then scaled and offset with two learnable vectors \\gamma and \\beta. The small epsilon term in the denominator (eps) is used to avoid a division by zero error. Layer norm is used instead of batch norm in the transformer for \n\nvarious reasons. The differences between various normalization techniques is outlined in \n\nthis excellent blog post. We apply layer normalization over the last axis of the input:\n\nx = np.array([[2, 2, 3], [-5, 0, 1]])\nx = layer_norm(x, g=np.ones(x.shape[-1]), b=np.zeros(x.shape[-1]))\nprint(x)\nprint(\"var:\", x.var(axis=-1))  # yields floating point shenanigans\nprint(\"mean:\", x.mean(axis=-1))  # same here\n\n\n\n","type":"content","url":"/micrograduate/picogpt#layer-normalization","position":37},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Linear","lvl2":"Basic Layers"},"type":"lvl3","url":"/micrograduate/picogpt#linear","position":38},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Linear","lvl2":"Basic Layers"},"content":"\n\nYour standard matrix multiplication + bias:\n\ndef linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n    return x @ w + b\n\n\n\nLinear layers are often referred to as projections (since they are projecting from one vector space to another vector space):\n\nx = np.random.normal(size=(64, 784))  # input dim = 784, batch/sequence dim = 64\nw = np.random.normal(size=(784, 10))  # output dim = 10\nb = np.random.normal(size=(10,))\nprint(x.shape)  # shape before linear projection\nprint(linear(x, w, b).shape)  # shape after linear projection\n\n\n\n","type":"content","url":"/micrograduate/picogpt#linear","position":39},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"GPT Architecture"},"type":"lvl2","url":"/micrograduate/picogpt#gpt-architecture","position":40},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"GPT Architecture"},"content":"\n\nThe GPT architecture follows that of the transformer:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"fig_1_from_attention_is_all_you_need_paper.png\"))\n\n\n\nBut uses only the decoder stack (the right part of the diagram):\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"transformer_decoder_stack.png\"))\n\n\n\nNote, the middle â€œcross-attentionâ€ layer is also removed since we got rid of the encoder. At a high level, the GPT architecture has three sections:\n\nText + positional embeddings\n\nA transformer decoder stack\n\nA projection to vocab step\n\nIn code, it looks like this:\n\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n    # token + positional embeddings\n    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n\n    # forward pass through n_layer transformer blocks\n    for block in blocks:\n        x = transformer_block(\n            x, **block, n_head=n_head\n        )  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # projection to vocab\n    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n\n\n\nLetâ€™s break down each of these three sections into more detail.\n\n","type":"content","url":"/micrograduate/picogpt#gpt-architecture","position":41},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Embeddings","lvl2":"GPT Architecture"},"type":"lvl3","url":"/micrograduate/picogpt#embeddings","position":42},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Embeddings","lvl2":"GPT Architecture"},"content":"\n\n","type":"content","url":"/micrograduate/picogpt#embeddings","position":43},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Token Embeddings","lvl3":"Embeddings","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#token-embeddings","position":44},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Token Embeddings","lvl3":"Embeddings","lvl2":"GPT Architecture"},"content":"\n\nToken IDs by themselves are not very good representations for a nn. For one, the relative magnitudes of the token IDs falsely communicate information (for example, if Apple = 5 and Table = 10 in our vocab, then we are implying that 2 * Apple = Table). Secondly, a single number is not a lot of dimensionality for a nn to work with. To address these limitations, weâ€™ll take advantage of \n\nword vectors, specifically via a learned embedding matrix:wte[inputs]  # [n_seq] -> [n_seq, n_embd]\n\nRecall, wte is a [n_vocab, n_embd] matrix. It acts as a lookup table, where the ith row in the matrix corresponds to the learned vector for the ith token in our vocabulary. wte[inputs] uses \n\ninteger array indexing to retrieve the vectors corresponding to each token in our input. Like any other parameter in our network, wte is learned. That is, it is randomly initialized at the start of training and then updated via gradient descent.\n\n","type":"content","url":"/micrograduate/picogpt#token-embeddings","position":45},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Positional Embeddings","lvl3":"Embeddings","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#positional-embeddings","position":46},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Positional Embeddings","lvl3":"Embeddings","lvl2":"GPT Architecture"},"content":"\n\nOne quirk of the transformer architecture is that it doesnâ€™t take into account position. That is, if we randomly shuffled our input and then accordingly unshuffled the output, the output would be the same as if we never shuffled the input in the first place (the ordering of inputs doesnâ€™t have any effect on the output). Of course, the ordering of words is a crucial part of language (duh), so we need some way to encode positional information into our inputs. For this, we can just use another learned embedding matrix:wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n\nRecall, wpe is a [n_ctx, n_embd] matrix. The ith row of the matrix contains a vector that encodes information about the ith position in the input. Similar to wte, this matrix is learned during gradient descent. Notice, this restricts our model to a maximum sequence length of n_ctx. That is, len(inputs) <= n_ctx must hold.\n\nNote: The original transformer paper used a \n\ncalculated positional embedding which they found performed just as well as learned positional embeddings, but has the distinct advantage that you can input any arbitrarily long sequence (you are not restricted by a maximum sequence length). However, in practice, your model is only going to be as the good sequence lengths that it was trained on. You canâ€™t just train a GPT on sequences that are 1024 long and then expect it to perform well at 16k tokens long. Recently however, there has been some success with relative positional embeddings, such as \n\nAlibi and \n\nRoPE.\n\n","type":"content","url":"/micrograduate/picogpt#positional-embeddings","position":47},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Combined","lvl3":"Embeddings","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#combined","position":48},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Combined","lvl3":"Embeddings","lvl2":"GPT Architecture"},"content":"\n\nWe can add our token and positional embeddings to get a combined embedding that encodes both token and positional information:# token + positional embeddings\nx = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n\n# x[i] represents the word embedding for the ith word + the positional\n# embedding for the ith position\n\n","type":"content","url":"/micrograduate/picogpt#combined","position":49},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Decoder Stack","lvl2":"GPT Architecture"},"type":"lvl3","url":"/micrograduate/picogpt#decoder-stack","position":50},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Decoder Stack","lvl2":"GPT Architecture"},"content":"\n\nThis is where all the magic happens and the â€œdeepâ€ in deep learning comes in. We pass our embedding through a stack of n_layer transformer decoder blocks:# forward pass through n_layer transformer blocks\nfor block in blocks:\n    x = transformer_block(\n        x, **block, n_head=n_head\n    )  # [n_seq, n_embd] -> [n_seq, n_embd]\n\nStacking more layers is what allows us to control how deep our network is. GPT-3 for example, has a whopping 96 layers. On the other hand, choosing a larger n_embd value allows us to control how wide our network is (for example, GPT-3 uses an embedding size of 12288).\n\n","type":"content","url":"/micrograduate/picogpt#decoder-stack","position":51},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Projection to Vocab","lvl2":"GPT Architecture"},"type":"lvl3","url":"/micrograduate/picogpt#projection-to-vocab","position":52},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Projection to Vocab","lvl2":"GPT Architecture"},"content":"\n\nIn our final step, we project the output of the final transformer block to a probability distribution over our vocab:# projection to vocab\nx = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\nreturn x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n\nCouple things to note here:\n\nWe first pass x through a final layer normalization layer before doing the projection to vocab. This is specific to the GPT-2 architecture (this is not present in the original GPT and Transformer papers).\n\nWe are reusing the embedding matrix wte for the projection.  Other GPT implementations may choose to use a separate learned weight matrix for the projection, however sharing the embedding matrix has a couple of advantages:\n\nYou save some parameters (although at GPT-3 scale, this is negligible).\n\nSince the matrix is both responsible for mapping both to words and from words, in theory, it may learn a richer representation compared to having two separate matrixes.\n\nWe donâ€™t apply softmax at the end, so our outputs will be logits instead of probabilities between 0 and 1. This is done for several reasons:\n\nsoftmax is \n\nmonotonic, so for greedy sampling np.argmax(logits) is equivalent to np.argmax(softmax(logits)) making softmax redundant\n\nsoftmax is irreversible, meaning we can always go from logits to probabilities by applying softmax, but we canâ€™t go back to logits from probabilities, so for maximum flexibility, we output the logits\n\nNumerically stability (for example, to compute cross entropy loss, taking log(softmax(logits)) is numerically unstable compared to log_softmax(logits)\n\nThe projection to vocab step is also sometimes called the language modeling head. What does â€œheadâ€ mean? Once your GPT is pre-trained, you can swap out the language modeling head with some other kind of projection, like a classification head for fine-tuning the model on some classification task. So your model can have multiple heads, kind of like a \n\nhydra. So thatâ€™s the GPT architecture at a high level, letâ€™s actually dig a bit deeper into what the decoder blocks are doing.\n\n","type":"content","url":"/micrograduate/picogpt#projection-to-vocab","position":53},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Decoder Block","lvl2":"GPT Architecture"},"type":"lvl3","url":"/micrograduate/picogpt#decoder-block","position":54},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Decoder Block","lvl2":"GPT Architecture"},"content":"\n\nThe transformer decoder block consists of two sublayers:\n\nMulti-head causal self attention\n\nPosition-wise feed forward nn\n\ndef transformer_block(\n    x, mlp, attn, ln_1, ln_2, n_head\n):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # multi-head causal self attention\n    x = x + mha(\n        layer_norm(x, **ln_1), **attn, n_head=n_head\n    )  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # position-wise feed forward network\n    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n    return x\n\n\n\nEach sublayer utilizes layer normalization on their inputs as well as a residual connection (i.e. add the input of the sublayer to the output of the sublayer). Some things to note:\n\nMulti-head causal self attention is what facilitates the communication between the inputs. Nowhere else in the network does the model allow inputs to â€œseeâ€ each other. The embeddings, position-wise feed forward network, layer norms, and projection to vocab all operate on our inputs position-wise. Modeling relationships between inputs is tasked solely to attention.\n\nThe Position-wise feed forward nn is just a regular 2 layer fully connected nn. This just adds a bunch of learnable parameters for our model to work with to facilitate learning.\n\nIn the original transformer paper, layer norm is placed on the output layer_norm(x + sublayer(x)) while we place layer norm on the input x + sublayer(layer_norm(x)) to match GPT-2. This is referred to as pre-norm and has been shown to be \n\nimportant in improving the performance of the transformer.\n\nResidual connections (popularized by ResNet) serve a couple of different purposes:\n\nMakes it easier to optimize nns that are deep (i.e. networks that have lots of layers). The idea here is that we are providing â€œshortcutsâ€ for the gradients to flow back through the network, making it easier to optimize the earlier layers in the network.\n\nWithout residual connections, deeper models see a degradation in performance when adding more layers (possibly because itâ€™s hard for the gradients to flow all the way back through a deep network without losing information). Residual connections seem to give a bit of an accuracy boost for deeper networks.\n\nCan help with the \n\nvanishing/exploding gradients problem.\n\nLetâ€™s dig a little deeper into the 2 sublayers.\n\n","type":"content","url":"/micrograduate/picogpt#decoder-block","position":55},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Position-wise Feed Forward Network","lvl2":"GPT Architecture"},"type":"lvl3","url":"/micrograduate/picogpt#position-wise-feed-forward-network","position":56},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Position-wise Feed Forward Network","lvl2":"GPT Architecture"},"content":"\n\nThis is just a simple multi-layer perceptron with 2 layers:\n\ndef ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # project up\n    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n\n    # project back down\n    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n\n    return x\n\n\n\nNothing super fancy here, we just project from n_embd up to a higher dimension 4*n_embd and then back down to n_embd[4].\n\nNote: Different GPT models may choose a different hidden width that is not 4*n_embd, however this is the common practice for GPT models. Also, we give the multi-head attention layer a lot of attention (pun intended) for driving the success of the transformer, but at the scale of GPT-3, 80\\% of the model parameters are contained in the feed forward layer. Just something to think about.\n\nRecall, from our params dictionary, that our mlp params look like this:\"mlp\": {\n    \"c_fc\": {\"b\": [4*n_embd], \"w\": [n_embd, 4*n_embd]},\n    \"c_proj\": {\"b\": [n_embd], \"w\": [4*n_embd, n_embd]},\n},\n\n","type":"content","url":"/micrograduate/picogpt#position-wise-feed-forward-network","position":57},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl3","url":"/micrograduate/picogpt#multi-head-causal-self-attention","position":58},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nThis layer is probably the most difficult part of the transformer to understand. So letâ€™s work our way up to â€œMulti-Head Causal Self Attentionâ€ by breaking each word down into its own section:\n\nAttention\n\nSelf\n\nCausal\n\nMulti-Head\n\n","type":"content","url":"/micrograduate/picogpt#multi-head-causal-self-attention","position":59},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#attention","position":60},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nChatGPT and other large language models use a special type of nn called the transformer. The transformer defining feature is the attention mechanism. Attention is defined by the equation:\\text{attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\nAttention can come in different forms, but this version of attention (known as scaled dot product attention) was first proposed in the \n\noriginal transformer paper. In this post, weâ€™ll build an intuition for the above equation by deriving it from the ground up.\n\n","type":"content","url":"/micrograduate/picogpt#attention","position":61},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Key-Value Lookups","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl5","url":"/micrograduate/picogpt#key-value-lookups","position":62},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Key-Value Lookups","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nA key-value (kv) lookup involves three components:\n\nA list of n_k keys\n\nA list of n_k values (that map 1-to-1 with the keys, forming key-value pairs)\n\nA query, for which we want to match with the keys and get some value based on the match\n\nYouâ€™re probably familiar with this concept as a dictionary or hash map:\n\nd = {\n    \"apple\": 10,\n    \"banana\": 5,\n    \"chair\": 2,\n}\nprint(d.keys())\nprint(d.values())\nquery = \"apple\"\nd[query]\n\n\n\n\n\nDictionaries let us perform lookups based on an exact string match. What if instead we wanted to do a lookup based on the meaning of a word?\n\n","type":"content","url":"/micrograduate/picogpt#key-value-lookups","position":63},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Key-Value Lookups based on Meaning","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl5","url":"/micrograduate/picogpt#key-value-lookups-based-on-meaning","position":64},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Key-Value Lookups based on Meaning","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nSay we wanted to look up the word â€œfruitâ€ in our previous example, how do we choose which key is the best match? Itâ€™s obviously not â€œchairâ€, but both â€œappleâ€ and â€œbananaâ€ seem like a good match. Itâ€™s hard to choose one or the other, fruit feels more like a combination of apple and banana rather than a strict match for either. So, letâ€™s not choose. Instead, weâ€™ll do exactly that, take a combination of apple and banana. For example, say we assign a 60\\% meaning based match for apple, a 40\\% match for banana, and 0\\% match for chair. We compute our final output value as the weighted sum of the values with the percentages:\n\nquery = \"fruit\"\nd = {\"apple\": 10, \"banana\": 5, \"chair\": 2}\n0.6 * d[\"apple\"] + 0.4 * d[\"banana\"] + 0.0 * d[\"chair\"]\n\n\n\nIn a sense, we are determining how much attention our query should be paying to each key-value pair based on meaning. The amount of â€œattentionâ€ is represented as a decimal percentage, called an attention score. Mathematically, we can define our output as a simple weighted sum:\\sum_{i} \\alpha_i v_i\n\nwhere \\alpha_i is our attention score for the ith kv pair and v_i is the ith value. Remember, the attention scores are decimal percentages, that is they must be between 0 and 1 inclusive (0 \\leq \\alpha_i \\leq 1) and their sum must be 1 (\\sum_{i} \\alpha_i = 1). Okay, but where did we get these attention scores from? In our example, we kind of chose them based on what we felt. While we did a pretty good job, this approach doesnâ€™t seem sustainable. Instead, letâ€™s take a look at how word vectors can help solve our problem of determining attention scores.\n\n","type":"content","url":"/micrograduate/picogpt#key-value-lookups-based-on-meaning","position":65},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Word Vectors and Similarity","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl5","url":"/micrograduate/picogpt#word-vectors-and-similarity","position":66},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Word Vectors and Similarity","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nImagine we represent a word with a vector of numbers. Ideally, the values in the vector should in some way capture the meaning of the word it represents. For example, imagine we have the following word vectors (visualized in 2D space):\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"2d_word_vectors_example.png\"))\n\n\n\nYou can see that words that are similar are clustered together. Fruits are clustered at the top right, vegetables are clustered at the top left, and furniture is clustered at the bottom. In fact, you can even see that the vegetable and fruit clusters are closer to each other than they are to the furniture cluster, since they are more closely related things. You can even imagine doing arithmetic on word vectors. For example, given the words â€œkingâ€, â€œqueenâ€, â€œmanâ€, and â€œwomanâ€ and their respective vector representations v_{king}, v_{queen}, v_{man}, and v_{woman}, we can imagine that:v_{queen} - v_{woman} + v_{man} \\sim v_{king}\n\nThat is, the vector for â€œqueenâ€ minus â€œwomanâ€ plus â€œmanâ€ should result in a vector that is similar to the vector for â€œkingâ€. But what does it exactly mean for two vectors to be similar? In the fruits/vegetables example, we were using distance as a measure of similarity (in particular, \n\neuclidean distance). There are also \n\nother ways to measure similarity between two vectors, each with its own advantages and disadvantages. Possibly the simplest measure of similarity between two vectors is their dot product:\\textbf{v} \\cdot \\textbf{w} = \\sum_{i} v_i w_i\n\n3blue1brown has a great video on the intuition behind dot product, but for our purposes all we need to know is:\n\nIf two vectors are pointing in the same direction, the dot product will be > 0 (i.e. similar)\n\nIf they are pointing in opposing directions, the dot product will be < 0 (i.e. dissimilar)\n\nIf they are exactly perpendicular, the dot product will be 0 (i.e. neutral)\n\nUsing this information, we can define a simple heuristic to determine the similarity between two word vectors: The greater the dot product, the more similar two words are in meaning.\n\nNote: Youâ€™ll note that the magnitude of the vectors have an influence on the output of dot product. For example, given 3 vectors, a = [1, 1, 1], b = [1000, 0, 0], c = [2, 2, 2], our dot product heuristic would tell us that because a \\cdot b > a \\cdot c, that a is more similar to c than a is to b. This doesnâ€™t seem right, since a and b are pointing in the exact same direction, while a and c are not. \n\nCosine similarity accounts for this normalizing the vectors to unit vectors before taking the dot product, essentially ignoring the magnitudes and only caring about the direction. So why donâ€™t we take the cosine similarity? In a deep learning setting, the magnitude of a vector might actually contain information we care about (and we shouldnâ€™t get rid of it). Also, if we regularize our networks properly, outlier examples like the above should not occur.\n\nOkay cool, but where do these word vectors actually come from? They usually come from some kind of learned embedding or latent representation. That is, initially the word vectors are just random numbers, but as the nn is trained, their values are adjusted to become better and better representations for words.\n\n","type":"content","url":"/micrograduate/picogpt#word-vectors-and-similarity","position":67},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Attention Scores using the Dot Product","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl5","url":"/micrograduate/picogpt#attention-scores-using-the-dot-product","position":68},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Attention Scores using the Dot Product","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nLetâ€™s return to our example of fruits, but this time around using word vectors to represent our words. That is \\textbf{q} = \\textbf{v}_{fruit} and \\textbf{k} = [\\textbf{v}_{apple} \\textbf{v}_{banana} \\textbf{v}_{chair}], such that \\textbf{v} \\in \\mathbb{R}^{d_k} (that is each vector has the same dimensionality of d_k, which is a value we choose when training a nn). Using our new dot product similarity measure, we can compute the similarity between the query and the ith key as:\\textbf{x}_i = \\textbf{q} \\cdot \\textbf{k}_i\n\nwhere \\textbf{a}_i is the attention score for the ith key-value pair.  Generalizing this further, we can compute the dot product for all n_k keys with:\\textbf{x} = \\textbf{q} \\cdot \\textbf{K}^T\n\nwhere \\textbf{x} is our vector of dot products \\textbf{x} = [x_1, x_2, ..., x_{n_k - 1}, x_{n_k}] and K is a row-wise matrix of our key vectors (i.e. our key vectors stacked on-top of each-other to form a n_k \\times d_k matrix such that k_i is the ith row of K). If youâ€™re having trouble understanding this, hereâ€™s an explanation:\n\nBasically, instead of computing each dot product separately:x_1 = \\textbf{q} \\cdot \\textbf{k}_1 = [2, 1, 3] \\cdot [-1, 2, -1] = -3x_2 = \\textbf{q} \\cdot \\textbf{k}_2 = [2, 1, 3] \\cdot [1.5, 0, -1] = 0x_3 = \\textbf{q} \\cdot \\textbf{k}_3 = [2, 1, 3] \\cdot [4, -2, -1] = 3\n\nYou compute it all at once:\\textbf{x} = \\textbf{q} \\cdot \\textbf{K}^T = [2, 1, 3] \\cdot \\begin{bmatrix} -1 & 2 & -1 \\\\ 1.5 & 0 & -1 \\\\ 4 & -2 & -1 \\end{bmatrix}^T = [2, 1, 3] \\cdot \\begin{bmatrix} -1 & 1.5 & 4 \\\\ 2 & 0 & -2 \\\\ -1 & -1 & -1 \\end{bmatrix} = [-3, 0, 3] = [x_1, x_2, x_3]\n\nNow, recall that our attention scores need to be decimal percentages (between 0 and 1 and sum to 1). Our dot product values however can be any real number (i.e. between -\\infty and \\infty). To transform our dot product values to decimal percentages, weâ€™ll use the softmax function:\\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n\ne.g.\n\nsoftmax(np.array([4.0, -1.0, 2.1]))\n\n\n\nNotice:\n\nâœ… Each number is between 0 and 1\n\nâœ… The numbers sum to 1\n\nâœ… The larger valued inputs get more â€œweightâ€\n\nâœ… The sorted order is preserved (i.e. the 4.0 is still the largest after softmax, and -1.0 is still the lowest), this is because softmax is a \n\nmonotonic function\n\nThis satisfies all the desired properties of an attention scores. Thus, we can compute the attention score for the ith key-value pair with:a_i = \\text{softmax}(x)_i = \\text{softmax}(\\textbf{q} \\textbf{K}^T)_i\n\nPlugging this into our weighted sum we get:\\sum_{i} a_i v_i = \\sum_{i} \\text{softmax}(\\textbf{x})_i v_i = \\sum_{i} \\text{softmax}(\\textbf{q} \\textbf{K}^T)_i v_i = \\text{softmax}(\\textbf{q} \\textbf{K}^T) \\textbf{v}\n\nNote: In the last step, we pack our values into a vector \\textbf{v} = [v_1, v_2, ..., v_{n_k - 1}, v_{n_k}], which allows us to get rid of the summation notation in favor of a dot product. And thatâ€™s it, we have a full working definition for attention:\\text{attention}(\\textbf{q}, K, \\textbf{v}) = \\text{softmax}(\\textbf{qK}^T)\\textbf{v}\n\nIn code:\n\ndef get_word_vector(word, d_k=8):\n    \"\"\"Hypothetical mapping that returns a word vector of size\n    d_k for the given word. For demonstrative purposes, we initialize\n    this vector randomly, but in practice this would come from a learned\n    embedding or some kind of latent representation.\"\"\"\n    return np.random.normal(size=(d_k,))\n\n\ndef attention(q, K, v):\n    # assumes q is a vector of shape (d_k)\n    # assumes K is a matrix of shape (n_k, d_k)\n    # assumes v is a vector of shape (n_k)\n    return softmax(q @ K.T) @ v\n\n\ndef kv_lookup(query, keys, values):\n    return attention(\n        q=get_word_vector(query),\n        K=np.array([get_word_vector(key) for key in keys]),\n        v=values,\n    )\n\n\n# returns some float number\nprint(kv_lookup(\"fruit\", [\"apple\", \"banana\", \"chair\"], [10, 5, 2]))\n\n\n\n","type":"content","url":"/micrograduate/picogpt#attention-scores-using-the-dot-product","position":69},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Scaled Dot Product Attention","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl5","url":"/micrograduate/picogpt#scaled-dot-product-attention","position":70},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl5":"Scaled Dot Product Attention","lvl4":"Attention","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nIn principle, the attention equation we derived in the last section is complete. However, weâ€™ll need to make a couple of changes to match the version in \n\nAttention is All You Need.\n\nValues as Vectors\n\nCurrently, our values in the key-value pairs are just numbers. However, we could also instead replace them with vectors of some size d_v. For example, with d_v = 4, you might have:d = {\n    \"apple\": [0.9, 0.2, -0.5, 1.0]\n    \"banana\": [1.2, 2.0, 0.1, 0.2]\n    \"chair\": [-1.2, -2.0, 1.0, -0.2]\n}\n\nWhen we compute our output via a weighted sum, weâ€™d be doing a weighted sum over vectors instead of numbers (i.e. scalar-vector multiplication instead of scalar-scalar multiplication). This is desirable because vectors let us hold/convey more information than just a single number. To adjust for this change in our equation, instead of multiplying our attention scores by a vector v, we multiply it by the row-wise matrix of our value vectors V (similar to how we stacked our keys to form K):\\text{attention}(\\textbf{q}, K, V) = \\text{softmax}(\\textbf{qK}^T)V\n\nOf course, our output is no longer a scalar, instead it would be a vector of dimensionality d_v.\n\nScaling\n\nThe dot product between our query and keys can get really large in magnitude if d_k is large. This makes the output of softmax more extreme. For example, softmax([3, 2, 1]) = [0.665, 0.244, 0.090], but with larger values softmax([30, 20, 10]) = [9.99954600e-01, 4.53978686e-05, 2.06106005e-09]. When training a nn, this would mean the gradients would become really small which is undesirable. As a solution, we scale our pre-softmax scores by \\frac{1}{\\sqrt{d_k}}:\\text{attention}(\\textbf{q}, K, V) = \\text{softmax}(\\frac{\\textbf{qK}^T}{\\sqrt{d_k}})V\n\nMultiple Queries\n\nIn practice, we often want to perform multiple lookups for n_q different queries rather than just a single query. Of course, we could always do this one at a time, plugging each query individually into the above equation. However, if we stack of query vectors row-wise as a matrix Q (in the same way we did for K and V), we can compute our output as an n_q \\times d_v matrix where row i is the output vector for the attention on the ith query:\\text{attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\nthat is, \\text{attention}(Q, K, V)_i = \\text{attention}(q_i, K, V)\n\nThis makes computation faster than if we ran attention for each query sequentially (say, in a for loop) since we can parallelize calculations (particularly when using a GPU). Note, our input to softmax becomes a matrix instead of a vector. When we write softmax here, we mean that we are taking the softmax along each row of the matrix independently, as if we were doing things sequentially.\n\nResult\n\nWith that, we have our final equation for scaled dot product attention as itâ€™s written in the \n\noriginal transformer paper:\\text{attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\nIn code:\n\ndef attention(q, k, v):  # [n_q, d_k], [n_k, d_k], [n_k, d_v] -> [n_q, d_v]\n    # assumes q is a matrix of shape [n_q, d_k]\n    # assumes k is a matrix of shape [n_k, d_k]\n    # assumes v is a matrix of shape [n_k, d_v]\n    # output is a matrix of shape [n_q, d_v]\n    d_k = k.shape[-1]\n    return softmax(q @ k.T / np.sqrt(d_k)) @ v\n\n\n\n","type":"content","url":"/micrograduate/picogpt#scaled-dot-product-attention","position":71},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Self","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#self","position":72},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Self","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nWhen q, k, and v all come from the same source, we are performing \n\nself-attention (i.e. letting our input sequence attend to itself):\n\ndef self_attention(x):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    return attention(q=x, k=x, v=x)\n\n\n\nFor example, if our input is Jay went to the store, he bought 10 apples., we would be letting the word he attend to all the other words, including Jay, meaning the model can learn to recognize that he is referring to Jay. We can enhance self attention by introducing projections for q, k, v and the attention output:\n\ndef self_attention(x, w_k, w_q, w_v, w_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # qkv projections\n    q = x @ w_q  # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n    k = x @ w_k  # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n    v = x @ w_v  # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n\n    # perform self attention\n    x = attention(q, k, v)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # out projection\n    x = x @ w_proj  # [n_seq, n_embd] @ [n_embd, n_embd] -> [n_seq, n_embd]\n\n    return x\n\n\n\nThis enables our model to learn a mapping for q, k, and v that best helps attention distinguish relationships between inputs. We can reduce the number of matrix multiplication from 4 to just 2 if we combine w_q, w_k and w_v into a single matrix w_fc, perform the projection, and then split the result:\n\ndef self_attention(x, w_fc, w_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # qkv projections\n    x = x @ w_fc  # [n_seq, n_embd] @ [n_embd, 3*n_embd] -> [n_seq, 3*n_embd]\n\n    # split into qkv\n    q, k, v = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n\n    # perform self attention\n    x = attention(q, k, v)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # out projection\n    x = x @ w_proj  # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n\n    return x\n\n\n\nThis is a bit more efficient as modern accelerators (GPUs) can take better advantage of one large matrix multiplication rather than 3 separate small ones happening sequentially. Finally, we add bias vectors to match the implementation of GPT-2, use our linear function, and rename our parameters to match our params dictionary:\n\ndef self_attention(x, c_attn, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # qkv projections\n    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n\n    # split into qkv\n    q, k, v = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n\n    # perform self attention\n    x = attention(q, k, v)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # out projection\n    x = linear(x, **c_proj)  # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n\n    return x\n\n\n\nRecall, from our params dictionary, our attn params look like this:\"attn\": {\n    \"c_attn\": {\"b\": [3*n_embd], \"w\": [n_embd, 3*n_embd]},\n    \"c_proj\": {\"b\": [n_embd], \"w\": [n_embd, n_embd]},\n},\n\n","type":"content","url":"/micrograduate/picogpt#self","position":73},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Causal","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#causal","position":74},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Causal","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nThere is a bit of an issue with our current self-attention setup, our inputs can see into the future! For example, if our input is [\"not\", \"all\", \"heroes\", \"wear\", \"capes\"], during self attention we are allowing \"wear\" to see \"capes\". This means our output probabilities for \"wear\" will be biased since the model already knows the correct answer is \"capes\". This is no good since our model will just learn that the correct answer for input i can be taken from input i+1. To prevent this, we need to somehow modify our attention matrix to hide or mask our inputs from being able to see into the future. For example, letâ€™s pretend our attention matrix looks like this:       not    all    heroes wear   capes\n   not 0.116  0.159  0.055  0.226  0.443\n   all 0.180  0.397  0.142  0.106  0.175\nheroes 0.156  0.453  0.028  0.129  0.234\n  wear 0.499  0.055  0.133  0.017  0.295\n capes 0.089  0.290  0.240  0.228  0.153\n\nEach row corresponds to a query and the columns to a key. In this case, looking at the row for wear, you can see that it is attending to capes in the last column with a weight of 0.295. To prevent this, we want to set that entry to 0.000:        not    all    heroes wear   capes\n   not 0.116  0.159  0.055  0.226  0.443\n   all 0.180  0.397  0.142  0.106  0.175\nheroes 0.156  0.453  0.028  0.129  0.234\n  wear 0.499  0.055  0.133  0.017  0.000\n capes 0.089  0.290  0.240  0.228  0.153\n\nIn general, to prevent all the queries in our input from looking into the future, we set all positions i, j where j > i to 0.000:        not    all    heroes wear   capes\n   not 0.116  0.000  0.000  0.000  0.000\n   all 0.180  0.397  0.000  0.000  0.000\nheroes 0.156  0.453  0.028  0.000  0.000\n  wear 0.499  0.055  0.133  0.017  0.000\n capes 0.089  0.290  0.240  0.228  0.153\n\nWe call this masking. One issue with our above masking approach is our rows no longer sum to 1 (since we are setting them to 0 after the softmax has been applied). To make sure our rows still sum to 1, we need to modify our attention matrix before the softmax is applied. This can be achieved by setting entries that are to be masked to -\\infty prior to the softmax.\n\nNote: If youâ€™re not convinced, stare at the softmax equation and convince yourself this is true (maybe even pull out a pen and paper):\\text{softmax}(\\vec{x})_{i} = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n\ndef attention(\n    q, k, v, mask\n):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\n\n\nwhere mask is the matrix (for n_seq=5):0 -1e10 -1e10 -1e10 -1e10\n0   0   -1e10 -1e10 -1e10\n0   0     0   -1e10 -1e10\n0   0     0     0   -1e10\n0   0     0     0     0\n\nWe use -1e10 instead of -np.inf as -np.inf can cause nans. Adding mask to our attention matrix instead of just explicitly setting the values to -1e10 works because practically, any number plus -inf is just -inf. We can compute the mask matrix in NumPy with (1 - np.tri(n_seq)) * -1e10. Putting it all together, we get:\n\ndef attention(\n    q, k, v, mask\n):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\n\ndef causal_self_attention(x, c_attn, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # qkv projections\n    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n\n    # split into qkv\n    q, k, v = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n\n    # causal mask to hide future inputs from being attended to\n    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n\n    # perform causal self attention\n    x = attention(q, k, v, causal_mask)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    # out projection\n    x = linear(x, **c_proj)  # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n\n    return x\n\n\n\n","type":"content","url":"/micrograduate/picogpt#causal","position":75},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Multi-Head","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"type":"lvl4","url":"/micrograduate/picogpt#multi-head","position":76},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Multi-Head","lvl3":"Multi-Head Causal Self Attention","lvl2":"GPT Architecture"},"content":"\n\nWe can further improve our implementation by performing n_head separate attention computations, splitting our queries, keys, and values into heads:\n\ndef mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n    # qkv projection\n    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n\n    # split into qkv\n    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n\n    # split into heads\n    qkv_heads = list(\n        map(lambda x: np.split(x, n_head, axis=-1), qkv)\n    )  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n\n    # causal mask to hide future inputs from being attended to\n    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n\n    # perform attention over each head\n    out_heads = [\n        attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)\n    ]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n\n    # merge heads\n    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n\n    # out projection\n    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n\n    return x\n\n\n\nThere are three steps added here:\n\nSplit q, k, v into n_head heads:# split into heads\nqkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [n_head, 3, n_seq, n_embd/n_head]\n\nCompute attention for each head:# perform attention over each head\nout_heads = [attention(q, k, v) for q, k, v in zip(*qkv_heads)]  # [n_head, 3, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n\nMerge the outputs of each head:# merge heads\nx = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n\nNotice, this reduces the dimension from n_embd to n_embd/n_head for each attention computation. This is a tradeoff. For reduced dimensionality, our model gets additional subspaces to work when modeling relationships via attention. For example, maybe one attention head is responsible for connecting pronouns to the person the pronoun is referencing. Maybe another might be responsible for grouping sentences by periods. Another could simply be identifying which words are entities, and which are not. Although, itâ€™s probably just another nn black box. The code we wrote performs the attention computations over each head sequentially in a loop (one at a time), which is not very efficient. In practice, youâ€™d want to do these in parallel. For simplicity, weâ€™ll just leave this sequential. With that, weâ€™re finally done our GPT implementation! Now, all thatâ€™s left to do is put it all together and run our code.\n\n","type":"content","url":"/micrograduate/picogpt#multi-head","position":77},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Putting it All Together"},"type":"lvl2","url":"/micrograduate/picogpt#putting-it-all-together","position":78},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"Putting it All Together"},"content":"\n\nHaving put everything together, we get the equivalent of \n\ngpt2.py, which in its entirety is a mere 120 lines of code (\n\n60 lines if you remove comments and whitespace!). All that remains, it to test our implementation by prompting our tiny GPT-2 model:\n\noutput = prompt_gpt(\"Alan Turing theorized that computers would one day become\", n_tokens_to_generate=8)\nassert output == \" the most powerful machines on the planet.\"\nprint(output)\n\n\n\n\n\n\n\nIt works! We can also test that our implementation gives identical results to OpenAIâ€™s official GPT-2 repo by building and executing the Docker container as follows:docker build -t \"openai-gpt-2\" - <<EOF\nFROM tensorflow/tensorflow:1.13.2-py3\n\nENV DEBIAN_FRONTEND=noninteractive\nRUN apt update -y && apt upgrade -y && apt install git -y\n\nRUN git clone https://github.com/openai/gpt-2 /gpt-2\nWORKDIR /gpt-2\n\nRUN python3 -m pip install --upgrade pip && python3 -m pip install -r requirements.txt\nRUN python3 download_model.py 124M\nRUN python3 download_model.py 355M\nRUN python3 download_model.py 774M\nRUN python3 download_model.py 1558M\nEOF\n\ndocker run -dt --name \"openai-gpt-2-app\" openai-gpt-2\ndocker exec -it \"openai-gpt-2-app\" /bin/bash -c 'python3 src/interactive_conditional_samples.py --length 8 --model_type 124M --top_k 1'\n\nand then pasting the following when prompted:\"Alan Turing theorized that computers would one day become\"\n\nThis should yield an identical result: the most powerful machines on the planet.\n\n","type":"content","url":"/micrograduate/picogpt#putting-it-all-together","position":79},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"What Next?"},"type":"lvl2","url":"/micrograduate/picogpt#what-next","position":80},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl2":"What Next?"},"content":"\n\nThis implementation is cool and all, but itâ€™s missing a ton of bells and whistles:\n\n","type":"content","url":"/micrograduate/picogpt#what-next","position":81},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"GPU/TPU Support","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#gpu-tpu-support","position":82},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"GPU/TPU Support","lvl2":"What Next?"},"content":"\n\nReplace NumPy with \n\nJAX:import jax.numpy as np\n\nThatâ€™s it. You can now use the code with GPUs and even \n\nTPUs! Just make sure you \n\ninstall JAX correctly.\n\n","type":"content","url":"/micrograduate/picogpt#gpu-tpu-support","position":83},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Backprop","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#backprop","position":84},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Backprop","lvl2":"What Next?"},"content":"\n\nAgain, if we replace NumPy with JAX:import jax.numpy as np\n\nThen computing the gradients is as easy as:def lm_loss(params, inputs, n_head) -> float:\n    x, y = inputs[:-1], inputs[1:]\n    logits = gpt2(x, **params, n_head=n_head)\n    loss = np.mean(-log_softmax(logits)[y])\n    return loss\n\ngrads = jax.grad(lm_loss)(params, inputs, n_head)\n\n","type":"content","url":"/micrograduate/picogpt#backprop","position":85},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Batching","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#batching","position":86},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Batching","lvl2":"What Next?"},"content":"\n\nOnce again, if we replace NumPy with JAX:import jax.numpy as np\n\nThen, making our gpt2 function batched is as easy as:gpt2_batched = jax.vmap(gpt2, in_axes=[0, None, None, None, None, None])\ngpt2_batched(batched_inputs)  # [batch, seq_len] -> [batch, seq_len, vocab]\n\n","type":"content","url":"/micrograduate/picogpt#batching","position":87},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"JAX test","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#jax-test","position":88},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"JAX test","lvl2":"What Next?"},"content":"\n\nLetâ€™s verify that switching to JAX is indeed as easy as described:\n\nimport jax.numpy as np\n\n# all references to np are now references to jax.numpy\noutput = prompt_gpt(\"Alan Turing theorized that computers would one day become\", n_tokens_to_generate=8)\nassert output == \" the most powerful machines on the planet.\"\nprint(output)\n\n\n\n\n\n\n\n","type":"content","url":"/micrograduate/picogpt#jax-test","position":89},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Inference Optimization","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#inference-optimization","position":90},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Inference Optimization","lvl2":"What Next?"},"content":"\n\nOur implementation is quite inefficient. The quickest and most impactful optimization you can make (outside of GPU + batching support) would be to implement a \n\nkv cache. Also, we implemented our attention head computations sequentially, when we should really be doing it in parallel. Using JAX, this is as simple asheads = jax.vmap(attention, in_axes=(0, 0, 0, None))(q, k, v, causal_mask)\n\nThereâ€™s many many more inference optimizations. Hereâ€™s two recommendations as a starting point:\n\nLillian Wengâ€™s Large Transformer Model Inference Optimization\n\nKipplyâ€™s Transformer Inference Arithmetic\n\n","type":"content","url":"/micrograduate/picogpt#inference-optimization","position":91},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Training","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#training-1","position":92},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Training","lvl2":"What Next?"},"content":"\n\nTraining a GPT is pretty standard for a nn (gradient descent w.r.t a loss function). Of course, you also need to use the standard bag of tricks when training a GPT (i.e. use the Adam optimizer, find the optimal learning rate, regularization via dropout and/or weight decay, use a learning rate scheduler, use the correct weight initialization, batching, etc ...). Though the real challenge to training a good GPT model is the ability to scale the data and the model. For scaling data, youâ€™ll want a corpus of text that is big, high quality, and diverse.\n\nBig means billions of tokens (terabytes of data). For example, check out \n\nThe Pile, which is an open source pre-training dataset for large language models.\n\nHigh quality means you want to filter out duplicate examples, unformatted text, incoherent text, garbage text, etc ...\n\nDiverse means varying sequence lengths, about lots of different topics, from different sources, with differing perspectives, etc ... Of course, if there are any biases in the data, it will reflect in the model, so you need to be careful of that as well.\n\nScaling the model to billions of parameters involves a cr*p ton of engineering (and money lol). Training frameworks can get \n\nabsurdly long and complex. A good place to start would be \n\nLillian Wengâ€™s How to Train Really Large Models on Many GPUs. On the topic thereâ€™s also the \n\nNVIDIAâ€™s Megatron Framework, \n\nCohereâ€™s Training Framework, \n\nGoogleâ€™s PALM, the open source \n\nmeshâ€‹-transformerâ€‹-jax (used to train EleutherAIâ€™s open source models), and \n\nmany \n\nmany \n\nmore.\n\n","type":"content","url":"/micrograduate/picogpt#training-1","position":93},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Evaluation","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#evaluation","position":94},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Evaluation","lvl2":"What Next?"},"content":"\n\nOh boy, how does one even evaluate LLMs? Honestly, itâ€™s really hard problem. \n\nHELM is pretty comprehensive and a good place to start, but you should always be skeptical of \n\nbenchmarks and evaluation metrics.\n\n","type":"content","url":"/micrograduate/picogpt#evaluation","position":95},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Architecture Improvements","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#architecture-improvements","position":96},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Architecture Improvements","lvl2":"What Next?"},"content":"\n\nYou can also take a look at \n\nPhil Wangâ€™s X-Transformerâ€™s. \n\nThis paper is also a pretty good summary (see Table 1). Facebookâ€™s \n\nLLaMA paper is also probably a good reference for standard architecture improvements (at least as of February 2023 it was).\n\n","type":"content","url":"/micrograduate/picogpt#architecture-improvements","position":97},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Stopping Generation","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#stopping-generation","position":98},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Stopping Generation","lvl2":"What Next?"},"content":"\n\nOur current implementation requires us to specify the exact number of tokens weâ€™d like to generate ahead of time. This is not a very good approach as our generations end up being too long, too short, or cutoff mid-sentence. To resolve this, we can introduce a special end of sentence (EOS) token. During pre-training, we append the EOS token to the end of our input (i.e. tokens = [\"not\", \"all\", \"heroes\", \"wear\", \"capes\", \".\", \"<|EOS|>\"]). During generation, we simply stop whenever we encounter the EOS token (or if we hit some maximum sequence length):def generate(inputs, eos_id, max_seq_len):\n\tprompt_len = len(inputs)\n\twhile inputs[-1] != eos_id and len(inputs) < max_seq_len:\n        output = gpt(inputs)\n        next_id = np.argmax(output[-1])\n        inputs.append(int(next_id))\n    return inputs[prompt_len:]\n\nGPT-2 was not pre-trained with an EOS token, so we canâ€™t use this approach in our code, but most LLMs nowadays use an EOS token.\n\n","type":"content","url":"/micrograduate/picogpt#stopping-generation","position":99},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Fine-tuning","lvl2":"What Next?"},"type":"lvl3","url":"/micrograduate/picogpt#fine-tuning","position":100},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl3":"Fine-tuning","lvl2":"What Next?"},"content":"\n\nWe briefly touched on fine-tuning in the training section. Recall, fine-tuning is when we re-use the pre-trained weights to train the model on some downstream task. We call this process transfer-learning. In theory, we could use zero-shot or few-shot prompting to get the model to complete our task, however, if you have access to a labelled dataset, fine-tuning a GPT is going to yield better results (results that can scale given additional data and higher quality data). There are a couple different topics related to fine-tuning, as described below:\n\n","type":"content","url":"/micrograduate/picogpt#fine-tuning","position":101},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Classification Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"type":"lvl4","url":"/micrograduate/picogpt#classification-fine-tuning","position":102},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Classification Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"content":"\n\nIn classification fine-tuning, we give the model some text and we ask it to predict which class it belongs to. For example, consider the \n\nIMDB dataset, which contains movie reviews that rate the movie as either good, or bad:--- Example 1 ---\nText: I wouldn't rent this one even on dollar rental night.\nLabel: Bad\n--- Example 2 ---\nText: I don't know why I like this movie so well, but I never get tired of watching it.\nLabel: Good\n--- Example 3 ---\n...\n\nTo fine-tune our model, we replace the language modeling head with a classification head, which we apply to the last token output:def gpt2(inputs, wte, wpe, blocks, ln_f, cls_head, n_head):\n    x = wte[inputs] + wpe[range(len(inputs))]\n    for block in blocks:\n        x = transformer_block(x, **block, n_head=n_head)\n    x = layer_norm(x, **ln_f)\n\n\t# project to n_classes\n\t# [n_embd] @ [n_embd, n_classes] -> [n_classes]\n    return x[-1] @ cls_head\n\nWe only use the last token output x[-1] because we only need to produce a single probability distribution for the entire input instead of n_seq distributions as in the case of language modeling. We take the last token in particular (instead of say the first token or a combination of all the tokens) because the last token is the only token that is allowed to attend to the entire sequence and thus has information about the input text as a whole. As per usual, we optimize w.r.t. the cross entropy loss:def singe_example_loss_fn(inputs: list[int], label: int, params) -> float:\n    logits = gpt(inputs, **params)\n    probs = softmax(logits)\n    loss = -np.log(probs[label]) # cross entropy loss\n    return loss\n\n","type":"content","url":"/micrograduate/picogpt#classification-fine-tuning","position":103},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Generative Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"type":"lvl4","url":"/micrograduate/picogpt#generative-fine-tuning","position":104},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Generative Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"content":"\n\nSome tasks canâ€™t be neatly categorized into classes. For example, consider the task of summarization. We can fine-tune these types of task by simply performing language modeling on the input concatenated with the label. For example, hereâ€™s what a single summarization training sample might look like:--- Article ---\nThis is an article I would like to summarize.\n--- Summary ---\nThis is the summary.\n\nWe train the model as we do during pre-training (optimize w.r.t language modeling loss). At predict time, we feed the model the everything up to --- Summary --- and then perform auto-regressive language modeling to generate the summary. The choice of the delimiters --- Article --- and --- Summary --- are arbitrary. How you choose to format the text is up to you, as long as it is consistent between training and inference. Notice, we can also formulate classification tasks as generative tasks (for example with IMDB):--- Text ---\nI wouldn't rent this one even on dollar rental night.\n--- Label ---\nBad\n\nHowever, this will probably perform worse than doing classification fine-tuning directly (loss includes language modeling on the entire sequence, not just the final prediction, so the loss specific to the prediction will get diluted)\n\n","type":"content","url":"/micrograduate/picogpt#generative-fine-tuning","position":105},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Instruction Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"type":"lvl4","url":"/micrograduate/picogpt#instruction-fine-tuning","position":106},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Instruction Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"content":"\n\nMost state-of-the-art large language models these days also undergo an additional instruction fine-tuning step after being pre-trained. In this step, the model is fine-tuned (generative) on thousands of instruction prompt + completion pairs that were human labeled. Instruction fine-tuning can also be referred to as supervised fine-tuning, since the data is human labelled (i.e. supervised). So whatâ€™s the benefit of instruction fine-tuning? While predicting the next word in a wikipedia article makes the model is good at continuing sentences, it doesnâ€™t make it particularly good at following instructions, or having a conversation, or summarizing a document (all the things we would like a GPT to do). Fine-tuning them on human labelled instruction + completion pairs is a way to teach the model how it can be more useful, and make them easier to interact with. This call this AI alignment, as we are aligning the model to do and behave as we want it to. Alignment is an active area of research, and includes more than just following instructions (bias, safety, intent, etc ...). What does this instruction data look like exactly? Googleâ€™s \n\nFLAN models were trained on various academic NLP datasets (which are already human labelled):\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"fig_3_from_flan_paper.png\"))\n\n\n\nOpenAIâ€™s \n\nInstructGPT on the other hand was trained on prompts collected from their own API. They then paid workers to write completions for those prompts. Hereâ€™s a breakdown of the data:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"table_1_and_2_from_instructgpt_paper.png\"))\n\n\n\n","type":"content","url":"/micrograduate/picogpt#instruction-fine-tuning","position":107},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Parameter Efficient Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"type":"lvl4","url":"/micrograduate/picogpt#parameter-efficient-fine-tuning","position":108},{"hierarchy":{"lvl1":"7. picoGPT: implementing a tiny GPT from scratch","lvl4":"Parameter Efficient Fine-tuning","lvl3":"Fine-tuning","lvl2":"What Next?"},"content":"\n\nWhen we talk about fine-tuning in the above sections, it is assumed that we are updating all of the model parameters. While this yields the best performance, it is costly both in terms of compute (need to back propagate over the entire model) and in terms of storage (for each fine-tuned model, you need to store a completely new copy of the parameters). For instruction fine-tuning, this is fine, we want maximum performance, but if you then wanted to fine-tune 100 different models for various downstream tasks, then youâ€™d have a problem. The most simple approach to this problem is to only update the head and freeze (i.e. make untrainable) the rest of the model. This would speed up training and greatly reduce the number of new parameters, however it would not perform nearly as well as a full fine-tune (we are lacking the deep in deep learning). We could instead selectively freeze specific layers (i.e. freeze all layers except the last 4, or freeze every other layer, or freeze all parameters except multi-head attention parameters), which would help restore some of the depth. This will perform a lot better, but we become a lot less parameter efficient and reduce our training speed ups. Instead, we can utilize parameter-efficient fine-tuning (PEFT) methods. PEFT is active area of research, and there are \n\nlots \n\nof \n\ndifferent \n\nmethods \n\nto \n\nchoose \n\nfrom. As an example, take the \n\nAdapters paper. In this approach, we add an additional â€œadapterâ€ layer after the FFN and MHA layers in the transformer block. The adapter layer is just a simple 2 layer fully connected nn, where the input and output dimensions are n_embd, and the hidden dimension is smaller than n_embd:\n\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"fig_2_from_the_adapters_paper.png\"))\n\n","type":"content","url":"/micrograduate/picogpt#parameter-efficient-fine-tuning","position":109}]}