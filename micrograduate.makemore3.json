{"version":3,"kind":"Notebook","sha256":"ad2199bdda365390e87e11530154e5af317344e9d8c84d18baeab814edaf8169","slug":"micrograduate.makemore3","location":"/micrograduate/makemore3.ipynb","dependencies":[],"frontmatter":{"title":"4. makemore (part 3): activations & gradients, batchnorm","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"micrograduate-env","language":"python"},"github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","source_url":"https://github.com/ckaraneen/micrograduate/blob/main/micrograduate/makemore3.ipynb","edit_url":"https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore3.ipynb","exports":[{"format":"ipynb","filename":"makemore3.ipynb","url":"/build/makemore3-d5f8fee3c921f673a942f3d66c1a1e4b.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt","key":"mHZyycL8Ni"},{"type":"outputs","id":"NtDro0L78krX8sDn2Zy9N","children":[],"key":"p88cAvc751"}],"key":"xsNMeCHYb5"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Intro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dkjYuiNFNp"}],"identifier":"intro","label":"Intro","html_id":"intro","implicit":true,"key":"WWTo4G1QSC"}],"key":"H3yxa3mSJh"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here, we will continue our implementation of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mcZwKBW6Yz"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d4YOwqzPFV"}],"key":"VE4Bd1RT8Z"},{"type":"text","value":". In the previous lesson, we implemented an character-level language model using a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DK1lrT1TPe"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NJPcKec65e"}],"key":"KtDtYoY4QV"},{"type":"text","value":" along the lines of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SYoXsq70QP"},{"type":"link","url":"https://doi.org/10.5555/944919.944966","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Bengio et al. 2003","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GNX6Jo0QdH"}],"urlSource":"https://dl.acm.org/doi/10.5555/944919.944966","data":{"doi":"10.5555/944919.944966"},"internal":false,"protocol":"doi","key":"vv8myFUN0O"},{"type":"text","value":". The model took as inputs a few past characters and predicted the next character in the sequence. What we would like to do is move on to more complex and larger ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ciLu9w7EEI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nwNjp2UNX2"}],"key":"D3GC8B9RD7"},{"type":"text","value":"s, like","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m4mWC1Z0ar"}],"key":"Ve61FbcFtY"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RNN, following ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"iHLeReSMah"},{"type":"link","url":"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Mikolov et al. 2010","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"updZME8xOY"}],"urlSource":"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf","key":"U0auQ80oOu"}],"key":"c6WtthkZ7x"}],"key":"gmhA54qK2M"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LSTM, following ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RMi8P69vN6"},{"type":"link","url":"https://arxiv.org/abs/1308.0850","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Graves et al. 2014","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aq56kcNM5C"}],"urlSource":"https://arxiv.org/abs/1308.0850","key":"PaGckUACLT"}],"key":"XKkmVt7UZR"}],"key":"s1t6DAtWXg"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"GRU, following ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"BGRHP2nhfc"},{"type":"link","url":"https://arxiv.org/abs/1409.1259","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Kyunghyun Cho et al. 2014","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"aKfHE1k8qc"}],"urlSource":"https://arxiv.org/abs/1409.1259","key":"BNHohicUgq"}],"key":"oQMwJwqLaR"}],"key":"vxNWvHBnkK"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"CNN, following ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"jxJoZjx0iN"},{"type":"link","url":"https://arxiv.org/abs/1609.03499","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Oord et al., 2016","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"InaBhzbh7A"}],"urlSource":"https://arxiv.org/abs/1609.03499","key":"WD4DwofFjV"}],"key":"U91A5pSep8"}],"key":"vDVfbOGTq2"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Transformer, following ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"VV8CQd0lwI"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Vaswani et al. 2017","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"LkwPeijwwv"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"EAg0Ydl96H"}],"key":"NeOJRc3La7"}],"key":"XLKIYmLix5"}],"key":"K9CA0QNYBl"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"But before we do so, let’s stick around at the level of the ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"YiUxRkGSKd"},{"type":"strong","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"Znvgq7PHAU"}],"key":"pMmLW7p6Zw"},{"type":"text","value":" for a little longer in order to develop an intuitive understanding of the activations during training, and especially the gradients flowing backwards: how they behave and how they look like. This is important for understanding the history of the development of newer architectures. Because, RNNs, as we’ll see, for example, although they are very expressive, are universal function approximators and can in principle implement all algorithms, we will see that they are not that easily optimizable with the first-order gradient-based techniques that we have available to us and that we use all the time. The key to understanding why they are not easily optimizable, is to understand the activations and the gradients and how they behave during training. What we’ll also see is that a lot of variants since RNNs, have tried to improve upon this situation. And so, that’s the path that we have to take.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"vM0EKYM6IP"}],"key":"lKMNjMIpbT"}],"key":"A74Qc4TRBs"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Rebuilding ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h680RR6s7O"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lNyoabg5fp"}],"key":"p7XeGSjII6"}],"identifier":"rebuilding-mlp","label":"Rebuilding mlp","html_id":"rebuilding-mlp","implicit":true,"key":"ZMAQHkhrP5"}],"key":"JSn4rtxwC9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, let’s get started by first building on the code from the previous lesson.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qZDbXuqTha"}],"key":"IxWLfhw7wq"}],"key":"FmCvcRJmFx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import random\nrandom.seed(42)\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\nSEED = 2147483647","key":"VN7EpbxyvJ"},{"type":"outputs","id":"I0BF7AcJF5Cn1dukKwloG","children":[],"key":"Ru7fzpSLOo"}],"key":"d2ERfY1o1B"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nwords[:8]","key":"OKkT062gN0"},{"type":"outputs","id":"gkYAra_Ga8Qd8QEKOwB2F","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']","content_type":"text/plain"}}},"children":[],"key":"lVTVhpumc9"}],"key":"prJfpHnVnq"}],"key":"b9OPId7J2M"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(words)","key":"xCp8MVhhpv"},{"type":"outputs","id":"9cJwnL1hPRIQ2Yt1TxczK","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"32033","content_type":"text/plain"}}},"children":[],"key":"JD6wg7tqo2"}],"key":"NftyDCrUKg"}],"key":"MdQabg3imq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {s: i + 1 for i, s in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: s for s, i in ctoi.items()}\nvocab_size = len(itoc)\nprint(itoc)\nprint(vocab_size)","key":"WqlpZC0eTF"},{"type":"outputs","id":"7G3fCN40YHaFIjkjZYKP_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n"},"children":[],"key":"lFokdaZ6kG"}],"key":"cpCd6kQKoF"}],"key":"Aw1aiPoC5u"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"block_size = 3\n\n\ndef build_dataset(words):\n    x, y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            x.append(context)\n            y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(x.shape, y.shape)\n    return x, y","key":"ta2ypZowQH"},{"type":"outputs","id":"c66LvDSZTw72_mqC0M1l6","children":[],"key":"eeFO6MVJqA"}],"key":"qQ6dTsXfdu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"random.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\nxtrain, ytrain = build_dataset(words[:n1])\nxval, yval = build_dataset(words[n1:n2])\nxtest, ytest = build_dataset(words[n2:])","key":"DMqyaS2W8V"},{"type":"outputs","id":"KdrmYOxE9X92lfabD08re","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n"},"children":[],"key":"otUe1U0CJE"}],"key":"iAlFFrQVre"}],"key":"emjFt80k5a"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, b1, w2, b2\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    b1 = torch.randn(n_hidden, generator=g) * b1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    parameters = [C, w1, b1, w2, b2]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return parameters","key":"zl8hO0FThG"},{"type":"outputs","id":"E5qNAaubzX5wxQfNkl7zd","children":[],"key":"AJ2yGfwBlS"}],"key":"exKPesBjtr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(x, y):\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss","key":"YmM6vMWErG"},{"type":"outputs","id":"yFC7_u1KSS9LG_SK0yzrO","children":[],"key":"Oz4bjCyyoL"}],"key":"H07WRB8ys4"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def backward(parameters, loss):\n    for p in parameters:\n        p.grad = None\n    loss.backward()","visibility":"show","key":"dTnsth9pyt"},{"type":"outputs","id":"vsemiftKt2-FBTTl8DJXA","children":[],"visibility":"show","key":"EYpS616KX4"}],"visibility":"show","key":"syGgtqIXfJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def update(parameters, lr):\n    for p in parameters:\n        p.data += -lr * p.grad","key":"CgPMN3PiNF"},{"type":"outputs","id":"VG6NE2ZQDzsKetY0SnwtV","children":[],"key":"OjR3NGNwM7"}],"key":"B4ZLzJEZej"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(x, y, initial_lr=0.1, maxsteps=200000, batchsize=32, redefine_params=False):\n    global parameters\n    lossi = []\n    if redefine_params:\n        parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for i in range(maxsteps):\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        hpreact, h, logits, loss = forward(xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 100000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n    return hpreact, h, logits, lossi","key":"A4j1j8QBeW"},{"type":"outputs","id":"rj70FZTzSdplPYStDbZwD","children":[],"key":"JctkqBFu0a"}],"key":"ULAoBZEqAZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@torch.no_grad()  # this decorator disables gradient tracking\ndef print_loss(x, y, prefix=\"\"):\n    _, _, _, loss = forward(x, y)\n    print(f\"{prefix} {loss}\")\n    return loss","key":"mYQokl80rA"},{"type":"outputs","id":"S-blBw9nl8jO0_083t6l_","children":[],"key":"h06v5wBzxp"}],"key":"kke3UvgnVt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn()\n_, _, _, lossi = train(xtrain, ytrain)\nprint_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\");","key":"bSa76OQp9v"},{"type":"outputs","id":"0oPimjTOcezhpmrQZVZ4Z","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/ 200000: 26.9154\n  10000/ 200000: 3.1635\n  20000/ 200000: 2.6757\n  30000/ 200000: 2.0344\n  40000/ 200000: 2.6388\n  50000/ 200000: 2.0378\n  60000/ 200000: 2.8707\n  70000/ 200000: 2.1878\n  80000/ 200000: 2.1205\n  90000/ 200000: 2.1872\n 100000/ 200000: 2.6178\n 110000/ 200000: 2.4887\n 120000/ 200000: 1.6539\n 130000/ 200000: 2.2179\n 140000/ 200000: 2.2891\n 150000/ 200000: 1.9963\n 160000/ 200000: 1.7527\n 170000/ 200000: 1.7564\n 180000/ 200000: 2.2431\n 190000/ 200000: 2.2670\ntrain 2.13726806640625\nval 2.1725592613220215\n"},"children":[],"key":"llJc0iBGdj"}],"key":"D07QUPSj1X"}],"key":"I4q9s4B732"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(lossi)","key":"wBkqTkCnXB"},{"type":"outputs","id":"B8CRVQT9XGfYJDWVX5XDc","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"2f462738d693496997f9881e754a78b7\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"33673b1115f2b1bb7df39bc6dedb342c","path":"/build/33673b1115f2b1bb7df39bc6dedb342c.png"},"text/html":{"content_type":"text/html","hash":"be5cc0fb539a274e243ab47d0a3b1fbb","path":"/build/be5cc0fb539a274e243ab47d0a3b1fbb.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"iGnHfg20Uj"}],"key":"pEONx08qyh"}],"key":"vKLIQMsvKv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def sample_from_model():\n    # sample from the model\n    g = torch.Generator().manual_seed(SEED + 10)\n    for _ in range(20):\n        out = []\n        context = [0] * block_size  # initialize with all ...\n        while True:\n            emb = C[torch.tensor([context])]  # (1,block_size,d)\n            h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n            logits = h @ w2 + b2\n            probs = F.softmax(logits, dim=1)\n            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n            context = context[1:] + [ix]\n            out.append(ix)\n            if ix == 0:\n                break\n        print(\"\".join(itoc[i] for i in out))","key":"UO0K8rsO8E"},{"type":"outputs","id":"WJnhP6g4tR0HJvs3oo3Ir","children":[],"key":"OP6llSeOei"}],"key":"RoQiVl6DB3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sample_from_model()","key":"PLSUSXKmsP"},{"type":"outputs","id":"oDUTJRp7xAJZYi1x8fAn9","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"eria.\nkayanniee.\nmed.\nryah.\nrethrus.\njernee.\naderedielin.\nshi.\njen.\neden.\nestana.\nselyn.\nmalyan.\nnyshabergiagriel.\nkinleeney.\npanthuon.\nubz.\ngeder.\nyarue.\nelsy.\n"},"children":[],"key":"a2Z9AAlbD4"}],"key":"XdtkodYnnN"}],"key":"teEpgZ3sdW"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So that’s our starting point. Awesome!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b26LQy7wCD"}],"key":"EfO3ZKMUpJ"}],"key":"vVgebSbXx4"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Dealing with bad weights","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IiEoljnXiw"}],"identifier":"dealing-with-bad-weights","label":"Dealing with bad weights","html_id":"dealing-with-bad-weights","implicit":true,"key":"KSnsGECKLV"}],"key":"FyS0OyzfLO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, the first thing to scrutinize is the initialization. An experienced person would tell you that our network is very improperly configured at initialization and there are multiple things wrong with it. Let’s start with the first one. If you notice the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wdp2b9oCJI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Cn4wGRdUIe"}],"key":"ir2pygAtxK"},{"type":"text","value":" at iteration ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mUzucr1XWM"},{"type":"inlineCode","value":"0/200000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Opn7rQ0yjr"},{"type":"text","value":", it is rather high. This rapidly comes down to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SFgRxSsI9m"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"URoCMhGYkD"},{"type":"text","value":" or so in the following training iterations. But you can tell that initialization is all messed up just by an initial ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cml1gO2KPr"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eAJuuykmnc"}],"key":"nW5zcL4bjW"},{"type":"text","value":" that is way too high. In the training of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x54QwjxwwH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b99WjLIiDC"}],"key":"IEBTnbER8z"},{"type":"text","value":"s, it is almost always the case that you’ll have a rough idea of what ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ND75zUvtjp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LAIGdM6sAx"}],"key":"hWWw1OoxVS"},{"type":"text","value":" to expect at initialization. And that just depends on the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lfftljSiqs"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wEkIK62lNr"}],"key":"KCXbh1LUyE"},{"type":"text","value":" function and the problem setup. In our case, we expect a number ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lHjUCZpMm4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"much lower","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gcKuauJjJp"}],"key":"xByH6QtTLt"},{"type":"text","value":" than what we get. Let’s calculate it together. Basically, there’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"inTGW4PoZp"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HzeYlzYlwg"},{"type":"text","value":" characters that can come next for any one training example. At initialization, we have no reason to believe that any characters to be much more likely than others. So, we’d expect that the probability distribution that comes out initially is a uniform distribution, assigning about-equal probability to all the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ByX96ASVAn"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a31e4AzhwH"},{"type":"text","value":" characters. This means that what we’d like the ideal probability we should record for any character coming next to be:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IdOXVC5YCQ"}],"key":"fkMt7EsKGf"}],"key":"otr7Bz6sCO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ideal_p = torch.tensor(1.0 / 27)\nideal_p.item()","key":"YEclmEMBdh"},{"type":"outputs","id":"KnKpmmciDGeujo7Cy9Ocf","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":18,"metadata":{},"data":{"text/plain":{"content":"0.03703703731298447","content_type":"text/plain"}}},"children":[],"key":"frTGwljVMt"}],"key":"trGsfjqRYg"}],"key":"zKZbtZh2OL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And then the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IzBzduQiwh"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J6ZBcrnuyy"}],"key":"tgXoXeG3RS"},{"type":"text","value":" we would expect is the negative log probability:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZPA77k5Ekq"}],"key":"OVf4RY2cbS"}],"key":"bREjXpN16C"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"expected_loss = -torch.log(ideal_p)\nexpected_loss.item()","key":"oQk1lZjsQv"},{"type":"outputs","id":"ndQTaaiLJhf2t1yeS0VdD","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":19,"metadata":{},"data":{"text/plain":{"content":"3.295836925506592","content_type":"text/plain"}}},"children":[],"key":"H8N96OUnDF"}],"key":"cupCOj1Tj2"}],"key":"ZH13bdngxU"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So what’s happening right now is that at initialization the network is creating probability distributions that are all messed up. Some characters are very confident and some characters are very not-confident. Basically, the network is very confidently wrong and that’s what makes it record a very high ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tXFQpdYfmI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oJB4ORT7ln"}],"key":"ks5l7vdgAC"},{"type":"text","value":". For simplicity, let’s see a smaller, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Fd9iynYwbW"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MaQVxcuHyr"},{"type":"text","value":"-dimensional example of the issue, by assuming we only have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kxvl2XMUtP"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GgeHig8wRw"},{"type":"text","value":" characters.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sdjHkHGpog"}],"key":"NGIP7yzmMz"}],"key":"P9wovfjBLe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def logits_4d(logits=torch.tensor([0.0, 0.0, 0.0, 0.0]), index=0):\n    probs = torch.softmax(logits, dim=0)\n    loss = -probs[index].log()\n    return probs, loss.item()","key":"OzZOMKBSMO"},{"type":"outputs","id":"2RZDW7ghKURdkxxnY8HQC","children":[],"key":"QXsUoqI8pw"}],"key":"vsBR4nPqz1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits_4d()","key":"rgItilDieu"},{"type":"outputs","id":"H-cnOdNmWW2c8TwIM9RZg","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":21,"metadata":{},"data":{"text/plain":{"content":"(tensor([0.2500, 0.2500, 0.2500, 0.2500]), 1.3862943649291992)","content_type":"text/plain"}}},"children":[],"key":"LgoR6OrDMr"}],"key":"INXORhhzYv"}],"key":"TZ3nXwsPp2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Suppose we have logits that come out of an ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ex8yULPO6Q"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sNya7xw9Ot"}],"key":"UqgmQZK4cH"},{"type":"text","value":" that are all ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EHwUtQ4XhS"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bgpWUwTvpC"},{"type":"text","value":". Then, when we calculate the softmax of these logits and get probabilities that are a diffused distribution that sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zJhgW6A7uy"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JG01Bnitnf"},{"type":"text","value":" and is exactly uniform. Whereas, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yIr0FYhek9"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dyYcsijha7"}],"key":"djLhT7sjqB"},{"type":"text","value":" we get is the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YqCUVwK1Sr"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Xvd8UR8l2V"}],"key":"S3inz3ZOup"},{"type":"text","value":" we would expect for a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eZgha7C3Y5"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aYVgpTv82J"},{"type":"text","value":"-dimensional example with a uniform probability distribution. And so it doesn’t matter whether the index is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K8mNCpu0W7"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XTsKkObNtu"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vt3UOTDvQQ"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FSVF0msegc"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZhD4Znkky4"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cT0iaekWyA"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R8AB5AkXOt"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OiHnUiE3Xu"},{"type":"text","value":". We’ll see of course that as we start to manipulate these logits, the loss changes. For example:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MEPcmphbqT"}],"key":"YbbZz6xFGT"}],"key":"HVqmqLCpva"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits_4d(logits=torch.tensor([0.0, 0.0, 5.0, 0.0]), index=2)","key":"ip79aQ8v0F"},{"type":"outputs","id":"uW2rOm7xsf_PQgEytptpk","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":22,"metadata":{},"data":{"text/plain":{"content":"(tensor([0.0066, 0.0066, 0.9802, 0.0066]), 0.020012274384498596)","content_type":"text/plain"}}},"children":[],"key":"sJKH75pyfu"}],"key":"KFDAVnR6Fw"}],"key":"fmB4SBgJJF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Yields a very low ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jgNLfcdhOT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OXq1OSAPBV"}],"key":"WwlOQ6ZZej"},{"type":"text","value":" since we are assigning the correct probability at initialization to the correct (3rd) label. Much more likely it is that some other dimension will have a high logit, e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eE4yAqxc4e"}],"key":"OkzgChSXcB"}],"key":"WerpbAorh6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits_4d(logits=torch.tensor([0.0, 5.0, 0.0, 0.0]), index=2)","key":"wReUtXdmiq"},{"type":"outputs","id":"WS-JBb9VkKCsuGy8CnS81","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":23,"metadata":{},"data":{"text/plain":{"content":"(tensor([0.0066, 0.9802, 0.0066, 0.0066]), 5.020012378692627)","content_type":"text/plain"}}},"children":[],"key":"qdmlCnnMWG"}],"key":"zIUZbZkk7O"}],"key":"PQTEVayNUQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"and then what happens is we start to record a much higher ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iOo0DoPtHW"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CZimrT3CCU"}],"key":"hKIfve4C2X"},{"type":"text","value":". So, what of course can happens is that the logits might take on extreme values and come out like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gfWpNzyQvk"}],"key":"OgKUQcHfXj"}],"key":"baPXh1UmKn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits_4d(logits=torch.tensor([-3.0, 5.0, 0.0, 2.0]), index=2)","key":"XBc13jOpIP"},{"type":"outputs","id":"-wI-RhyqkSPUvk3PVvOEQ","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":24,"metadata":{},"data":{"text/plain":{"content":"(tensor([3.1741e-04, 9.4620e-01, 6.3754e-03, 4.7108e-02]), 5.055302619934082)","content_type":"text/plain"}}},"children":[],"key":"AxLvxIT7xC"}],"key":"dOJK8CvSiJ"}],"key":"HQxxmtufLI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"which also leads to a very high ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aMa5lmhuyX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YKPsfBCHnP"}],"key":"TOafPb5Kpt"},{"type":"text","value":". For example, if logits are be relatively close to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RxjpkCP2Uy"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PgApldWNnT"},{"type":"text","value":", the loss is not too big. For example:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M3w4VY1ElN"}],"key":"KWJBCO6Fzi"}],"key":"vcE0bstdnR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"randn_logits = torch.randn(4)\nprint(randn_logits)\nlogits_4d(logits=randn_logits, index=2)","key":"N5zHyWvNUn"},{"type":"outputs","id":"M0zgcF2MZtSg3S8_UxU82","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([ 0.6490,  0.7479, -0.3871, -0.5356])\n"},"children":[],"key":"Q2WjfxjzqA"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":25,"metadata":{},"data":{"text/plain":{"content":"(tensor([0.3617, 0.3993, 0.1284, 0.1106]), 2.0529625415802)","content_type":"text/plain"}}},"children":[],"key":"cMOqlSC93w"}],"key":"YcOaNBgEXX"}],"key":"wd80BwxTH9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"However, if they are larger, it’s very unlikely that you are going to be guessing the correct bucket and so you’d be confidently wrong and usually record a very high ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ux7X7eSHvq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zGYXizqOSB"}],"key":"nf4IudtRKb"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xiGnUqpw3e"}],"key":"osad8PEdL5"}],"key":"QTOstBmHw7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"big_randn_logits = torch.randn(4) * 10\nprint(big_randn_logits)\nlogits_4d(logits=big_randn_logits, index=3)","key":"zmWuYvU3b0"},{"type":"outputs","id":"e_V1SGpxmsDwHKJU91_XD","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([  9.4665,   7.1429,   2.0826, -18.9976])\n"},"children":[],"key":"flk9q1XWOe"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":26,"metadata":{},"data":{"text/plain":{"content":"(tensor([9.1030e-01, 8.9138e-02, 5.6545e-04, 3.9573e-13]), 28.55805015563965)","content_type":"text/plain"}}},"children":[],"key":"rYuLE9kvdH"}],"key":"I791l7TYQG"}],"key":"s9mNkawERj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For even more extreme logits, you might get extreme loss values:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Xz9wvpdXTw"}],"key":"aVyiZAvKaE"}],"key":"kkexURx6e8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"huge_randn_logits = torch.randn(4) * 100\nprint(huge_randn_logits)\nlogits_4d(logits=huge_randn_logits, index=1)","key":"TD24QhRPAE"},{"type":"outputs","id":"DqF1kGelRam2cyiJxAm-Q","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([   8.3889, -112.5325,  -85.9192, -154.8166])\n"},"children":[],"key":"o9GBw2H66k"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":27,"metadata":{},"data":{"text/plain":{"content":"(tensor([1.0000e+00, 0.0000e+00, 1.1028e-41, 0.0000e+00]), inf)","content_type":"text/plain"}}},"children":[],"key":"xUejWD4Yg2"}],"key":"NF16Uo1s3w"}],"key":"NsEDfNfALi"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Basically, such logits are not good and we want the logits to be roughly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W1ylyEQwJ1"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SY8BMH5ipg"},{"type":"text","value":" when the network is initialized. In fact, the logits don’t need to be zero, they just have to be equal, e.g.:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uncATIfpIg"}],"key":"vgJnxf67ck"}],"key":"nsy9lD255Z"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits_4d(logits=torch.tensor([3., 3., 3., 3.]), index=2)","key":"sKOSC6Ty5k"},{"type":"outputs","id":"ElztUS03oTAln0NeFLdNY","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":28,"metadata":{},"data":{"text/plain":{"content":"(tensor([0.2500, 0.2500, 0.2500, 0.2500]), 1.3862943649291992)","content_type":"text/plain"}}},"children":[],"key":"mRnUSOlFtD"}],"key":"dfAt1bwCwF"}],"key":"Kwqggr3vu3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Because of the normalization inside softmax, this will actually come out ok. But, for symmetry, we don’t want it to be any arbitrary positive or negative number, just zero. So let’s now concretely see where things go wrong in our initial example. First, let’s reinitialize our network:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZpDwRv4YAg"}],"key":"WOV3VdhoCK"}],"key":"mbmCChbWum"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn()","key":"DUDkbYwzXo"},{"type":"outputs","id":"RG2ePdZ793x7mx8XJCxo-","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n"},"children":[],"key":"ZxxBVVNwdB"}],"key":"qmbn21EE6g"}],"key":"wwY6GvXrUh"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then let’s train it only for one step:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RJXqcn23aR"}],"key":"f9lD79g7fz"}],"key":"uEQRDz6Wvd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"_, _, logits, _ = train(xtrain, ytrain, maxsteps=1)","key":"vzymmRcQdj"},{"type":"outputs","id":"-h71jAahGJNZT_SFOtDp_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"      0/      1: 29.0502\n"},"children":[],"key":"FKJIH4Qvm9"}],"key":"FNo0QWteVx"}],"key":"tZLu36pOts"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If we print the logits, we’ll see that they take on quite extreme values:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QsFnCjFuLn"}],"key":"NSoA27eTwf"}],"key":"hDnksNv9Hu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits[0]","key":"uneVGGtyzy"},{"type":"outputs","id":"c0Y8gdWKO1id1GLmLrNBV","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":31,"metadata":{},"data":{"text/plain":{"content":"tensor([ 1.8587e+01, -8.8322e+00, -4.3481e+00,  9.0512e+00,  2.1609e+00,\n         5.8525e+00, -1.5959e+01,  1.7691e-01,  1.4306e+01,  5.5933e+00,\n        -1.0145e+01, -2.8939e+00, -2.0576e+01,  1.0257e+01,  1.2336e+01,\n        -1.0782e+01, -3.0768e+01, -4.1870e+00, -1.3147e-02,  2.1114e+01,\n        -6.2802e+00, -5.8485e+00, -9.8297e-01,  2.2049e+01, -4.3106e+00,\n         1.4430e+01, -6.5003e+00], grad_fn=<SelectBackward0>)","content_type":"text/plain"}}},"children":[],"key":"gPmoOWYIxZ"}],"key":"scBthJQb8R"}],"key":"j03Oxlgk1H"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"which is what is creating the fake confidence and why the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c3LBTyA4ve"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HFzWMSunLn"}],"key":"mASdXmuFd1"},{"type":"text","value":" is so high. Let’s now try to scale down the values of the some of our parameters (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R02uZipfuO"},{"type":"inlineCode","value":"w2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hlErpXOAAh"},{"type":"text","value":") and retrain for a step:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KnyY7nMjB4"}],"key":"R8miBSO7fl"}],"key":"j4972FpAJs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w2_factor=0.1, b2_factor=0.0)\n_ = train(xtrain, ytrain, maxsteps=1)","key":"LStJVdkusQ"},{"type":"outputs","id":"SB08_E4chaZ4cCfVtdsXy","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/      1: 4.3367\n"},"children":[],"key":"RL2rnLuIe7"}],"key":"ZOT24grWOZ"}],"key":"fYMIGSyQMX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Aha! The loss is lower, which makes sense. Let’s try a smaller factor:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uaQCptrE1g"}],"key":"JiLOKD4tLO"}],"key":"dQv2wC0C0n"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w2_factor=0.01, b2_factor=0.0)\n_ = train(xtrain, ytrain, maxsteps=1)","key":"r5XbORLO2a"},{"type":"outputs","id":"OaMnwIeEL1gbqfdeT1WkR","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/      1: 3.2853\n"},"children":[],"key":"THzpyWTZb7"}],"key":"UWQvPSFyrF"}],"key":"C5LWMBQKn3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The loss decreases further. Alright, so we’re getting closer and closer... So, you might ask, why not just initialize the weights to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TcLlmihMdo"},{"type":"text","value":"0.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OP92nxQsPk"},{"type":"text","value":"?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ovqxquDWSs"}],"key":"mQxuNb5AIi"}],"key":"j6PAubt2ZF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w2_factor=0.0, b2_factor=0.0)\n_ = train(xtrain, ytrain, maxsteps=1)","key":"dToDwQRsgm"},{"type":"outputs","id":"LMh8Hak0XVCDjcGbYVdvg","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/      1: 3.2958\n"},"children":[],"key":"LcuCIYRxI8"}],"key":"gyGW4iMEbz"}],"key":"HWsgosZShn"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Besides, it does yield an acceptable initial ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gujzbxlPU8"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JrfDQ5N1Xo"}],"key":"ldblzpTN3K"},{"type":"text","value":" value. Well, you don’t want to be setting the parameters of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zFVyH3veRh"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YGgzfa9bYJ"}],"key":"XRxVdG8bts"},{"type":"text","value":" exactly to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OKVhh7f7ud"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vSBFSmapKp"},{"type":"text","value":". You usually want it to be small numbers instead of exactly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gESWD72JCB"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TMsK9c0Ht7"},{"type":"text","value":". Let’s see soon where things might go wrong if we set the initial parameters to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hadc5d6zJb"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mDDmvcicfi"},{"type":"text","value":". For now, let’s just consider the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qc2q5w8M4Q"},{"type":"text","value":"0.01","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NyPiPsgtoR"},{"type":"text","value":" factor, which yields a small-enough initial ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ij0i71BxBC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZObXcVIVUh"}],"key":"LpNIMCbbv2"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dB2aEm6WNa"}],"key":"JE4j6OLtOD"}],"key":"kRC6l6hJrH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w2_factor=0.01, b2_factor=0.0)\n_, _, logits, _ = train(xtrain, ytrain, maxsteps=1)","key":"CHeZtE7GOC"},{"type":"outputs","id":"8ThPC1stErVRReDPu-tyM","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/      1: 3.3213\n"},"children":[],"key":"LLPQ01fZ9a"}],"key":"FGlR0HwFqR"}],"key":"hbyZz0M7ZE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The logits are now coming out as closer to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OgiIODyIu6"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GHx6xrDdzV"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ACFtW8zpQx"}],"key":"Kirb4CM6lV"}],"key":"rHjMagZOm7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits[0]","key":"me5vxwfUzh"},{"type":"outputs","id":"2zl6l6tumDfvAaUgHcxGx","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":36,"metadata":{},"data":{"text/plain":{"content":"tensor([ 0.0719,  0.0493, -0.2910,  0.0210,  0.2192,  0.0624,  0.2226,  0.2487,\n         0.1420,  0.1322,  0.0790, -0.0102, -0.0382,  0.1264,  0.0133, -0.0155,\n         0.0955, -0.1007,  0.0885,  0.0645,  0.0264,  0.1433,  0.0642, -0.1751,\n        -0.0414, -0.1055, -0.1209], grad_fn=<SelectBackward0>)","content_type":"text/plain"}}},"children":[],"key":"qOCrWBmtYN"}],"key":"dM1hbNkStk"}],"key":"JWbu8rdzfL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Cool. So, let’s now train the network completely, and see what losses we get.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K1aUL76DmU"}],"key":"s41wCc5L5I"}],"key":"BbPXA1wUQq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w2_factor=0.01, b2_factor=0.0)\n_, _, _, lossi = train(xtrain, ytrain)","key":"gwqJl8uNzc"},{"type":"outputs","id":"gy_MMC2XH9d-MJx7jNl4q","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/ 200000: 3.2738\n  10000/ 200000: 2.2549\n  20000/ 200000: 2.1941\n  30000/ 200000: 2.0484\n  40000/ 200000: 2.1231\n  50000/ 200000: 2.3176\n  60000/ 200000: 1.9067\n  70000/ 200000: 2.3038\n  80000/ 200000: 2.2954\n  90000/ 200000: 2.3491\n 100000/ 200000: 2.4612\n 110000/ 200000: 1.8579\n 120000/ 200000: 1.8499\n 130000/ 200000: 2.0158\n 140000/ 200000: 2.2136\n 150000/ 200000: 1.8362\n 160000/ 200000: 1.7483\n 170000/ 200000: 1.9169\n 180000/ 200000: 2.3354\n 190000/ 200000: 2.1250\n"},"children":[],"key":"vfdufxAWtn"}],"key":"N4BMIwfGeO"}],"key":"NjWxNnFSSJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(lossi)","key":"nqPOnsyGYf"},{"type":"outputs","id":"JlS0PMAYQ3f6i-IULODIe","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"25e7d03ead9b4bd28ce332c5de803f3a\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"540b9305b86ef2a403e229844be9ebc6","path":"/build/540b9305b86ef2a403e229844be9ebc6.png"},"text/html":{"content_type":"text/html","hash":"9e7b5031492e62dd5c7d33a8015aafca","path":"/build/9e7b5031492e62dd5c7d33a8015aafca.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"tLAPBAIwNT"}],"key":"SYgAYJGn86"}],"key":"voZR0w3wJl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\")","key":"PWRIOoIHWW"},{"type":"outputs","id":"C-HUsOCNYFbzpRJcpq_Sk","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.068324327468872\nval 2.128187656402588\n"},"children":[],"key":"lX2MTnqtXU"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"tensor(2.1282)","content_type":"text/plain"}}},"children":[],"key":"PbtiUpvNFS"}],"key":"C6be4Kxbbw"}],"key":"eIjf8qVqyw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h6fXZM4ogk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AZDuFj1BYV"}],"key":"sE0ro5XhNc"},{"type":"text","value":" gets smaller after the first step. Now, notice that our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JgePoheRBk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aGynFxz0I8"}],"key":"BvV9HSr2e6"},{"type":"text","value":" plot does not have the previous ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h0h4hRNkXt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lVvESoqSex"}],"key":"L9l5dFB98s"},{"type":"text","value":" plot’s hockey stick appearance. The reason is that that shape came from the optimization process basically squashing down the weights to a much smaller range than the initial one. But, now since we’ve already initialized the weights with small values, no such significant shrinking takes places, and thus no big ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aLlt63NrtL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iRbvDNcUWp"}],"key":"pzOJHGKdan"},{"type":"text","value":" drop happens between the first couple training steps. Therefore, we are not getting any easy gains, as we previously did in the beginning, but only just the hard gains from training. One important point to keep in mind is that the training and validation ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YCq5H0zPe7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"losses","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bQ0eTmOfkV"}],"key":"CwsCwuXjYB"},{"type":"text","value":" are now a bit better, since training now goes on for a bit longer, since the first epochs are no longer spent for squashing the parameters.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uPebmMCNwc"}],"key":"zskDayFjHF"}],"key":"k6zV96iYcB"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Dealing with dead neurons","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v8tChuf66d"}],"identifier":"dealing-with-dead-neurons","label":"Dealing with dead neurons","html_id":"dealing-with-dead-neurons","implicit":true,"key":"JEliMBSYJU"}],"key":"JYbHJ2wAkm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, time to deal with a second problem. Although our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i3KCl2owQP"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bx3Cq2tBTr"}],"key":"zzWA3CvdHQ"},{"type":"text","value":" after initializing with smaller weights is low:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mqiFtHAtcN"}],"key":"iOT4Aj3E5f"}],"key":"bmwXfGcYu6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w2_factor=0.01, b2_factor=0.0)\nhpreact, h, logits, _ = train(xtrain, ytrain, maxsteps=1)","key":"ycF9rdCFIT"},{"type":"outputs","id":"3XVt9wXaArJb8BrMxzWVo","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/      1: 3.3179\n"},"children":[],"key":"XHn7n3ToiV"}],"key":"pdigYLlbLY"}],"key":"Oy723sKqUR"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"the activation variable contains many ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zs85NJmCak"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qvIwWwwtF9"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ze5QH8uXZC"},{"type":"text","value":"-1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ztURPas30g"},{"type":"text","value":" values:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YSJdux4YiR"}],"key":"PmtkOm8yFC"}],"key":"jF1ghudGBE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"h","key":"U7HPww1Tve"},{"type":"outputs","id":"dCvv3PzdHge1TGMdWD9Ma","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/plain":{"content":"tensor([[-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],\n        [-1.0000,  0.4796, -0.9999,  ..., -0.9951,  0.9976,  1.0000],\n        [-0.9999, -0.1726, -1.0000,  ..., -0.9927,  1.0000,  1.0000],\n        ...,\n        [ 0.0830, -0.9999,  0.9990,  ..., -0.7998,  0.9251,  1.0000],\n        [-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],\n        [-1.0000, -0.9665, -1.0000,  ..., -0.9994,  0.9962,  0.9919]],\n       grad_fn=<TanhBackward0>)","content_type":"text/plain"}}},"children":[],"key":"etB6QIfbCn"}],"key":"PRM8kN15lS"}],"key":"subveHwFGi"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IS5Cc8JzoZ"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iM8QIlaQh4"},{"type":"text","value":" is the result of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pk2F3FIkaF"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"qJ8Gcr4rld"},{"type":"text","value":" activation function which is basically a squashing function that maps values within the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kv352Cvkgb"},{"type":"inlineMath","value":"[-1.0, 1.0]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1.0</mn><mo separator=\"true\">,</mo><mn>1.0</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-1.0, 1.0]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1.0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1.0</span><span class=\"mclose\">]</span></span></span></span>","key":"MSqnqekqwP"},{"type":"text","value":" range. To get an idea of the distribution of the values of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QDkDhLjDew"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"drLCzjaa72"},{"type":"text","value":", let’s look at its histogram.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LmoyuEpKwY"}],"key":"oWz0BAtATc"}],"key":"cGZu5wIKvM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.hist(h.view(-1).tolist(), 50);","key":"KKKLSOA6yx"},{"type":"outputs","id":"sLgRG3f5u6hdSzJt6ZAha","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"6b673d5ac3624cd0bdd0c980d7a72be2\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"0d9cbee2d53f7a513b4dd3d0a9a5732b","path":"/build/0d9cbee2d53f7a513b4dd3d0a9a5732b.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=640.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Hncoqa4rR3"}],"key":"Krobt9t79W"}],"key":"JQvWqXOkJl"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We clearly see that most of the values of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c9Zm0siEDD"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PjuGWuwGDe"},{"type":"text","value":" are either ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I6F74jV4sM"},{"type":"text","value":"-1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vnJ2JmMpsY"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xJzXo5a90K"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MaEAYvtgIp"},{"type":"text","value":". So, this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ASlOyaH3Wn"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"aDv5AoastQ"},{"type":"text","value":" is very very active. We can look at why that is by plotting the pre-activations that feed into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"idd5CZc7J7"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"rxePrpUAls"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FNOzhcNvWB"}],"key":"Uo7NbjVuGt"}],"key":"T3ZgCiQyDz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.hist(hpreact.view(-1).tolist(), 50);","key":"aey7RJhX2u"},{"type":"outputs","id":"PM3ByBFQ0VDogb-DiInb6","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"2ddab9572ce044578af8473046a73470\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"f50a05c5db0ed5a4638a16220b333c20","path":"/build/f50a05c5db0ed5a4638a16220b333c20.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=640.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"bHE4vjzSGx"}],"key":"ndxdFlWiN4"}],"key":"OChySNHLmZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And we can see that the distribution of the preactivations is very very broad, with numbers between ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PKpUrsHCPq"},{"type":"text","value":"-20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IbkYANpcJA"},{"type":"text","value":" and around ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FTPFyFsylP"},{"type":"text","value":"20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b4Ca7ntMuM"},{"type":"text","value":". That is why in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yJ4Mogy0cZ"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"hvW4PVByQB"},{"type":"text","value":" output values of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AmTwZBi0SK"},{"type":"inlineMath","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">h</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"qKDeCtOnCg"},{"type":"text","value":", everything is being squashed and capped to be in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gzoTd9XiN6"},{"type":"inlineMath","value":"[-1.0, 1.0]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1.0</mn><mo separator=\"true\">,</mo><mn>1.0</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-1.0, 1.0]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1.0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1.0</span><span class=\"mclose\">]</span></span></span></span>","key":"UGEe2k3pDQ"},{"type":"text","value":" range, with many extreme ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WvYzoaEfVf"},{"type":"text","value":"-1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y6ZHoMmPG6"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FUYIfz5OX7"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ABVGW7MciY"},{"type":"text","value":" values. If you are new to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O3hgwjzZrU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d6OQdpAayV"}],"key":"iwzNOfVODb"},{"type":"text","value":"s, you might not see this as an issue. But if you’re well-versed in the dark arts of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mz49W1Ct15"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ixXhD8fRon"}],"key":"Ndzo0PXYJP"},{"type":"text","value":" and have an intuitive sense of how these gradients flow through a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ty5FbRSR0I"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SQyBIjFjdp"}],"key":"DRKcU8iQxH"},{"type":"text","value":", you are looking at how the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sWBrHiAQ4i"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"mpoUSFItkk"},{"type":"text","value":" values are distributed and you are sweating! Either case, let’s see why this is an issue. First and foremost, we have to keep in mind that during ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nRK166ivSn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZHeVfWgoiX"}],"key":"Dlhz7Zw9pd"},{"type":"text","value":", we do a backward pass by starting at the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RgmlXhP6ZW"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sy8RqJmaer"}],"key":"MZBBjDbLN3"},{"type":"text","value":" and flowing through the network backwards. In particular, we get to a point where we ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BCvM9NwxX2"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KnUYM0I2p4"}],"key":"Oosu8PM03N"},{"type":"text","value":" through the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NUL5CjNeDR"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"CfUpLtt1DY"},{"type":"text","value":" function. If you scroll up to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UOiCtYYe5G"},{"type":"inlineCode","value":"forward()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xUga3p7JDW"},{"type":"text","value":" function, you’ll see that the layer we first ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o25AwHpqnb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ba27B9KLoe"}],"key":"Yu6VpnvDIQ"},{"type":"text","value":" through is the hidden ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TXTJQJkzRK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FMmvinOk83"}],"key":"anaZ2upw50"},{"type":"text","value":" layer (with parameters ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qbCMJaN6Rd"},{"type":"inlineCode","value":"w2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rgpinqqaYa"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cPktWCVaJl"},{"type":"inlineCode","value":"b2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HKLssR5PdI"},{"type":"text","value":"), with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XlgQev7Nuw"},{"type":"inlineCode","value":"n_hidden","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wYDfZILhIB"},{"type":"text","value":" number of neurons, that implements an element-wise ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K6BuJZpaqK"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"RDe5qD0DT5"},{"type":"text","value":" non-linearity. Now, let’s look at what happens in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WohlwMEaKs"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"EoNfJIKAHv"},{"type":"text","value":" in the backward pass. We can actually go back to our very first ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GDF4lgBQVh"},{"type":"link","url":"#1.-micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LFgcCAkEMp"}],"key":"OpDPlDqae1"}],"urlSource":"#1.-micrograd","key":"SpQNufEVfp"},{"type":"text","value":" implementation, in the first notebook and see how we implement ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zrrltpJ7V7"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"JLNAzp6Cgf"},{"type":"text","value":". This is how the gradient of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LopXVSDqmA"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"KI1y7IYZ9e"},{"type":"text","value":" is calculated: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oD6sdDavXd"},{"type":"inlineMath","value":"(1 - t^2) \\cdot \\dfrac{\\partial L}{\\partial out}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>t</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mo>⋅</mo><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">(1 - t^2) \\cdot \\dfrac{\\partial L}{\\partial out}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.0574em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.3714em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathnormal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>","key":"c1qkPwx1Jz"},{"type":"text","value":".  If the value of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VTTFZPEY2n"},{"type":"inlineMath","value":"t","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span>","key":"pkPm77iOy3"},{"type":"text","value":", the output of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XeetWsijfZ"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"LVyvLPTWn0"},{"type":"text","value":" is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tNfprWlYHy"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nmpjLwnKiT"},{"type":"text","value":", then the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kMJ2I6Cjan"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"JE4tvZRuch"},{"type":"text","value":" neuron is basically ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YRIiV9faUZ"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"inactive","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"krRwXtrIct"}],"key":"xci5VZgTqr"},{"type":"text","value":" and the gradient of the previous layer just passes through. Whereas, if ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yT04uZQ3GP"},{"type":"inlineMath","value":"t","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span>","key":"SsCqPnwGPi"},{"type":"text","value":" is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BnmUjvr1fl"},{"type":"text","value":"-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gc85t3r0Gb"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nd5YTjQvRr"},{"type":"text","value":"+1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wGCYWpPz7V"},{"type":"text","value":", then the gradient becomes ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"owyQMP8eUr"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"elcGcs30po"},{"type":"text","value":". This means that if most of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EeEtlxPUyI"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E1KukSfHFw"},{"type":"text","value":" values (outputs of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GobzqeFtvn"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"SFk86JAtM7"},{"type":"text","value":") are close to the flat ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lNF1djgxGY"},{"type":"text","value":"-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YXsilIsqkL"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gfc2H5IvQK"},{"type":"text","value":"+1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mrGtaurm0x"},{"type":"text","value":" regions of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wYMEWurewc"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"Myy6dOeeyg"},{"type":"text","value":" output value range, then the gradients that are flowing through the network are getting destroyed at this layer: an unwanted side-effect. Let’s further investigate the amount of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LRsRUzVGGL"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SeLuNHicsZ"},{"type":"text","value":" activation values that are concentrated at the flat regions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gmVy4Xrb50"}],"key":"mvOxKNFpDj"}],"key":"O1MU6uKJ65"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure(figsize=(20, 10))\nplt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\");","key":"NTkHx0rlku"},{"type":"outputs","id":"0Tcn7XSlsTg-Iqcl5qtPZ","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ac84eb5cfce4435399de6c8e27c3dfcc\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"62fb3ce2e5ab8f5d62445c3fa7f2b584","path":"/build/62fb3ce2e5ab8f5d62445c3fa7f2b584.png"},"text/html":{"content_type":"text/html","hash":"d047e9a908ef239a581f78cc763f29c4","path":"/build/d047e9a908ef239a581f78cc763f29c4.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"TcYpjL5CrE"}],"key":"l7RIs7RPky"}],"key":"Yi76u3T4z2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What we see in this data display are each one of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fW4aNelyyc"},{"type":"text","value":"200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kVqe1l0l7d"},{"type":"text","value":" neurons (columns) per each of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xufGQs4FVf"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QmeP71z5h8"},{"type":"text","value":" examples/batches (rows). A white pixel represents a neuron whose output is in the flat ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WFL6Lvh2qT"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"Ssk09BqQn0"},{"type":"text","value":" region: either ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I9mxNVQcfL"},{"type":"text","value":"-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lbh6sdoajp"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N47NQD7MrH"},{"type":"text","value":"+1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Fknxkqh6Uu"},{"type":"text","value":". Whereas, a black pixel represents a neuron whose output is in-between those flat region values. In other words, the white neurons are all the maximum-valued neurons that block the flow of gradients during ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZA0GHZ61ot"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BeqZE9XC06"}],"key":"YDYSrgKcwI"},{"type":"text","value":". Of course, we would be in grave trouble if for all of these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P74943oc4a"},{"type":"text","value":"200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VjzJZoL1Ze"},{"type":"text","value":" neurons in each column (across all batches) were white. Because in that case we would have what we call a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gJxtLXrN9f"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dead neuron","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RKu1gOL8wt"}],"key":"Kv5mI6XUhY"},{"type":"text","value":". This would be a case wherein the initialization of weights and biases is such that no single example (batch) ever activates a neuron in the active region of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nsd1oVVwwS"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"qkelgQEQOA"},{"type":"text","value":" value range, in between the flat, saturated regions. Since our display does not contain any column of all-whites, for each neuron of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BtVz7GIjzH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LqClwRYvoD"}],"key":"zvD1p4EHt7"},{"type":"text","value":", there are least one or a couple of neurons that activate in the active region, meaning that some gradients will flow through and each neuron will learn. Nevertheless, cases of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oMkaAUouxf"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dead neurons","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nIjNEzBGCj"}],"key":"WqdGEBEo6d"},{"type":"text","value":" are possible and the way this manifests (e.g. for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y2B2LnYCUB"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"w6uG8vf8Dp"},{"type":"text","value":" neurons) is that no matter what inputs you plug in from your dataset, these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ITik1kD5Hj"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dead neurons","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iP2e2xyC8b"}],"key":"bVvDVCFTRE"},{"type":"text","value":" only fire either completely ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ss1i2VXxMC"},{"type":"text","value":"+1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IsstdzXViE"},{"type":"text","value":" or completely ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"joeYwIuiki"},{"type":"text","value":"-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"frXwjnUYPa"},{"type":"text","value":" and then these neurons will just not learn, because all the gradients will be zeroed out. These scenarios are not only true for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"elYueBu0kK"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"PgOrOG5gUH"},{"type":"text","value":", but for many other non-linearities that people use in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lheon01moj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mOwlyfpu8F"}],"key":"AwXv2hGd2m"},{"type":"text","value":"s.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YRNgUtNXEA"}],"key":"ZPqK3IPoEC"}],"key":"dtfwwa1ReD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\ndisplay(Image(filename='activations.png'))","key":"ngK1Zl3nge"},{"type":"outputs","id":"bxVx1bnZgNk8eZIIpMx5U","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"9d5e98524d07afb70a21b4574e95f8f7","path":"/build/9d5e98524d07afb70a21b4574e95f8f7.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"b6hYpLu4nJ"}],"key":"irAWERT5Ah"}],"key":"QDiJTG8s34"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For example, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bl8vTxJgkw"},{"type":"inlineMath","value":"sigmoid","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">sigmoid</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">d</span></span></span></span>","key":"I3qleRtzt1"},{"type":"text","value":" activation function will have the exact same issues, as it is a similar squashing function. Now, consider ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CHptiaOoYA"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"KHIR2uSfiq"},{"type":"text","value":", which has a completely flat region for negative input values. So, if you have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P44OBnivu7"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"uteRSoNTbh"},{"type":"text","value":" neuron, it is a pass-through if it is positive and if the pre-activation value is negative, it will just shut it off, giving an output value of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GLTCwp6JwS"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g1ipn4ICx6"},{"type":"text","value":". Therefore, if a neuron with a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gcbrp4VLwG"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"cOcApsAiLZ"},{"type":"text","value":" non-linearity never activates, so for any inputs you feed it from the dataset it never turns on and remains always in its flat region, then this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lc4wRT5rio"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"kTukQqznev"},{"type":"text","value":" neuron is considered a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rKtHdzenhN"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dead neuron","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pys34ajM1m"}],"key":"gVi1DWUnTn"},{"type":"text","value":": its weights and bias will never receive a gradient and will thus never learn, simply because the neuron never activated. And this can sometimes happen at initialization, because the weights and biases just make it so that by chance some neurons are forever dead. But it can also happen during optimization. If you have too high of a learning rate for example, sometimes you have these neurons that get too much of a gradient and get knocked out of the data manifold, resulting in no example ever activating such a neuron. Consequently, one danger with large gradient is knocking off neurons and making them forever dead. Other non-linearities such as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FQIswTFt3k"},{"type":"inlineMath","value":"leaky ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">leaky ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">ak</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"Tb2NunBII3"},{"type":"text","value":" will not suffer from this issue as much, because of the lack of flat tails, as they’ll almost always yield gradients. But, to return to our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"txiWJ3qVbh"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"FnIcM8YRHs"},{"type":"text","value":" issue, the problem is that our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AgPWVpDL3T"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"tggGa4eUIk"},{"type":"text","value":" pre-activation ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LD3vHC40qX"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GXcNvNshAP"},{"type":"text","value":" values are too far away from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mDVndnlniB"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HrK9rVqJ4X"},{"type":"text","value":", thus yielding a flat region activation distribution that is too saturated at the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r4t3pfUbkw"},{"type":"inlineCode","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fvZXOfBT7C"},{"type":"text","value":" flat regions, which leads to a suppression of learning for many neurons. How do we fix this? One easy way is to decrease the initial value of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fCx04iz44c"},{"type":"inlineCode","value":"w1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ys0yieU1Oz"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tR2InEDvAv"},{"type":"inlineCode","value":"b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"efqxHWV4M8"},{"type":"text","value":" parameters:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Es2zrvfA3G"}],"key":"WVfMJyCcd1"}],"key":"KIsiv4xtpY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w1_factor=0.2, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0)\nhpreact, h, logits, _ = train(xtrain, ytrain, maxsteps=1)\nplt.figure(figsize=(20, 10))\nplt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\");","key":"BUyHs0l8qS"},{"type":"outputs","id":"d965qCH6kq61gWuNjyOdS","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/      1: 3.3174\n"},"children":[],"key":"xsEhRno4eX"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"dc6df1fdb2d04150aa45ce47e5e81eff\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"cea4b537d93550d7e9fcf9adbfd8092b","path":"/build/cea4b537d93550d7e9fcf9adbfd8092b.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=2000.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"EaufkzifxY"}],"key":"OGbEDra3bG"}],"key":"BPv4UoSJkd"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, our activations are not as saturated above 0.99 as they were before, with only a few white neurons. What is more, the activations are now more evenly distributed and the range of pre-activations is now significantly narrower:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KQp3MpdyiH"}],"key":"AaK1GEHmJD"}],"key":"pCQzXNFZdZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.hist(h.view(-1).tolist(), 50)\nplt.figure()\nplt.hist(hpreact.view(-1).tolist(), 50);","key":"ZK7SOXuKzn"},{"type":"outputs","id":"q3ZmvxPwc4jfGynMIThd_","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9a39a55168e54b7287ab443f4a13fa59\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"4de34a5746b137dae102ef44408a70ca","path":"/build/4de34a5746b137dae102ef44408a70ca.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=640.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"QOItJYtsfb"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"5b8ce4511b554762a6b736293c96a9e3\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"93fbf982e5ef8f8b2fb18d18a5547190","path":"/build/93fbf982e5ef8f8b2fb18d18a5547190.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=640.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"mvIovNEVuO"}],"key":"P090SdQSmM"}],"key":"vWD0QUqnCL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Since distributions look nicer now, perhaps this is what our initialization should be. Let’s now train a new network with this initialization setting and print the losses:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JwssHDP409"}],"key":"dqw5aFhEWo"}],"key":"ksKH3YUdZY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(w1_factor=0.2, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0)\n_ = train(xtrain, ytrain)","key":"dvguW6GJxJ"},{"type":"outputs","id":"IOWS-Zk0UKwvxwQ1tTx9C","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/ 200000: 3.3052\n  10000/ 200000: 2.6664\n  20000/ 200000: 2.5232\n  30000/ 200000: 2.0007\n  40000/ 200000: 1.8163\n  50000/ 200000: 2.1677\n  60000/ 200000: 2.2280\n  70000/ 200000: 2.5228\n  80000/ 200000: 2.1911\n  90000/ 200000: 2.4983\n 100000/ 200000: 2.1451\n 110000/ 200000: 1.7719\n 120000/ 200000: 1.9741\n 130000/ 200000: 1.6981\n 140000/ 200000: 1.8294\n 150000/ 200000: 1.8194\n 160000/ 200000: 2.0302\n 170000/ 200000: 2.0787\n 180000/ 200000: 1.9397\n 190000/ 200000: 2.0921\n"},"children":[],"key":"w1TCwpvbuB"}],"key":"lSU3VQeMJ1"}],"key":"aMdNFLgA5d"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_loss(xtrain, ytrain, prefix='train')\nprint_loss(xval, yval, prefix='val');","key":"HcYDUbG1Kc"},{"type":"outputs","id":"WUYVuPx9_4VWdzh3lb3gO","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.0368359088897705\nval 2.1042771339416504\n"},"children":[],"key":"dCzubI2n2g"}],"key":"l6WLFMn9Iz"}],"key":"JKDagwOHAQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The validation loss continues to improve! This exercise clarifies the effect of good initialization on performance and emphasizes being aware of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wcLD3PAjdn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P8620YMe1A"}],"key":"mVSgHc6V2k"},{"type":"text","value":" internals like activations and gradients. Now, we’re working with a very small network which is basically just a 1-hidden layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qPF55ylGBP"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fmEJEAmIP9"}],"key":"bAhp4jGv8Y"},{"type":"text","value":". Because the network is so shallow, the optimization problem is quite easy and very forgiving. So, even though our initialization in the beginning was terrible, the network still learned eventually. It just got a bit of a worse result. This is not the case in general though. Once we actually start working with much deeper networks that have say 50 layers, things can get much more complicated and these problems stack up, and it is often not surprising to get into a place where a network is basically not training at all, if your initialization is bad enough. Generally, the deeper and more complex a network is, the less forgiving it is to some of the aforementioned errors. But what has worked so far with our simple example is great! However, we have come up with a bunch of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PuR6Xppf08"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"magic","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HSyeN3mKLI"}],"key":"zMu6TP5ce4"},{"type":"text","value":" weight and bias factors (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CmTydXuF1j"},{"type":"inlineCode","value":"w1_factor","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CNaYaAoJKN"},{"type":"text","value":"). How did we come up with these? And how are we supposed to set these if we have a large ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K65KINV2Ln"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ebN1LTCdMs"}],"key":"D0SQxhHxLx"},{"type":"text","value":" with lots and lots of layers? As you might assume, no one sets these by hand. And there’s rather principled ways of setting these scales that I’d like to introduce to you now.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BWh2G3XDte"}],"key":"UjecaHZ2wK"}],"key":"xeqfZgp3wN"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Learning to set the factors","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nbg401Q7iv"}],"identifier":"learning-to-set-the-factors","label":"Learning to set the factors","html_id":"learning-to-set-the-factors","implicit":true,"key":"LDsyuxoyPT"}],"key":"SbJy33uVrb"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s start with a short snippet, just to begin to motivate this discussion by defining an input tensor of many multi-dimensional examples and a weight tensor of a hidden layer, both drawn from a Gaussian distribution. We’ll calculate the mean and standard deviation of these inputs and the corresponding pre-activations:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z9eavU83dU"}],"key":"gyWk3oCfiE"}],"key":"qVDXePz9I6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def plot_x_y_distributions(n_inputs=10, weight_factor=1.0):\n    x = torch.randn(1000, n_inputs)  # many examples of inputs\n    w = torch.randn(n_inputs, 200) * weight_factor  # weights of the hidden layer\n    y = x @ w  # pre-activations\n    print(x.mean(), x.std())\n    print(y.mean(), y.std())\n    plt.figure(figsize=(20, 5))\n    plt.subplot(121)\n    plt.hist(x.view(-1).tolist(), 50, density=True)\n    plt.subplot(122)\n    plt.hist(y.view(-1).tolist(), 50, density=True)","key":"LcQHI2xrl0"},{"type":"outputs","id":"WTaPqsubWzGsCPhjLu4E4","children":[],"key":"Ks9fcZUaC7"}],"key":"GHjJdeG33t"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_x_y_distributions()","key":"uwMXU3yK1H"},{"type":"outputs","id":"ckorfSqmEPLyCpW8LPaJc","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(-0.0020) tensor(1.0074)\ntensor(0.0011) tensor(3.2319)\n"},"children":[],"key":"Yvj9dx14Rm"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"5975360a86ad458cac35510b0ae38867\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"3530ac3265311f41d67d9502b1757e2f","path":"/build/3530ac3265311f41d67d9502b1757e2f.png"},"text/html":{"content_type":"text/html","hash":"f7ada954b29e489b09cfad97a019b232","path":"/build/f7ada954b29e489b09cfad97a019b232.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"QZX7CrIDVq"}],"key":"s7PhVMLKpf"}],"key":"ex2PuCIMFs"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If you notice, the std of the pre-activations ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jBQc2s3RxR"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lyqOmI6SV8"},{"type":"text","value":" has increased compared to that of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sP7aBJgFun"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jAJlpHa5Cd"},{"type":"text","value":", as can also be seen by the widening of the Gaussian. The left Gaussian has basically undergone a stretching operation, resulting in the expanded right plot. We don’t want that. We want most of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tlxcQuLfhp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HnTLmpoeGo"}],"key":"Y2VuQKUwJw"},{"type":"text","value":" to have relatively similar activations with a relatively uniform Gaussian throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zQvJdc3wBQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bDnn5nGWTm"}],"key":"C7mRt1Txb8"},{"type":"text","value":". So the question becomes, how do we scale these weight vectors (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IvsI2GZ9AU"},{"type":"inlineCode","value":"w","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xRvi43cbnp"},{"type":"text","value":") to preserve the Gaussian distribution of the inputs (e.g. left)? Let’s do some experiments. If we scale ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F5FBpxeobQ"},{"type":"inlineCode","value":"w","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qBBlJfRNSZ"},{"type":"text","value":" by a large number, e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bAxtnKKeg9"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zSwjBQ2SEG"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YJKdw1dBCv"}],"key":"dXFcLXYshl"}],"key":"rQV3w698a8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_x_y_distributions(weight_factor=5)","key":"kZNf86ZVDw"},{"type":"outputs","id":"vsQBmfg9VNHDbjvgkRWB6","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(0.0091) tensor(0.9923)\ntensor(-0.0045) tensor(15.6379)\n"},"children":[],"key":"V7qnvMVnsM"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"31a68d0130ad40e986412ddc5b6ca6ec\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"2b8d661ad297ce792e6e2edd46524dc4","path":"/build/2b8d661ad297ce792e6e2edd46524dc4.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=2000.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Fg8dG6Sseg"}],"key":"bEWxT4KNxN"}],"key":"MeTiY19vyT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"this Gaussian grows and grows in std, with the outputs in the x-axis taking on more and more extreme values (right plot). But if we scale the weights down, e.g. by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xYXoK6GuxJ"},{"type":"text","value":"0.2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Us04w5XZbb"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aFj3yoUykr"}],"key":"UsqmMDiRdr"}],"key":"Uamv4G7Qg3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_x_y_distributions(weight_factor=0.2)","key":"zuFRW6f1Mj"},{"type":"outputs","id":"g9NXxoSZYx4Mi7KsC5ebD","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(-0.0002) tensor(1.0079)\ntensor(-0.0013) tensor(0.6582)\n"},"children":[],"key":"f6RsMdsaVh"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"7e28a2a6002a4958933032f4ee98a4f4\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"4cc29256ebb3780dfa446011dcb7d09f","path":"/build/4cc29256ebb3780dfa446011dcb7d09f.png"},"text/html":{"content":"\n            <div style=\"display: inline-block;\">\n                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n                    Figure\n                </div>\n                <img src='data:image/png;base64,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' width=2000.0/>\n            </div>\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"n14YcpBEzc"}],"key":"TYAhF52TxE"}],"key":"knAGOXYLZ5"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"then, conversely, the Gaussian gets smaller and smaller and it’s shrinking. Notice the std of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CdT1GVRAsM"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WYAdbBwHTi"},{"type":"text","value":" now being smaller than that of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NAaep0D323"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c0xCslkBfi"},{"type":"text","value":". The question then becomes: what is the appropriate factor to exactly preserve the std of the inputs? And it turns out that the correct answer, mathematically, (when you work out the variance of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RsbUpphP4R"},{"type":"inlineCode","value":"x @ w","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HwtqyjH0Sf"},{"type":"text","value":" multiplication), is that you are supposed to divide by the square root of the fan-in. Meaning, the square root of the number of inputs. Therefore if the number of inputs is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S9IxmujT1m"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PKJSMYlWzl"},{"type":"text","value":" then the appropriate factor for preserving the Gaussian distribution of the inputs is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MYzqacbxYf"},{"type":"inlineMath","value":"10^{-1/2}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">10^{-1/2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.888em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.888em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1/2</span></span></span></span></span></span></span></span></span></span></span></span>","key":"kyp7kWhG4K"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"enjzs5Qj8N"}],"key":"hHDG9JmVJB"}],"key":"Mf5VNl3tMH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_x_y_distributions(n_inputs=10, weight_factor=10**-0.5)","key":"WEhZqZuQhl"},{"type":"outputs","id":"DsGZLeH34JC7i2s-Unlk5","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(0.0140) tensor(1.0000)\ntensor(0.0028) tensor(1.0127)\n"},"children":[],"key":"XzUQ025EHJ"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"df97daf34e904125be75d0176356288b\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"72df299f343451b15dffea3827ed5fd6","path":"/build/72df299f343451b15dffea3827ed5fd6.png"},"text/html":{"content_type":"text/html","hash":"36e2fdae31b8d09b47858f693dcac504","path":"/build/36e2fdae31b8d09b47858f693dcac504.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"mqvn7G0EYJ"}],"key":"nzgxVXGszT"}],"key":"Rak0xTbCN6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now we see that the std remains roughly the same! Now, unsuprisingly, a number of papers have looked into how to best initialize ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XRNBXMjhnS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SgFWJpo3YX"}],"key":"b4Wey60xQO"},{"type":"text","value":"s and in the case of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z4uGeWbczJ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pQIPLrjFMv"}],"key":"H2AqcXWuj1"},{"type":"text","value":"s, we can have these fairly deep networks that have these nonlinearities in between layers and we want to make sure that the activations are well-behaved and they don’t expand to infinity or shrink all the way to zero. And the question is, how do we initialize the weights so that they take on reasonable values throughout the network. In ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ds2QDHHOie"},{"type":"link","url":"https://arxiv.org/abs/1502.01852","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N8ZWTz1tfi"}],"urlSource":"https://arxiv.org/abs/1502.01852","key":"ICTVcmIdk1"},{"type":"text","value":", they study convolutional ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z68iBacQX8"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"plDB0FNtjM"}],"key":"OTQ87dysVu"},{"type":"text","value":"s (CNNs) and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hd7e5q0gjN"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"Fqrrw9CeBR"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FAoERNpsNc"},{"type":"inlineMath","value":"PReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">PReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">PR</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"dnWL3AkvlG"},{"type":"text","value":" non-linearities. But the analysis is very similar to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cx2GAkfPW6"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"ojRflzqRUY"},{"type":"text","value":" non-linearity. As we saw previously, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vDs42FKXD0"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"Kh7KorDZkY"},{"type":"text","value":" is somewhat of a squashing function where all the negative values are simply clamped to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PDDr9dY0iE"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DlqeKqO3Wi"},{"type":"text","value":". Because with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qC4b0wtOKE"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"yT1EULGWly"},{"type":"text","value":"s half of the distribution is thrown away, they find in their analysis of the forward activations of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jWGs7xictl"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V8GXzfPMo2"}],"key":"ANQ8w3DHaG"},{"type":"text","value":", that you have to compensate for that with a gain. They find that to initialize their weights they have to do it with a zero-mean Gaussian whose std is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zPxsdPiiDP"},{"type":"inlineMath","value":"\\sqrt{2/n_l}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><mrow><mn>2</mn><mi mathvariant=\"normal\">/</mi><msub><mi>n</mi><mi>l</mi></msub></mrow></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{2/n_l}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.24em;vertical-align:-0.305em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.935em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mord\">2/</span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.895em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.305em;\"><span></span></span></span></span></span></span></span></span>","key":"r0wDYuJH3Z"},{"type":"text","value":". We just did the same, multiplying our weights by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"afD038h1lK"},{"type":"inlineMath","value":"\\sqrt{1/10}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><mrow><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>10</mn></mrow></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{1/10}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.24em;vertical-align:-0.305em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.935em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mord\">1/10</span></span></span><span style=\"top:-2.895em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.305em;\"><span></span></span></span></span></span></span></span></span>","key":"opdQfTsToD"},{"type":"text","value":" (the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z9eINuOipY"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pYhHOxOqaS"},{"type":"text","value":" has to do with the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uxnSUj90XC"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"H90VIUymry"},{"type":"text","value":" activation function they use). They also study the backward propagation case, finding that the backward pass is also approximately initialized up to a constant factor ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RatBCiPhBm"},{"type":"inlineMath","value":"c_2/d_L","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>c</mi><mn>2</mn></msub><mi mathvariant=\"normal\">/</mi><msub><mi>d</mi><mi>L</mi></msub></mrow><annotation encoding=\"application/x-tex\">c_2/d_L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">c</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">/</span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"R6atmOJ1kA"},{"type":"text","value":" that has to do with the number of hidden neurons in early and late layer. Now, this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UgsdXxy545"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Kaiming initialization is also implemented in pytorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dzfR0xnmDZ"}],"urlSource":"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_","key":"DQ8ViiAURq"},{"type":"text","value":" and it is probably the most common way of initializing ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yu8LoTc1Ee"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P6nShMk0Sy"}],"key":"E24QB2kh7A"},{"type":"text","value":"s now. This PyTorch method takes a mode and nonlinearity argument among others, with the latter determining the gain factor (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xOUOvQ9FC4"},{"type":"inlineMath","value":"\\sqrt{2}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.04em;vertical-align:-0.1328em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9072em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\">2</span></span></span><span style=\"top:-2.8672em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1328em;\"><span></span></span></span></span></span></span></span></span>","key":"Hv7W9ROgXE"},{"type":"text","value":"). Why do we need a gain? For example, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M11XXWOgxS"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"CHbjxmowvE"},{"type":"text","value":" is a contractive transformation that squashes the output distribution by taking any negative value and clamping it to zero. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AgGAbKezjV"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"eWiIGasQIj"},{"type":"text","value":" also squashes in some way, as it will squeeze values at the tails of its range. Therefore, in order to fight the squeezing-in of these activation functions, we have to boost the weights a little bit in order to counteract this effect and re-normalize everything back to standard unit deviation. So that’s why there’s a little bit of a gain that comes out. Now we’re actually intentionally skipping through this section quickly. The reason for that is the following. Around 2015, when this paper was written, you had to actually be extremely careful with the activations and the gradients, their ranges, their histograms, the precise setting of gains and the scrutinizing of the non-linearities and so on... So, everything was very finicky and very fragile and everything had to be very properly arranged in order to train a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kyNYa5Ih8x"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tXH6Wjz5ZN"}],"key":"SPGqBvg175"},{"type":"text","value":". But there are a number of modern innovations that made everything significantly more stable and well-behaved. And it has become less important to initialize these networks “exactly right”. Some of those innovations are for example: residual connections (which we will cover in the next notebooks), a number of normalization layers (e.g. batch normalization, layer normalization, group normalization) and of course much better optimizers: not just stochastic gradient descent (the simple optimizer we have been using), but slightly more complex optimizers such as RMSProp and especially Adam. All of these modern innovations make it less important for you to precisely calibrate the initialization of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xuuM3wTPAx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UGbS819zsd"}],"key":"lCsxYMk1kZ"},{"type":"text","value":". So, what do people do in practice? They usually initialize their weights with Kaiming-normally, like we did. Now notice how the following multiplier ends up being the std of Gaussian distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jrc4qzJX7U"}],"key":"iwGt2n2mXM"}],"key":"VMphiZJgGY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"multiplier = 0.2\n(torch.randn(10000) * multiplier).std().item()","key":"Wo5TmMwoB3"},{"type":"outputs","id":"wEtVrCw6T72jArU4uOOTu","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":55,"metadata":{},"data":{"text/plain":{"content":"0.1984076201915741","content_type":"text/plain"}}},"children":[],"key":"sCviqWdBQ0"}],"key":"TJrt43ib4v"}],"key":"ZcQktR5cCB"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"But, according to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VJE7kKYmWE"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"kaiming PyTorch docs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EpV182aS9r"}],"urlSource":"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_","key":"F794JKugLo"},{"type":"text","value":", we want an std of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zm7VgLN1fT"},{"type":"inlineMath","value":"\\frac{gain}{\\sqrt{fan\\_in}}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mi>g</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow><msqrt><mrow><mi>f</mi><mi>a</mi><mi>n</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi></mrow></msqrt></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{gain}{\\sqrt{fan\\_in}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.7374em;vertical-align:-0.8296em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9078em;\"><span style=\"top:-2.5046em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord sqrt mtight\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9791em;\"><span class=\"svg-align\" style=\"top:-3.4286em;\"><span class=\"pstrut\" style=\"height:3.4286em;\"></span><span class=\"mord mtight\" style=\"padding-left:1.19em;\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal mtight\">an</span><span class=\"mord mtight\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal mtight\">in</span></span></span><span style=\"top:-2.9511em;\"><span class=\"pstrut\" style=\"height:3.4286em;\"></span><span class=\"hide-tail mtight\" style=\"min-width:0.853em;height:1.5429em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.5429em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4774em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.4461em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal mtight\">ain</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8296em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>","key":"uoyABWn2nm"},{"type":"text","value":". Therefore, for a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nqiMkeyBlT"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"OTjxTOQtSy"},{"type":"text","value":" nonlinearity:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F1fhsTM0Op"}],"key":"oTz1WIcic7"}],"key":"YTgzqG7ZMS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"n_embd = 10\nkaiming_w1_factor = (5 / 3) / ((n_embd * block_size) ** 0.5)","key":"KfmnjGyGsY"},{"type":"outputs","id":"04Mm_t4bUEWQnVuJMCAHa","children":[],"key":"dISnQi1Kwf"}],"key":"nayI3vSLud"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now let’s re-initialize and re-train our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VOdn3Qmf9i"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lvtIpGRwCI"}],"key":"nvCDcfb5Lf"},{"type":"text","value":" with this initilization:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x7fByGqjFl"}],"key":"iFy3rum9dk"}],"key":"QPVuwLR5wV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(\n    w1_factor=kaiming_w1_factor, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0\n)\nhpreact, _, _, _ = train(xtrain, ytrain)","key":"JNDi9hqm1v"},{"type":"outputs","id":"kxoqftYAhIfRSDNcIKCJ_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"11897\n      0/ 200000: 3.3202\n  10000/ 200000: 2.0289\n  20000/ 200000: 2.2857\n  30000/ 200000: 1.9499\n  40000/ 200000: 1.8151\n  50000/ 200000: 2.2310\n  60000/ 200000: 2.1923\n  70000/ 200000: 1.9824\n  80000/ 200000: 2.1736\n  90000/ 200000: 2.1285\n 100000/ 200000: 2.0951\n 110000/ 200000: 1.9936\n 120000/ 200000: 1.8634\n 130000/ 200000: 2.3813\n 140000/ 200000: 1.7260\n 150000/ 200000: 1.7054\n 160000/ 200000: 1.8842\n 170000/ 200000: 2.1857\n 180000/ 200000: 1.7953\n 190000/ 200000: 1.6770\n"},"children":[],"key":"pJ8xA8Dtoo"}],"key":"fZ31BfqttF"}],"key":"HcMSnsDflH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_loss(xtrain, ytrain, prefix='train')\nprint_loss(xval, yval, prefix='val');","key":"JcrioE1rsW"},{"type":"outputs","id":"YKYjZpj0020hIzWPzZjen","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.040121078491211\nval 2.1033377647399902\n"},"children":[],"key":"k5ZqSd1uJf"}],"key":"dXWEKF7i8k"}],"key":"EWiLWqLK36"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Of course, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LR1Ol2KHw2"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Pdm7DylqTW"}],"key":"FujYNRfV3l"},{"type":"text","value":" is quite similar to before. The difference now though is that we don’t need to inspect histograms and introduce arbitrary factors. On the contrary, we now have a semi-principled way to initialize our weights that is also scalable to much larger networks which we can use as a guide. However, this precise weight initialization is not as important as we might think nowadays, due to some modern innovations.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cnEpBNVv24"}],"key":"L7qVGfWmrI"}],"key":"lrIzZpkypC"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E5Lz0XSlcS"}],"identifier":"batchnorm","label":"Batchnorm","html_id":"batchnorm","implicit":true,"key":"swT4X7Ee2b"}],"key":"kEiqwnJDJe"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s now introduce one of them. Batch Normalization (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aYbGIni1AO"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FPXDuziuLx"}],"key":"OuYaYKp22B"},{"type":"text","value":") came out in 2015 from a team at Google in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YHb6RbgmCD"},{"type":"link","url":"https://arxiv.org/abs/1502.03167","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"an extremely impactful paper","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xaAo72e1u7"}],"urlSource":"https://arxiv.org/abs/1502.03167","key":"PARmBCLNvO"},{"type":"text","value":", making it possible to train deep neural networks quite reliably. It basically just worked. Here’s what ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i68ZT0DWUn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RlgIRg2ofj"}],"key":"qIuVG0f6a8"},{"type":"text","value":" does and how it’s implemented. Like we mentioned before, we don’t want the pre-activations (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WUxiCZRYbO"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vObnEdClxm"},{"type":"text","value":") to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K2wj7YZE7R"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"vEtybTjAaR"},{"type":"text","value":" to be too small, nor too large because then the outputs will turn out either close to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aKbqI0GtSV"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gmNX9vmsNZ"},{"type":"text","value":" or saturated. Instead, we want the pre-activations to be roughly Gaussian (with a zero mean and a std of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mQnvuGgAe0"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G87URBRBaZ"},{"type":"text","value":"), at least at initialization. So, the insight from the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YFX6s5KG1d"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V4zpShJjmo"}],"key":"PlFleZhigh"},{"type":"text","value":" paper is: ok, we have these hidden pre-activation states/values that we’d like to be Gaussian, then why not take them and just normalize them to be Gaussian? Haha. I know, it sounds kinda crazy but you can just do that, because ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tNhPAEOsgU"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"standardizing","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s56x5lcBDD"}],"key":"Bhs6WcfbzB"},{"type":"text","value":" hidden states so that they become Gaussian is a perfectly differentiable operation. And so the gist of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uF87ymCd5Z"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zLAO3mYisP"}],"key":"u3v3n8zTdo"},{"type":"text","value":" is that if we want unit Gaussian hidden states in our network, then we can just normalize them to be so. Let’s see how that works. If you scroll up to our definition of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rVn5Af4e8A"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Wj6WDFKypm"},{"type":"text","value":" function we can see the pre-activations ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OBIiOzz50q"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v7t9RwAsKY"},{"type":"text","value":" before they are fed into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yal5optBkt"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"CeNmgjxrhY"},{"type":"text","value":" function. Now, the idea, remember, is we are trying to make these roughly Gaussian. If the values are too small, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jg9xzoUmEC"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"Ub6EKogoTX"},{"type":"text","value":" is kind of inactive, whereas if they are very large, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z9WKxiDGi9"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"odjeKSrbiT"},{"type":"text","value":" becomes very saturated and the gradients don’t flow. So, let’s learn how to standardize ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r0nAS9qAbU"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dzjMWQu4O0"},{"type":"text","value":" to be roughly Gaussian.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O0tBEvqllI"}],"key":"qEmTpw3bK1"}],"key":"C9ywQ1fRQA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"hpreact.shape","key":"hJyNMYFH2K"},{"type":"outputs","id":"NyBG-d7TYwcmx7h81sU73","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":59,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 200])","content_type":"text/plain"}}},"children":[],"key":"aQBiNmtXgV"}],"key":"YTAUMzdWg4"}],"key":"xP4xY2Tdt3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"hpmean = hpreact.mean(0, keepdim=True)\nhpmean.shape","key":"jgPDEWUnV4"},{"type":"outputs","id":"iWmEHWX7MD1j8qiYuMXeR","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":60,"metadata":{},"data":{"text/plain":{"content":"torch.Size([1, 200])","content_type":"text/plain"}}},"children":[],"key":"vZbUp82wJL"}],"key":"mVSlvlELBl"}],"key":"My8OCq3JtK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"hpstd = hpreact.std(0, keepdim=True)\nhpstd.shape","key":"tQQ5bm6Hqy"},{"type":"outputs","id":"AZMYDBOBkMJWnVospCw6x","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":61,"metadata":{},"data":{"text/plain":{"content":"torch.Size([1, 200])","content_type":"text/plain"}}},"children":[],"key":"wKODRGvFW8"}],"key":"uktMhHbxjd"}],"key":"VzAhT4KhIt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"After calculating the mean and std across batches of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RYvddjaXK2"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MswoPYdHNp"},{"type":"text","value":", which in the paper are referred to the “mini-batch mean” and “mini-batch variance”, respectively, next, following along the paper, we are going to normalize or standardize the inputs (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"giipJ6Qlzz"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fiJR7a8231"},{"type":"text","value":") by subtracting the mean and dividing by the std. Basically:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XYB7c9oHha"}],"key":"X63etqiSdA"}],"key":"dDR9VEC41b"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"hpreact = (hpreact - hpmean) / hpstd","key":"gQMDOCtJyK"},{"type":"outputs","id":"I-u7VSA_-rJ8r9kV7Sm6n","children":[],"key":"J1FyJ7DOq4"}],"key":"u56acjj6WL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What normalization does is that now every single neuron and its firing rate will be exactly unit Gaussian for each batch (which is why it’s called ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q8IBLJ3sKU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"InQSnfVtt4"}],"key":"cfM8KxHSzI"},{"type":"text","value":"). Now, we could in principle train using normalization. But we would not achieve a very good result. And the reason for that is that we want the pre-activations to be roughly Gaussian, but only at initialization. But we don’t want these to be forced to be Gaussian always. We’d like to allow the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MWvFJu1hEP"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"efi9KUAxyb"}],"key":"epKH4cH98n"},{"type":"text","value":" to move the distributions around, such as making them more diffuse, more sharp, perhaps to make some ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cUzAfuGAxd"},{"type":"inlineCode","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MQXNub2qr6"},{"type":"text","value":" neurons to be more trigger-happy or less trigger-happy. So we’d like this distribution to move around and we’d like the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wCxVb2BK87"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n6MWkJvhoi"}],"key":"YuRtn3VH9Q"},{"type":"text","value":" to tell us how that distribution should move around. So in addition to standardization of any point in the network, we have to also introduce this additional component mentioned in the paper described as “scale and shift”. Basically, what we want to be doing is multiplying the normalized values by a gain ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bkLUsDzCnr"},{"type":"inlineMath","value":"\\gamma","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>","key":"X60QX01avV"},{"type":"text","value":" and then addding a bias ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rwfub8dBsT"},{"type":"inlineMath","value":"\\beta","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span>","key":"T5YqDOTp7a"},{"type":"text","value":" to get a final output of each layer. Let’s define them:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vnOgwVGoTK"}],"key":"oZqSKKSmnG"}],"key":"au6tR3AtSK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bngain = torch.ones(1, hpreact.shape[1])\nbnbias = torch.zeros(1, hpreact.shape[1])","key":"aS2ivgyGEy"},{"type":"outputs","id":"NdP0QV1XnxPBpU3mTdUat","children":[],"key":"pF56RCap5o"}],"key":"RbdObUKWBw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"so that:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B7kOZFSAbv"}],"key":"RsDDpiIDmE"}],"key":"FURU934SlQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"hpreact = bngain * (hpreact - hpmean) / hpstd + bnbias","key":"XjkPEzUBOo"},{"type":"outputs","id":"wCBLLeieJR05QDTL10OeW","children":[],"key":"FnLSmGwpUO"}],"key":"WD4we1ImbO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Because the gain is initialized to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ofMIqrcBUN"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WW2kPdB5tb"},{"type":"text","value":" and the bias to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nejjuEVLvA"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S86Rhf4GWC"},{"type":"text","value":", at initialization, each neuron’s firing values in this batch will be exactly unit Gaussian and we’ll have nice numbers, regardless of what the distribution of the incoming (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wx9wnM9o7z"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rGXOWdTq8J"},{"type":"text","value":") tensors are. That is roughly what we want, at least during initialization. And during optimization, we’ll be able to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vHmiGpm10M"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QtxTuw0kVO"}],"key":"pCV0OCsExv"},{"type":"text","value":" and change the gain and the bias, so the network is given the full ability to do with this whatever it wants internally. In order to train these, we have to make sure to include these in the parameters of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a8HBxDfJr0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uetSpWHKCG"}],"key":"nggsCY7FSp"},{"type":"text","value":". To do so, and by effect facilitate ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q7s0iaMIa7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S6CW6xQTiW"}],"key":"O4buDO5BRU"},{"type":"text","value":", let’s update our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jxSPmM9uam"},{"type":"inlineCode","value":"define_nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DFP9BVtvn3"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gKfQLqIYir"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ioZbfg2ugS"},{"type":"text","value":" functions accordingly:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JQY4TvFk4i"}],"key":"x0S6rcijbt"}],"key":"FWoaBkpmXs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, b1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    b1 = torch.randn(n_hidden, generator=g) * b1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    bngain = torch.ones((1, n_hidden))\n    bnbias = torch.zeros((1, n_hidden))\n    parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return parameters","key":"dbLRIS9UMl"},{"type":"outputs","id":"GkeyynfcjfART_iT-LTTm","children":[],"key":"WtoTgoLjzW"}],"key":"VXTyo8xaaX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(x, y):\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    hpreact = (\n        bngain\n        * (hpreact - hpreact.mean(0, keepdim=True))\n        / hpreact.std(0, keepdim=True)\n        + bnbias\n    )\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss","key":"vGM6ybDwLm"},{"type":"outputs","id":"U3aoV2kzLomTeaTX3Gjh_","children":[],"key":"bOlH1mnAUE"}],"key":"t3pJ3b2PbY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And now, let’s initialize our new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"reIEQXnXHi"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rEjWaKmt9t"}],"key":"E1o1JGoZOj"},{"type":"text","value":" and train!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kar1k56Z1G"}],"key":"H6gjNCgLHH"}],"key":"BLkK66oE0E"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(\n    w1_factor=kaiming_w1_factor, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0\n)\n_ = train(xtrain, ytrain)\nprint_loss(xtrain, ytrain, prefix=\"train\")\nprint_loss(xval, yval, prefix=\"val\");","key":"pB559fZSw3"},{"type":"outputs","id":"1m5VwVpw4LQTtzGWImvIA","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"12297\n      0/ 200000: 3.3033\n  10000/ 200000: 2.4509\n  20000/ 200000: 2.2670\n  30000/ 200000: 2.2647\n  40000/ 200000: 2.2298\n  50000/ 200000: 1.9236\n  60000/ 200000: 2.1204\n  70000/ 200000: 2.3262\n  80000/ 200000: 2.1840\n  90000/ 200000: 1.9809\n 100000/ 200000: 2.2282\n 110000/ 200000: 2.0057\n 120000/ 200000: 2.0795\n 130000/ 200000: 1.8743\n 140000/ 200000: 2.2611\n 150000/ 200000: 1.4639\n 160000/ 200000: 2.2464\n 170000/ 200000: 2.2788\n 180000/ 200000: 2.3267\n 190000/ 200000: 1.7778\ntrain 2.0695457458496094\nval 2.1072680950164795\n"},"children":[],"key":"gmeJtE3dUm"}],"key":"SvO3F7LnUU"}],"key":"uPEg9qQf6N"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We get a loss that is comparable to our previous results. Here’s a rough summary of our losses (re-running this notebook might yield slightly different values, but you get the point):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JknvVcISyi"}],"key":"A71esP2e86"}],"key":"d7aUr3WxxN"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"# loss log\n\n# original:\ntrain 2.127638339996338\nval 2.171938180923462\n\n# fix softmax confidently wrong\ntrain 2.0707266330718994\nval 2.1337196826934814\n\n# fix tanh layer saturated at init\ntrain 2.0373899936676025\nval 2.1040639877319336\n\n# use semi-principled kaiming initialization instead of hacky way:\ntrain 2.038806438446045\nval 2.108304977416992\n\n# add a batchnorm layer\ntrain 2.0688135623931885\nval 2.10699462890625","position":{"start":{"line":1,"column":1},"end":{"line":23,"column":1}},"key":"NY8wjZwygM"}],"key":"d7WLe4iMsb"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"However, we should not actually be expecting an improvement in this case. And that’s because we are dealing with a very simple ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ko5GVGEhqX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J2puB8SatI"}],"key":"uLul3oRVnZ"},{"type":"text","value":" that has just a single hidden layer. In fact, in this very simple case of just one hidden layer, we were actually able to calculate what the scale of the weights should be to have the activations have a roughly Gaussian shape. So, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ljlJDfw1ts"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ISiQxqcvws"}],"key":"szbt0k5sif"},{"type":"text","value":" is not doing much here. But you might imagine that once you have a much deeper ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C1MGLVNEis"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ni9QfYZtHC"}],"key":"JVkYBahIn6"},{"type":"text","value":", that has lots of different types of operations and there’s also, for example, residual connections (which we’ll cover) and so on, it will become very very difficult to tune the scales of the weight matrices such that all the activations throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ABvcOu5gkn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YNt25FfT4a"}],"key":"mECi9rwo5G"},{"type":"text","value":" are roughly Gaussian: at scale, an intractable approach. Therefore, compared to that, it is much much easier to sprinkle ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yswc2EWDH0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Dq0t5Hh9iE"}],"key":"vQj6BZEbHL"},{"type":"text","value":" layers throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hyKt4HKuui"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LrCfeDocKu"}],"key":"SkvHuPQngb"},{"type":"text","value":".  In particular, it’s common to look at every single linear layer like this one ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DvwFzG9rQC"},{"type":"inlineCode","value":"hpreact = embcat @ w1 + b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s272UWpjTt"},{"type":"text","value":" (multiply by a weight matrix and add a bias), or for example convolutions that also perform matrix multiplication, (just in a more “structured” format) and append a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bPy0yhDPls"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XdP2jd0zUJ"}],"key":"K1xdiS8RXj"},{"type":"text","value":" layer right after it to control the scale of these activations at every point in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xIVG01LyYb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j2oUiy0urt"}],"key":"GJ3lhKQPn5"},{"type":"text","value":". So, we’d be adding such normalization layers throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DpEc42LDlC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qPiapv5eYY"}],"key":"LV9Qo9f2dL"},{"type":"text","value":" to control the scale of these activation (again, throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xwvDVrr7LC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MhtIWPdBC9"}],"key":"gA8WWpInPF"},{"type":"text","value":") without requiring us to do perfect mathematics in order to manually control individual activation distributions for any “building block” (e.g. layer) we might want to introduce into our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RQ0W9TxUX6"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SiIEJyajme"}],"key":"Kg4vxPBgEk"},{"type":"text","value":". So, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pm3XI4J9Jg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IQ7cVf6lgQ"}],"key":"VJraTghADr"},{"type":"text","value":" significantly stabilizes training and that’s why these layers are quite popular. Beware though, the stability offered by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"in6JXzko9Z"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NLv5feCGb6"}],"key":"r2ZMqI9t9w"},{"type":"text","value":" often comes at a terrible cost. If you think about it, what is happening at a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lO6OTW9cOY"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dSW5odyJg1"}],"key":"Aq64zb7y8Z"},{"type":"text","value":" layer (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bIZbOqN6P4"},{"type":"inlineCode","value":"hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qPttFrKaY2"},{"type":"text","value":") is something strange and terrible. Before introducing such a layer, it used to be the case that a single example was fed into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GSL5BtYtOS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ho5YyCM7Hs"}],"key":"QrE8nGLxFh"},{"type":"text","value":" and then we calculated its activations and its logits in a deterministic manner in which a specific example yields specific logits. Then, for reasons of efficiency of learning, we started to use batches of examples. Those batches of examples were processed independently, but this was just an efficient thing. But now suddenly, with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MDTgPuhed2"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J3L4xeskZX"}],"key":"ef1NjZ3e55"},{"type":"text","value":", because of the normalization through the batch, we are mathematically coupling these examples in the forward pass and the backward pass of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vxc9jwVCgQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LyqtNVmOLD"}],"key":"sXRhAz8EiD"},{"type":"text","value":". So with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BI3IZ09mzF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lm2i96LAj7"}],"key":"fTZGOAqzP8"},{"type":"text","value":", the hidden states (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"URnB8w42p2"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eWcXMFe8OM"},{"type":"text","value":") and the output states (e.g. logits), are not just a function of the inputs of a specific example, but they’re also a function of all the other examples that happen to come for a ride in that batch. Damn! So what’s happening is, if you see for example the activations ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rQDQ59shFY"},{"type":"inlineCode","value":"h = torch.tanh(hpreact)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BZKgGdiTdA"},{"type":"text","value":", for every different example/batch, the activations are going to actually change slightly, depending on what other examples there are in the batch. Thus depending on what examples there are, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eaoBKGQaDq"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mupg02cBo2"},{"type":"text","value":" is going to jitter if you sample from many examples, since the statistics of the mean and std are going to be impacted. So, you’ll get a jitter for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hnzEG9ISU0"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SZjdOMEENx"},{"type":"text","value":" and for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PaoXvhwuCr"},{"type":"inlineCode","value":"logits","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qXqt4gjbrd"},{"type":"text","value":" values. And you’d think that this would be a bug or something undesirable, but in a very strange way, this actually turns out to be good in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zoqA8Gnqzr"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PaCNdaTbXA"}],"key":"nPGaNtuYrQ"},{"type":"text","value":" training as a side effect. The reason for that is that you can think of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GjhAPYPWFS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"barchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lhl9pvzBYX"}],"key":"CZlyF9jrGk"},{"type":"text","value":" as some kind of regularizer. Because what is happening is the you have your input and your ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uVXRmRRZzI"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gRjQ4kCI6S"},{"type":"text","value":" and because of the other examples the value of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l5wRMgUAFy"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M3CSeGPmr8"},{"type":"text","value":" is jittering a bit. What that does is that is effectively padding-out any one of these input examples and it’s introducing a little bit of entropy. And because of the padding-out, the jittering effect is actually kind of like a form of data augmentation, making it harder for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bMB0aMizqc"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rvg9nV9JTO"}],"key":"HsRTC5mSTz"},{"type":"text","value":" to overfit for these concrete specific examples. So, by introducing all this noise, it actually pads out the examples and it regularizes the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VM61wSUckA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IJNOelNNDl"}],"key":"LX8eg1U8GA"},{"type":"text","value":". And that is the reason why, deceivingly, as a second-order effect, this is acts like a regularizer, making it harder for the us as a community to remove or do without ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SRH1IlkfqT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gsBb9vqKxe"}],"key":"UYcxTDXgPi"},{"type":"text","value":". Because, basically, no one likes this property that the examples in a batch are coupled mathematically in the forward pass and it leads to all kinds of strange results, bugs and so on. Therefore, people do not like these side effects so many have advocated for deprecating the use of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V2zERiyEu6"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c19iNdtm7M"}],"key":"Svduh4rQCk"},{"type":"text","value":" and move to other normalization techniques that do not couple the examples of a batch. Examples are layer normalization, instance normalization, group normalization, and so on. But basically, long story short, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AWUsbBysKI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mEYytXpC03"}],"key":"N2NI4ILs8W"},{"type":"text","value":" is the first kind of normalization layer to be introduced, it worked extremely well, it happened to have this regularizing effect, it stabilized training and people have been trying to remove it and move unto the other normalization techniques. But it’s been hard, because it just works quite well. And some of the reason it works quite well is because of this regularizing effect and because it is quite effective at controlling the activations and their distributions. So, that’s the brief story of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x9nJUSlHFg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oVLg9Jx5L6"}],"key":"OLT0Dy2ldl"},{"type":"text","value":". But let’s see one of the other weird outcomes of this coupling. Basically, once we’ve trained a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vb0gGIaw6V"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZYC9ivLJWv"}],"key":"Kb7R0aUnuh"},{"type":"text","value":", we’d like to deploy it in some kind of setting and we’d like to feed in a single individual example and get a prediction out from our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ok4l6FQRTs"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BdwwmV9KpM"}],"key":"YEQsCSXSnm"},{"type":"text","value":". But how can we do that when our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i0B3c6Lk3S"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tvJQhl9hFS"}],"key":"puhJRV3Gqd"},{"type":"text","value":" now with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qmUfIfMMRS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IQJ1fyLzdz"}],"key":"ReUkQRUx0M"},{"type":"text","value":" in a forward pass estimates the statistics of the mean and the std of a batch and not a single example? The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rKaYhJPVem"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rvkq36Q4sj"}],"key":"FgnTVcxzNn"},{"type":"text","value":" expects batches as an input now. So how do we feed in a single example and get sensible results out? The proposal in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dYzkBOlVPH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eTrsVFmPPH"}],"key":"JzCynEHA9v"},{"type":"text","value":" paper is the following. What we would like to do is implement a step after training that calculates and sets the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k3cDRVFp0E"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AiLYKv4dtg"}],"key":"PJ5OCnKKUL"},{"type":"text","value":" mean and std a single time over the training dataset. Basically, calibrate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h3x7s8Wjly"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jMTZfZR4CN"}],"key":"iUPLY5MiJT"},{"type":"text","value":" statistics at the end of training as such. We are going to get the training dataset and get the pre-activations for every single training example, and then one single time estimate the mean and std over the entire training set: two fixed numbers.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LRPnhtMg13"}],"key":"UKgZxRbzOt"}],"key":"LGH2ogHG2g"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@torch.no_grad()  # disable gradient calculation\ndef infer_mean_and_std_over_trainset():\n    # pass the entire training set through\n    emb = C[xtrain]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # measure the mean/std over the entire training set\n    bnmean_xtrain = hpreact.mean(dim=0, keepdim=True)\n    bnstd_xtrain = hpreact.std(dim=0, keepdim=True)\n    return bnmean_xtrain, bnstd_xtrain","key":"uvYMAjYWbJ"},{"type":"outputs","id":"3iiGO8IY2C9Oq_RkBWSP6","children":[],"key":"ESzlaorbLL"}],"key":"lZ5iD2aP1C"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bnmean_xtrain, bnstd_xtrain = infer_mean_and_std_over_trainset()","key":"mIfJ6PMpy9"},{"type":"outputs","id":"mjUUuG9PuGopKp_xtzF1P","children":[],"key":"uonpkoWQzY"}],"key":"QoRMaAQ2Ef"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And so after calculating these values, at test time we are going to clamp them to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EHiglG6ElN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PWHyfXleps"}],"key":"E4WwWPUctU"},{"type":"text","value":" calculation. To do so, let’s extend the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZPQjFaoXC8"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dIDdDKA861"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S4rGKSuDtr"},{"type":"inlineCode","value":"print_loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YhyHg4k9hM"},{"type":"text","value":" functions as such:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xDP6abwzF7"}],"key":"I9H1H13QK4"}],"key":"lCirlvkXaZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(x, y, bnmean=None, bnstd=None):\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:\n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss","key":"VrYsaOu2iq"},{"type":"outputs","id":"JcBhONqiALuBI_L36t5nc","children":[],"key":"D1MDBGA0zd"}],"key":"p9r9IVPn0X"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@torch.no_grad()  # this decorator disables gradient tracking\ndef print_loss(x, y, prefix=\"\", bnmean=None, bnstd=None):\n    _, _, _, loss = forward(x, y, bnmean=bnmean, bnstd=bnstd)\n    print(f\"{prefix} {loss}\")\n    return loss","key":"vzfUtdNtMA"},{"type":"outputs","id":"hngZ0W5QP8QmmhnYzh2wa","children":[],"key":"tcEOroSLbM"}],"key":"TIb82tu9le"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, if we do an inference with these mean and std values, instead of the batch-specific ones:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kbo3cKA9LX"}],"key":"PbEdB85ppU"}],"key":"wHZPphE6yN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_loss(xtrain, ytrain, bnmean=bnmean_xtrain, prefix='train')\nprint_loss(xval, yval, bnstd=bnstd_xtrain, prefix='val');","key":"ZrkJREAXny"},{"type":"outputs","id":"hD6kyKmEzK_dI1MU5GTfp","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.0695457458496094\nval 2.107233762741089\n"},"children":[],"key":"vQEKXSJsOs"}],"key":"btrF2q3xZ5"}],"key":"saRXi9DqVJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The losses we get may be more or less the same as our last losses before, but the benefit we have gained is that we can now forward a single example, because now the mean and std are fixed tensors. That said, because everyone is lazy, nobody wants to estimate the mean and std as a second stage after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gKaEYYKVVm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hVFRa9Uans"}],"key":"noyEmIRoJe"},{"type":"text","value":" training. So, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nj6MiH7oZo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SLmR3uTKTp"}],"key":"YnQf0wAqOA"},{"type":"text","value":" paper also introduced one more idea: that we can estimate these mean and std values in a running manner during the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vpv9jg0MXT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zOmtQsopxY"}],"key":"EtnpzcR0EJ"},{"type":"text","value":" training phase. Let’s see what that would look like. First, we’ll define running value variables in the definition of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nNw8vEwKbQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zcprt9LvfQ"}],"key":"KOmvzihW70"},{"type":"text","value":". Then we’ll modify the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zaibyqdVyG"},{"type":"inlineCode","value":"train","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VAfVto7ei1"},{"type":"text","value":" function and calculate the running values:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MuhjzfTfy6"}],"key":"V0WAudb8j5"}],"key":"c0dfsqTeJL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, b1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    b1 = torch.randn(n_hidden, generator=g) * b1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    bngain = torch.ones((1, n_hidden))\n    bnbias = torch.zeros((1, n_hidden))\n    bnmean_running = torch.ones((1, n_hidden))\n    bnstd_running = torch.zeros((1, n_hidden))\n    parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return bnmean_running, bnstd_running, parameters","key":"JCjuRHH8R2"},{"type":"outputs","id":"JWep5AEh4PhKreBmmMjLl","children":[],"key":"Fcyc097xxC"}],"key":"swTAaRwbjY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(x, y, bnmean=None, bnstd=None):\n    global bnmean_running, bnstd_running\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:\n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    with torch.no_grad():  # disable gradient calculation\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd\n    h = torch.tanh(hpreact)\n    logits = h @ w2 + b2\n    loss = F.cross_entropy(logits, y)\n    return hpreact, h, logits, loss","key":"AH20JIemps"},{"type":"outputs","id":"I79QkytmORy3WgkWwgdTQ","children":[],"key":"uh6Zrc9mvN"}],"key":"yQYJrFexLA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(x, y, initial_lr=0.1, maxsteps=200000, batchsize=32, redefine_params=False):\n    global parameters\n    lossi = []\n    if redefine_params:\n        parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for i in range(maxsteps):\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        hpreact, h, logits, loss = forward(xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 100000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n    return hpreact, h, logits, lossi","key":"S0hJtbVWeQ"},{"type":"outputs","id":"7p4BdMm-6Pl0kdHrLNrfi","children":[],"key":"CxPFk6yjqn"}],"key":"NhHVQvMQ3s"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now if we train, we will be calculating the running values of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hOUFiTyypD"},{"type":"inlineCode","value":"bnmean","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cUI9GXWYP6"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AltEuR8ck4"},{"type":"inlineCode","value":"bnstd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FQf3g1iRy8"},{"type":"text","value":" without requiring a second step after training. This also happens when using PyTorch ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qWFlZ5oTuU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MvArbiUfO2"}],"key":"u2ZCjhFlFj"},{"type":"text","value":" layers: running values are calculated and then are used during inference. Now, let’s re-define our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"erBRlbEhFy"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RSzILyo1lM"}],"key":"I0h4ocCvPs"},{"type":"text","value":" and train it.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r1P2qxSNpO"}],"key":"PTNXjUiPnS"}],"key":"jMzxUrYuPq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bnmean_running, bnstd_running, parameters = define_nn(\n    w1_factor=kaiming_w1_factor, b1_factor=0.01, w2_factor=0.01, b2_factor=0.0\n)\n_ = train(xtrain, ytrain)","key":"DzvtoEKsiH"},{"type":"outputs","id":"PaPKMjl7tLe2CbaTARmMM","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"12297\n      0/ 200000: 3.3186\n  10000/ 200000: 2.5451\n  20000/ 200000: 2.1227\n  30000/ 200000: 2.3208\n  40000/ 200000: 2.3080\n  50000/ 200000: 2.2920\n  60000/ 200000: 2.1421\n  70000/ 200000: 2.2069\n  80000/ 200000: 2.3143\n  90000/ 200000: 1.6854\n 100000/ 200000: 2.1594\n 110000/ 200000: 2.1690\n 120000/ 200000: 2.2234\n 130000/ 200000: 2.2345\n 140000/ 200000: 1.8121\n 150000/ 200000: 2.3733\n 160000/ 200000: 1.7584\n 170000/ 200000: 2.7041\n 180000/ 200000: 2.0920\n 190000/ 200000: 1.8872\n"},"children":[],"key":"wab9xTg573"}],"key":"pMzScB3myB"}],"key":"LMebn3fd3U"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bnmean_xtrain, bnstd_xtrain = infer_mean_and_std_over_trainset()\nprint_loss(xtrain, ytrain, bnmean=bnmean_xtrain, prefix=\"train\")\nprint_loss(xval, yval, bnstd=bnstd_xtrain, prefix=\"val\");","key":"u8TQtrdUHw"},{"type":"outputs","id":"_HaVCk3BAyp22AWL13Lmy","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.06760311126709\nval 2.109143018722534\n"},"children":[],"key":"PFrYpq1gsx"}],"key":"fZGwr8u5Wy"}],"key":"a70OOYEnIj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If we calculate the mean over the whole training set and compare it with the running mean, we notice they are quite similar:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UyQAweZUM1"}],"key":"yKl4N1qP3y"}],"key":"DwHdnfUnQf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"torch.set_printoptions(sci_mode=False)\nbnmean_xtrain","key":"xwe3JCpRl0"},{"type":"outputs","id":"a7N1bK5ZxPM_WK2Accidp","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":78,"metadata":{},"data":{"text/plain":{"content":"tensor([[-2.6770, -0.1693, -0.6069,  0.4962,  0.7990,  0.6392,  2.3859, -1.3855,\n          1.1074,  1.4398, -1.2298, -2.4216, -0.5599,  0.1668, -0.2828, -0.7317,\n          1.0945, -1.9686, -1.1664,  0.6019, -0.1889, -0.8343, -0.6375,  0.6903,\n          0.6620,  0.0190,  1.1514, -0.0258,  0.3537,  1.8418,  0.2683, -0.8331,\n          0.5717, -0.5169, -0.0296, -1.6526,  0.7317, -0.2649, -0.1199,  0.3907,\n         -0.2578, -1.1813, -0.3795,  0.0315,  0.6800, -0.7146,  1.4800, -1.1142,\n          1.3800,  1.3333,  1.7228, -0.1832,  1.6916,  0.8562,  1.4595, -2.2924,\n         -0.3804,  0.4507,  1.9066, -1.4633, -0.8475,  1.2721,  0.8002,  0.2051,\n          2.0326,  1.2502, -1.0770,  1.3650, -0.9620,  0.4069,  0.3910,  0.6054,\n          0.0738, -1.3951, -2.4795,  0.1532,  1.1407, -0.4320,  0.6385,  0.3845,\n          0.3345,  0.9669,  1.5173,  0.5538,  0.8198, -0.2719, -0.7971, -0.3735,\n          2.4336, -0.6536, -1.1261,  0.8431,  0.0744, -0.9863, -1.0063,  0.1658,\n          0.4939, -1.2384, -0.7562, -0.8595, -0.2615,  0.1969, -1.7003,  1.0725,\n          1.0187,  0.2188, -0.4287, -0.2156,  0.7122, -1.0895,  1.0372,  0.1750,\n          0.0708,  1.3315,  2.8986,  1.5759,  1.1428, -0.4351,  0.4545, -0.2242,\n         -1.2595, -1.5032,  0.3134,  1.1210, -0.5699, -0.1829,  1.0623, -1.5076,\n         -1.3623, -0.6535,  2.5082, -0.4506,  0.7244,  1.3152,  0.9770,  0.9000,\n         -0.8565,  1.5871,  0.7384,  0.3593,  1.2161,  0.8446,  1.6187,  0.0483,\n          0.3879,  0.9822,  0.3694, -1.1177,  0.0051,  0.5489, -1.0130,  0.4538,\n          1.4678,  2.0332,  0.7353, -0.3556,  1.6172, -1.8053, -0.2439,  0.9442,\n          0.0504, -0.7963,  0.2883, -2.1548, -0.5377, -0.6621, -0.0440, -0.2134,\n         -2.3616, -0.7478,  0.3349, -2.1589,  0.3803, -1.2049, -0.9475,  0.7523,\n          1.8645, -0.7137,  1.1013, -1.0613,  1.6492,  1.3798,  0.7756, -0.9489,\n         -0.1432, -0.2982, -0.4837,  0.3259,  2.6390,  0.8259,  0.2949,  1.7561,\n         -0.7375, -0.1671,  0.7696,  1.0035,  1.2708, -0.7615, -0.1892,  1.1808]])","content_type":"text/plain"}}},"children":[],"key":"atnRGNAWk4"}],"key":"A1V9mwy7bl"}],"key":"Blbjklmp3S"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bnmean_running","key":"YPdCgRuMnE"},{"type":"outputs","id":"Zmvop2cqycZn8HPdj38rO","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":79,"metadata":{},"data":{"text/plain":{"content":"tensor([[-2.6439, -0.1617, -0.6136,  0.4968,  0.7971,  0.6411,  2.3763, -1.3774,\n          1.1354,  1.4372, -1.2115, -2.3877, -0.5565,  0.1727, -0.2845, -0.7350,\n          1.1087, -1.9475, -1.1556,  0.6244, -0.1834, -0.8322, -0.6170,  0.6846,\n          0.6547,  0.0314,  1.1314, -0.0138,  0.3604,  1.8411,  0.2521, -0.8311,\n          0.5715, -0.5084, -0.0265, -1.6514,  0.7295, -0.2540, -0.1140,  0.4024,\n         -0.2471, -1.1745, -0.3745,  0.0327,  0.6927, -0.7181,  1.4660, -1.1102,\n          1.3809,  1.3116,  1.7163, -0.1866,  1.6785,  0.8492,  1.4503, -2.3023,\n         -0.3868,  0.4478,  1.8868, -1.4559, -0.8400,  1.2690,  0.7847,  0.2059,\n          2.0220,  1.2630, -1.0618,  1.3643, -0.9662,  0.3970,  0.3910,  0.5977,\n          0.0795, -1.3865, -2.4648,  0.1534,  1.1447, -0.4265,  0.6496,  0.4061,\n          0.3352,  0.9816,  1.5135,  0.5539,  0.8205, -0.2807, -0.8094, -0.3802,\n          2.4052, -0.6548, -1.1304,  0.8636,  0.0803, -0.9660, -1.0143,  0.1948,\n          0.5108, -1.2296, -0.7239, -0.8933, -0.2621,  0.2005, -1.6910,  1.0689,\n          1.0074,  0.2274, -0.4184, -0.2276,  0.7187, -1.0911,  1.0455,  0.1610,\n          0.0798,  1.3174,  2.9053,  1.5687,  1.1363, -0.4269,  0.4681, -0.2275,\n         -1.2518, -1.5101,  0.3332,  1.1115, -0.5765, -0.1845,  1.0573, -1.5042,\n         -1.3581, -0.6503,  2.4976, -0.4533,  0.7215,  1.3105,  0.9769,  0.8943,\n         -0.8746,  1.5900,  0.7509,  0.3625,  1.2261,  0.8343,  1.6215,  0.0652,\n          0.3875,  1.0001,  0.3721, -1.1022, -0.0122,  0.5340, -1.0139,  0.4521,\n          1.4687,  2.0395,  0.7374, -0.3479,  1.6156, -1.7936, -0.2443,  0.9528,\n          0.0623, -0.7957,  0.2997, -2.1441, -0.5244, -0.6628, -0.0482, -0.1997,\n         -2.3657, -0.7569,  0.3328, -2.1407,  0.3706, -1.2124, -0.9397,  0.7629,\n          1.8574, -0.7038,  1.1025, -1.0656,  1.6602,  1.3632,  0.7866, -0.9584,\n         -0.1377, -0.2888, -0.4677,  0.3213,  2.6165,  0.8160,  0.2933,  1.7396,\n         -0.7484, -0.1775,  0.7626,  1.0181,  1.2647, -0.7750, -0.1872,  1.1865]])","content_type":"text/plain"}}},"children":[],"key":"g9P50QdM6J"}],"key":"o3BIcrRl2r"}],"key":"IjcKV22Qhz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Similarly:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qucp4S3AMi"}],"key":"TKOqCGeR0C"}],"key":"DAla64wMva"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bnstd_xtrain","key":"KhGMXxWhTq"},{"type":"outputs","id":"UTEBMUvz6ddqiODlJk70e","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":80,"metadata":{},"data":{"text/plain":{"content":"tensor([[2.3470, 2.1151, 2.1772, 2.0503, 2.2926, 2.3693, 2.1253, 2.5290, 2.3184,\n         2.1632, 2.2796, 2.2958, 2.1392, 2.3227, 2.0948, 2.6618, 2.3286, 1.9229,\n         2.2178, 2.6903, 2.2699, 2.4215, 2.1179, 2.0361, 2.0093, 1.8318, 2.1781,\n         2.3840, 2.2855, 2.4143, 1.6977, 1.7918, 2.0156, 2.0423, 1.9195, 1.7332,\n         2.1424, 2.2936, 1.8010, 1.8278, 2.2029, 2.1118, 2.3197, 1.7458, 2.3419,\n         1.9642, 2.1103, 2.4709, 2.0671, 2.4320, 2.0708, 1.6809, 2.0100, 1.8361,\n         2.4507, 2.2494, 1.9286, 2.2677, 2.6837, 1.9560, 2.2003, 2.0271, 1.9214,\n         2.2211, 2.3771, 2.3485, 1.9970, 2.1871, 2.1062, 2.1121, 1.9403, 2.0357,\n         2.0631, 2.1141, 2.0321, 1.4313, 2.3739, 2.3750, 1.7508, 2.3588, 1.9391,\n         2.0428, 1.9524, 2.1434, 2.4703, 2.3452, 2.1779, 2.3140, 2.5282, 2.6035,\n         2.0373, 1.9570, 2.4558, 1.9520, 2.0133, 2.3092, 2.0963, 1.9307, 2.1936,\n         2.0732, 2.2833, 1.9115, 2.1663, 2.0248, 1.7752, 2.3723, 2.0549, 2.2011,\n         1.9060, 2.1103, 2.3679, 2.2174, 2.3703, 2.4740, 2.7726, 2.4209, 1.8527,\n         1.9249, 1.9286, 2.1562, 2.1385, 2.1264, 2.0558, 2.0809, 1.9615, 2.0763,\n         2.0409, 2.3690, 1.8694, 2.3763, 2.0429, 2.6295, 2.1289, 1.8621, 1.9486,\n         2.1482, 2.2445, 3.0612, 1.9641, 1.9791, 2.0894, 1.7463, 2.1585, 1.9308,\n         1.9275, 2.3182, 2.3112, 2.1799, 1.9871, 1.7467, 1.7394, 2.1327, 2.0271,\n         2.2697, 2.1686, 2.1339, 1.9851, 1.8926, 1.8833, 1.9115, 2.2966, 1.9413,\n         2.1535, 2.2445, 2.2070, 1.6808, 2.2534, 1.7394, 1.9822, 2.1503, 1.9299,\n         2.2190, 2.2701, 2.1555, 2.3559, 2.0457, 2.2009, 2.0695, 2.2631, 1.9018,\n         2.4972, 2.1612, 2.2842, 1.8935, 2.0535, 2.2222, 2.0146, 2.2677, 2.3287,\n         2.1751, 2.2328, 2.1815, 2.0942, 1.8494, 2.1692, 2.1498, 2.0431, 2.6586,\n         2.3651, 1.8138]])","content_type":"text/plain"}}},"children":[],"key":"JAuPWpYSyT"}],"key":"MxMUvWbCOm"}],"key":"ZNTFhQyMxq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"bnstd_running","key":"kHLo2H09jC"},{"type":"outputs","id":"P7q0r5EviI5pFwDHgSzcp","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":81,"metadata":{},"data":{"text/plain":{"content":"tensor([[2.3397, 2.1127, 2.1607, 2.0195, 2.2593, 2.3516, 2.1103, 2.5075, 2.3009,\n         2.1339, 2.2692, 2.2746, 2.1203, 2.3036, 2.0772, 2.6334, 2.3094, 1.9119,\n         2.1908, 2.6739, 2.2396, 2.3972, 2.0972, 2.0194, 1.9913, 1.8106, 2.1576,\n         2.3601, 2.2604, 2.4005, 1.6662, 1.7701, 1.9929, 2.0250, 1.9011, 1.7245,\n         2.1089, 2.2855, 1.7875, 1.8095, 2.1857, 2.0851, 2.2970, 1.7297, 2.3121,\n         1.9505, 2.0847, 2.4411, 2.0630, 2.4087, 2.0420, 1.6596, 1.9859, 1.8215,\n         2.4230, 2.2367, 1.9207, 2.2545, 2.6714, 1.9398, 2.1691, 2.0143, 1.9043,\n         2.1866, 2.3438, 2.3331, 1.9744, 2.1716, 2.0918, 2.0947, 1.9186, 2.0143,\n         2.0387, 2.0704, 2.0176, 1.4192, 2.3597, 2.3436, 1.7193, 2.3276, 1.9210,\n         2.0164, 1.9422, 2.1131, 2.4389, 2.3320, 2.1649, 2.2978, 2.5055, 2.5902,\n         2.0084, 1.9485, 2.4278, 1.9296, 2.0035, 2.2858, 2.0765, 1.9142, 2.1631,\n         2.0530, 2.2614, 1.9054, 2.1492, 2.0110, 1.7607, 2.3342, 2.0385, 2.1851,\n         1.8868, 2.0890, 2.3435, 2.1972, 2.3476, 2.4317, 2.7641, 2.3959, 1.8332,\n         1.9081, 1.9148, 2.1321, 2.1238, 2.1028, 2.0405, 2.0542, 1.9374, 2.0692,\n         2.0270, 2.3360, 1.8488, 2.3442, 2.0212, 2.6016, 2.1188, 1.8529, 1.9363,\n         2.1399, 2.2261, 3.0514, 1.9437, 1.9584, 2.0580, 1.7288, 2.1423, 1.8827,\n         1.9192, 2.3113, 2.2884, 2.1547, 1.9679, 1.7264, 1.7157, 2.1096, 2.0085,\n         2.2409, 2.1413, 2.1120, 1.9617, 1.8712, 1.8732, 1.8953, 2.2746, 1.9251,\n         2.1248, 2.2261, 2.1831, 1.6496, 2.2212, 1.7222, 1.9535, 2.1321, 1.9156,\n         2.1960, 2.2431, 2.1481, 2.3300, 2.0081, 2.1739, 2.0539, 2.2523, 1.8856,\n         2.4582, 2.1390, 2.2654, 1.8744, 2.0369, 2.1935, 2.0023, 2.2502, 2.3119,\n         2.1646, 2.2128, 2.1717, 2.0855, 1.8304, 2.1477, 2.1137, 2.0246, 2.6384,\n         2.3380, 1.7943]])","content_type":"text/plain"}}},"children":[],"key":"BBruUvgojB"}],"key":"zbVMdZRz0B"}],"key":"vvrYNCQctP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Therefore, we can easily infer the loss using the running values:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sfiGGy8i4r"}],"key":"ysfUXR7MT1"}],"key":"VudtumLWP6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_loss(xtrain, ytrain, bnmean=bnmean_running, prefix=\"train\")\nprint_loss(xval, yval, bnstd=bnstd_running, prefix=\"val\");","key":"OCT5q2SCXB"},{"type":"outputs","id":"f-_fn71CkY6_s_VfanWzT","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.0676608085632324\nval 2.1091856956481934\n"},"children":[],"key":"GGgSRoKzwp"}],"key":"ydacQd0W6r"}],"key":"EXqHgtNcPI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And the resulting losses are basically identical. So, calculating running mean and std values eliminates the need for calculating them in a second step after training. Ok, so we are almost done with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DKFMxbuFu4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NjnHQiLk1P"}],"key":"oeDLMcAvHJ"},{"type":"text","value":". There are two more notes to make. First, is that we skipped the discussion of what the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DBStqrBl0o"},{"type":"inlineMath","value":"\\epsilon","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">ϵ</span></span></span></span>","key":"mZOCpW7jzC"},{"type":"text","value":" term is that is added to the normalization step’s denominator square root. It is usually a small, fixed number (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xJKAryEuwN"},{"type":"inlineCode","value":"1e-05","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GVUPV52zPO"},{"type":"text","value":") by default. What this number does is that it prevents a division by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qei6caU5Ft"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fhCn9cZgN0"},{"type":"text","value":" in the case that the variance over the batch is exactly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rXMRKsZlBp"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v63qP0pYDM"},{"type":"text","value":". We could add it in our example and feel free to, but we are just going to skip it since a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XT0fOF2Zqq"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gt1kIS6ylm"},{"type":"text","value":" variance is very very unlikely in our very simple example. Second note is that we are being wasteful with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jIewMhmGVf"},{"type":"inlineCode","value":"b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ksfJLh6NXP"},{"type":"text","value":" in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W8AJ9ShAWV"},{"type":"inlineCode","value":"forward()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fxnBTNbhfJ"},{"type":"text","value":". There, we are first adding ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CHCuzXgyTF"},{"type":"inlineCode","value":"b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UEQBATtxmt"},{"type":"text","value":" to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W9hfB5qdOd"},{"type":"inlineCode","value":"embcat @ w1 ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fyKLCZr7u8"},{"type":"text","value":" to calculate ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EpDRC8E7uW"},{"type":"inlineCode","value":"hpreact","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w7NBplSY3r"},{"type":"text","value":", but then, within the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xmzhNUyNFw"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"svcYLwBS5K"}],"key":"tLSrwJSLPB"},{"type":"text","value":" layer, we are normalizing by subtracting the pre-activation mean (that contains ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GJFcCRo2ka"},{"type":"inlineCode","value":"b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xLGjUTqaJg"},{"type":"text","value":"), which basically subtracts ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CYv1toGE8g"},{"type":"inlineCode","value":"b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gQja9btK9i"},{"type":"text","value":" out, rendering it redundant:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TnXzmuZgOb"}],"key":"AHFoNnxILe"}],"key":"wkceU3b72Z"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"    ...\n    hpreact = embcat @ w1 + b1  # hidden layer pre-activation\n    # batchnorm\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:        \n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    ...","position":{"start":{"line":1,"column":1},"end":{"line":11,"column":1}},"key":"cmOUyW7dT9"}],"key":"YGqDq8HMUy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Since it is being subtracted out, as a parameter it is neither contributing to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DNb54d0Aht"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cKjhBQVm3P"}],"key":"LlIb7HN40d"},{"type":"text","value":" training or inference nor is it being optimized. If we inspect it’s gradient attribute, it is zero:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IHLqVlGrzu"}],"key":"zBwzsYxvbI"}],"key":"Gb1ayvCPrd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(b1.grad)\ntorch.testing.assert_close(b1.grad, torch.zeros(b1.shape))","key":"KOAsG3KgeK"},{"type":"outputs","id":"Mh1-FaAQB06WAGBV1M5yM","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([     0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,\n             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,\n             0.0000,     -0.0000,     -0.0000,      0.0000,     -0.0000,\n            -0.0000,     -0.0000,      0.0000,      0.0000,     -0.0000,\n             0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n            -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,\n             0.0000,     -0.0000,      0.0000,      0.0000,     -0.0000,\n             0.0000,     -0.0000,     -0.0000,     -0.0000,      0.0000,\n            -0.0000,      0.0000,     -0.0000,      0.0000,      0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n             0.0000,     -0.0000,      0.0000,     -0.0000,     -0.0000,\n             0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,\n            -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,\n             0.0000,     -0.0000,      0.0000,     -0.0000,     -0.0000,\n            -0.0000,      0.0000,     -0.0000,      0.0000,     -0.0000,\n            -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,\n             0.0000,     -0.0000,     -0.0000,     -0.0000,     -0.0000,\n             0.0000,     -0.0000,     -0.0000,      0.0000,     -0.0000,\n             0.0000,      0.0000,     -0.0000,      0.0000,     -0.0000,\n             0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,\n            -0.0000,      0.0000,     -0.0000,      0.0000,      0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n            -0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,\n            -0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,\n             0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,\n             0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,\n            -0.0000,     -0.0000,     -0.0000,     -0.0000,     -0.0000,\n             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,\n            -0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,\n            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,\n            -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,\n             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n             0.0000,      0.0000,     -0.0000,      0.0000,     -0.0000])\n"},"children":[],"key":"QpFOhILMX8"}],"key":"FYHvyOT6mm"}],"key":"oZibWiW61s"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Therefore, whenever using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MoIUG54OUe"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q0iOtsfPD1"}],"key":"wbFq6sLlSg"},{"type":"text","value":", then if you have any layers with weights before it, like a linear layer or a convolutional layer or something like that, you are better off disabling the bias parameter for that layer, since we have the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TOBmyY9ljp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pxPmxCclAi"}],"key":"CvaOVq3HNi"},{"type":"text","value":" bias (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oIWKK8TOl8"},{"type":"inlineCode","value":"bnbias","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NgKv87cWx4"},{"type":"text","value":" in our case) which compensates for it. To sum up this point: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ScfBFxSz7e"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uIz4DrvvY6"}],"key":"nTSitMvj7l"},{"type":"text","value":" has its own bias and thus there’s no need to have a bias in the layer before it, because that bias is going to be subtracted out anyway. So that’s the other small detail to be careful of sometimes. Of course, keeping a unnecessary bias in a layer is not going to do anything catastrophic but it is not going to be doing anything and is just wasteful, so it is better to remove it. Therefore, let’s deprecate ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"djKo4obJva"},{"type":"inlineCode","value":"b1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bKezbW2bPi"},{"type":"text","value":", the first layer bias, from our network and add some nice comments:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yBHNsJFASd"}],"key":"FRG77VXAu7"}],"key":"hG24EKzvJi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(\n    n_hidden=200, n_embd=10, w1_factor=1.0, b1_factor=1.0, w2_factor=1.0, b2_factor=1.0\n):\n    global C, w1, w2, b2, bngain, bnbias\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    w1 = torch.randn(n_embd * block_size, n_hidden, generator=g) * w1_factor\n    w2 = torch.randn(n_hidden, vocab_size, generator=g) * w2_factor\n    b2 = torch.randn(vocab_size, generator=g) * b2_factor\n    # batchnorm layer parameters\n    bngain = torch.ones((1, n_hidden))\n    bnbias = torch.zeros((1, n_hidden))\n    bnmean_running = torch.ones((1, n_hidden))\n    bnstd_running = torch.zeros((1, n_hidden))\n    parameters = [C, w1, w2, b2, bngain, bnbias]\n    print(sum(p.nelement() for p in parameters))\n    for p in parameters:\n        p.requires_grad = True\n    return bnmean_running, bnstd_running, parameters","key":"VeIVpeTzjJ"},{"type":"outputs","id":"12QNJ6DrhcC1HLbBAa94W","children":[],"key":"c4VR242VIN"}],"key":"XI1larxTcb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(x, y, bnmean=None, bnstd=None):\n    global bnmean_running, bnstd_running\n    emb = C[x]\n    embcat = emb.view(emb.shape[0], -1)\n    # linear layer\n    hpreact = embcat @ w1  # hidden layer pre-activation\n    # batchnorm layer\n    if bnmean is None:\n        bnmean = hpreact.mean(0, keepdim=True)\n    if bnstd is None:\n        bnstd = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n    with torch.no_grad():  # disable gradient calculation\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd\n    # non-linearity\n    h = torch.tanh(hpreact)  # hidden layer\n    logits = h @ w2 + b2  # output layer\n    loss = F.cross_entropy(logits, y)  # loss function\n    return bnmean, bnstd, hpreact, h, logits, loss","key":"LhFswWGrBq"},{"type":"outputs","id":"Wuqfn8zZfL2TY2De9MXDl","children":[],"key":"DxHdYcil8f"}],"key":"YGAhaV71MU"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For a final sum up: we use ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w4mIbhxBWE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fp65IOiMx3"}],"key":"YKq6TSMaRh"},{"type":"text","value":" to control the statistics of activations in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dKhTanWBJH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KN1tK82AvW"}],"key":"yI0WNz2uad"},{"type":"text","value":". It is common to sprinkle ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HvCK2jE5Y1"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CbWLK4cKkY"}],"key":"BIreaky1dx"},{"type":"text","value":" layers across the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cgsxDVhf7v"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ASFk6O8nVJ"}],"key":"qFdUKVFSPb"},{"type":"text","value":" and usually we will place it after layers that have multiplications (linear, convolutional, etc.). Internally, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hJ25vn1yI7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e7uoHsST1g"}],"key":"nsu78MFKNh"},{"type":"text","value":" has parameters for the gain (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S7rfIXQYwD"},{"type":"inlineCode","value":"bngain","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c1ckATCspJ"},{"type":"text","value":") and the bias (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OcK8acGjPI"},{"type":"inlineCode","value":"bnbias","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QkalFRqqLJ"},{"type":"text","value":"). And these are trained using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T1RSIXwe8m"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C8zF2Np9VC"}],"key":"pk7V2Tg3Yy"},{"type":"text","value":". It also has two buffers. These are the running mean and the running mean of the std, which are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XNNCweTCDd"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"not","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MZPPRAuXof"}],"key":"E8aJwsx3r2"},{"type":"text","value":" trained using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yICUbRgaBo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NyWs5UFhaj"}],"key":"WOqSauxqiQ"},{"type":"text","value":" but which are updated during, and finally calculated after, training. So, what ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"olkOor0COR"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ArYbMtINCS"}],"key":"LlLcR3laPo"},{"type":"text","value":" does is it calculates the batch mean and std of the activations that are feeding into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aVTYymit8r"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OsGwOlCjqI"}],"key":"rcC9JzOyxk"},{"type":"text","value":" layer, then it’s centering that batch to be unit Gaussian and then it’s offsetting and scaling it by the learned bias (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JnzYqFBKBS"},{"type":"inlineCode","value":"bnbias","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LhgQ6yLkxY"},{"type":"text","value":") and gain (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yifkATH737"},{"type":"inlineCode","value":"bngain","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SGP73vIS9v"},{"type":"text","value":"). And then, on top of that, it’s keeping track of the mean and std of the inputs, which are then used during inference. In addition, this allows us to forward individual examples during test time. So, that’s the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yfNp1SYeNd"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZTPzHb2jPI"}],"key":"IBySD8VOGz"},{"type":"text","value":" layer, which is a fairly complicated layer, but this is a simple example of what it’s doing internally. Now, we are going to go through a real example.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o0mxi9GjZx"}],"key":"n60BVH4LCN"}],"key":"hrC0n0zKEX"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"ResNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mToraOgKWz"}],"identifier":"resnet","label":"ResNet","html_id":"resnet","implicit":true,"key":"oGhpm3QuOK"}],"key":"n0Nt1t9wbg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"link","url":"https://en.wikipedia.org/wiki/Residual_neural_network","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"residual ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bTRBKuoNqt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bcnvLRCWDr"}],"key":"HiMuyVWgdH"},{"type":"text","value":"s","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ugfdt7o0XI"}],"urlSource":"https://en.wikipedia.org/wiki/Residual_neural_network","data":{"page":"Residual_neural_network","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"R5eiqiChA3"},{"type":"text","value":" ( ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PHi1fV1V43"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"resnet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MTvwLAkVXf"}],"key":"QeG0kZRrPJ"},{"type":"text","value":"s) are common types of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yh2jOJRvzt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jeRJyD9Bqw"}],"key":"ecQW0ogMeG"},{"type":"text","value":"s used for image classification. Although we haven’t yet nor will we be covering or explaining all the pieces of these networks in detail, it is still worth noting that an image basically feeds into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"itVAg7a2ka"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"resnet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j4Qs1rPg0A"}],"key":"mLM6OzVG2o"},{"type":"text","value":", and there are many many layers with repeating structure all the way to the output layer the gives us the predictions (e.g. what is inside the input image).","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DWZ7CVdcLS"}],"key":"YB3GW5OTYb"}],"key":"LYC2yxPWWy"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"resnet.png\"))","key":"XWwYArEuwh"},{"type":"outputs","id":"qhjf6NEFYeE-YiXLdMbcr","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"48d4fd6c25a36103aa331b474d1c9ea7","path":"/build/48d4fd6c25a36103aa331b474d1c9ea7.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"YwtZWf0iiA"}],"key":"H6HP4FhIUI"}],"key":"EANIBSaW6q"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"resnet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iqSH8AQ3b0"}],"key":"rVzqpuFEyX"},{"type":"text","value":"s (top network in the above image) are a repeating structure made up of blocks that are sequentially stacked-up. In PyTorch, each such block is defined as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U2yuDIvYua"},{"type":"link","url":"https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"Bottleneck","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fenY0s3Q7q"},{"type":"text","value":" object","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VuSz5Lsy3B"}],"urlSource":"https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108","data":{"kind":"file","org":"pytorch","repo":"vision","reference":"main","file":"torchvision/models/resnet.py","from":108,"raw":"https://raw.githubusercontent.com/pytorch/vision/main/torchvision/models/resnet.py"},"internal":false,"protocol":"github","key":"ina30BNQgv"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P0f519Ww25"}],"key":"SYVKpLe1y0"}],"key":"yZAkAW0mg7"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"class Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        downsample: Optional[nn.Module] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out","position":{"start":{"line":1,"column":1},"end":{"line":53,"column":1}},"key":"S6hbDd5sRJ"}],"key":"TTiyQlHxsi"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Although we haven’t covered all the components of the above pytorch snippet (e.g. CNNs), let’s point out some small pieces of it. The constructor, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YfPfiPvuXD"},{"type":"inlineCode","value":"__init__","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vgXeR0JUiM"},{"type":"text","value":", basically initializes the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cNuIY82Cel"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CiCJFRvUU8"}],"key":"uxsadJzY7q"},{"type":"text","value":", similarly to our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rzgaiua7CG"},{"type":"inlineCode","value":"define_nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bdPLfMhfrd"},{"type":"text","value":" function. And, similarly to our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FONWtGPMTY"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"svZUSmsddG"},{"type":"text","value":" function, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CluylYJ0wZ"},{"type":"inlineCode","value":"Bottleneck.forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d63dnLBYPK"},{"type":"text","value":" method specifies how the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QVZfD4e4pY"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mtZNe5tMau"}],"key":"OTA34v7DSM"},{"type":"text","value":" acts for a given input ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P0NvMTiMph"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NGMH6drV4o"},{"type":"text","value":". Now, if you initialize a bunch of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cir7rXh00b"},{"type":"inlineCode","value":"Bottleneck","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nCC0MKtX0T"},{"type":"text","value":" blocks and stack them up serially, you get a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"llAfui2lcm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"resnet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QC0sC7nHvV"}],"key":"Oxa2UdAoTj"},{"type":"text","value":". Notice what is happening here. We have convolutional layers (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v4cZ5ScK4p"},{"type":"inlineCode","value":"conv1x1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iiRLLIuV6R"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VMeHiltq2l"},{"type":"inlineCode","value":"conv3x3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rz3feqSajR"},{"type":"text","value":"). These are the same thing as a linear layer, except that convolutional layers are used for images and so they have spatial structure. What this means is that the linear multiplication and bias offset (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"avPCzVqPLp"},{"type":"inlineCode","value":"logits = h @ w2 + b2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rxuqM9cyUN"},{"type":"text","value":") are done on overlapping patches, or parts, of the input, instead on the full input (since the images have spatial structure). Otherwise though, convolutional layers basically do an ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sVxYR8MtCb"},{"type":"inlineCode","value":"wx + b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BrakigcUpG"},{"type":"text","value":" type of operation. Then, we have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vlL6biBPnn"},{"type":"inlineCode","value":"norm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c43fBUc2kz"},{"type":"text","value":" layer (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dF8ksN1NLL"},{"type":"inlineCode","value":"bn1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KjbrFJ2ug8"},{"type":"text","value":"), which is initialized to be a 2D ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CrhSKpaUkM"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"txkvTaDBjs"}],"key":"tFJKGxmqQX"},{"type":"text","value":" layer (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wdAziczRxx"},{"type":"inlineCode","value":"BatchNorm2d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cY5bANdc9U"},{"type":"text","value":"). And then, there is a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JPefv2eyoK"},{"type":"inlineCode","value":"relu","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gKj2y1OzJz"},{"type":"text","value":" non-linearity. We have used ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ii7YMrNA9T"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"cekvEy5VYa"},{"type":"text","value":" so far, but these are both common non-linearities that can be used relatively interchangeably. But for very deep networks, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NgWEZcwsq8"},{"type":"inlineMath","value":"ReLU","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">ReLU</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">LU</span></span></span></span>","key":"XQmq1upCJN"},{"type":"text","value":" typically and empirically works a bit better. And in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mOJAIhyRlQ"},{"type":"inlineCode","value":"Bottleneck.forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bl25Nh8aE3"},{"type":"text","value":" method, you’ll notice the following pattern: conv layer -> ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pUnoIdfIp4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a8aW5QVv54"}],"key":"uGnu3e5fiO"},{"type":"text","value":" layer -> relu, repeated three times. This however is basically almost exactly the same pattern employed in our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sdAFgMnAKp"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uhCjCFW5nS"},{"type":"text","value":" function: linear layer -> ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AvZwPFF974"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pQQO1EELl7"}],"key":"QY2trU37Rb"},{"type":"text","value":" layer -> tanh. And that’s the motif that you would be stacking up when you would be creating these deep ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FM3cOsyT4W"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jvpHHYr6xl"}],"key":"FgtPthIzBe"},{"type":"text","value":"s that are called ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hMIpgQ0NCo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"resnet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UenLv8XdXa"}],"key":"yN8CF5q7os"},{"type":"text","value":"s. Now, if you dig deeper into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E6su3SOMBt"},{"type":"link","url":"https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PyTorch ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z6fEox54sA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"resnet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fdBAnWQ642"}],"key":"yTYkCJAzqe"},{"type":"text","value":" implementation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ti7a2WOJuG"}],"urlSource":"https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py","data":{"kind":"file","org":"pytorch","repo":"vision","reference":"main","file":"torchvision/models/resnet.py","raw":"https://raw.githubusercontent.com/pytorch/vision/main/torchvision/models/resnet.py"},"internal":false,"protocol":"github","key":"JianWYKtC1"},{"type":"text","value":", you’ll notice that in the functions that return a convolutional layer, e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yqFFqD0yOP"},{"type":"inlineCode","value":"conv1x1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Kn2AnWCLqH"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jXkYM55UoB"}],"key":"j9nzOirAFF"}],"key":"lcdy4cZH7m"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)","position":{"start":{"line":1,"column":1},"end":{"line":5,"column":1}},"key":"hnPiazhjY1"}],"key":"ffF8yiSRk8"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Interim summary","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pcSRGwJfvW"}],"identifier":"interim-summary","label":"Interim summary","html_id":"interim-summary","implicit":true,"key":"YdRyM81k60"}],"key":"UHGLSfczl3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"the bias is disabled (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RoEveUHJJp"},{"type":"inlineCode","value":"bias=False","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r4iANPNx7b"},{"type":"text","value":") for the exact same reason we deprecated the bias in our layer that precedes our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gRhBqfH6yF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RjnxePHVri"}],"key":"nnsj17vdD4"},{"type":"text","value":" layer (like we said, keeping these parameters wouldn’t hurt performance, but they are practically useless). So, because of the motif, the convolutional layers don’t need a bias, as there is a bias in the following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ox4BSlVyav"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"barchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lSPA2Hqifu"}],"key":"UKWWHPwyje"},{"type":"text","value":" layers to make up for them. Let’s now also briefly descend into the definitions of similar pytorch layers and the parameters that they take. Instead of a convolutional layer, we’re going to look at the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YLsNwqjDIX"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"linear layer as implemented by PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PLm7YR6Ip3"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html","key":"bS5UyOmA4F"},{"type":"text","value":". As we discussed, convolutional layers are basically linear ones except on patches of the image. So a linear layer performs a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"esfXv9vVLR"},{"type":"inlineCode","value":"wx + b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ScEZYvnei1"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Wk6z3VurrI"},{"type":"inlineMath","value":"xA^T + b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">xA^T + b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9247em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span>","key":"N2zmP6q3NW"},{"type":"text","value":" as described in the docs. And to initiliaze this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sRxuJFJbkU"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GwtaWqOn5F"},{"type":"text","value":" layer object, you need to know the fan-in (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xqNFZLwxL5"},{"type":"inlineCode","value":"in_features","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uStGkDX8Gv"},{"type":"text","value":") and the fan-out (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZTGw3bsg1F"},{"type":"inlineCode","value":"out_features","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"khXSGQZZ3c"},{"type":"text","value":") in order to construct a weight matrix that has a shape ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oe1qSHWO7Q"},{"type":"inlineMath","value":"[in\\_features \\times out\\_features]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>i</mi><mi>n</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo>×</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[in\\_features \\times out\\_features]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">in</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">res</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\"></span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">t</span><span class=\"mord\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">a</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">u</span><span class=\"mord mathnormal\">res</span><span class=\"mclose\">]</span></span></span></span>","key":"VIz43tQLNF"},{"type":"text","value":". In our case, the equivalent parameters for the first layer are: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uhnFptysS5"},{"type":"inlineCode","value":"n_embd * block_size, n_hidden","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bA5OP5eXZQ"},{"type":"text","value":". Also there is an option to enable or disable the bias. Furthermore, if you see the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MvfA9knamF"},{"type":"inlineCode","value":"Variables","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bqnAlgIRpF"},{"type":"text","value":" section of the docs, there’s a weight and a bias bulletpoint whose default initialization is described. So by default, PyTorch initializes the weights by taking the fan-in and then calculating the square root of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yF005cix8A"},{"type":"inlineMath","value":"k = \\frac{1}{in\\_features}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>i</mi><mi>n</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">k = \\frac{1}{in\\_features}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.4071em;vertical-align:-0.562em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8451em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">in</span><span class=\"mord mtight\" style=\"margin-right:0.02778em;\">_</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">t</span><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">res</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.562em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>","key":"VMGIbHfKDa"},{"type":"text","value":". And then,  the weights are drawn from a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zmh5pcrtns"},{"type":"inlineMath","value":"U(-\\sqrt{k}, \\sqrt{k})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi><mo stretchy=\"false\">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator=\"true\">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">U(-\\sqrt{k}, \\sqrt{k})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1822em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mopen\">(</span><span class=\"mord\">−</span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9322em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span><span style=\"top:-2.8922em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1078em;\"><span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9322em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span><span style=\"top:-2.8922em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1078em;\"><span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>","key":"H5tJn0tz8M"},{"type":"text","value":" uniform distribution. Despite the lack of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YS0gvoRHZi"},{"type":"inlineCode","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JPqxtBBLmU"},{"type":"text","value":" gain ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZrpWVcEmvw"},{"type":"inlineMath","value":"5/3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>5</mn><mi mathvariant=\"normal\">/</mi><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">5/3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">5/3</span></span></span></span>","key":"dqnMFVyAbG"},{"type":"text","value":" that we are using, this is the same kaiming initialization as we have described throughout this lesson. The reason is that if you have a roughly Gaussian input, a kaiming initialization will ensure that out of this layer (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kwroh3P6ZU"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I664qMtUip"},{"type":"text","value":") you will have a roughly Gaussian output. Let’s now look at the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XQHW9XsNmX"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PyTorch ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MqSXPvsQ73"},{"type":"inlineCode","value":"BatchNorm1D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XzPLysnoCG"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J3i3O91V3f"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZOETjlNwmD"}],"key":"AsOKIr5dlT"},{"type":"text","value":" layer","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pua2qn4KPg"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","key":"rwfzh0pgtg"},{"type":"text","value":". It takes a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jNTVVteHcV"},{"type":"inlineCode","value":"num_features","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"imMqtwBI0L"},{"type":"text","value":" argument (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RQyKAeNM1a"},{"type":"inlineCode","value":"n_hidden","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZNH6Cqf75K"},{"type":"text","value":" in our case) in order to initialize the gain, bias and running parameters, as well as an ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ru7ovEaEAP"},{"type":"inlineMath","value":"\\epsilon","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">ϵ</span></span></span></span>","key":"AIHYsXa1fO"},{"type":"text","value":" parameter that is used for the square root of the normalization denominator. There is also a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VbzYNIz4G4"},{"type":"inlineCode","value":"momentum=0.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VlxaV3IGIJ"},{"type":"text","value":" parameter that is used in the calculation of the running mean and std values (our equivalent is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IjuGXABI8g"},{"type":"text","value":"0.001","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UqV8DkuKJ4"},{"type":"text","value":", e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lLxiYRx4ge"},{"type":"inlineCode","value":"bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CCSyMaeGVm"},{"type":"text","value":") which you may want to change sometimes. Roughly speaking, if you have a very large batch size, typically what you’ll see is that when you estimate the mean and std, for every single batch size, if it’s large enough, you’re going to get roughly the same result. And therefore, slightly higher momentum like the default ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v0b8a5YYU4"},{"type":"text","value":"0.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tNh8Zawkta"},{"type":"text","value":". However, for a batch size as small as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KaqXKeC2wd"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f6HLjvZr33"},{"type":"text","value":" (e.g. the one we use), the mean and std here might take on slightly different numbers, because there’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UyYfQaxaTm"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"only","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vrFvFqXe3V"}],"key":"Ihr70s3iPX"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zuu33Sge9D"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fEp4XbNvq0"},{"type":"text","value":" (instead of let’s say ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E6mUCNfLE7"},{"type":"text","value":"128","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qJT0jdbwZ8"},{"type":"text","value":") to estimate the mean and std. So in that case, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x6fvze748H"},{"type":"text","value":"0.001","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wxKgODu4Ef"},{"type":"text","value":" in our example is more appropriate for convergence than the larger, potentially dangerous ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gYcwrEcARL"},{"type":"text","value":"0.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oHXO0Uu9Hl"},{"type":"text","value":" that would cause more thrashing and higher inaccuracies in the calculations. There’s also the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QELOoFZAif"},{"type":"inlineCode","value":"affine","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tqlm55mpzl"},{"type":"text","value":" boolean parameter, that determines whether the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xNpNDrHwOB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sOJpYX6zmr"}],"key":"iqHm4FCZJu"},{"type":"text","value":" layer’s gain and bias parameters are learnable, and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tfL8ILwKAR"},{"type":"inlineCode","value":"track_running_stats","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QvA9MRVZEO"},{"type":"text","value":" boolean parameter. One reason you may want to skip running stats is because you may want to, for example, calculate them after training, as a second stage (e.g. through ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UKmdJ0Z5Ql"},{"type":"inlineCode","value":"mean_and_std_over_trainset()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FTy0Odn01o"},{"type":"text","value":"). And so in that case, you wouldn’t want the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xiYyqkEArg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uIA2aqET4u"}],"key":"eGkGBwLheI"},{"type":"text","value":" layer to do all this extra compute that you’re not gonna use. Finally, you can also specify the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FmScZk32vj"},{"type":"inlineCode","value":"device","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oNKyyV7VGk"},{"type":"text","value":" that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LFVHKOR9mL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hw9VTrbmpg"}],"key":"bhi7EWjuLz"},{"type":"text","value":" layer pass is going to happen on (either ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PO8IuetveQ"},{"type":"inlineCode","value":"cpu","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bWIRsFhqjK"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SBqlflRmhJ"},{"type":"inlineCode","value":"gpu","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e2vUiWMRoC"},{"type":"text","value":") and what the datatype is going to be (half-precision, single-precision, double-precision, and so on). So that is more or less the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I1z0YqY4Zj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rm706uhKai"}],"key":"LrngbW7xa9"},{"type":"text","value":" layer covered in the paper, as implemented by us and as quite-similarly provided in PyTorch. And that’s all we wanted to cover in this lecture: the importance of understanding the activations and the gradients and their statistics in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gMQk1CuWz2"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hBO9T63yXI"}],"key":"z0oIPn9Aw3"},{"type":"text","value":"s. And this becomes increasingly important especially as you make your ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qhb1NaVAgv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XCz6Nz8Gb6"}],"key":"wLlktqALni"},{"type":"text","value":"s bigger, larger and deeper. We looked at the distributions at the output layer and we saw that if you have too confident mispredictions, because the activations are too messed up at the last layer, you can end up with these hockey stick losses. And if you fix this, you get a better loss at the end of training, because your training is not doing wasteful work. Then, we also saw that we need to control the activations as we don’t want them to squash to zero or explode to the flat regions of the non-linearity’s output range, because you can run into trouble (e.g. dead neurons). Basically, you want everything to be fairly homogeneous throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oqAPrhKC60"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d7XpHnLLga"}],"key":"mTFiVwc7HS"},{"type":"text","value":". You want roughly Gaussian activations throughout the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XJ8YcdDBor"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F3fw5Vtp6I"}],"key":"U1FrkwWaUS"},{"type":"text","value":". And then we pondered, if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kJ8bSGMW2i"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l6ScQtnEv6"}],"key":"ROaNaZBYFW"},{"type":"text","value":" so that everything is as controlled as possible. By a bit of trial and error, we found some appropriate scaling factors that gave us the uniform activations that we seeked. Of course, we realize that scaling parameters like that is very very hard and practically unsustainable a method when the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Tl7z8HG5mw"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PgBguKxojQ"}],"key":"a4dsrb7Q9b"},{"type":"text","value":" is much much deeper. So then we introduced the notion of the normalization layer that people use in practice: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vMTwiza6Qm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PZe4JsOdOq"}],"key":"JKEsIi60mS"},{"type":"text","value":", layer normalization, instance normalization, group normalization. And we introduced and covered the one that came out first. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Iaw6jqdRfe"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ByZ11AG4lk"}],"key":"JreWHAaMeS"},{"type":"text","value":" is layer that you can sprinkle throughout your deep ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zE4s0ACnQE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GVE28YvZeC"}],"key":"lJwzQHHAod"},{"type":"text","value":" and the basic idea is that if you want roughly Gaussian activations, well then take your activations, find their mean and std and center your data. And you can do that because the centering operation is differentiable. On top of that, we had to add a lot of bells and whistles, giving us a sense of the complexity of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nXR0WBeIbx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e65a33I6yv"}],"key":"GD2Gu8SMU8"},{"type":"text","value":". Because, ok now we’re centering the data: that’s great. But suddenly we need the gain and bias parameters and now those are trainable. And because we are coupling all the training examples, the questioning is how do you do the inference? To do the inference, we then realized that we have to estimate the mean and std once on the entire training set and then use those at inference. But then, no one likes to do that as a second stage after training. So calculate those values as running averages during training and estimate these in a running manner so that everything is a bit simpler. And again! That was the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jCezryzTRo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I4nBcwozOb"}],"key":"wh5anXMM5F"},{"type":"text","value":" layer. Last time, I promise. Lol. Although helpful, no one likes this layer! It causes a huge amount of bugs and intuitively that’s because it’s coupling different examples (per batch) in the forward pass of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j5biQtEhEp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oF5Qvq01wm"}],"key":"TJt7BP8FHp"},{"type":"text","value":". And many have shot themselves in the foot with this layer, over and over again in their lifetimes. So, in order to avoid sufferring, try to avoid it as much as possible (e.g. by using other normalization alternatives). Nevertheless, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nhzDdskUkW"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"riJajblRYW"}],"key":"uCHC2KIK0Z"},{"type":"text","value":" proven to be very influential when it came out in 2015 because that was the first time that you could train reliably much deeper ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q0DRKBIFoZ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"chDDDUDWSv"}],"key":"bRyPPoIr4L"},{"type":"text","value":"s. The reason for that is that this layer is very effective at controlling the statistics of the activations in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zhFNTMCTmt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y43KNhbvYt"}],"key":"Orag2Mu9Zg"},{"type":"text","value":". Now, that’s all for now. In the next notebooks, we can start going fully into recurrent ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ITkmy4qBe5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xXlPfWzGJf"}],"key":"gTVUeMg7mt"},{"type":"text","value":"s which are very very deep networks (due to unrolling during optimization). And that is where a lot of this analysis around the activation statistics and all these normalization layers will become very very important for good performance. So, we’ll see that next time. Bye!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Za3CoAFm0e"}],"key":"r3oSC3D2YC"}],"key":"XgScFVyb2c"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"torch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i50sMD84f8"}],"key":"gxJn9JdMWV"},{"type":"text","value":"ification","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o4DGncixpi"}],"identifier":"torchification","label":"torchification","html_id":"torchification","implicit":true,"key":"A5dWAADtem"}],"key":"eVTTbln5lM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Just kidding!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xtQuk6oxFf"}],"key":"idS7iHwhQ4"},{"type":"text","value":" - As a bonus, before the next lesson, we will cover one more summary of everything we have presented in this lesson so far. But also, it would be very useful to “torchify” our code a little bit so it looks much more like what you would encounter in PyTorch. We will structure our code into modules. Then we will construct our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d1Br6vUslU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BJtEWGB3hJ"}],"key":"lrATpVbDG7"},{"type":"text","value":" like we would in PyTorch and we will run our training loop to optimize it. Then, as one last thing we will visualize the activation statistics both in the forward pass and in the backward pass, before evaluating and sampling just like we have done before. Let’s start. Similar to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zi29hzEZwS"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"torch.nn.Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z0JdCPbR0G"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html","key":"bya9yCF4BI"},{"type":"text","value":", we will implement our own linear layer. By default, we initialize the weights by drawing numbers from a Gaussian distribution and doing a kaiming initialization and we initialize the bias to zero. When calling this module, we do a forward pass and calculate ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DLTxxOAVhW"},{"type":"inlineCode","value":"x @ w + b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tPwMwycMgq"},{"type":"text","value":", whereas calling a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m9nOH0ICnx"},{"type":"inlineCode","value":"parameters","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KZyIQxzVbW"},{"type":"text","value":" method will return the weight and bias tensors of this layer.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GJg1PXdoCv"}],"key":"BHIstopHCS"}],"key":"mC5nxx1bRW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class Linear:\n    def __init__(self, fan_in, fan_out, generator, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=generator) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])","key":"E4Z60B64L8"},{"type":"outputs","id":"4j--nHRtqaymZ8Iifjff2","children":[],"key":"mK12ZJ9um2"}],"key":"MawQRmtp31"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Similar to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uqNayrwfL6"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PyTorch ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kAcK4Kpc0v"},{"type":"inlineCode","value":"BatchNorm1D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NKQRUw7Nhq"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IxJQO5M3Ie"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VXKesgHSSz"}],"key":"lksu9QIQD3"},{"type":"text","value":" layer","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aBjrzRxsEN"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","key":"xG38jjfUbk"},{"type":"text","value":", we will define our own. Apart from the parameters we discussed previously (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"reUnTkHHKn"},{"type":"inlineCode","value":"dim","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QCOVBdxMMk"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P43My62Yfg"},{"type":"inlineCode","value":"eps","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KWLIjxta5O"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NQ26pvn2vI"},{"type":"inlineCode","value":"momentum","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yMRD6PSuuS"},{"type":"text","value":"), we will also define the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sLuGrE7yfy"},{"type":"inlineCode","value":"training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IKyXrSTQIw"},{"type":"text","value":" attribute. When this boolean flag is enabled, the running mean values are calculated (training mode) and when it is disabled, they are not (testing mode). When calling this layer, we do a forward pass, wherein mean values are assigned and an output value is calculated (as described previously) and saved (in order to comfortably plot them later on) and finally we update the moving average buffers. Notice the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JtEOyYnCdT"},{"type":"inlineCode","value":"torch.no_grad()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"imTKOSAz3O"},{"type":"text","value":" context manager we use in order to make our code more efficient by bypassing unnecessary saving into a maintained computation graph (since we do not care about gradients for the buffer variables, there is no point in wasting memory for the allocation of gradient-related data). This context manager essentially signifies that we will not be calling ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bOBuwVZwSR"},{"type":"inlineCode","value":"backward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lqqt9CMBts"},{"type":"text","value":" on the variables inside it.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dg1dOuRmxJ"}],"key":"Shd48JU0gh"}],"key":"ukv1by9cA8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (\n                    1 - self.momentum\n                ) * self.running_mean + self.momentum * xmean\n                self.running_var = (\n                    1 - self.momentum\n                ) * self.running_var + self.momentum * xvar\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]","key":"pbUdWT2Vfc"},{"type":"outputs","id":"_iM5JORrd32AoKVegAIOt","children":[],"key":"AEet00Xnv2"}],"key":"Frk6x7hueA"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Lastly, in PyTorch fashion, we also calculate an equivalent ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pbdeHUA9R1"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"torch.nn.Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LnjW9KiIY5"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html","key":"xO9DMAmwGx"},{"type":"text","value":" layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rMhW9Ws62i"}],"key":"EVkA28enwu"}],"key":"DTeK7r4vG5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []","key":"CDlJyXXmKB"},{"type":"outputs","id":"1v4RHdrxZ7c8Lyc9AAjD6","children":[],"key":"gfVMHfahvD"}],"key":"ybXFrqf0UT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, by defining everything in layers it now becomes very easy to stack them up into a list and more intuitively “define” any ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uHf0EWjDjV"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vGKZemjqgy"}],"key":"yFPUqvU7gZ"},{"type":"text","value":". Let’s see how by updating our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gBUTJ4UVcq"},{"type":"inlineCode","value":"define_nn()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rM8O7KuM0J"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RvCdHpumKh"}],"key":"rvtMoeG6i1"}],"key":"XfYfViC236"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(\n    n_embd=10,\n    hidden_dims=[100, 100, 100, 100, 100],\n    weight_gain=5 / 3,\n    batchnorm_enabled=False,\n    add_batchnorm_last_layer=False,\n    tanh_enabled=True,\n):\n    global C, g\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((vocab_size, n_embd), generator=g)\n    input_size = n_embd * block_size\n    output_size = vocab_size\n    # define input layer\n    layers = [\n        Linear(input_size, hidden_dims[0], generator=g, bias=False),\n    ]\n    if batchnorm_enabled:\n        layers.append(BatchNorm1d(hidden_dims[0]))\n    if tanh_enabled:\n        layers.append(Tanh())\n    # define hidden layers\n    for i, n_hidden in enumerate(hidden_dims[:-1]):\n        layers.append(Linear(n_hidden, hidden_dims[i + 1], generator=g, bias=False))\n        if batchnorm_enabled:\n            layers.append(BatchNorm1d(n_hidden))\n        if tanh_enabled:\n            layers.append(Tanh())\n    # define output layer\n    layers.append(Linear(hidden_dims[-1], output_size, generator=g, bias=False))\n    if add_batchnorm_last_layer:\n        layers.append(BatchNorm1d(output_size))\n    # scale parameters\n    with torch.no_grad():\n        # last layer: make less confident\n        if add_batchnorm_last_layer:\n            layers[-1].gamma *= 0.1\n        else:\n            layers[-1].weight *= 0.1\n        # all other layers: apply gain\n        for layer in layers[:-1]:\n            if isinstance(layer, Linear):\n                layer.weight *= weight_gain\n    # collect parameters\n    parameters = [C] + [p for layer in layers for p in layer.parameters()]\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters","key":"Bqofy12GVb"},{"type":"outputs","id":"82HLVk5XDo82r8gv-rlTx","children":[],"key":"vjVzT4FPyR"}],"key":"vs8i1oqTKO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now let’s update the our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E9ecSG3gMb"},{"type":"inlineCode","value":"forward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rh1OeziSiE"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DOma3a3nrc"},{"type":"inlineCode","value":"backward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TtFswv8ZoT"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lInCHRgJau"},{"type":"inlineCode","value":"train","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w09r5GaRvT"},{"type":"text","value":" functions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LdTHQjiPRP"}],"key":"H6rOAOj8wY"}],"key":"qbHHGRu9LM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(layers, xb, yb):\n    emb = C[xb]  # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, yb)  # loss function\n    return loss","key":"k5fHkMEp0K"},{"type":"outputs","id":"HlVmj1saNJ8QjMkL5mttu","children":[],"key":"OZlybM3zr6"}],"key":"kyTQe25nqA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def backward(layers, parameters, loss, debug=False):\n    if debug:\n        for layer in layers:\n            layer.out.retain_grad()\n    for p in parameters:\n        p.grad = None\n    loss.backward()","key":"mg0p7PqQzz"},{"type":"outputs","id":"IJnK_mI7FOrqZx9kfazlk","children":[],"key":"NoKrNl4mz8"}],"key":"TcTOvMzGfD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(\n    x,\n    y,\n    layers,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(layers, xb, yb)\n        backward(layers, parameters, loss, debug=(break_at_step is not None))\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        if break_at_step is not None and i >= break_at_step:\n            break  # AFTER_DEBUG: would take out obviously to run full optimization\n    return lossi","key":"f86DGihiw2"},{"type":"outputs","id":"PFwXhBomHy7SmavJAdJ_q","children":[],"key":"WiT1QKmqyK"}],"key":"N1Q89eKR9V"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we’ll define a new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NFVBSbnX7C"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"heHfHGHP7y"}],"key":"PRVtllDU3Y"},{"type":"text","value":" and train in debug mode, for only one step.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tMGqsyuEQc"}],"key":"oPUbxotPSU"}],"key":"tNR4tyiYkj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)","key":"QRABYrgtgI"},{"type":"outputs","id":"0ijEdfLSb5HyjG6b4t3s-","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2946\n"},"children":[],"key":"FzAX2orEi8"}],"key":"Gjd8DF19Mb"}],"key":"ZbhWO3NICP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Visualizing activations and gradients","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rc2op61IJr"}],"identifier":"visualizing-activations-and-gradients","label":"Visualizing activations and gradients","html_id":"visualizing-activations-and-gradients","implicit":true,"key":"bBQqejkHTP"}],"key":"JvDHb1brBf"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, since we defined an ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vpw09s9Zy0"},{"type":"inlineCode","value":"out","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Bv4tcCWxE5"},{"type":"text","value":" attribute in our custom ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rsRxyzIzod"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Pvauacptq7"},{"type":"text","value":" layer, after training we have saved the activations which we can visualize! Specifically, we will plot the histogram of each layer’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VWypsFvFnX"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"nhQVnSdmiM"},{"type":"text","value":" activations.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K7HhvsO5VH"}],"key":"Q4CxBxqhoP"}],"key":"g50Jr7E6Gf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def visualize_layer_values(layers, grad=False, layer_cls=Tanh):\n    plt.figure(figsize=(20, 4))  # width and height of the plot\n    legends = []\n    for i, layer in enumerate(layers[:-1]):  # note: exclude the output layer\n        if isinstance(layer, layer_cls):\n            t = layer.out.grad if grad else layer.out\n            if grad:\n                print(\n                    \"layer %d (%10s): mean %+f, std %e\"\n                    % (i, layer.__class__.__name__, t.mean(), t.std())\n                )\n            else:\n                print(\n                    \"layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%\"\n                    % (\n                        i,\n                        layer.__class__.__name__,\n                        t.mean(),\n                        t.std(),\n                        (t.abs() > 0.97).float().mean() * 100,\n                    )\n                )\n            hy, hx = torch.histogram(t, density=True)\n            plt.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f\"layer {i} ({layer.__class__.__name__}\")\n    plt.legend(legends)\n    if grad:\n        plt.title(\"gradient distribution\")\n    else:\n        plt.title(\"activation distribution\")","key":"qAEnTZj6Qf"},{"type":"outputs","id":"CIsl1MstHJCzu0HuspD4j","children":[],"key":"F4IVREXmT2"}],"key":"B2Mmssa26Q"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_layer_values(layers)","key":"oTocAfaT3X"},{"type":"outputs","id":"kAo844wcR5S6qzahkn1dq","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"layer 1 (      Tanh): mean -0.03, std 0.77, saturated: 22.97%\nlayer 3 (      Tanh): mean -0.01, std 0.69, saturated: 8.75%\nlayer 5 (      Tanh): mean +0.02, std 0.67, saturated: 7.37%\nlayer 7 (      Tanh): mean -0.00, std 0.65, saturated: 5.34%\nlayer 9 (      Tanh): mean -0.02, std 0.66, saturated: 6.03%\n"},"children":[],"key":"zJMOSmDu8b"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8f455c8952e74c55bb0ce824bb543e4e\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"ca0cf8a79e8d2043b7c62140df10c969","path":"/build/ca0cf8a79e8d2043b7c62140df10c969.png"},"text/html":{"content_type":"text/html","hash":"289bd22c94b73af5c070e6f826abf759","path":"/build/289bd22c94b73af5c070e6f826abf759.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"psYi9eiVqU"}],"key":"zGNKMLASNq"}],"key":"CGK9KtmcuI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This histogram shows us how many values in these tensors take on any of the values on the x-axis. layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fByau8HLVU"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bPyWZTAGS8"},{"type":"text","value":" is fairly saturated (~20%), with a significant amount of values being close to the saturation points at the tails (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TRJVRn4Fpf"},{"type":"text","value":"-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bMQq3hjD5d"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZRrsQjUVDK"},{"type":"text","value":"+1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lZ18vcKS3U"},{"type":"text","value":") and the subsequent layers being more stable. And why the values are pretty stable is because the weight values of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SMQa6pBvoQ"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ShUUAw9ijd"},{"type":"text","value":" layer are boosted by a gain of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yYVoDBrI7G"},{"type":"inlineMath","value":"5/3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>5</mn><mi mathvariant=\"normal\">/</mi><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">5/3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">5/3</span></span></span></span>","key":"nyHePeYEjP"},{"type":"text","value":". If we use a gain of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QbdyUG70SQ"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dLNBB8kjBq"},{"type":"text","value":" (aka no gain), let’s see what happens:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PG60bMlhHy"}],"key":"aLFjEAEoxa"}],"key":"wpPz7vzekI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(weight_gain=1)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)","key":"LMS6cFsXL8"},{"type":"outputs","id":"NezmPKdWEet7w4X2CGET_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.3090\nlayer 1 (      Tanh): mean -0.03, std 0.63, saturated: 4.47%\nlayer 3 (      Tanh): mean -0.02, std 0.48, saturated: 0.06%\nlayer 5 (      Tanh): mean +0.01, std 0.41, saturated: 0.03%\nlayer 7 (      Tanh): mean +0.00, std 0.35, saturated: 0.00%\nlayer 9 (      Tanh): mean -0.01, std 0.33, saturated: 0.00%\n"},"children":[],"key":"It66FWFzOo"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ec87cf391e6d4a0883f734704f46336d\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"44d6e827afca2a77861a25977c0dbb2b","path":"/build/44d6e827afca2a77861a25977c0dbb2b.png"},"text/html":{"content_type":"text/html","hash":"553f5a40acc524602715e7a467d28df7","path":"/build/553f5a40acc524602715e7a467d28df7.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"hExcc5BuWI"}],"key":"gxkr4PXmJJ"}],"key":"W2W09zmWxN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, from the first to the last ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a8aQJO6dhH"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"ptoPXMZlCc"},{"type":"text","value":" layer, the std shrinks and the saturation goes to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vjdqgx1sWb"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mJwnE4FNpv"},{"type":"text","value":". What this means is that the activations are being shrunk to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MG9S1DfxJ7"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vXZ4pWpbIq"},{"type":"text","value":". The reason for that is that when you just have a sandwich of linear layer, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TUknCXXtXz"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"Fk9syUCOGr"},{"type":"text","value":" layer pairs, these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rxQaKintR6"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"xU5P5mSvFe"},{"type":"text","value":" layers act as squashing functions that take a distribution and slightly squeeze it towards zero. Therefore, some gain is necessary in order to keep expanding the distributions and by doing so to fight the squashing phenomenon. So, if the gain is close to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xWG7CckUfx"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q451HogvC9"},{"type":"text","value":", the activations will then come towards ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dCjjSXY5rQ"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NLc31oA5JH"},{"type":"text","value":", but if it is something too big (such as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G3G7YwGcjI"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v6q9I67hc3"},{"type":"text","value":"), then, on the contrary, the saturations end up way too large:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zPW5isNOj2"}],"key":"GO5e2kEv6p"}],"key":"f9A5YBVRBo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(weight_gain=3)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)","key":"w0QDLuWRUQ"},{"type":"outputs","id":"rONXE4FOo_rCVbNqBoP6T","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2942\nlayer 1 (      Tanh): mean -0.07, std 0.86, saturated: 50.00%\nlayer 3 (      Tanh): mean +0.00, std 0.84, saturated: 40.91%\nlayer 5 (      Tanh): mean -0.00, std 0.84, saturated: 43.16%\nlayer 7 (      Tanh): mean -0.03, std 0.84, saturated: 41.12%\nlayer 9 (      Tanh): mean -0.00, std 0.84, saturated: 41.31%\n"},"children":[],"key":"o9VJZWKHrF"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"0bad81f17047447195d03d3a87713883\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"19dde7eafe6547aab49d33cf378605ec","path":"/build/19dde7eafe6547aab49d33cf378605ec.png"},"text/html":{"content_type":"text/html","hash":"6d26bb1b6ed6c236468dd0daa6c0f89a","path":"/build/6d26bb1b6ed6c236468dd0daa6c0f89a.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"dIHCNCDOzC"}],"key":"ZjrtfBTQer"}],"key":"ZjRe3nMbd1"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cG7XACHdHT"},{"type":"inlineMath","value":"5/3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>5</mn><mi mathvariant=\"normal\">/</mi><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">5/3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">5/3</span></span></span></span>","key":"MmrhSUa4WV"},{"type":"text","value":" (the default value) is a good setting for a sandwich of linear layers with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WZK5OMWJcS"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"mmImiQ87no"},{"type":"text","value":" activations. And it roughly stabilizes the std at a reasonable value (~5%), which is a pretty good number and this is a good setting of the gain in this context:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EJIu1g6uCc"}],"key":"ybEEQkaiqb"}],"key":"SITwWxlP7S"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)","key":"cQ88Av2ZcF"},{"type":"outputs","id":"7Y4EPDGQGUOB8pjcqtJC7","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.3047\nlayer 1 (      Tanh): mean -0.03, std 0.76, saturated: 19.34%\nlayer 3 (      Tanh): mean +0.00, std 0.70, saturated: 9.50%\nlayer 5 (      Tanh): mean +0.01, std 0.68, saturated: 7.69%\nlayer 7 (      Tanh): mean +0.01, std 0.66, saturated: 6.12%\nlayer 9 (      Tanh): mean -0.01, std 0.65, saturated: 6.25%\n"},"children":[],"key":"qVhZEKl2Qy"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"6b2cbd427ba848df8d6b1c73c5dc70d4\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"51116a92bfe21dd3764380915856221b","path":"/build/51116a92bfe21dd3764380915856221b.png"},"text/html":{"content_type":"text/html","hash":"3ec6e41ee0c450f0ba206c8b3dd798b9","path":"/build/3ec6e41ee0c450f0ba206c8b3dd798b9.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"z2Lx3QXcKI"}],"key":"X2xP3rx1pN"}],"key":"R7tezluXgo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Similarly, we can do the exact same thing with the gradients. So, here we will run the exact same loop by using the exact same function, but instead of the layer outputs we will now visualize the gradients (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DEkswnMWl8"},{"type":"inlineCode","value":".grad","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c2ehAD4G6Z"},{"type":"text","value":"):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UiEjLgTtrf"}],"key":"aMssCkti8t"}],"key":"uEWCzDWVA8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, grad=True)","key":"xWDPYbXGKy"},{"type":"outputs","id":"QLhRZIIP_lsCM-QmVSjgx","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2861\nlayer 1 (      Tanh): mean +0.000005, std 4.419007e-04\nlayer 3 (      Tanh): mean -0.000000, std 4.158158e-04\nlayer 5 (      Tanh): mean +0.000005, std 3.875846e-04\nlayer 7 (      Tanh): mean +0.000005, std 3.389598e-04\nlayer 9 (      Tanh): mean -0.000002, std 3.052316e-04\n"},"children":[],"key":"UpieETCQLi"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"2ca0079a03e043aa9e2ed482dddee185\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"159c24455a9d4f4aa55c6576c660c5e4","path":"/build/159c24455a9d4f4aa55c6576c660c5e4.png"},"text/html":{"content_type":"text/html","hash":"5a26b293a7a4e68a6d163018f37757b4","path":"/build/5a26b293a7a4e68a6d163018f37757b4.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"cQR483IJDE"}],"key":"ZRj7ZVHnBv"}],"key":"ac9GiY5Tsw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here, you will see that the gradient distribution is fairly reasonable. And in particular, what we are looking for is that all of these layers (layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AmROQ1jwPi"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BhlPTXsDUT"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zAlLgeiPs6"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wqernW5fg3"},{"type":"text","value":", etc.) in this “sandwich” have roughly the same gradient. Things are not shrinking or exploding. So, let’s train and set the gain as way too small, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QXrHjW9nF7"},{"type":"text","value":"0.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VL8EbKuSRl"},{"type":"text","value":" and see what happens to the activations and the gradients:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gLvso5PfyM"}],"key":"LnmZkzb5ES"}],"key":"BBdhjOPjMz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(weight_gain=0.5)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)","key":"rK6Ngd7U8t"},{"type":"outputs","id":"vKVK6gU8GgKmSy1G4oiTI","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2965\nlayer 1 (      Tanh): mean -0.01, std 0.42, saturated: 0.09%\nlayer 3 (      Tanh): mean -0.00, std 0.20, saturated: 0.00%\nlayer 5 (      Tanh): mean +0.00, std 0.10, saturated: 0.00%\nlayer 7 (      Tanh): mean +0.00, std 0.05, saturated: 0.00%\nlayer 9 (      Tanh): mean -0.00, std 0.02, saturated: 0.00%\nlayer 1 (      Tanh): mean -0.000000, std 1.770416e-05\nlayer 3 (      Tanh): mean -0.000001, std 3.721016e-05\nlayer 5 (      Tanh): mean +0.000002, std 7.565098e-05\nlayer 7 (      Tanh): mean +0.000004, std 1.499158e-04\nlayer 9 (      Tanh): mean -0.000008, std 3.020476e-04\n"},"children":[],"key":"LwoFXavoiF"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"7f1cd4ddb21044c7a09d237efc2d2b40\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"98cfab3f96079f90031a1c80e1988ecf","path":"/build/98cfab3f96079f90031a1c80e1988ecf.png"},"text/html":{"content_type":"text/html","hash":"9848a50c351dba4536e586f487d5e06c","path":"/build/9848a50c351dba4536e586f487d5e06c.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"mAhZpu06fO"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"a92a9706a26f4a0d881ffed67adbcc6b\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"dfe8d3a9076e87b5931b82d372d50d3a","path":"/build/dfe8d3a9076e87b5931b82d372d50d3a.png"},"text/html":{"content_type":"text/html","hash":"b3815eb0fe855e764eab784579637c91","path":"/build/b3815eb0fe855e764eab784579637c91.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"ec5cRsUbTq"}],"key":"h6829fUFKr"}],"key":"UsOG2qDnKy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"First of all, now, the activations are shrinking to zero but also the gradients are doing something weird: they start off very narrow, around ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KBVKMO1zWt"},{"type":"text","value":"0.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PjFuMlbcHr"},{"type":"text","value":" (see layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c5djDhpJXc"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o6u1e02140"},{"type":"text","value":"), but then in layers that follow, they are expanding out (layer 3, 5, etc.). If we now use a too-high of a gain, e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G9k4EOx7zX"},{"type":"text","value":"3.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gplDTTHB7R"},{"type":"text","value":", like we did before:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nbS7TuaIUd"}],"key":"dLj9rp2SmT"}],"key":"aLMSP9hkj4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(weight_gain=3.0)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, grad=True)","key":"B7rJAt1YrP"},{"type":"outputs","id":"OY3kjepVCOIM8FhniKQad","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2981\nlayer 1 (      Tanh): mean +0.000022, std 1.098993e-03\nlayer 3 (      Tanh): mean -0.000023, std 7.887821e-04\nlayer 5 (      Tanh): mean +0.000010, std 6.063787e-04\nlayer 7 (      Tanh): mean +0.000011, std 4.196224e-04\nlayer 9 (      Tanh): mean -0.000010, std 2.980209e-04\n"},"children":[],"key":"mW7m3ajArG"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ef876bcd9f874b1f8532a53bb056d0bf\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"f46a4708678695e9ba0baf84554698b6","path":"/build/f46a4708678695e9ba0baf84554698b6.png"},"text/html":{"content_type":"text/html","hash":"10aefbcea9d458e1865b118292db2bed","path":"/build/10aefbcea9d458e1865b118292db2bed.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"GDBVTEOAou"}],"key":"JXHTsDrtf0"}],"key":"DrzVkVMx0i"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"then we see that for the gradients there is some asymmetry going on where, as you go into deeper and deeper layers, the activations are also changing. Therefore, we have to very carefully set the grains to get nice activations in both the forward and backard passes. Now, before we move on to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B1QQJghKCJ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O2zSKW2ZZz"}],"key":"t1nmu933Fk"},{"type":"text","value":", let’s see what happens with the activations when we remove the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QwgLIfow5c"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HhWOKHmX8x"},{"type":"text","value":" units and thus only a giant linear sandwich remains as our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CbOcvFuMOW"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fOjLTKsq5F"}],"key":"jtG3DfePTv"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B02bQWTaM3"}],"key":"BnALuL50y4"}],"key":"GlDc2lSaEG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(tanh_enabled=False)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, layer_cls=Linear)","key":"z6zPPUB8Ls"},{"type":"outputs","id":"UnOyKxuLYPCThcmASmFzH","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 4.4966\nlayer 0 (    Linear): mean -0.14, std 1.70, saturated: 57.47%\nlayer 1 (    Linear): mean +0.01, std 2.76, saturated: 71.41%\nlayer 2 (    Linear): mean +0.04, std 4.79, saturated: 82.16%\nlayer 3 (    Linear): mean +0.06, std 7.88, saturated: 90.72%\nlayer 4 (    Linear): mean -0.40, std 13.66, saturated: 94.12%\n"},"children":[],"key":"FJJDb2OeZ6"},{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"/tmp/ipykernel_1430637/3179552959.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  plt.figure(figsize=(20, 4))  # width and height of the plot\n"},"children":[],"key":"MFrkIK89cC"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8e38fe847ef34a448499d4463eaaa596\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"8fda9018afae64835ab021b18256b4b4","path":"/build/8fda9018afae64835ab021b18256b4b4.png"},"text/html":{"content_type":"text/html","hash":"baff4176d83aba459d4640dc9f0de8ef","path":"/build/baff4176d83aba459d4640dc9f0de8ef.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"mPrZSlUu3f"}],"key":"q8Xhds3tEg"}],"key":"jqMCv6pYgF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What we are seeing is that the activations started out on the blue (layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x2ZMSWGy4f"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bsjHHD0Eam"},{"type":"text","value":") and by layer 4 they have become very diffuse, so what is happening to the activations is that they are expanding.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E1ApxEyVFR"}],"key":"mMGVuXzLWK"}],"key":"AAW6AhRN34"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_layer_values(layers, grad=True, layer_cls=Linear)","key":"WCSACZA7j7"},{"type":"outputs","id":"xYVPl29HyOZll0iR9kOX6","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"layer 0 (    Linear): mean +0.000016, std 2.696820e-03\nlayer 1 (    Linear): mean -0.000022, std 1.605610e-03\nlayer 2 (    Linear): mean +0.000048, std 9.567880e-04\nlayer 3 (    Linear): mean -0.000002, std 5.531530e-04\nlayer 4 (    Linear): mean -0.000003, std 3.283872e-04\n"},"children":[],"key":"j2mHFGrFfl"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"36c0c1b313ab4675bf2472913eed00ff\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"a531114c2211462d4a937232c5236afb","path":"/build/a531114c2211462d4a937232c5236afb.png"},"text/html":{"content_type":"text/html","hash":"58634ef05ba30127a15bbd635d4de7ee","path":"/build/58634ef05ba30127a15bbd635d4de7ee.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"MkKAz2wR2j"}],"key":"EkP6laNszp"}],"key":"Zz8tDPpQBM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Conversely, the gradients follow the opposite pattern, as you go down deeper in the layers. So basically you have an asymmetry in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nstl2oAQuK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r9VCqmLkU2"}],"key":"sg0XSMI3ZY"},{"type":"text","value":". And you might imagine that if you have very deep ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PcSWTYJeAo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TCUt1sZLZd"}],"key":"nXQZGg1fyM"},{"type":"text","value":"s, say like 50 layers or something like that, the above pattern is not a good place to be! That’s why before the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lAQSQeOLwm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lpnX4R4qiN"}],"key":"kBILpDqWtl"},{"type":"text","value":" technique, the grain was incredibly tricky to set. See what happens, for a very small gain:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kCroiJVQqV"}],"key":"wETWldJg67"}],"key":"Kzxh6btUp7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(tanh_enabled=False, weight_gain=0.5)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, layer_cls=Linear)\nvisualize_layer_values(layers, grad=True, layer_cls=Linear)","key":"WzfEg68Unz"},{"type":"outputs","id":"aIBOPdnQulIoo3i86lYe0","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2954\nlayer 0 (    Linear): mean -0.04, std 0.49, saturated: 5.09%\nlayer 1 (    Linear): mean -0.01, std 0.24, saturated: 0.03%\nlayer 2 (    Linear): mean +0.00, std 0.12, saturated: 0.00%\nlayer 3 (    Linear): mean +0.00, std 0.06, saturated: 0.00%\nlayer 4 (    Linear): mean -0.00, std 0.03, saturated: 0.00%\nlayer 0 (    Linear): mean +0.000000, std 1.994393e-05\nlayer 1 (    Linear): mean -0.000000, std 4.033995e-05\nlayer 2 (    Linear): mean +0.000003, std 8.008869e-05\nlayer 3 (    Linear): mean +0.000005, std 1.561930e-04\nlayer 4 (    Linear): mean +0.000003, std 3.092947e-04\n"},"children":[],"key":"Pt2wKk0eFV"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"035d3ebce40a4b3290328add55192b18\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"6d0724098aad17c99cd5832e494b580d","path":"/build/6d0724098aad17c99cd5832e494b580d.png"},"text/html":{"content_type":"text/html","hash":"c4a7fd2363ede99526023e8f813be5f5","path":"/build/c4a7fd2363ede99526023e8f813be5f5.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"j5UnbumWx8"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"49b789bb79d444eab794a1937765e259\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"978000066aea7365206111e9fe0d79c3","path":"/build/978000066aea7365206111e9fe0d79c3.png"},"text/html":{"content_type":"text/html","hash":"f7fd9ad18ca37731da6210990137bbcd","path":"/build/f7fd9ad18ca37731da6210990137bbcd.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"QGZ1TnpUbL"}],"key":"lhSpCsqMFJ"}],"key":"uxfHWewL0W"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Basically, the reverse occurs: activations shrink and gradients diffuse, as we go deeper in the layers. Therefore, certainly these patterns are not what we would want and in this case the correct setting of the gain is exactly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vuaiK7LbVx"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fr4YVFi4sm"},{"type":"text","value":", just as we are doing at initialization:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nad9AGCkMr"}],"key":"TDi5SoU68Z"}],"key":"pIFYgtmLyP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(tanh_enabled=False, weight_gain=1.0)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_layer_values(layers, layer_cls=Linear)\nvisualize_layer_values(layers, grad=True, layer_cls=Linear)","key":"ec1H8wN0y3"},{"type":"outputs","id":"sYTEJjdh-J_duLAg1Kjdh","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.3175\nlayer 0 (    Linear): mean -0.04, std 0.97, saturated: 31.66%\nlayer 1 (    Linear): mean -0.01, std 0.98, saturated: 30.91%\nlayer 2 (    Linear): mean +0.01, std 1.01, saturated: 32.06%\nlayer 3 (    Linear): mean +0.01, std 0.99, saturated: 32.44%\nlayer 4 (    Linear): mean -0.06, std 1.00, saturated: 32.16%\nlayer 0 (    Linear): mean -0.000002, std 3.170026e-04\nlayer 1 (    Linear): mean -0.000003, std 3.147987e-04\nlayer 2 (    Linear): mean +0.000015, std 3.173883e-04\nlayer 3 (    Linear): mean +0.000002, std 3.088438e-04\nlayer 4 (    Linear): mean -0.000003, std 2.986307e-04\n"},"children":[],"key":"rd3IRMuzLk"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"dfae44f1594a4d849eda23cc8f864b1f\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"af295a618ce7ad811b54a6e2c9435096","path":"/build/af295a618ce7ad811b54a6e2c9435096.png"},"text/html":{"content_type":"text/html","hash":"095812ed0b67b1e622592e58f0c40852","path":"/build/095812ed0b67b1e622592e58f0c40852.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"vlCKpm3Wjq"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"187766cf5fe148ce846fdb0fb90fff09\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"4d2e4542fe4c0ec91cc78feed6c9692e","path":"/build/4d2e4542fe4c0ec91cc78feed6c9692e.png"},"text/html":{"content_type":"text/html","hash":"d7f148818c07ba91134feebfad266394","path":"/build/d7f148818c07ba91134feebfad266394.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"p6OLYAlu6n"}],"key":"z9HbTIyEHj"}],"key":"NHbtjUfbW0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now we see that the statistics for the forward and backward passes are well behaved! And so the reason we are demonstrating these phenomena is to highlight how getting ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zkFU0UaBwL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xeQmKkkVqJ"}],"key":"AZhxtT7EtY"},{"type":"text","value":"s to train before these normalization layers and before the use of advanced optimizers like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RlulHYtyDh"},{"type":"inlineCode","value":"Adam","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DkMAK5yKA1"},{"type":"text","value":" (which we still have to cover) and residual connections and so on, training ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Shtw92DXZf"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jFv4IuuZJp"}],"key":"vVUD0UWZjm"},{"type":"text","value":"s basically looked like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xkJeB2QFdz"}],"key":"kEFpM53KR3"}],"key":"K6VIpenIeE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"pencil_balancing.jpeg\"))","key":"UsBvPTSNOV"},{"type":"outputs","id":"LNZLX6sUfKgShskqbaoSq","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"2037737ec8e3ad0252f481e8d6a9cd3b","path":"/build/2037737ec8e3ad0252f481e8d6a9cd3b.jpeg"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"khn4b6taZY"}],"key":"wkBc9kg6GQ"}],"key":"CDrthmuaB0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Haha, like a total balancing act. You have to make sure that everything is precisely orchestrated and have to care about the activations and the gradients and the statistics and then maybe you can train something. But, it was basically impossible to train very deep networks, and this was fundamentally the reason for that. You would have to be very very careful with your initialization. The other point to make here is the question: why do we need ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DLQ8aP8KOh"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"HK5scJlwHb"},{"type":"text","value":" layers at all? Why do we include them and then have to worry about the gain? The reason for that of course is that if you just have a stack of linear layers, then certainly we are very easily getting nice activations and so on but this is just a massive linear sandwich. And it turns out that it collapses to a single linear layer in terms of its representation power. So, if you were to plot the output as a function of the input, in that case, you are just getting a linear function. No matter how many linear layers you stack up, you still end up with just a linear transformation: all the sets of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AR5AidGunU"},{"type":"inlineMath","value":"wx + b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">wx + b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span>","key":"t9KtoJiGBh"},{"type":"text","value":" just collapse into a large ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QOFwg0wmA1"},{"type":"inlineMath","value":"WX + B","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><mi>X</mi><mo>+</mo><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">WX + B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span>","key":"riDuLfUFXL"},{"type":"text","value":" with a slightly different weight and bias matrix. Interestingly though, even though in that case, the forward pass collapses to just a linear layer, because of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"seqdOAZVWl"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rYaRJOftJM"}],"key":"KhmNbPbRMF"},{"type":"text","value":" and the dynamics of the backward pass, the optimization is really not identical. You actually end up with all kinds of interesting dynamics in the backward pass because of the way the chain rule is calculating it. And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layer, in both cases, those are just a linear transformation in the forward pass, but the training dynamics would be different. And there are actually in fact entire papers that analyze infinitely layered linear layers, etc. and so as you can imagine there’s a lot of things too that you can play with there. Basically, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R1w0CiOt0X"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cYbpsbbqzj"},{"type":"text","value":" non-linearities allow us to turn this sandwich from just a linear transformation into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aDn3SAgDfZ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ytZsv9nUfu"}],"key":"SBtM0frliy"},{"type":"text","value":" that can in priciple approximate any arbitrary function.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sy7SUnqB6d"}],"key":"J2wMWaA9CK"}],"key":"Qd09RvYtA9"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Further visualization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o9QY6YKa4f"}],"identifier":"further-visualization","label":"Further visualization","html_id":"further-visualization","implicit":true,"key":"XY8KmGeAC0"}],"key":"qStmAGWLgt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we’ll define a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x5IHftVUya"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"takBWD6F2E"}],"key":"R6RxBT7HmS"},{"type":"text","value":" with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pMdGxcZYyJ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H8srKw0Fyb"}],"key":"nMG6oOj3UO"},{"type":"text","value":" layers between the linear and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"psZSow88v8"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"nJT7C2jb9r"},{"type":"text","value":" layers and we will look at another kind of visualization that is very important to consider when training ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SeTzv00UQ3"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qZamAFCd0T"}],"key":"ulKYZJKKoC"},{"type":"text","value":"s:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qux4QQJprV"}],"key":"m4vEt4iFhY"}],"key":"vvSIeivwvF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def visualize_weight_gradients(parameters):\n    plt.figure(figsize=(20, 4))  # width and height of the plot\n    legends = []\n    for i, p in enumerate(parameters):\n        t = p.grad\n        if p.ndim == 2:\n            print(\n                \"weight %10s | mean %+f | std %e | grad:data ratio %e\"\n                % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std())\n            )\n            hy, hx = torch.histogram(t, density=True)\n            plt.plot(hx[:-1].detach(), hy.detach())\n            legends.append(f\"{i} {tuple(p.shape)}\")\n    plt.legend(legends)\n    plt.title(\"weights gradient distribution\")","key":"XHobe9U2is"},{"type":"outputs","id":"HQlyrX2eENmCHm3aB_HBK","children":[],"key":"N51zR2eFHx"}],"key":"qfp80IQ1X3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(batchnorm_enabled=True)\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=0)\nvisualize_weight_gradients(parameters)","key":"VaspOT8CHV"},{"type":"outputs","id":"UJ2lFuywNaBEBZCp9yDU5","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"46970\n      0/ 200000: 3.2891\nweight   (27, 10) | mean -0.000000 | std 1.300287e-03 | grad:data ratio 1.299346e-03\nweight  (30, 100) | mean -0.000036 | std 1.190012e-03 | grad:data ratio 3.815812e-03\nweight (100, 100) | mean -0.000003 | std 1.107649e-03 | grad:data ratio 6.667744e-03\nweight (100, 100) | mean +0.000001 | std 9.681268e-04 | grad:data ratio 5.766639e-03\nweight (100, 100) | mean -0.000003 | std 8.500073e-04 | grad:data ratio 5.084338e-03\nweight (100, 100) | mean +0.000012 | std 7.212228e-04 | grad:data ratio 4.309830e-03\nweight  (100, 27) | mean +0.000000 | std 2.098679e-02 | grad:data ratio 2.072655e+00\n"},"children":[],"key":"QcbzJWYXbS"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"94c89df07fcb4143b6969f4923a872cd\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"f97845b09a1aebe0a2b1a4871a1389fa","path":"/build/f97845b09a1aebe0a2b1a4871a1389fa.png"},"text/html":{"content_type":"text/html","hash":"59de9ad9ebb9e025e96b7daed75190fd","path":"/build/59de9ad9ebb9e025e96b7daed75190fd.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Dxbta6AywN"}],"key":"DT1sWlY2WD"}],"key":"mKDztF2QTl"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, ultimately what we are doing during training is that we are updating the parameters of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UAv2a6fuRo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UTLNkGHtoq"}],"key":"XdI8BUUnM0"},{"type":"text","value":". So, we care about the parameters, their values and their gradients. Therefore, in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W8EBR5xTl7"},{"type":"inlineCode","value":"visualize_weight_gradients","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F7gyi12b3d"},{"type":"text","value":" what we are doing is we are iterating over all the available parameters and then we are only considering the 2-dimensional ones (by checking ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rOddVkuFTH"},{"type":"inlineCode","value":"if p.ndim == 2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ngmf7k2Sxq"},{"type":"text","value":"), which are basically the weights of these linear layers. We are skipping the biases, the gammas and the betas in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HXMVHM7Ftc"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UdY9VrYamO"}],"key":"S0CpUoV5bO"},{"type":"text","value":" layer just for simplicity, because what is happening with the weights is instructive by itself. Here, we have printed the mean, std and gradient-to-data ratio, which is helpful for getting a sense of the scale of the gradient compared to the scale of the actual values. This is important because we are going to be taking a step update that is the learning rate times the gradient onto the data. And so if the gradient has too large of a magnitude (if the numbers in that tensor are too large) compared to the data (the numbers in the data tensor), then you are in trouble. But in our case, our grad-to-data ratios are low numbers (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zAPb5TkBIK"},{"type":"inlineCode","value":"1.209762e-03","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EzZSNT09T1"},{"type":"text","value":") and the grad values are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z4hRcrv9bt"},{"type":"text","value":"100","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"okENRL2Fxa"},{"type":"text","value":" to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nevk0GyJm4"},{"type":"text","value":"1000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wfOfUD7iIn"},{"type":"text","value":" times smaller than the data values of these weight parameters. Notably, this is not true about the last layer (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YWPbBVsLa1"},{"type":"text","value":"16","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IvhRo9S0Ke"},{"type":"text","value":", pink) which is a bit of a troublemaker in the way that it is currently arranged. Because you can see that this layer takes on values that are much larger than some of the other layer’s values inside the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R5quHzr4eb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pB8VL64ZfW"}],"key":"XIsE8DpXTD"},{"type":"text","value":". And so the std values are roughly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yldrXvVK7W"},{"type":"span","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"10","key":"xk2VStKLD7"},{"type":"superscript","children":[{"type":"text","value":"-3","key":"EAjffDyMUk"}],"key":"MQI7rvLkft"}],"key":"hArHmiwyDJ"},{"type":"text","value":" throughout the layers, except for the last linear layer that has an std of roughly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kIHxm1ALor"},{"type":"span","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"10","key":"eQehqSFbVK"},{"type":"superscript","children":[{"type":"text","value":"-2","key":"zK86FP3fpI"}],"key":"oFvpMCprxf"}],"key":"S3p6dSUztd"},{"type":"text","value":". That is problematic, because in the simple stochastic gradient descent setup, you would be training the last layer about ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LZ9SSSEcLP"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f5TcGOmWAt"},{"type":"text","value":" times faster than you would be training the other layers at initialization. Now this actually fixes itself a little bit if you train for a bit longer. So, for example if we stop the training at step ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qAkyp2aaYW"},{"type":"text","value":"1000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LrP3aZZeWh"},{"type":"text","value":" and plot the distributions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T2gqPBSlGc"}],"key":"YuJdk0JoBH"}],"key":"FE3NlzGSgR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)","key":"SKhV12xtZk"},{"type":"outputs","id":"fzs5_GifPQ_SqSH1vKMYg","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.3036\nlayer 1 (      Tanh): mean -0.07, std 0.77, saturated: 22.59%\nlayer 3 (      Tanh): mean -0.00, std 0.73, saturated: 13.75%\nlayer 5 (      Tanh): mean -0.01, std 0.74, saturated: 13.34%\nlayer 7 (      Tanh): mean -0.02, std 0.73, saturated: 11.97%\nlayer 9 (      Tanh): mean -0.04, std 0.71, saturated: 9.81%\n"},"children":[],"key":"G4eCjwgcu7"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"1a665f840df346d49514e40ef06e0d09\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"1c41dda4a3ea94bd0e3a4513cc44f7b9","path":"/build/1c41dda4a3ea94bd0e3a4513cc44f7b9.png"},"text/html":{"content_type":"text/html","hash":"aabee436a6779a84fbf9b85eacdbaa3f","path":"/build/aabee436a6779a84fbf9b85eacdbaa3f.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"xaDk35J9zt"}],"key":"dej3XLdlsb"}],"key":"wvI9f2ljQx"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here we see how in the forward pass the neurons are saturating just a bit (~21% for layer 1, ~11% for layers 2+).","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L72dwfqs28"}],"key":"vNlWxRnBCL"}],"key":"VTM8DpXrxr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_layer_values(layers, grad=True)","key":"ukTkztnpZH"},{"type":"outputs","id":"FT5waBTKlNYkTbxuvotA2","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"layer 1 (      Tanh): mean +0.000007, std 3.624916e-03\nlayer 3 (      Tanh): mean +0.000012, std 3.253933e-03\nlayer 5 (      Tanh): mean +0.000041, std 3.013918e-03\nlayer 7 (      Tanh): mean +0.000042, std 2.847814e-03\nlayer 9 (      Tanh): mean -0.000004, std 2.390941e-03\n"},"children":[],"key":"rNFI0W1qWg"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"48a615d69ab042328643db409c5ad5d9\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"3d8f512eccb13f4394231d9121d087cd","path":"/build/3d8f512eccb13f4394231d9121d087cd.png"},"text/html":{"content_type":"text/html","hash":"e09510e860f7b797831753f5292bdfac","path":"/build/e09510e860f7b797831753f5292bdfac.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"mX2r3fEhOv"}],"key":"KolKngTlqH"}],"key":"dy7Jpca7lv"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And if we also look at the backward pass, the stds are more or less equal and there is no shrinking to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mi8UybAeOR"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XIUESyi9K1"},{"type":"text","value":" or exploding to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W0ERwLrAiU"},{"type":"inlineMath","value":"\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord\">∞</span></span></span></span>","key":"nnw9Gy9P6W"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s4eQQ3AZod"}],"key":"TRdAXAQIak"}],"key":"zl5yErhWaC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_weight_gradients(parameters)","key":"sW6fMnTq6C"},{"type":"outputs","id":"uO5Xphgr4wNzAw75jYj4l","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"weight   (27, 10) | mean +0.001014 | std 1.351586e-02 | grad:data ratio 1.350231e-02\nweight  (30, 100) | mean +0.000045 | std 1.049890e-02 | grad:data ratio 3.353235e-02\nweight (100, 100) | mean -0.000138 | std 8.640624e-03 | grad:data ratio 5.135019e-02\nweight (100, 100) | mean -0.000059 | std 7.113333e-03 | grad:data ratio 4.192131e-02\nweight (100, 100) | mean -0.000067 | std 6.203464e-03 | grad:data ratio 3.675764e-02\nweight (100, 100) | mean +0.000043 | std 4.948972e-03 | grad:data ratio 2.940613e-02\nweight  (100, 27) | mean -0.000000 | std 1.747323e-02 | grad:data ratio 2.483876e-01\n"},"children":[],"key":"uaFSjv5ajk"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"4b567166e2834649b7f05d9c7cb47206\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"063b315617202ee72da0bc42d84be336","path":"/build/063b315617202ee72da0bc42d84be336.png"},"text/html":{"content_type":"text/html","hash":"ac65025a385668e0dda3270fd7c3543e","path":"/build/ac65025a385668e0dda3270fd7c3543e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"I0kQ0HO7Vu"}],"key":"o6ho7F49vz"}],"key":"IsXd1EEhzj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And last but not least, you can see here, in the weight gradients, things are also stabilizing a little bit. So the tails of the last layer (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lA0G9KdUqN"},{"type":"text","value":"6","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q7LP1FJ3Uh"},{"type":"text","value":", pink) are being drawn to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uuN9eHJl7r"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jhozLBRBnF"},{"type":"text","value":" during the optimization. But this is certainly a little bit troubling. Especially if you are using a very simple update rule like stochastic gradient descent, instead of a modern optimizer like Adam. Now, let’s look at another plot that is very useful to look at when training ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ngfOhQEpT3"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tAwRcYiS6x"}],"key":"uE0MKicIC1"},{"type":"text","value":"s. First of all, let’s agree that the grad-to-data ratio is actually not that informative because what matters at the end instead is actually the update-to-data ratio. Because that is the amount by which we will actually change the data in these tensors. So, now let’s update the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LeNvoxRA5s"},{"type":"inlineCode","value":"train","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ge5yEzXVca"},{"type":"text","value":" function by introducing a new update-to-data ratio list (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XwRag7Y9XU"},{"type":"inlineCode","value":"ud","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m5GwjDUkaQ"},{"type":"text","value":") that we are going to be building up for every single training iteration in order to keep track of this ratio:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MTIgtcR8PE"}],"key":"p6373teotT"}],"key":"DsgQZF0kjz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(\n    x,\n    y,\n    layers,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    ud = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(layers, xb, yb)\n        backward(layers, parameters, loss, debug=(break_at_step is not None))\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        with torch.no_grad():\n            ud.append(\n                [\n                    ((lr * p.grad).std() / p.data.std()).log10().item()\n                    for p in parameters\n                ]\n            )\n        if break_at_step is not None and i >= break_at_step:\n            break  # AFTER_DEBUG: would take out obviously to run full optimization\n    return lossi, ud","key":"qxna3JxAiw"},{"type":"outputs","id":"Si54wzqvy7t5d5Zh9RuEy","children":[],"key":"uFWMsdSh1Q"}],"key":"H64N7n9RF6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, let’s initialize a new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UrwUQAnrwl"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CjG67h50TN"}],"key":"fWlSjAikyE"},{"type":"text","value":" and train for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m2xikpnoYq"},{"type":"text","value":"1000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XkdMX8cY28"},{"type":"text","value":" iterations:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wLfSbuDUch"}],"key":"w6GqjjuvHn"}],"key":"eSr1AD6rgl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)","key":"SxAFqrgzRh"},{"type":"outputs","id":"OYA9dkojM5Ie0eB2KDAmH","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2851\n"},"children":[],"key":"a2S28c4Cw8"}],"key":"pEoZHAkl3L"}],"key":"QcyK7HlUR2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And look at the activations, the gradients and the weight gradients:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vbbygVx8rw"}],"key":"nAeZ8LdCYA"}],"key":"CuX9pgnpZy"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)","key":"dFdDL7JnBc"},{"type":"outputs","id":"94CpoUpUnjyITaRIfOu3x","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"layer 1 (      Tanh): mean -0.05, std 0.76, saturated: 21.22%\nlayer 3 (      Tanh): mean +0.04, std 0.71, saturated: 10.84%\nlayer 5 (      Tanh): mean -0.01, std 0.73, saturated: 11.84%\nlayer 7 (      Tanh): mean -0.03, std 0.74, saturated: 12.34%\nlayer 9 (      Tanh): mean -0.02, std 0.71, saturated: 11.22%\nlayer 1 (      Tanh): mean +0.000147, std 3.620432e-03\nlayer 3 (      Tanh): mean -0.000034, std 3.273357e-03\nlayer 5 (      Tanh): mean -0.000040, std 3.102373e-03\nlayer 7 (      Tanh): mean -0.000053, std 3.089136e-03\nlayer 9 (      Tanh): mean +0.000043, std 2.550589e-03\nweight   (27, 10) | mean +0.001451 | std 1.459943e-02 | grad:data ratio 1.458455e-02\nweight  (30, 100) | mean -0.000039 | std 1.180491e-02 | grad:data ratio 3.770098e-02\nweight (100, 100) | mean -0.000031 | std 9.201036e-03 | grad:data ratio 5.468920e-02\nweight (100, 100) | mean +0.000058 | std 7.712632e-03 | grad:data ratio 4.546781e-02\nweight (100, 100) | mean -0.000025 | std 7.597501e-03 | grad:data ratio 4.500666e-02\nweight (100, 100) | mean +0.000049 | std 7.046165e-03 | grad:data ratio 4.183645e-02\nweight  (100, 27) | mean +0.000000 | std 2.525010e-02 | grad:data ratio 3.572145e-01\n"},"children":[],"key":"onB0Rao0UG"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"fa4aefbe2ba4477c9c11347940b8e2dc\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"71d991d904c0883f772abbb70dae0c81","path":"/build/71d991d904c0883f772abbb70dae0c81.png"},"text/html":{"content_type":"text/html","hash":"688eb8b6fb30fc35c003116cf4ed7c63","path":"/build/688eb8b6fb30fc35c003116cf4ed7c63.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"ZlFUCrTvwt"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"5a18c3073fc54c8885b53e12d52581c4\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"83ee061f37e6d451aa4ab777330ff825","path":"/build/83ee061f37e6d451aa4ab777330ff825.png"},"text/html":{"content_type":"text/html","hash":"d53cf0dfb4ab7016b8e21e92d3f6706d","path":"/build/d53cf0dfb4ab7016b8e21e92d3f6706d.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"TY17XVbxIQ"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"c76a73e714ae41b6a4ad14e0600db112\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"8a85ff52d22b050781738913c612f595","path":"/build/8a85ff52d22b050781738913c612f595.png"},"text/html":{"content_type":"text/html","hash":"797e95873b85777556597e0aa504d9eb","path":"/build/797e95873b85777556597e0aa504d9eb.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"ZTmgRkKfY2"}],"key":"Imp7SBEBkg"}],"key":"s3Ecp5sJED"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"but also one more plot we will now introduce:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pF06IBSBlM"}],"key":"ZVz36uMuiY"}],"key":"bLKIlhK8eJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def visualize_update_ratios(parameters, ud):\n    plt.figure(figsize=(20, 4))\n    legends = []\n    for i, p in enumerate(parameters):\n        if p.ndim == 2:\n            plt.plot([ud[j][i] for j in range(len(ud))])\n            legends.append(\"param %d\" % i)\n    plt.plot(\n        [0, len(ud)], [-3, -3], \"k\"\n    )  # these ratios should be ~1e-3, indicate on plot\n    plt.legend(legends)","key":"Uqt9SjOvp2"},{"type":"outputs","id":"IkrdGg5yfi4iXspqK8baY","children":[],"key":"QcMsjNeYvI"}],"key":"eeYrFNsvz2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_update_ratios(parameters, ud)","key":"lG8Age9Ycx"},{"type":"outputs","id":"o59bp50mUWbJlEYIOOUKX","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"c68bfd222a61409aa2c28cd9d58e8e8a\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"11a5fda9394415c56721208714208b44","path":"/build/11a5fda9394415c56721208714208b44.png"},"text/html":{"content_type":"text/html","hash":"e4bb501c9445700ec65e025471a58d5a","path":"/build/e4bb501c9445700ec65e025471a58d5a.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"LHH3d07e8p"}],"key":"zVsrF9pcz6"}],"key":"k6BII2zBnH"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, when we plot the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JjwKyW1Gu4"},{"type":"inlineCode","value":"ud","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PwAt6vHd4p"},{"type":"text","value":" ratios, you can see that they evolve over time. During initialization they take on certain values and these updates sort of like start stabilizing during training. But you’ll also notice we have plotted a straight black line. This is an approximate value that is a rough guide for what the ratios should roughly be, which in this case is roughly ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p6U2bbI4TI"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WyiEd4KvEB"},{"type":"text","value":". That basically means that there are some certain values in the data tensor and the updates to those values at every single iteration are no more than roughly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ze7cwN5QnD"},{"type":"inlineMath","value":"1000th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1000</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">1000th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">1000</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"bn2nwZZKN4"},{"type":"text","value":" of the actual magnitude in those tensors. If instead of roughly ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QV46QObykC"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ps7mccuDRT"},{"type":"text","value":", the desired ratio value are much larger (e.g. ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QM5g5BwZuf"},{"type":"inlineCode","value":"1e-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qXmCnWA2Nu"},{"type":"text","value":" or a log value of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E36VL2pE8G"},{"type":"inlineCode","value":"-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WTIuhrzZr5"},{"type":"text","value":" in this plot), then the data values are updating a lot, meaning that they are undergoing a lot of change. This is the case for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hj03UAb10Z"},{"type":"inlineCode","value":"ud","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uWCWImvi0Z"},{"type":"text","value":" ratio values of the last layer, layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SIuHPEs1i0"},{"type":"text","value":"6","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dim0VcGQVo"},{"type":"text","value":". The reason why this layer is an outlier, is because this layer was artificially shrunk down to keep the softmax unconfident: see the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NzFvywqkyx"},{"type":"inlineCode","value":"define_nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tKRD0nHRNf"},{"type":"text","value":" function where we specifically do: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"plkO85f4fF"},{"type":"inlineCode","value":"layers[-1].weight *= 0.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nk545fDpk6"},{"type":"text","value":". This artificially made inside that last layer tensor way too low and that is why we are temporarily getting a very high ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dc9h3BhotO"},{"type":"inlineCode","value":"ud","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XKmHRrkikD"},{"type":"text","value":" ratio. But as you can see, that ratio does decrease and then stabilizes over time, once that weight starts to learn. In general, it’s helpful to look at the evolution of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q4a9de7wcs"},{"type":"inlineCode","value":"ud","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TdZprvaC6g"},{"type":"text","value":" ratio and as a rule of thumb to make sure that the values are not too much above roughly ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KSnQRfpNNg"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mire3WEreL"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jBLNTr0SDD"},{"type":"text","value":"-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"amj3In1l6o"},{"type":"text","value":" on this log plot). If it’s below ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iOLXdKN3op"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LRWlIA0cSZ"},{"type":"text","value":", usually this means that the parameters are not training fast enough. So, if our learning rate was very low, let’s say ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yeTVoElNZc"},{"type":"text","value":"0.001","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P2vATzrrre"},{"type":"text","value":", this plot will typically reveal it:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HlaBq3v5iO"}],"key":"UVqJ7Dir8D"}],"key":"dMGz5wvhhR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi, ud = train(\n    xtrain, ytrain, layers, parameters, break_at_step=1000, initial_lr=0.001\n)\nvisualize_update_ratios(parameters, ud)","key":"RaOknnPiNj"},{"type":"outputs","id":"3XRvIp9Pjzqt1wvqyUlHL","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2872\n"},"children":[],"key":"PGQfDu1Zsw"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"42b6940fd57c48b3b09c0914839efb5e\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"8a2abc69c17e84a81b373774c16fa52e","path":"/build/8a2abc69c17e84a81b373774c16fa52e.png"},"text/html":{"content_type":"text/html","hash":"3f29f0ea40abe0e361ffec02d6cb6f77","path":"/build/3f29f0ea40abe0e361ffec02d6cb6f77.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"iDZcfD7AxN"}],"key":"AjK5nPRJtL"}],"key":"iybv6GojzF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So you see how all of these updates are way too small. The size of the update is ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QtWOMcQ0mg"},{"type":"inlineCode","value":"1e-5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tETF7QlMEf"},{"type":"text","value":" times smaller than the size of the data tensor values. And this is essentially a symptop of training way too slow. So this is another way to sometimes set the learning rate and to get a sense of what that learning rate should be after inspecting the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vl8Z1YiaLz"},{"type":"inlineCode","value":"ud","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xOQLSZh0dd"},{"type":"text","value":" ratio evolution. If anything, the default learning rate of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q9aiK8kVrR"},{"type":"text","value":"0.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WQwBHmfbEt"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IgjsAx3Mn8"}],"key":"jZmxNHhSDJ"}],"key":"qQgja0EBIq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_update_ratios(parameters, ud)","key":"vDW4KG4y2v"},{"type":"outputs","id":"V9i48QXh-d7AN_-dkRBEc","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n      0/ 200000: 3.2913\n"},"children":[],"key":"w36ZrYUVbo"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9d25f01ab38846749f956cb9ffd566b7\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"bdfae7ccab9ae1ba7dc45fe6c37c0378","path":"/build/bdfae7ccab9ae1ba7dc45fe6c37c0378.png"},"text/html":{"content_type":"text/html","hash":"ddafe65e492227d2e041eb3bd75f943c","path":"/build/ddafe65e492227d2e041eb3bd75f943c.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"eX2cHOvvHL"}],"key":"wI7EPBK4Xc"}],"key":"o6OQYfamho"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"is a little bit on the higher side. Because you see that we’re above the black line of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hbz2MzkxHR"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KEybrJYsp2"},{"type":"text","value":" a little bit, but everything is somewhat stabilizing. So this looks like a pretty decent setting of learning rates. But this is something to look at in general. And when something is miscalibrated you will quickly realize it. So for example, everything looks pretty well behaved, right? But, just as a comparison, when things are not properly calibrated, what does that look like? For example, let’s simulate the scenario were we initialize the weights of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gLr5sszh11"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tigSxll8DG"},{"type":"text","value":" layers from a Gaussian distribution ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zfg7Z4BAKL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"without","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZclDYmstql"}],"key":"SPzKrP913S"},{"type":"text","value":" the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RyLvzToaiK"},{"type":"inlineCode","value":"fan_in","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FU1vOlQz6R"},{"type":"text","value":" normalization (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zYt4Anv3rk"},{"type":"inlineCode","value":"torch.randn((fan_in, fan_out), generator=generator)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ndz4aPjDmJ"},{"type":"text","value":" and not ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wgFP3Lrd69"},{"type":"inlineCode","value":"torch.randn((fan_in, fan_out), generator=generator) / fan_in**0.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u4l1aGIyiD"},{"type":"text","value":"). An easy way to do this without having to re-define the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J755bkgfog"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DkWJkceVhR"},{"type":"text","value":" class and re-write stuff is to simply call a function after defining our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yuNZ3Ko9kE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ovM9wObodr"}],"key":"jXMpZzx6Ay"},{"type":"text","value":" that multiplies each layer’s weight tensor with a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z8LbSCeGzh"},{"type":"inlineCode","value":"fan_in**0.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g2plyv3s4H"},{"type":"text","value":" in order to revert the effect of division by the same number (that happened during initialization):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aNEemuMxG7"}],"key":"L61h8JGmfq"}],"key":"BjRPW9Mo2V"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def revert_fan_in_normalization(layers):\n    for layer in layers:\n        if isinstance(layer, Linear):\n            fan_in = layer.weight.shape[0]\n            layer.weight.requires_grad = False\n            # revert division by fan_in**0.5 and simulate initialization\n            # of weight by sampling from plain Gaussian distribution:\n            layer.weight *= fan_in**0.5\n            layer.weight.requires_grad = True","key":"GtutbVoJOX"},{"type":"outputs","id":"qJ6SWuq1o92X4KLNlQmCW","children":[],"key":"NjvK9HpPtl"}],"key":"xdJaNLWLbc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn()\nrevert_fan_in_normalization(layers)","key":"LYxLzZ0ykt"},{"type":"outputs","id":"caU9FupHbkme-DLwtgslV","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"45970\n"},"children":[],"key":"POWvhcL3FI"}],"key":"eOGxbH7TLK"}],"key":"AkrSvcDmy4"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, how do we notice in this case that something is off? Well, after training, the activations plot should tell you “woaw! your neurons are way too saturated”:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m8yjuSJgTE"}],"key":"YlZyoUrWXK"}],"key":"Ryhb6zeUmJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)","key":"uoFEuAukyX"},{"type":"outputs","id":"6r6xluIxDaARcXqsbwzfA","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"      0/ 200000: 3.6046\nlayer 1 (      Tanh): mean +0.04, std 0.99, saturated: 97.16%\nlayer 3 (      Tanh): mean +0.08, std 0.98, saturated: 91.34%\nlayer 5 (      Tanh): mean +0.00, std 0.98, saturated: 90.28%\nlayer 7 (      Tanh): mean +0.01, std 0.98, saturated: 89.53%\nlayer 9 (      Tanh): mean +0.03, std 0.98, saturated: 90.56%\n"},"children":[],"key":"HosAChj7Ld"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"52aa839cf20344cb94b5732e65f12b88\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"cd7db62c6627620ae5e35103ab8e7ec8","path":"/build/cd7db62c6627620ae5e35103ab8e7ec8.png"},"text/html":{"content_type":"text/html","hash":"8b3a740876046c7741cc1b6290d3f4b6","path":"/build/8b3a740876046c7741cc1b6290d3f4b6.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"HjkEMayot9"}],"key":"ZtEfejh0SG"}],"key":"CVlYKf7gHg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Also, the gradients and weight gradients are going to be all messed up:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aTCWJUBW4x"}],"key":"s00kVoi63R"}],"key":"MkmosG5U4E"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)","key":"qRzu4nxLxP"},{"type":"outputs","id":"e2_derYoCBr36Zfny1CHV","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"layer 1 (      Tanh): mean +0.002450, std 1.395893e-01\nlayer 3 (      Tanh): mean +0.000484, std 5.279354e-02\nlayer 5 (      Tanh): mean +0.000254, std 1.787987e-02\nlayer 7 (      Tanh): mean -0.000173, std 6.437032e-03\nlayer 9 (      Tanh): mean -0.000014, std 2.148476e-03\nweight   (27, 10) | mean +0.004719 | std 2.368491e-01 | grad:data ratio 8.764252e-02\nweight  (30, 100) | mean +0.001443 | std 1.174272e-01 | grad:data ratio 5.857516e-02\nweight (100, 100) | mean -0.000085 | std 3.704451e-02 | grad:data ratio 2.215002e-02\nweight (100, 100) | mean -0.000166 | std 1.271184e-02 | grad:data ratio 7.566377e-03\nweight (100, 100) | mean -0.000024 | std 4.991787e-03 | grad:data ratio 2.985652e-03\nweight (100, 100) | mean +0.000010 | std 1.871466e-03 | grad:data ratio 1.118368e-03\nweight  (100, 27) | mean +0.000000 | std 3.144594e-02 | grad:data ratio 4.646735e-01\n"},"children":[],"key":"jPPFBUEvdz"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"e2cc699e5d414f6d9e8099ba9a9c94a4\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"b0a6e390458bb41d8be83be923b3cb12","path":"/build/b0a6e390458bb41d8be83be923b3cb12.png"},"text/html":{"content_type":"text/html","hash":"eb289aad155e274f506995b2206fae48","path":"/build/eb289aad155e274f506995b2206fae48.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"avngLJGh84"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"3a4ea20d8368450d8678707b9dcd289d\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"9b84a304230b74eafd6f9002a057f8f7","path":"/build/9b84a304230b74eafd6f9002a057f8f7.png"},"text/html":{"content_type":"text/html","hash":"31953315b9a352a529ae91411dae985e","path":"/build/31953315b9a352a529ae91411dae985e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"XD45nYtyKQ"}],"key":"SiwljUJTmQ"}],"key":"ElHiQuvXoi"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And if we look at the update-to-data ratios, they are also quite messed up and all over the places:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gMr6Ms4zIl"}],"key":"gTXGvMyY82"}],"key":"is8ciF4F28"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_update_ratios(parameters, ud)","key":"m6hnIKKvCC"},{"type":"outputs","id":"AKgLgto5PAxCcqMzqrD0s","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"0def0cb5eaec45c59b38a9465d7e8d15\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"03fe25aead64446b601001ce40350baa","path":"/build/03fe25aead64446b601001ce40350baa.png"},"text/html":{"content_type":"text/html","hash":"45a79aea0d7bd655352cc07401604ca8","path":"/build/45a79aea0d7bd655352cc07401604ca8.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"BI7dpSef7f"}],"key":"F81jxNrUDB"}],"key":"fx5esvNBUG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Yikes! As you can see, there is a lot of discrepancy in how fast these layers are learning and some of them are learning way too fast. And so ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ugw1C8Bcc7"},{"type":"inlineCode","value":"1e-1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hb1E8kvKnH"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EsNuZ5sMCz"},{"type":"inlineCode","value":"1e-1.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cTvJ93iFk9"},{"type":"text","value":", etc. are very large numbers in terms of this ratio. Again, we should be somewhere around ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mfrTEoqIXL"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ATixgXAPUP"},{"type":"text","value":" and not much more above that. So, this is how miscalibrations of your ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M1xy9TeToD"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ak09uKqlWK"}],"key":"yEyx2H4Z01"},{"type":"text","value":"s are going to manifest. And therefore such plots are a good way of bringing those miscalibrations to your attention, so you can address them. Okay so so far we have seen that when we have such a linear ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EhCu5toJ8o"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"dQ4mhAoavt"},{"type":"text","value":" sandwich as the one we have constructed, we can actually precisely calibrate the gains and make the activations, the gradients and the parameters and the updates all look pretty decent. But it definitely does feel like trying to balance a pencil on your finger and that’s because the gain has to be very precisely calibrated. So now let’s introduce ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bMYO0lupIs"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cWLIU0gcpl"}],"key":"QH1CyXSDtG"},{"type":"text","value":" layers into the magical sandwich and let’s see how that helps fix the problem, by placing them in-between our linear and tanh layers (note: placing them after the tanh layers would also yield similar results). Luckily, we have already implemented an option for that and all we have to do is to enable the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iWjqHO93m4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xhHNm0tYSQ"}],"key":"RBHR1LDqIQ"},{"type":"text","value":" option. But now we will also add a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vckfrg9fnr"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Jyspstq1Hw"}],"key":"sz3bJuJjkB"},{"type":"text","value":" after the last layer too using the corresponding option:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m01HiUUBO9"}],"key":"vrkd446rHK"}],"key":"s3FlYy53uk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(batchnorm_enabled=True, add_batchnorm_last_layer=True)","key":"D9t0x2x2mi"},{"type":"outputs","id":"yaebAsSXNzVnrYjAnlpuC","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"47024\n"},"children":[],"key":"HRYbt8XkBN"}],"key":"ytdqv3ZRkl"}],"key":"ZSB5IAzylj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, let’s train and look at the distributions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UUjewwRyoG"}],"key":"t6kFMPfIi2"}],"key":"VGUzpF34I7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)","key":"PH9yzpQDL8"},{"type":"outputs","id":"me8NWEtkPMzMvFhSrWTRU","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"      0/ 200000: 3.3037\nlayer 2 (      Tanh): mean -0.01, std 0.63, saturated: 2.84%\nlayer 5 (      Tanh): mean +0.00, std 0.64, saturated: 2.78%\nlayer 8 (      Tanh): mean +0.01, std 0.64, saturated: 2.22%\nlayer 11 (      Tanh): mean -0.00, std 0.65, saturated: 1.69%\nlayer 14 (      Tanh): mean -0.01, std 0.65, saturated: 1.62%\nlayer 2 (      Tanh): mean +0.000000, std 3.910133e-03\nlayer 5 (      Tanh): mean +0.000000, std 3.199076e-03\nlayer 8 (      Tanh): mean +0.000000, std 2.847068e-03\nlayer 11 (      Tanh): mean +0.000000, std 2.580181e-03\nlayer 14 (      Tanh): mean -0.000000, std 2.521838e-03\nweight   (27, 10) | mean -0.000000 | std 1.063054e-02 | grad:data ratio 1.061966e-02\nweight  (30, 100) | mean +0.000073 | std 9.105187e-03 | grad:data ratio 2.913094e-02\nweight (100, 100) | mean +0.000007 | std 7.453867e-03 | grad:data ratio 4.459023e-02\nweight (100, 100) | mean -0.000043 | std 6.202964e-03 | grad:data ratio 3.678501e-02\nweight (100, 100) | mean +0.000010 | std 5.674492e-03 | grad:data ratio 3.381422e-02\nweight (100, 100) | mean +0.000030 | std 5.529360e-03 | grad:data ratio 3.293567e-02\nweight  (100, 27) | mean +0.000064 | std 1.152379e-02 | grad:data ratio 6.914081e-02\n"},"children":[],"key":"eX9nlktHTr"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"de751e0bc3ec40a7bf50e86a07e1a0cc\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"17459d88cde6d719df5f39804fa3d337","path":"/build/17459d88cde6d719df5f39804fa3d337.png"},"text/html":{"content_type":"text/html","hash":"4b9ed028c7d576b9a67d820826af8cdc","path":"/build/4b9ed028c7d576b9a67d820826af8cdc.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"LwvhnZwvnH"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9b8387007985401fa7f0c015038c1fa7\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"2a5d450ff3ea061fc7aff22a6600a903","path":"/build/2a5d450ff3ea061fc7aff22a6600a903.png"},"text/html":{"content_type":"text/html","hash":"efb678d32965e5aeab4b9c795cb540a5","path":"/build/efb678d32965e5aeab4b9c795cb540a5.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"sWNUqf5dv8"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"6309aa034ad24661864761344fb4b71b\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"b92067de2ba5c2c1961193a7b3415384","path":"/build/b92067de2ba5c2c1961193a7b3415384.png"},"text/html":{"content_type":"text/html","hash":"004aa3c8a5757c46e86c8eef99d8a988","path":"/build/004aa3c8a5757c46e86c8eef99d8a988.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"KNSgP1W0fT"}],"key":"LRiumiUvMS"}],"key":"PzvCJVrfho"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"which of course look very good. And they are necessarily going to look good because now before every single ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qcn25D3D6B"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"saFHCFH7z2"},{"type":"text","value":" layer there’s a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p5ZQbIobpS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hUqHSpFaMK"}],"key":"EPWWgnucKe"},{"type":"text","value":" happening. This yields a saturation of ~","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VYd6vOwrDw"},{"type":"inlineMath","value":"2\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">2%</span></span></span></span>","key":"AqV95g7ZJw"},{"type":"text","value":" and roughly equal std across all layers and everything looks very homogeneous in the activations distribution, with the gradient and weight gradient distributions also looking great. Also, the updates:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t7aBPyAtNN"}],"key":"i756pXBz03"}],"key":"GQxJ6kJmr0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_update_ratios(parameters, ud)","key":"LsTTY4PLSM"},{"type":"outputs","id":"h4XlRR2raoeb9tiMC1iYl","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"116940beed29420babd2ee4bf7c5cdc0\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"5242532851fed46dc51c5f8dc9c167d9","path":"/build/5242532851fed46dc51c5f8dc9c167d9.png"},"text/html":{"content_type":"text/html","hash":"912dac2d0eb0817f31cf7a5a7e016909","path":"/build/912dac2d0eb0817f31cf7a5a7e016909.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"baAbIGLqy9"}],"key":"Xg3Vjuvdao"}],"key":"NNYkzeWXbe"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"also look pretty reasonable, with all parameters training at around roughly the same rate. Now, what we have gained is that we can now be slightly less brittle with respect to the gain values of the weights. Meaning, that if for example we make the gain be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cT90iu6ky0"},{"type":"text","value":"0.2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uOraJ47bRZ"},{"type":"text","value":" (much lower than the default ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eRnrDf4B67"},{"type":"inlineMath","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">anh</span></span></span></span>","key":"fndaGKAZ0R"},{"type":"text","value":" gain of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W2LjoMr8q6"},{"type":"inlineMath","value":"5/3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>5</mn><mi mathvariant=\"normal\">/</mi><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">5/3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">5/3</span></span></span></span>","key":"OIS0aXB0O4"},{"type":"text","value":") and then train and print the same distributions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pIPotriTKH"}],"key":"Q1tyEadT1n"}],"key":"o5Kq7dgLJc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=0.2)\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)","key":"Y9zJkPTfn4"},{"type":"outputs","id":"G6zH4gLYe_Wu1i9xhDyfw","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"47024\n      0/ 200000: 3.3041\nlayer 2 (      Tanh): mean -0.01, std 0.64, saturated: 3.00%\nlayer 5 (      Tanh): mean +0.00, std 0.65, saturated: 2.09%\nlayer 8 (      Tanh): mean -0.00, std 0.65, saturated: 1.38%\nlayer 11 (      Tanh): mean -0.00, std 0.66, saturated: 0.81%\nlayer 14 (      Tanh): mean -0.00, std 0.67, saturated: 0.72%\nlayer 2 (      Tanh): mean +0.000000, std 1.356849e-03\nlayer 5 (      Tanh): mean +0.000000, std 1.092489e-03\nlayer 8 (      Tanh): mean -0.000000, std 1.002747e-03\nlayer 11 (      Tanh): mean +0.000000, std 1.016688e-03\nlayer 14 (      Tanh): mean -0.000000, std 1.143892e-03\nweight   (27, 10) | mean -0.000000 | std 8.272509e-03 | grad:data ratio 8.264745e-03\nweight  (30, 100) | mean +0.000205 | std 1.605308e-02 | grad:data ratio 2.640624e-01\nweight (100, 100) | mean -0.000010 | std 7.176930e-03 | grad:data ratio 2.413751e-01\nweight (100, 100) | mean -0.000048 | std 6.421504e-03 | grad:data ratio 2.342190e-01\nweight (100, 100) | mean -0.000007 | std 6.354468e-03 | grad:data ratio 2.399155e-01\nweight (100, 100) | mean +0.000019 | std 6.318578e-03 | grad:data ratio 2.404839e-01\nweight  (100, 27) | mean -0.000009 | std 1.378118e-02 | grad:data ratio 3.142304e-01\n"},"children":[],"key":"V9UN5DNvrJ"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"d6de78cf046a492396da4a64544620da\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"1fecc134ccc6fd773176f2d5593afdd6","path":"/build/1fecc134ccc6fd773176f2d5593afdd6.png"},"text/html":{"content_type":"text/html","hash":"2b2c3fc548bf1faa8f696923173dd30e","path":"/build/2b2c3fc548bf1faa8f696923173dd30e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"yxPs9oW4wU"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"e23c3ad52eb14df387903ce6b4748031\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"c3276caf213b7a1f5993ba03daca069d","path":"/build/c3276caf213b7a1f5993ba03daca069d.png"},"text/html":{"content_type":"text/html","hash":"1fa104fb69a861373927abbf927ec121","path":"/build/1fa104fb69a861373927abbf927ec121.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"nZ98mqlyVe"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"253447916936472ab2942f0117fc00c9\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"c6bec9c0e8c0bed51b09a1ce01c772f3","path":"/build/c6bec9c0e8c0bed51b09a1ce01c772f3.png"},"text/html":{"content_type":"text/html","hash":"9bde047e7223a37f1116d1d20cc8a752","path":"/build/9bde047e7223a37f1116d1d20cc8a752.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"tkuNQpLVW9"}],"key":"zIJRX15Lvj"}],"key":"JyVgnZ1DSu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"They will all look pretty ok and unaffected! However, if we plot the updates:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DdOdLxOgVl"}],"key":"uki4D0cnpm"}],"key":"JYQ3dw4YV3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualize_update_ratios(parameters, ud)","key":"KaxLEkofjj"},{"type":"outputs","id":"0A6fVx3zg313Y9ldgbuG2","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"10a28ea1308441f380026200cff8f029\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"8850fd87ebc4649d7ac5f6b465374457","path":"/build/8850fd87ebc4649d7ac5f6b465374457.png"},"text/html":{"content_type":"text/html","hash":"800a7673de46fc96d93cbe312f923a2d","path":"/build/800a7673de46fc96d93cbe312f923a2d.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Ss5CfFj3jt"}],"key":"whJNDcEsPv"}],"key":"uk9wWdVhO0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"we see that these do in fact change. And so even though the forward and backward pass to a very large extent look okay because of the backward pass of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X0bTyDApqo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e9vQXdTZbY"}],"key":"AOUEbHz7lZ"},{"type":"text","value":" and how the specifically the scale of the incoming activations interacts in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ErGHdsxP3R"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QtFkZUYhOA"}],"key":"D1e9d3RXr1"},{"type":"text","value":" and its backward pass, the decrease of the gain is actually changing the scale of the updates on these parameters. So, the gradients on these weights are affected. So, we still don’t get a completely free pass to pass any arbitrary weight gain, but everything else is significantly more robust in terms of the forward and backward passes and the weight gradients. It’s just that you may in such a case need to retune the learning rate if you are changing sufficiently the scale of the activations that are coming into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pxsGcFhSW0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uMywzapFI3"}],"key":"V1Vd5RysXH"},{"type":"text","value":" layers. To verify this, we can see how making the gain to a greater value like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JPeNRIHbkp"},{"type":"text","value":"5.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LGQ80z5xYJ"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OrQzJR7chc"}],"key":"ZbjnW1BdgX"}],"key":"RV9zMXDXuY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=5.0)\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\nvisualize_update_ratios(parameters, ud)","key":"PFSfhUo7NR"},{"type":"outputs","id":"fd_oi1nedRd9X9m6HnH1M","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"47024\n      0/ 200000: 3.2738\nlayer 2 (      Tanh): mean -0.01, std 0.63, saturated: 2.84%\nlayer 5 (      Tanh): mean -0.00, std 0.63, saturated: 2.44%\nlayer 8 (      Tanh): mean +0.00, std 0.64, saturated: 2.31%\nlayer 11 (      Tanh): mean -0.00, std 0.64, saturated: 2.09%\nlayer 14 (      Tanh): mean +0.00, std 0.64, saturated: 2.72%\nlayer 2 (      Tanh): mean -0.000000, std 2.696700e-03\nlayer 5 (      Tanh): mean -0.000000, std 2.428798e-03\nlayer 8 (      Tanh): mean +0.000000, std 2.221820e-03\nlayer 11 (      Tanh): mean +0.000000, std 2.076553e-03\nlayer 14 (      Tanh): mean +0.000000, std 1.946961e-03\nweight   (27, 10) | mean -0.000000 | std 6.479451e-03 | grad:data ratio 6.473200e-03\nweight  (30, 100) | mean -0.000037 | std 2.023350e-03 | grad:data ratio 2.162614e-03\nweight (100, 100) | mean +0.000024 | std 2.014056e-03 | grad:data ratio 4.040985e-03\nweight (100, 100) | mean -0.000014 | std 1.705210e-03 | grad:data ratio 3.385467e-03\nweight (100, 100) | mean +0.000003 | std 1.588601e-03 | grad:data ratio 3.167232e-03\nweight (100, 100) | mean +0.000005 | std 1.499648e-03 | grad:data ratio 2.987035e-03\nweight  (100, 27) | mean +0.000037 | std 2.765074e-03 | grad:data ratio 5.592591e-03\n"},"children":[],"key":"AL0RNSQAeX"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"e163bac57c0a4d4bbd186748f456149c\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"f0999a9ceaf7d84de6408ca51c521e8c","path":"/build/f0999a9ceaf7d84de6408ca51c521e8c.png"},"text/html":{"content_type":"text/html","hash":"e0dbb9eb90240f1baaaff1af312242dc","path":"/build/e0dbb9eb90240f1baaaff1af312242dc.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"fSyrbtRbbV"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"cb4fa5f26b6047ec81dc7d0bc25682a5\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"5be0008919803c0f64ccb6f9ed0904d5","path":"/build/5be0008919803c0f64ccb6f9ed0904d5.png"},"text/html":{"content_type":"text/html","hash":"1a4c171742dd4d09457ebf09f9a5b799","path":"/build/1a4c171742dd4d09457ebf09f9a5b799.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"TJwJlNdoQM"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"a925269c8ca64634b613424645b2e64a\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"56163f392c00ab2022dee64b9b745ab0","path":"/build/56163f392c00ab2022dee64b9b745ab0.png"},"text/html":{"content_type":"text/html","hash":"8dc2e2eba69047e8d683fbd5d52b8241","path":"/build/8dc2e2eba69047e8d683fbd5d52b8241.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"WlB5NHZtIS"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"0540cd5f274f48c08f5bfe57c45330f3\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"50f641b47ab6ecd7c74794a63c6cb1af","path":"/build/50f641b47ab6ecd7c74794a63c6cb1af.png"},"text/html":{"content_type":"text/html","hash":"7ab80c486896f94d469b31b7e994e841","path":"/build/7ab80c486896f94d469b31b7e994e841.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"NIUa0CqtJq"}],"key":"mfcbh8MKqz"}],"key":"qMIyJ2u3os"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"now causes the updates to come out lower, as a result. Finally let’s now remove the weight gain by setting it to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q8pgpVPhL5"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F1ouHR3EbG"},{"type":"text","value":" notice that with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iDNhh6PJYM"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BfCphRcg5e"}],"key":"qc38WBLLKQ"},{"type":"text","value":" enabled we can now also skip the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sTr6BAshfj"},{"type":"inlineCode","value":"fan_in","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XL70UfgItm"},{"type":"text","value":" normalization at initialization. So, like we did before, if we define our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZvIwrjFlAj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BdPUytuXYt"}],"key":"Eu6lyngsqV"},{"type":"text","value":" by sampling the initial weights from a plain Gaussian:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QS7V2cnb9t"}],"key":"cgBBcetX06"}],"key":"MBrGbvkyci"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(\n    batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=1.0\n)\nrevert_fan_in_normalization(layers)\nlossi, ud = train(xtrain, ytrain, layers, parameters, break_at_step=1000)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\nvisualize_update_ratios(parameters, ud)","key":"RPRfPDL6Os"},{"type":"outputs","id":"2hWnoxcyskYFnp_SL3R5o","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"47024\n      0/ 200000: 3.2920\nlayer 2 (      Tanh): mean -0.00, std 0.64, saturated: 2.78%\nlayer 5 (      Tanh): mean -0.01, std 0.64, saturated: 1.97%\nlayer 8 (      Tanh): mean -0.00, std 0.64, saturated: 2.53%\nlayer 11 (      Tanh): mean -0.00, std 0.64, saturated: 2.25%\nlayer 14 (      Tanh): mean +0.00, std 0.63, saturated: 2.59%\nlayer 2 (      Tanh): mean -0.000000, std 3.312148e-03\nlayer 5 (      Tanh): mean -0.000000, std 3.053182e-03\nlayer 8 (      Tanh): mean +0.000000, std 2.762014e-03\nlayer 11 (      Tanh): mean -0.000000, std 2.388142e-03\nlayer 14 (      Tanh): mean -0.000000, std 2.036850e-03\nweight   (27, 10) | mean +0.000000 | std 5.950560e-03 | grad:data ratio 5.945425e-03\nweight  (30, 100) | mean +0.000005 | std 2.227116e-03 | grad:data ratio 2.173013e-03\nweight (100, 100) | mean -0.000008 | std 1.128165e-03 | grad:data ratio 1.131871e-03\nweight (100, 100) | mean +0.000001 | std 9.868351e-04 | grad:data ratio 9.796767e-04\nweight (100, 100) | mean +0.000009 | std 8.436788e-04 | grad:data ratio 8.410787e-04\nweight (100, 100) | mean +0.000007 | std 7.515551e-04 | grad:data ratio 7.485138e-04\nweight  (100, 27) | mean -0.000007 | std 1.182972e-03 | grad:data ratio 1.196482e-03\n"},"children":[],"key":"dasBFDGVKd"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8f2c91bd3a664b709e0a51cbb04d9f20\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"e5e16e8cd669845673c3c29fcb5577f4","path":"/build/e5e16e8cd669845673c3c29fcb5577f4.png"},"text/html":{"content_type":"text/html","hash":"3a10ae0b4f675656726d9478091e769e","path":"/build/3a10ae0b4f675656726d9478091e769e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"OeEKu5bhs1"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"4f680bfb726242d7a873af0ea83c8d5b\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"2e05c58157e428f93c0280988c44bfc0","path":"/build/2e05c58157e428f93c0280988c44bfc0.png"},"text/html":{"content_type":"text/html","hash":"a9006f80d91f22f3c944841c23b25652","path":"/build/a9006f80d91f22f3c944841c23b25652.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"WcBitahbOa"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9673baae9300458bbc3d7f9de36bb8b5\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"96825d7369ad2ae2a03f49d1758c8a38","path":"/build/96825d7369ad2ae2a03f49d1758c8a38.png"},"text/html":{"content_type":"text/html","hash":"912db62c87db34d32704a0e66e82f48e","path":"/build/912db62c87db34d32704a0e66e82f48e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"d1LyaoBP2D"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"a65a0948e2fc46d6991041333e6e5b89\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"7fe9ce92d9dd5bceca76dd98e59db9b9","path":"/build/7fe9ce92d9dd5bceca76dd98e59db9b9.png"},"text/html":{"content_type":"text/html","hash":"4f8b254bf5311f4bbbefe8e3c91bd28d","path":"/build/4f8b254bf5311f4bbbefe8e3c91bd28d.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"kdQJlnRgNl"}],"key":"V66xjkPmUC"}],"key":"IYQgmaLVqq"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"everything looks pretty much ok. But from the update plot you can see that everything looks below ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wT7ZvfEXuP"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EI06omP9ke"},{"type":"text","value":", so we would have to bump up the learning rate in order to make sure that we are training more properly. Intuitively, we would probably need to 10x the learning rate from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QUf62k26ty"},{"type":"text","value":"0.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lKq0JkHsow"},{"type":"text","value":" (default) to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ayiYza6MFz"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HsaiD7p9H8"},{"type":"text","value":". Let’s try it out:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QmlBJg1hHV"}],"key":"xKAugdoMUJ"}],"key":"X4wQ1MlQT0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(\n    batchnorm_enabled=True, add_batchnorm_last_layer=True, weight_gain=1.0\n)\nrevert_fan_in_normalization(layers)\nlossi, ud = train(\n    xtrain, ytrain, layers, parameters, break_at_step=1000, initial_lr=1.0\n)\nvisualize_layer_values(layers)\nvisualize_layer_values(layers, grad=True)\nvisualize_weight_gradients(parameters)\nvisualize_update_ratios(parameters, ud)","key":"PAFVPGLBzR"},{"type":"outputs","id":"_lx7tkabdTC2_uKCC3Cpj","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"47024\n      0/ 200000: 3.2936\nlayer 2 (      Tanh): mean -0.00, std 0.63, saturated: 4.19%\nlayer 5 (      Tanh): mean -0.02, std 0.63, saturated: 3.72%\nlayer 8 (      Tanh): mean +0.00, std 0.64, saturated: 3.56%\nlayer 11 (      Tanh): mean -0.02, std 0.64, saturated: 3.41%\nlayer 14 (      Tanh): mean +0.00, std 0.64, saturated: 2.78%\nlayer 2 (      Tanh): mean +0.000000, std 3.989876e-03\nlayer 5 (      Tanh): mean +0.000000, std 3.745467e-03\nlayer 8 (      Tanh): mean +0.000000, std 3.609461e-03\nlayer 11 (      Tanh): mean +0.000000, std 3.665544e-03\nlayer 14 (      Tanh): mean +0.000000, std 3.469911e-03\nweight   (27, 10) | mean -0.000000 | std 8.855599e-03 | grad:data ratio 8.541719e-03\nweight  (30, 100) | mean +0.000016 | std 2.512573e-03 | grad:data ratio 2.443840e-03\nweight (100, 100) | mean +0.000017 | std 1.302984e-03 | grad:data ratio 1.306176e-03\nweight (100, 100) | mean +0.000009 | std 1.208597e-03 | grad:data ratio 1.199037e-03\nweight (100, 100) | mean +0.000001 | std 1.189904e-03 | grad:data ratio 1.185508e-03\nweight (100, 100) | mean +0.000007 | std 1.228369e-03 | grad:data ratio 1.222727e-03\nweight  (100, 27) | mean +0.000006 | std 2.232471e-03 | grad:data ratio 2.253838e-03\n"},"children":[],"key":"cMkME3u6ZF"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"a112f6ced1d049f68592e9946fdf2128\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"583726c5ddc031f56194387151ca5a53","path":"/build/583726c5ddc031f56194387151ca5a53.png"},"text/html":{"content_type":"text/html","hash":"2a248277f890f4726cb1e7f199fb2b51","path":"/build/2a248277f890f4726cb1e7f199fb2b51.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Wm75tjZVHK"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"d78263a2b02d491586587ab7a5103077\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"c60f8e7d6ba46cb1e747ca094aeca5d7","path":"/build/c60f8e7d6ba46cb1e747ca094aeca5d7.png"},"text/html":{"content_type":"text/html","hash":"2eb658cd68ca9654ad2e34a8335d28ed","path":"/build/2eb658cd68ca9654ad2e34a8335d28ed.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"NxNJNwjZ7S"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"900dc521dff840a4bf951c331c4d9332\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"5fc21a55aeae63820805527eec41def8","path":"/build/5fc21a55aeae63820805527eec41def8.png"},"text/html":{"content_type":"text/html","hash":"f89f24bf7d665585070c1fa01a67ada1","path":"/build/f89f24bf7d665585070c1fa01a67ada1.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"EFxIvEOVPX"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"2a6531da7da4487dbd0c28415341395f\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"0bd632eb37407d01b893eaa8a8f62735","path":"/build/0bd632eb37407d01b893eaa8a8f62735.png"},"text/html":{"content_type":"text/html","hash":"03ba97212b344b13d9985ee7cfc59fbd","path":"/build/03ba97212b344b13d9985ee7cfc59fbd.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"qOkVuAOnEl"}],"key":"KBhGa9gsvG"}],"key":"LGzaYoL0dk"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"everything again looks good and voilà! Now our updates are more reasonable. So, long story short, with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qWbX8YMyYv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"barchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DdNDkBs3Uv"}],"key":"VHw2l5RWBf"},{"type":"text","value":", we are now significantly more robust to the gain of these linear layers, whether or not we have to apply the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TfzFqYTSFV"},{"type":"inlineCode","value":"fan_in","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MMM5Eps9xA"},{"type":"text","value":" normalization, with the caveat (in terms of the former) that we do have to worry about the update scales and making sure that the learning rate is properly calibrated here. So, the forward and backward pass statistics are all looking significantly more behaved, except for the scales of the updates that should be taken into consideration.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pKb5DkEDfB"}],"key":"LKv7clFYDg"}],"key":"viwU4vjiFB"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Summary","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vuKNDSnnrZ"}],"identifier":"summary","label":"Summary","html_id":"summary","implicit":true,"key":"cEpbFzBG08"}],"key":"rCg0CnVSsb"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Ok, so now let’s summarize (again, lol). There are three things this section was intended to teach:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KkFpRr2zC5"}],"key":"NIZq0WDDDP"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"introducing you to ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"tC3qBcL2NI"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"azzq9c7i0j"}],"key":"Y6UgQrJTYD"},{"type":"text","value":", which is one of the first modern innovations that helped stabilize very deep ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lY1Y4Sdvk2"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"jzSmaZ0pF1"}],"key":"QVnIkLpCf1"},{"type":"text","value":"s and their training","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HcNXKmm9yL"}],"key":"WYCHZVj1dB"}],"key":"W6Bn51bplB"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"PyTorch-ifying some of our code by wrapping it up into layer modules (","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"EIIjoDgQKF"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"AUlWUqAEGg"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"OTIrUlNPSv"},{"type":"inlineCode","value":"BatchNorm1D","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"LulFk3rTCY"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"AUSN28k8El"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"HUCmUt4m8d"},{"type":"text","value":", etc.) that can be stacked up into ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"JB6ri8K8VF"},{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Ik35SgmhH5"}],"key":"xuqmH0U49U"},{"type":"text","value":" like lego building blocks. Since these synonymous layers exist as objects in the ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"x1rsQkEJiT"},{"type":"inlineCode","value":"torch.nn","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"lPXI7jRR5l"},{"type":"text","value":" API, the way we have constructed it, we could easily replace each one of our custom modules (","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"XSpxCAlZbd"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"g7fFjdJkuN"},{"type":"text","value":" with ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ArbwVV0yku"},{"type":"inlineCode","value":"nn.Linear","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"iIvydYEYDP"},{"type":"text","value":" and so on) and everything would probably work just fine.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"k3VR1ggHKL"}],"key":"eV8STDJwVp"}],"key":"u9QYIIkWMo"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"present you with the diagnostic tools that you would use to understand whether your ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"qsFiskXaY2"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"QcykuEzUfp"}],"key":"k69l7b9hjC"},{"type":"text","value":" is in a good state dynamically. This means looking at histograms of the forward pass activations and backward pass gradients. And then also the weights that are going to be activated as part of stochastic gradient descent by looking at their means, stds and also the gradient-to-data ratios or even better, the update-to-data ratios. And we saw that what people usually do is look at the evolution of these update-to-data ratios, instead of single step snapshots frozen in time, and make sure everything looks fine. In particular, we highlighted that around ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"VkV3qEXiYh"},{"type":"inlineCode","value":"1e-3","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pUdtz53yzY"},{"type":"text","value":" (","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"uvxXOF5cp9"},{"type":"text","value":"-3","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"FNptkrfJ2G"},{"type":"text","value":" on the log scale) is a good rough heuristic of what you want this ratio to be and if it’s way too high, then probably the learning rate is a little too big. Whereas, if it’s too small, then the learning rate is probably too small. So, these are the things that you might want to play with when you want to get your ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"U4ota1hEsl"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"VtqDiemjur"}],"key":"J0B6UJyExe"},{"type":"text","value":" to work very well.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"WIMjxwLYyq"}],"key":"C7zd9A4dsB"}],"key":"LTi8JNGNRz"}],"key":"QjO2krxXGx"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Now, there are certain things we did not try to achieve in this lesson. As an example, we did not try to beat the performace from our previous lessons. If we do actually try to, by using ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"BL9NRrNSpQ"},{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"qa1gfmML2H"}],"key":"fuaRkjrk0S"},{"type":"text","value":" layers (by using the learning rate finding mechanism described in the previous lesson), we would end up with results that are very very similar to the ones that we obtained before. And in that case, that would be because our performance now is not bottlenecked by the optimization, which is what ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"dclaOv0Olj"},{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"WxfzbsQZk0"}],"key":"AkJhnHkZbU"},{"type":"text","value":" is helping with. But, the performance in actually most likely bottlenecked by the context length we are choosing as our context. Currently we are taking in ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"TG1LB4hz1U"},{"type":"text","value":"3","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"m7KLtWLT9a"},{"type":"text","value":" characters in order to predict the ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"WYNOxAbjyB"},{"type":"inlineMath","value":"4th","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">4th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"XZ60XOMhof"},{"type":"text","value":" one. To go beyond that, we would need to look at more powerful architectures, like RNNs and Transformers, in order to further push the log probabilities that we’re achieving on this dataset. Also, we did not give a full explanation of all of these activations and the gradients (e.g. from the backward pass or the weights). Maybe you found those parts slightly unintuitive and maybe you’re slightly confused about: okay, if I change the gain, how come that we need a different learning rate? And the reason we didn’t go into full detail to make such questions clearer is because we’d have to actually look at the backward pass of ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"AKdcZbwYlm"},{"type":"emphasis","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"all","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"BpJVCj663g"}],"key":"R3EJslKezu"},{"type":"text","value":" these different layers and get an intuitive understanding of how that works, and so we did not go into that in this lesson. The purpose really was just to introduce you to the diagnostic tools and what they look like. But of course there’s still a lot of work remaining on the intuitive level to understand the initialization, the backward pass and how all of these interact.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"SzpfNODcpn"}],"key":"tUr44UhXHI"}],"key":"MXpjLZGpJo"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Outro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z21ULlGQU1"}],"identifier":"outro","label":"Outro","html_id":"outro","implicit":true,"key":"YVDUt9b0ke"}],"key":"mTXXZSYGly"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We certainly haven’t ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"udaIQErcLR"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"solved","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"btXHT9poyx"}],"key":"Fiay4chfer"},{"type":"text","value":" initialization, nor have we ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E9hOVqkp6z"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"solved","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tgIoSWIa6x"}],"key":"S1Ge7bM9R4"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ae6AYyTU7h"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ccsrm7xmVe"}],"key":"XrLElKULUk"},{"type":"text","value":", or anything of that sorts. These are still very much an active area of research with lots of people trying to figure out what the best way is to initialize these networks, what is the best update rule to use and so on. So none of all this is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jwV6Z1G8KB"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"solved","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cDgbrJU7wq"}],"key":"oGHmSlyUit"},{"type":"text","value":" and we don’t really have all the answers to all these cases but at least we are making progress and at least we have some tools to tell us whether or not things are on the right track, for now. So, all in all, we have made progress in this lesson and I hope you enjoyed it. See you in the next lesson!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vmRBTEL0AG"}],"key":"pq40DhSar5"}],"key":"vWZ7iGSWcx"}],"key":"mPXDvkPwcV"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"3. makemore (part 2): mlp","url":"/micrograduate/makemore2","group":"microgra∇uate"},"next":{"title":"5. makemore (part 4): becoming a backprop ninja","url":"/micrograduate/makemore4","group":"microgra∇uate"}}},"domain":"http://localhost:3000"}