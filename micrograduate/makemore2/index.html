<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>3. makemore (part 2): mlp - microgra∇uate</title><meta property="og:title" content="3. makemore (part 2): mlp - microgra∇uate"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><meta name="image" content="/build/heading-2992afcd5615ae46f403e88bcb8569f4.png"/><meta property="og:image" content="/build/heading-2992afcd5615ae46f403e88bcb8569f4.png"/><link rel="stylesheet" href="/build/_assets/app-OIHP3NGU.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="myst-skip-to-article fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article content</a></div><dialog id="myst-no-css" style="position:fixed;left:0px;top:0px;width:100vw;height:100vh;font-size:4rem;padding:1rem;color:black;background:white"><strong>Site not loading correctly?</strong><p>This may be due to an incorrect <code>BASE_URL</code> configuration. See<!-- --> <a href="https://mystmd.org/guide/deployment#deploy-base-url">the MyST Documentation</a> <!-- -->for reference.</p><script>
    (() => {
            // Test for has-styling variable set by the MyST stylesheet
            const node = document.currentScript.parentNode;
            const hasCSS = window.getComputedStyle(node).getPropertyValue("--has-styling");
            if (hasCSS === ""){
                    node.showModal();
            }

    })()
</script></dialog><div class="myst-top-nav bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="myst-top-nav-bar flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="myst-top-nav-menu-button flex items-center justify-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100 w-10 h-10"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="myst-home-link flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><div class="myst-home-link-logo mr-3 flex items-center dark:bg-white dark:rounded px-1"><img src="/build/logo-1a1c7119355e3dbc339d456e6b58f518.png" class="h-9" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5 sr-only">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R75cp:" data-state="closed" class="myst-search-bar flex items-center h-10 aspect-square sm:w-64 text-left text-gray-600 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 myst-search-bar-disabled hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="myst-search-text-placeholder hidden sm:block grow">Search</span><div aria-hidden="true" class="myst-search-shortcut items-center hidden mx-1 font-mono text-sm text-gray-600 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="myst-theme-button theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-10 h-10 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-moon-icon h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-sun-icon h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="myst-primary-sidebar fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="myst-primary-sidebar-pointer pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="myst-primary-sidebar-nav flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="myst-primary-sidebar-topnav overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="myst-primary-sidebar-toc flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="myst-toc w-full px-1 dark:text-white"><a title="microgra∇uate" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/">microgra∇uate</a><a title="1. micrograd: implementing an autograd engine" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/micrograd">1. micrograd: implementing an autograd engine</a><a title="2. makemore (part 1): implementing a bigram character-level language model" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore1">2. makemore (part 1): implementing a bigram character-level language model</a><a title="3. makemore (part 2): mlp" aria-current="page" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg myst-toc-item-exact bg-blue-300/30 active" href="/micrograduate/makemore2">3. makemore (part 2): mlp</a><a title="4. makemore (part 3): activations &amp; gradients, batchnorm" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore3">4. makemore (part 3): activations &amp; gradients, batchnorm</a><a title="5. makemore (part 4): becoming a backprop ninja" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore4">5. makemore (part 4): becoming a backprop ninja</a><a title="6. makemore (part 5): building a WaveNet" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore5">6. makemore (part 5): building a WaveNet</a><a title="7. picoGPT: implementing a tiny GPT from scratch" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/picogpt">7. picoGPT: implementing a tiny GPT from scratch</a></div></nav></div><div class="myst-primary-sidebar-footer flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="myst-made-with-myst flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="myst-fm-block mb-8 pt-9"><div class="myst-fm-block-header flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><div class="myst-fm-block-badges"><a href="https://github.com/ckaraneen/micrograduate" title="GitHub Repository: ckaraneen/micrograduate" target="_blank" rel="noopener noreferrer" class="myst-fm-github-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-github-icon inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a></div><a href="https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore2.ipynb" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="myst-fm-edit-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-edit-icon inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="myst-fm-downloads-dropdown relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="myst-fm-downloads-button relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8ucp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-downloads-icon"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="myst-fm-block-title mb-0">3. makemore (part 2): mlp</h1></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="bPtSIx8P4Q" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import sys

IN_COLAB = &quot;google.colab&quot; in sys.modules
if IN_COLAB:
    print(&quot;Cloning repo...&quot;)
    !git clone --quiet https://github.com/ckaraneen/micrograduate.git &gt; /dev/null
    %cd micrograduate
    print(&quot;Installing requirements...&quot;)
    !pip install --quiet uv
    !uv pip install --system --quiet -r requirements.txt</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="LN_YwL7eySP1qrZkvoshL" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="gGtxN08mDO" class="myst-jp-nb-block relative group/block"><h2 id="intro" class="relative group"><span class="heading-text">Intro</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#intro" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="btveSTaexH" class="myst-jp-nb-block relative group/block"><p>Time to make more out of <strong>makemore</strong>! In the last lesson, we implemented the bigram language model, both using counts and a super simple, 1-linear-layer <strong>nn</strong>. How we approach training is that we looked only at the single previous character and we predicted a distribution for the character coming next in the sequence. We did that by taking counts and normalizing them into probabilities so that each row in the count matrix sums to <!-- -->1<!-- -->. This method is great if you only have one character of previous context. The problem with that model though is that predictions are not very good. Another problem, if we are to take more context into account, is that the counts in the matrix grow exponentially as we increase the length of the context. For just <!-- -->1<!-- --> character of context we have <!-- -->27<!-- --> rows, each representing the next possible character. For <!-- -->2<!-- --> characters, the number of rows would grow to <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>27</mn><mo>⋅</mo><mn>27</mn><mo>=</mo><mn>729</mn></mrow><annotation encoding="application/x-tex">27 \cdot 27 = 729</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">27</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">27</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">729</span></span></span></span></span>. Whereas for <!-- -->3<!-- --> characters, it would explode to <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>27</mn><mo>⋅</mo><mn>27</mn><mo>⋅</mo><mn>27</mn><mo>=</mo><mn>19683</mn></mrow><annotation encoding="application/x-tex">27 \cdot 27 \cdot 27 = 19683</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">27</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">27</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">27</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">19683</span></span></span></span></span>, and so on. This solution simply doesn’t scale well and explodes. That is why we are going to move on and instead implement an <strong>mlp</strong> model to predict the next character in a sequence.</p></div><div id="AaeuZnKoG7" class="myst-jp-nb-block relative group/block"><h2 id="building-a-mlp-language-model" class="relative group"><span class="heading-text">Building a <strong>mlp</strong> language model</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#building-a-mlp-language-model" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="ODhf1ipLk8" class="myst-jp-nb-block relative group/block"><p>The modeling approach we are going to adopt follows <a target="_blank" rel="noreferrer" href="https://doi.org/10.5555/944919.944966" class="link">Bengio et al. 2003<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>, an important paper that we are going to implement. Although they implement a word-level language model, we are going to stick to our character-level language model, but follow the same approach. The authors propose associating each and every word (out of e.g. <!-- -->17000<!-- -->) with a feature vector (e.g. of <!-- -->30<!-- --> dimensions). In other words, every word is a point that is embedded into a <!-- -->30<!-- -->-dimensional space. You can think of it this way. We have <!-- -->17000<!-- --> point-vectors in a <!-- -->30<!-- -->-dimensional space. As you can imagine, that is very crowded, that’s lots of points for a very small space. Now, in the beginning, these words are initialized completely randomly: they are spread out at random. But, then we are going to tune these embeddings of these words using <strong>backprop</strong>. So during the course of training of this <strong>nn</strong>, these point-vectors are going to basically be moved around in this space. And you might imagine that, for example, words that have very similar meanings or that are indeed synonyms of each other might end up in a very similar part of the vector space, and, conversely, words with very different meanings will go somewhere else in that space. Now, their modeling approach otherwise is identical to what ours has been so far. They are using a <strong>mlp</strong> <strong>nn</strong> to predict the next word, given the previous words and to train the <strong>nn</strong> they are maximizing the log-likehood of the training data, just like we did. Here, is their example of this intuition: suppose the exact phrase <em><code>a dog was running in a</code></em> has never occured and at test time we want our model to complete the sentence by predicting the word that might follow it (e.g. <em><code>room</code></em>). Because the model has never encountered this exact phrase in the training set, it is out of distribution, as we say. Meaning, you don’t have fundamentally any reason to suspect what might come next. However, the approach we are following allows you to get around such suspicion. Maybe we haven’t seen the exact phrase, but maybe we have seen similar phrases like: <em><code>the dog was running in a</code></em> and maybe your <strong>nn</strong> has learned that <em><code>a</code></em> and <em><code>the</code></em> are frequently interchangeble with each other. So maybe our model took the embeddings for <em><code>a</code></em> and <em><code>the</code></em> and it actually put them nearby eachother in the vector space. Thus, you can transfer knowledge through such an embedding and generalize in that way. Similarly, it can do the same with other similar words such that a phrase such as <em><code>The cat is walking in the bedroom</code></em> can help us generalize to a diserable or at least valid sentence like <em><code>a dog was running in a</code></em> <strong><code>room</code></strong> by merit of the magic of feature vector similarity after training! To put it more simply, manipulating the embedding space allows us to transfer knowledge, predict and generalize to novel scenarios even when fed inputs like the sequence of words mentioned that we have not trained on. If you scroll down the paper, you will see the following diagram:</p></div><div id="eRhgkNQy9Z" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">from IPython.display import Image, display
display(Image(filename=&#x27;bengio2003nn.jpeg&#x27;))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="KdrQgPgAnxLA3D6HiJQ7X" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-image"><img src="/build/a21abcc7498c74c85d4a3cd5f51b3817.jpeg" alt="&lt;IPython.core.display.Image object&gt;"/></div></div></div><div id="Lc6JDEg7fe" class="myst-jp-nb-block relative group/block"><p>This is the <strong>nn</strong> where we are taking e.g. three previous words and we are trying to predict the fourth word in a sequence, where <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{t-3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{t-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span> are the indeces of each incoming word. Since there are <!-- -->17000<!-- --> possible words, these indeces are integers between <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>−</mo><mn>16999</mn></mrow><annotation encoding="application/x-tex">0-16999</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16999</span></span></span></span></span>. There’s also a lookup table <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span>, a matrix that has <!-- -->17000<!-- --> rows (one for each word embedding) and <!-- -->30<!-- --> columns (one for each feature vector/embedding dimension). Every index basically plucks out a row of this embedding matrix so that each index is converted to the <!-- -->30<!-- -->-dimensional embedding vector corresponding to that word. Therefore, each word index corresponds to <!-- -->30<!-- --> neuron activations exiting the first layer: <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>3</mn></mrow></msub><mo>→</mo><mi>C</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>3</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w_{t-3} \rightarrow C(w_{t-3})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub><mo>→</mo><mi>C</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w_{t-2} \rightarrow C(w_{t-2})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>→</mo><mi>C</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w_{t-1} \rightarrow C(w_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>. Thus, the first layer contains <!-- -->90<!-- --> neurons in total. Notice how the <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span> matrix is shared, which means that we are indexing the same matrix over and over. Next up is the hidden layer of this <strong>nn</strong> whose size is a hyperparameter, meaning that it is up to the choice of the designer how wide, aka how many neurons, it is going to have. For example it could have <!-- -->100<!-- --> or any other number that endows the <strong>nn</strong> with the best performance, after evaluation. This hidden layer is fully connected to the input layer of <!-- -->90<!-- --> neurons, meaning each neuron is connected to each one of this layer’s neurons. Then there’s a <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>tanh</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mop">tanh</span></span></span></span></span> non-linearity, and then there’s an output layer. And because of course we want the <strong>nn</strong> to give us the next word, the output layer has <!-- -->17000<!-- --> neurons that are also fully connected to the previous (hidden) layer’s neurons. So, there’s a lot of parameters, as there are a lot of words, so most computation happens in the output layer. Each of this layer’s <!-- -->17000<!-- --> logits is passed through a <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span></span> function, meaning they are all exponentiated and then everything is normalized to sum to <!-- -->1<!-- -->, so that we have a nice probability distribution <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>=</mo><mi>i</mi><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(w_{t}=i\ |\ context)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathnormal">co</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span> for the next word <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> in the sequence. During training of course, we have the label or target index: the index of the next word in the sequence which we use to pluck out the probability of that word from that distribution. The point of training is to maximize the probability of that word <strong>w.r.t.</strong> the <strong>nn</strong> parameters, meaning the weights and biases of the output layer, of the hidden layer and of the embedding lookup table <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></span>. All of these parameters are optimized using <strong>backprop</strong>. Ignore the green dashed arrows in the diagram, they represent a variation of the <strong>nn</strong> we are not going to explore in this lesson. So, what we  described is the setup. Now, let’s implement it!</p></div><div id="ETFPBhrgU0" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
if IN_COLAB:
    %matplotlib inline
else:
    %matplotlib ipympl</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="cOrRKrMTFtAuzWqgU13DB" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="UYBXSNv2ct" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># read in all the words
words = open(&quot;names.txt&quot;, &quot;r&quot;).read().splitlines()
words[:8]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="AhKg5PbAZjWmRswIsXuGs" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>[&#x27;emma&#x27;, &#x27;olivia&#x27;, &#x27;ava&#x27;, &#x27;isabella&#x27;, &#x27;sophia&#x27;, &#x27;charlotte&#x27;, &#x27;mia&#x27;, &#x27;amelia&#x27;]</span></code></div></div></div><div id="nLuzFoFPcl" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">len(words)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="gw5wMOeyZcC_l3QjoDxny" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>32033</span></code></div></div></div><div id="d5ozLWF0YK" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># build the vocabulary of characters and mappings to/from integers
chars = sorted(list(set(&quot;&quot;.join(words))))
ctoi = {c: i + 1 for i, c in enumerate(chars)}
ctoi[&quot;.&quot;] = 0
itoc = {i: c for c, i in ctoi.items()}
print(itoc)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="ZhDmFVsNXCDSYd8pwSg09" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>{1: &#x27;a&#x27;, 2: &#x27;b&#x27;, 3: &#x27;c&#x27;, 4: &#x27;d&#x27;, 5: &#x27;e&#x27;, 6: &#x27;f&#x27;, 7: &#x27;g&#x27;, 8: &#x27;h&#x27;, 9: &#x27;i&#x27;, 10: &#x27;j&#x27;, 11: &#x27;k&#x27;, 12: &#x27;l&#x27;, 13: &#x27;m&#x27;, 14: &#x27;n&#x27;, 15: &#x27;o&#x27;, 16: &#x27;p&#x27;, 17: &#x27;q&#x27;, 18: &#x27;r&#x27;, 19: &#x27;s&#x27;, 20: &#x27;t&#x27;, 21: &#x27;u&#x27;, 22: &#x27;v&#x27;, 23: &#x27;w&#x27;, 24: &#x27;x&#x27;, 25: &#x27;y&#x27;, 26: &#x27;z&#x27;, 0: &#x27;.&#x27;}
</span></code></pre></div></div></div></div><div id="scGWdAvH9f" class="myst-jp-nb-block relative group/block"><p>As you can see, we are reading <code>32033</code> words into a list and we are creating character-to/from-index mappings. From here, the first thing we want to do is compile the dataset for the <strong>nn</strong>:</p></div><div id="LI23rjbATX" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># context length: how many characters do we take to predict the next one?
block_size = 3


def build_dataset(words, verbose=False):
    x, y = [], []
    for w in words:
        if verbose:
            print(w)
        context = [0] * block_size
        for ch in w + &quot;.&quot;:
            ix = ctoi[ch]
            if verbose:
                print(&quot;&quot;.join(itoc[i] for i in context), &quot;---&gt;&quot;, itoc[ix])
            context = context[1:] + [ix]  # crop and append
            x.append(context)
            y.append(ix)
    x = torch.tensor(x)
    y = torch.tensor(y)
    print(f&quot;{x.shape=}, {y.shape=}&quot;)
    print(f&quot;{x.dtype=}, {y.dtype=}&quot;)
    return x, y


x, y = build_dataset(words[:5], verbose=True)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="8d3HW0n4sIuIOFYTFpGF9" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>emma
... ---&gt; e
..e ---&gt; m
.em ---&gt; m
emm ---&gt; a
mma ---&gt; .
olivia
... ---&gt; o
..o ---&gt; l
.ol ---&gt; i
oli ---&gt; v
liv ---&gt; i
ivi ---&gt; a
via ---&gt; .
ava
... ---&gt; a
..a ---&gt; v
.av ---&gt; a
ava ---&gt; .
isabella
... ---&gt; i
..i ---&gt; s
.is ---&gt; a
isa ---&gt; b
sab ---&gt; e
abe ---&gt; l
bel ---&gt; l
ell ---&gt; a
lla ---&gt; .
sophia
... ---&gt; s
..s ---&gt; o
.so ---&gt; p
sop ---&gt; h
oph ---&gt; i
phi ---&gt; a
hia ---&gt; .
x.shape=torch.Size([32, 3]), y.shape=torch.Size([32])
x.dtype=torch.int64, y.dtype=torch.int64
</span></code></pre></div></div></div></div><div id="lajPHVQePY" class="myst-jp-nb-block relative group/block"><p>We first define the <code>block_size</code> which is how many characters we need to predict the next one. In the example I just described, we used <!-- -->3<!-- --> words to predict the next one. Here, we also use a block size of <!-- -->3<!-- --> and do the same thing, but remember, instead of words we expect characters as inputs and predictions. After defining the block size, we construct <code>x</code>: a feature list of word index triplets (e.g. <code>[[ 0,  0,  5], [ 0,  5, 13], ...]</code>) that represent the context inputs, and <code>y</code>: a list of corresponding target word indeces (e.g. <code>[ 5, 13, ...]</code>). In the printout above, you can see for each word’s context character triplet, the corresponding target character. E.g. for an input of  <code>...</code> the target character is <code>e</code>, for <code>..e</code> it’s <code>m</code>, and so on! Change the <code>block_size</code> and see the print out for yourself. Notice how we are using dots as padding. After building the dataset, inputs and targets look like this:</p></div><div id="qazYGLebGh" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">x, y</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="7va8qFY39_OPECVxOyehP" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>(tensor([[ 0,  0,  5],
         [ 0,  5, 13],
         [ 5, 13, 13],
         [13, 13,  1],
         [13,  1,  0],
         [ 0,  0, 15],
         [ 0, 15, 12],
         [15, 12,  9],
         [12,  9, 22],
         [ 9, 22,  9],
         [22,  9,  1],
         [ 9,  1,  0],
         [ 0,  0,  1],
         [ 0,  1, 22],
         [ 1, 22,  1],
         [22,  1,  0],
         [ 0,  0,  9],
         [ 0,  9, 19],
         [ 9, 19,  1],
         [19,  1,  2],
         [ 1,  2,  5],
         [ 2,  5, 12],
         [ 5, 12, 12],
         [12, 12,  1],
         [12,  1,  0],
         [ 0,  0, 19],
         [ 0, 19, 15],
         [19, 15, 16],
         [15, 16,  8],
         [16,  8,  9],
         [ 8,  9,  1],
         [ 9,  1,  0]]),
 tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,
          1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))</span></code></div></div></div><div id="jafkSctMCJ" class="myst-jp-nb-block relative group/block"><p>Given these, let’s write a <strong>nn</strong> that takes the <code>x</code> and predicts <code>y</code>. First, let’s build the embedding lookup table <code>C</code>. In the paper, they have <!-- -->17000<!-- --> words and embed them in spaces as low-dimensional as <!-- -->30<!-- -->, so they cram <!-- -->17000<!-- --> into a <!-- -->30<!-- -->-dimensional space. In our case, we have only <!-- -->27<!-- --> possible characters, so let’s cram them into as small as -let’s say- a <!-- -->2<!-- -->-dimensional space:</p></div><div id="kq2y94hw6R" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">SEED = 2147483647
g = torch.Generator().manual_seed(SEED)
C = torch.randn((27, 2), generator=g)
C</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="8wvEOtdci-XePangG9v5Q" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[ 1.5674, -0.2373],
        [-0.0274, -1.1008],
        [ 0.2859, -0.0296],
        [-1.5471,  0.6049],
        [ 0.0791,  0.9046],
        [-0.4713,  0.7868],
        [-0.3284, -0.4330],
        [ 1.3729,  2.9334],
        [ 1.5618, -1.6261],
        [ 0.6772, -0.8404],
        [ 0.9849, -0.1484],
        [-1.4795,  0.4483],
        [-0.0707,  2.4968],
        [ 2.4448, -0.6701],
        [-1.2199,  0.3031],
        [-1.0725,  0.7276],
        [ 0.0511,  1.3095],
        [-0.8022, -0.8504],
        [-1.8068,  1.2523],
        [ 0.1476, -1.0006],
        [-0.5030, -1.0660],
        [ 0.8480,  2.0275],
        [-0.1158, -1.2078],
        [-1.0406, -1.5367],
        [-0.5132,  0.2961],
        [-1.4904, -0.2838],
        [ 0.2569,  0.2130]])</span></code></div></div></div><div id="bU1cVi989R" class="myst-jp-nb-block relative group/block"><p>Each of our <!-- -->27<!-- --> characters will have a <!-- -->2<!-- -->-dimensional embedding. Therefore, our table <code>C</code> will have <!-- -->27<!-- --> rows (one for each character) and <!-- -->2<!-- --> columns (number of dimensions per character embedding). Before we embed all the integers inside input <code>x</code> using this lookup table <code>C</code>, let’s first embed a single, individual character, let’s say, <!-- -->5<!-- -->, so we get a sense of how this works. One way to do it is to simply index the table using the character index:</p></div><div id="KWyyW5bXNN" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">C[5]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="nQGVf2xIkyLo1qNcT8Pwc" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([-0.4713,  0.7868])</span></code></div></div></div><div id="kGdMwMvHSh" class="myst-jp-nb-block relative group/block"><p>Another way, as we saw in the previous lesson, is to one-hot encode the character:</p></div><div id="H5eCMIiyBz" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ohv = F.one_hot(torch.tensor(5), num_classes=27)
print(ohv)
print(ohv.shape)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="hFLS6LBceEGMarsRuPJWo" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0])
torch.Size([27])
</span></code></pre></div></div></div></div><div id="fk7MHm3eF4" class="myst-jp-nb-block relative group/block"><p>With this way we get a one-hot encoded representation whose <!-- -->5<!-- -->-th element is <!-- -->1<!-- --> and all the rest are <!-- -->0<!-- -->. Now, notice how, just as we previously alluded to in the previous lesson, if we take this one-hot vector and we multiply it by <code>C</code>:</p></div><div id="VP74CPbJhs" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ohv_matmul_C = ohv.float() @ C
ohv_matmul_C</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="X1pTzQ9ANNYLwOJDbK9QB" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([-0.4713,  0.7868])</span></code></div></div></div><div id="Qn0Eq4osGe" class="myst-jp-nb-block relative group/block"><p>as you can see, they are identical:</p></div><div id="SOUCHcVwNy" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">assert C[5].equal(ohv_matmul_C)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="fYXt0C9UdP14V5-95o9pZ" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="uoeKDHvLU6" class="myst-jp-nb-block relative group/block"><p>Multiplying a one-hot encoded vector and an appropriate matrix acts like indexing that matrix with the index of the vector that points to element <!-- -->1<!-- -->! And so, we actually arrive at the same result. This is interesting since it points out how the first layer of this <strong>nn</strong> (see diagram above) can be thought of as a set of neurons whose weight matrix is <code>C</code>, when the inputs (the integer character indeces) are one-hot encoded. Note aside, in order to embed a character, we are just going to simply index the table as it’s much faster:</p></div><div id="LRZ3GcqBFN" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">C[5]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="mmxq93uqsw_3dkKW0soO7" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([-0.4713,  0.7868])</span></code></div></div></div><div id="qwumtg2yGL" class="myst-jp-nb-block relative group/block"><p>which is easy for a single character. But what if we want to index more simultaneously? That’s also easy:</p></div><div id="JOxxlZskcQ" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">C[[5, 6, 7]]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="eCQMv22CzFrR-NNPo9XAf" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[-0.4713,  0.7868],
        [-0.3284, -0.4330],
        [ 1.3729,  2.9334]])</span></code></div></div></div><div id="x6RTgc4PHq" class="myst-jp-nb-block relative group/block"><p>Cool! You can actually index a PyTorch tensor with a list or another tensor. Therefore, to easily get all character embeddings, we can simply do:</p></div><div id="twHdRLijpm" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">emb = C[x]
print(emb)
print(emb.shape)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="xW2SvnClekUIVu7f9-6ac" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>tensor([[[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.4713,  0.7868]],

        [[ 1.5674, -0.2373],
         [-0.4713,  0.7868],
         [ 2.4448, -0.6701]],

        [[-0.4713,  0.7868],
         [ 2.4448, -0.6701],
         [ 2.4448, -0.6701]],

        [[ 2.4448, -0.6701],
         [ 2.4448, -0.6701],
         [-0.0274, -1.1008]],

        [[ 2.4448, -0.6701],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-1.0725,  0.7276]],

        [[ 1.5674, -0.2373],
         [-1.0725,  0.7276],
         [-0.0707,  2.4968]],

        [[-1.0725,  0.7276],
         [-0.0707,  2.4968],
         [ 0.6772, -0.8404]],

        [[-0.0707,  2.4968],
         [ 0.6772, -0.8404],
         [-0.1158, -1.2078]],

        [[ 0.6772, -0.8404],
         [-0.1158, -1.2078],
         [ 0.6772, -0.8404]],

        [[-0.1158, -1.2078],
         [ 0.6772, -0.8404],
         [-0.0274, -1.1008]],

        [[ 0.6772, -0.8404],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [-0.0274, -1.1008]],

        [[ 1.5674, -0.2373],
         [-0.0274, -1.1008],
         [-0.1158, -1.2078]],

        [[-0.0274, -1.1008],
         [-0.1158, -1.2078],
         [-0.0274, -1.1008]],

        [[-0.1158, -1.2078],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.6772, -0.8404]],

        [[ 1.5674, -0.2373],
         [ 0.6772, -0.8404],
         [ 0.1476, -1.0006]],

        [[ 0.6772, -0.8404],
         [ 0.1476, -1.0006],
         [-0.0274, -1.1008]],

        [[ 0.1476, -1.0006],
         [-0.0274, -1.1008],
         [ 0.2859, -0.0296]],

        [[-0.0274, -1.1008],
         [ 0.2859, -0.0296],
         [-0.4713,  0.7868]],

        [[ 0.2859, -0.0296],
         [-0.4713,  0.7868],
         [-0.0707,  2.4968]],

        [[-0.4713,  0.7868],
         [-0.0707,  2.4968],
         [-0.0707,  2.4968]],

        [[-0.0707,  2.4968],
         [-0.0707,  2.4968],
         [-0.0274, -1.1008]],

        [[-0.0707,  2.4968],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373]],

        [[ 1.5674, -0.2373],
         [ 1.5674, -0.2373],
         [ 0.1476, -1.0006]],

        [[ 1.5674, -0.2373],
         [ 0.1476, -1.0006],
         [-1.0725,  0.7276]],

        [[ 0.1476, -1.0006],
         [-1.0725,  0.7276],
         [ 0.0511,  1.3095]],

        [[-1.0725,  0.7276],
         [ 0.0511,  1.3095],
         [ 1.5618, -1.6261]],

        [[ 0.0511,  1.3095],
         [ 1.5618, -1.6261],
         [ 0.6772, -0.8404]],

        [[ 1.5618, -1.6261],
         [ 0.6772, -0.8404],
         [-0.0274, -1.1008]],

        [[ 0.6772, -0.8404],
         [-0.0274, -1.1008],
         [ 1.5674, -0.2373]]])
torch.Size([32, 3, 2])
</span></code></pre></div></div></div></div><div id="aJrCkwskFE" class="myst-jp-nb-block relative group/block"><p>Notice the shape: <code>[&lt;number of character input sets&gt;, &lt;input size&gt;, &lt;number of character embedding dimensions&gt;]</code>. Indexing as following, we can assert that both ways of representation are valid:</p></div><div id="LegP6QLpFg" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">assert emb[13, 2].equal(C[x[13, 2]])</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="x22gPDmiq5AlcKB8X5pD0" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="ouRcIN80pn" class="myst-jp-nb-block relative group/block"><p>Long story short, PyTorch indexing is awesome and tensors such as embedding tables can be indexed by other tensors, e.g. inputs. One last thing, as far as the first layer is concerned. Since each embedding of our <code>3</code> inputs has <code>2</code> dimensions, the output dimension of our first layer is basically <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>⋅</mo><mn>2</mn><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">3 \cdot 2 = 6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">6</span></span></span></span></span>. Usually, a <strong>nn</strong> layer is described by a pair of input and output dimensions. The input dimension of our first, embeddings layer is <!-- -->32<!-- --> (<code>&lt;number of character inputs&gt;</code>). To get the output dimension we have to concatenate the following <code>&lt;inputs size&gt;</code> and <code>&lt;number of character embedding dimensions&gt;</code> tensor dimensions into one dimension:</p></div><div id="wR7Ud9N0Dw" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">last_dims = emb.shape[1:]  # get dimensions after the first one
last_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them
emb_proper = emb.view(-1, last_dims_product)
emb_proper.shape  # tada!</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="HR8YbC5YG7utPtJcltWdz" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([32, 6])</span></code></div></div></div><div id="RFRylnRvrR" class="myst-jp-nb-block relative group/block"><p>We have prepared the dimensionality of the first layer. Now, let’s implement the hidden, second layer:</p></div><div id="drjhghWXeE" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">l0out = emb_proper.shape[-1]
l1in = l0out
l1out = 100  # neurons of hidden layer
w1 = torch.randn(l1in, l1out, generator=g)
b1 = torch.randn(l1out, generator=g)
print(w1.shape)
print(b1.shape)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="4lDy1CbRKXk0z1yZzfTsw" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>torch.Size([6, 100])
torch.Size([100])
</span></code></pre></div></div></div></div><div id="DrvSOLvtG5" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">h = torch.tanh(emb_proper @ w1 + b1)
h</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="HuSazjmXOgO6_GwZjrsql" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[ 0.2797,  0.9997,  0.7675,  ...,  0.9929,  0.9992,  0.9981],
        [-0.9960,  1.0000, -0.8694,  ..., -0.5159, -1.0000, -0.0069],
        [-0.9968,  1.0000,  0.9878,  ...,  0.4976, -0.9297, -0.8616],
        ...,
        [-0.9043,  1.0000,  0.9868,  ..., -0.7859, -0.4819,  0.9981],
        [-0.9048,  1.0000,  0.9553,  ...,  0.9866,  1.0000,  0.9907],
        [-0.9868,  1.0000,  0.5264,  ...,  0.9843,  0.0223, -0.1655]])</span></code></div></div></div><div id="HQtDBFabD7" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">h.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="f2JVa5EdVWJshintv_WzL" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([32, 100])</span></code></div></div></div><div id="hiqXvo8nsA" class="myst-jp-nb-block relative group/block"><p>Done! And now, to create the output layer:</p></div><div id="SRHd0LFJ8x" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">l2in = l1out
l2out = 27  # number of characters
w2 = torch.randn(l2in, l2out, generator=g)
b2 = torch.randn(l2out, generator=g)
print(w2.shape)
print(b2.shape)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="ql9vlG_s560N3pVs4lGss" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>torch.Size([100, 27])
torch.Size([27])
</span></code></pre></div></div></div></div><div id="JEzKbuU68S" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">logits = h @ w2 + b2
logits.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="h_t6TEGRCsV_EWSgQaodI" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([32, 27])</span></code></div></div></div><div id="iO8btmFnYG" class="myst-jp-nb-block relative group/block"><p>Exactly as we saw in the previous lesson, we want to take these logits and we want to first exponentiate them to get our fake counts. Then, we want to normalize them to get the probabilities of how likely it is for each character to come next:</p></div><div id="xsXPa9yCqa" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">counts = logits.exp()
prob = counts / counts.sum(1, keepdims=True)
prob.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="NKtLyPh22gSeWOMNauhRr" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([32, 27])</span></code></div></div></div><div id="uB2MEHQ7OY" class="myst-jp-nb-block relative group/block"><p>Remember, we also have the the target values <code>y</code>, the actual characters that come next that we would like our <strong>nn</strong> to be able to predict:</p></div><div id="bGjLZZ0ami" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">y</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="sEorjMtEYBngjmd82hD7s" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,
         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])</span></code></div></div></div><div id="Wm5Q9BxB6p" class="myst-jp-nb-block relative group/block"><p>So, what we would like to do now is index into the rows of <code>prob</code> and for each row to pluck out the probability given to the correct character:</p></div><div id="hVCAydHj04" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">prob[range(len(y)), y]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="AUfXOmEasFi-6EqtxMwQK" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([9.9994e-12, 1.9647e-08, 4.0322e-07, 3.0845e-09, 4.6517e-11, 7.4238e-12,
        2.0297e-09, 9.9179e-01, 1.7138e-02, 3.2410e-03, 2.8552e-06, 1.0565e-06,
        2.6391e-09, 4.1804e-06, 3.5843e-08, 7.7737e-07, 3.5022e-02, 2.7425e-10,
        1.7086e-08, 6.3572e-02, 1.1315e-08, 1.6961e-09, 2.1885e-11, 1.5201e-10,
        1.0528e-03, 3.6704e-08, 9.5847e-02, 3.1954e-12, 8.5695e-17, 2.5576e-03,
        9.1782e-12, 1.0565e-06])</span></code></div></div></div><div id="MeHQywhQ2X" class="myst-jp-nb-block relative group/block"><p>This gives the current probabilities for these specific correct, target characters that come next after each character sequence, given the current <strong>nn</strong> configuration (weights and biases). Currently these probabilities are pretty bad and most characters are pretty unlikely to occur next. Of course, we haven’t trained the <strong>nn</strong> yet. So, we want to train it so that each probability approximates <code>1</code>. As we saw previously, to do so, we have to define the <strong>loss</strong> and then minimize it:</p></div><div id="GG0AYr5QuG" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">loss = -prob[range(len(y)), y].log().mean()
loss</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="7wtdiT-9OIiMP8SA6tPOG" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(16.0342)</span></code></div></div></div><div id="E7GEdfU36u" class="myst-jp-nb-block relative group/block"><p>Pretty big loss! Haha. Now we will minimize it so our <strong>nn</strong> able to predict the next character in each sequence correctly. To do so, we have to optimize the parameters. Let’s define a function that defines them and collects them all into a list just so we have easy access:</p></div><div id="x0aOSjL7zz" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import random

# context length: how many characters do we take to predict the next one?
block_size = 3


# build the dataset
def build_dataset(words):
    X, Y = [], []
    for w in words:
        # print(w)
        context = [0] * block_size
        for ch in w + &quot;.&quot;:
            ix = ctoi[ch]
            X.append(context)
            Y.append(ix)
            # print(&#x27;&#x27;.join(itos[i] for i in context), &#x27;---&gt;&#x27;, itos[ix])
            context = context[1:] + [ix]  # crop and append

    X = torch.tensor(X)
    Y = torch.tensor(Y)
    print(X.shape, Y.shape)
    return X, Y


random.seed(42)
random.shuffle(words)
n1 = int(0.8 * len(words))
n2 = int(0.9 * len(words))

Xtr, Ytr = build_dataset(words[:n1])
Xdev, Ydev = build_dataset(words[n1:n2])
Xte, Yte = build_dataset(words[n2:])


def define_nn(l1out=100, embsize=2):
    global C, w1, b1, w2, b2
    g = torch.Generator().manual_seed(SEED)
    C = torch.randn((27, embsize), generator=g)
    l1in = embsize * block_size
    # l1out: neurons of hidden layer
    w1 = torch.randn(l1in, l1out, generator=g)
    b1 = torch.randn(l1out, generator=g)
    l2in = l1out
    l2out = 27  # neurons of output layer, number of characters
    w2 = torch.randn(l2in, l2out, generator=g)
    b2 = torch.randn(l2out, generator=g)
    parameters = [C, w1, b1, w2, b2]
    return parameters


parameters = define_nn()
sum(p.nelement() for p in parameters)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="bennTUAXbm82fKJ2LiCZT" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>3481</span></code></div></div></div><div id="BRdKoI2ouc" class="myst-jp-nb-block relative group/block"><p>To recap the forward pass:</p></div><div id="RNZoPdHcLE" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">emb = C[x]  # [32, 3, 2]
last_dims = emb.shape[1:]  # get dimensions after the first one
last_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them
emb_proper = emb.view(-1, last_dims_product)  # [32, 6]
h = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]
logits = h @ w2 + b2  # [32, 27]
counts = logits.exp()
prob = counts / counts.sum(1, keepdims=True)
loss = -prob[range(len(y)), y].log().mean()
loss</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="52Sl6R8je95adS719PPWI" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(16.0342)</span></code></div></div></div><div id="U5niBFPaSL" class="myst-jp-nb-block relative group/block"><p>A better and more efficient way to calculate the <strong>loss</strong> from logits and targets is through the <a target="_blank" rel="noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html" class="link">cross entropy loss function<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>:</p></div><div id="WWSfaFjfz6" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">F.cross_entropy(logits, y)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="VaD0XuY2FCHZWCZOvEbHG" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(16.0342)</span></code></div></div></div><div id="GzdY2sJuhd" class="myst-jp-nb-block relative group/block"><p>Let’s tidy up the forward pass:</p></div><div id="mg8GfbNqmF" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def forward_pass(x, y):
    emb = C[x]  # [32, 3, 2]
    last_dims = emb.shape[1:]  # get dimensions after the first one
    last_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them
    emb_proper = emb.view(-1, last_dims_product)  # [32, 6]
    h = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]
    logits = h @ w2 + b2  # [32, 27]
    loss = F.cross_entropy(logits, y)
    return loss</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="UekCWVq_qgaFlGbuPG2Nj" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="PEMzM9JzdT" class="myst-jp-nb-block relative group/block"><h2 id="training-the-model" class="relative group"><span class="heading-text">Training the model</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#training-the-model" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="W0FREMMmtM" class="myst-jp-nb-block relative group/block"><p>And now, let’s train our <strong>nn</strong>:</p></div><div id="FH2ida8OOh" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def train(x, y, epochs=10):
    parameters = define_nn()
    for p in parameters:
        p.requires_grad = True
    for _ in range(epochs):
        loss = forward_pass(x, y)
        print(loss.item())
        for p in parameters:
            p.grad = None
        loss.backward()
        for p in parameters:
            p.data += -0.1 * p.grad</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="NLK9ZxQMl7KGnTyIvz_zT" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="tvXprFRBif" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x, y)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="-1jo6gUeE7K9BFTKMU9yx" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>16.034189224243164
11.740133285522461
9.195296287536621
7.302927017211914
5.805147647857666
4.850736618041992
4.184849739074707
3.644319534301758
3.207334518432617
2.843700885772705
</span></code></pre></div></div></div></div><div id="KNGS3e1rp1" class="myst-jp-nb-block relative group/block"><p>The <strong>loss</strong> keeps decreasing, which means that the training process is working! Now, since we are only training using a dataset of <!-- -->5<!-- --> words, and since our parameters are many more than the samples we are training on, our <strong>nn</strong> is probably overfitting. What we have to do now, is train on the whole dataset.</p></div><div id="LJK7nCWT5k" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">x_all, y_all = build_dataset(words)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="Zo4YYQNFXS22HZvgVgfMH" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>torch.Size([228146, 3]) torch.Size([228146])
</span></code></pre></div></div></div></div><div id="ICPEn1m4XJ" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x_all, y_all)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="mcY3SA64SeQtHkcejs5na" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>19.505229949951172
17.084487915039062
15.776533126831055
14.83334732055664
14.002612113952637
13.253267288208008
12.579923629760742
11.983108520507812
11.470502853393555
11.05186653137207
</span></code></pre></div></div></div></div><div id="PphZRw1Tjo" class="myst-jp-nb-block relative group/block"><p>Same, the loss for all input samples also keeps decreasing. But, you’ll notice that training takes longer now. This is happening because we are doing a lot of work, forward and backward passing on <!-- -->228146<!-- --> examples. That’s way too much work! In practice, what people usually do in such cases is they train on minibatches of the whole dataset. So, what we want to do, is we want to randomly select some portion of the dataset, and that’s a minibatch! And then, only forward, backward and update on that minibatch, likewise iterate and train on those minibatches. A simple way to implement minibatching is to set a batch size, e.g.:</p></div><div id="FcTCZBF3YY" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">batchsize = 32</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="r7HqiA56Qif3gK9aD1sSv" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="QaDnOsbXjK" class="myst-jp-nb-block relative group/block"><p>and then to randomly select <code>batchsize</code> number of indeces referencing the subset of input data to be used for minibatch training. To get the indeces you can do something like this:</p></div><div id="V5elJDmAjP" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">batchix = torch.randint(0, x_all.shape[0], (batchsize,))
print(batchix)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="FQXxp4r1yTOo4eAZttgQz" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>tensor([ 74679, 122216,  57092, 133769, 226181,  38045, 126099,  23446, 218663,
         17662, 225764, 199486, 185049,  64041, 217855, 198821, 192633,  84825,
         44722,  46171, 182390,  99196, 102624,    409, 168159, 182770, 142590,
        173184,  86521,   1596, 158516, 206175])
</span></code></pre></div></div></div></div><div id="tW6UPgg6R2" class="myst-jp-nb-block relative group/block"><p>Then, to actually get a minibatch per epoch, just create a new, random set of indeces and index the samples and targets from the dataset before each forward pass. Like this:</p></div><div id="D3aBbGeoYu" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def train(x, y, lr=0.1, epochs=10, print_all_losses=True):
    parameters = define_nn()
    for p in parameters:
        p.requires_grad = True
    for _ in range(epochs):
        batchix = torch.randint(0, x.shape[0], (batchsize,))
        bx, by = x[batchix], y[batchix]
        loss = forward_pass(bx, by)
        if print_all_losses:
            print(loss.item())
        for p in parameters:
            p.grad = None
        loss.backward()
        for p in parameters:
            p.data += -lr * p.grad
    if not print_all_losses:
        print(loss.item())
    return loss.item()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="ULNAEPPF3XzvddUmibdRM" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="L9pSETl5IX" class="myst-jp-nb-block relative group/block"><p>Now, if we train using minibatches...</p></div><div id="qqx3PtIUbo" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x_all, y_all)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="ZZUH2CeV0MtRZwt65zsn8" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>17.94289779663086
15.889695167541504
15.060649871826172
13.832050323486328
16.023155212402344
14.010979652404785
16.336170196533203
13.788375854492188
11.292967796325684
13.045702934265137
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>13.045702934265137</span></code></div></div></div><div id="CFbUBJ7qV7" class="myst-jp-nb-block relative group/block"><p>training is much much faster, almost instant! However, since we are dealing with minibatches, the quality of our gradient is lower, so the direction is not as reliable. It’s not the actual exact gradient direction, but the gradient direction is good enough even though it’s being estimated for only <code>batchsize</code> (e.g. <code>32</code>) examples. In general, it is better to have an approximate gradient and just make more steps than it is to compute the exact gradient and take fewer steps. And that is why in practice, minibatching works quite well.</p></div><div id="DiZS8oJVFU" class="myst-jp-nb-block relative group/block"><h2 id="finding-a-good-learning-rate" class="relative group"><span class="heading-text">Finding a good learning rate</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#finding-a-good-learning-rate" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="tq0dkgTUPt" class="myst-jp-nb-block relative group/block"><p>Now, one issue that has popped up as you may have noticed, is that during minibatch training the <strong>loss</strong> seems to fluctuate. For some epochs it decreases, but then it increases again, and vice versa. That question that arises from this observation is this: are we stepping too slow or too fast? Meaning, are we updating the parameters by a fraction of their gradients that is too small or too large? Such magnitude is determined by the step size, aka the learning rate. Therefore, the overarching question is: how do you determine this learning rate? How do we gain confidence that we are stepping with the right speed? Let’s see one way to determine the learning rate. We basically want to find a reasonable search range, if you will. What people usually do is they pick different learning rate values until they find a satisfactory one. Let’s try to find one that is better. We see for example if it is very small:</p></div><div id="l9lgrdQOXD" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x_all, y_all, lr=0.0001, epochs=100)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="bGrpeSeWIW9_MYzgFATlR" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>21.48345375061035
17.693952560424805
20.993595123291016
19.23766326904297
17.807458877563477
19.426368713378906
20.208740234375
21.426673889160156
20.107091903686523
16.44301986694336
16.98691749572754
16.693296432495117
16.654979705810547
18.207632064819336
20.281587600708008
19.277870178222656
19.782976150512695
20.056819915771484
19.198200225830078
15.863028526306152
18.550344467163086
19.435653686523438
19.682138442993164
17.305164337158203
21.181236267089844
19.23027992248535
18.67540168762207
18.95297622680664
21.35270881652832
21.46781349182129
21.018564224243164
20.318994522094727
21.243608474731445
19.62767791748047
19.476289749145508
17.74656867980957
20.23328399658203
20.085819244384766
16.801542282104492
18.122915267944336
19.09043312072754
19.84799575805664
20.199235916137695
16.658361434936523
19.510778427124023
19.398319244384766
18.517004013061523
19.53419303894043
22.490541458129883
20.45920753479004
17.721420288085938
18.58787727355957
20.76034927368164
20.696556091308594
18.54053497314453
19.546337127685547
15.577354431152344
18.100522994995117
15.600821495056152
21.15610122680664
20.79819107055664
18.512712478637695
17.394367218017578
15.756057739257812
21.389039993286133
16.85922622680664
13.484357833862305
19.010683059692383
18.83637046813965
19.841796875
18.28095054626465
20.777664184570312
19.818172454833984
18.778358459472656
20.82563591003418
19.217248916625977
18.208587646484375
19.463356018066406
16.181228637695312
16.927345275878906
18.849687576293945
19.017803192138672
18.24212074279785
20.15293312072754
19.38414764404297
19.442598342895508
22.70920181274414
19.071269989013672
17.25360679626465
16.035856246948242
19.327434539794922
20.848506927490234
19.198562622070312
18.62538719177246
20.031288146972656
22.616220474243164
18.733247756958008
20.26487159729004
18.593149185180664
24.60611343383789
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>24.60611343383789</span></code></div></div></div><div id="BdBc7qdd1x" class="myst-jp-nb-block relative group/block"><p>The loss barely decreases. So this value is too low. Let’s try something bigger, e.g.</p></div><div id="IJWJ6gYaqz" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x_all, y_all, lr=0.001, epochs=100)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="YwanthjGC6gI_9N-gDBWb" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>20.843355178833008
21.689167022705078
19.067970275878906
17.899921417236328
20.831283569335938
21.015792846679688
21.317489624023438
18.584075927734375
19.30166244506836
17.16819190979004
20.15867805480957
18.009313583374023
22.017822265625
18.148967742919922
17.53749656677246
19.633005142211914
17.75607681274414
17.64638900756836
18.511669158935547
20.743165969848633
18.751705169677734
18.893756866455078
19.241785049438477
19.387001037597656
18.413681030273438
20.803842544555664
18.513309478759766
20.150976181030273
19.927082061767578
20.02385711669922
19.09459686279297
16.731922149658203
19.16921615600586
19.426868438720703
17.71548843383789
17.51811408996582
18.333765029907227
20.96685028076172
19.99691390991211
19.345508575439453
19.923913955688477
16.887836456298828
17.372751235961914
18.805681228637695
18.897857666015625
16.86487579345703
17.781234741210938
20.2587833404541
17.451217651367188
18.460481643676758
17.9292049407959
20.8989315032959
20.129817962646484
17.018564224243164
19.071075439453125
15.609376907348633
18.350452423095703
14.199233055114746
18.659013748168945
17.954235076904297
17.826528549194336
18.58924102783203
17.669662475585938
16.46000862121582
15.66697883605957
17.6021785736084
17.65107536315918
16.883989334106445
14.59417724609375
16.17646026611328
18.381986618041992
19.10284423828125
15.856918334960938
18.458749771118164
18.598033905029297
17.683555603027344
17.749269485473633
17.12112808227539
20.2098445892334
18.316301345825195
16.487417221069336
14.472514152526855
16.50566864013672
19.501144409179688
18.444271087646484
17.818748474121094
12.876835823059082
17.16472816467285
15.761727333068848
18.426593780517578
17.760990142822266
18.603355407714844
16.690837860107422
16.553945541381836
15.75294303894043
17.358163833618164
16.67896842956543
17.08140754699707
18.213592529296875
17.534658432006836
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>17.534658432006836</span></code></div></div></div><div id="Mpj9z4qqNv" class="myst-jp-nb-block relative group/block"><p>This is ok, but still not good enough. The <strong>loss</strong> value decreases but not steadily and fluctuates a lot. For an even bigger value:</p></div><div id="LEHMHhHROR" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x_all, y_all, lr=1, epochs=100)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="nSJFHxxNF210vvmpktqjo" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>18.0602970123291
15.639816284179688
14.974983215332031
16.521020889282227
10.994373321533203
19.934722900390625
13.230535507202148
10.555554389953613
11.227700233459473
8.685490608215332
11.373151779174805
9.253009796142578
8.64297866821289
9.108452796936035
7.178742408752441
9.831550598144531
5.762266635894775
9.50097370147705
10.957695960998535
12.328742980957031
8.328937530517578
8.131488800048828
11.364215850830078
9.899446487426758
7.836635589599609
8.942032814025879
11.561779022216797
9.97337532043457
8.2869291305542
9.61953067779541
9.630435943603516
14.119514465332031
12.620526313781738
8.802383422851562
8.957738876342773
11.694437980651855
16.162137985229492
10.493476867675781
7.7210798263549805
10.860843658447266
8.748751640319824
13.449786186218262
10.955209732055664
8.923118591308594
6.181601047515869
8.725625991821289
6.119848251342773
11.221086502075195
8.663549423217773
9.03221607208252
8.159632682800293
11.553065299987793
7.1041059494018555
6.436527729034424
9.19931697845459
6.504988670349121
8.564536094665527
6.59806489944458
8.718829154968262
7.369975566864014
11.306722640991211
10.493293762207031
7.680598735809326
8.20093059539795
7.427743911743164
7.3400983810424805
8.856118202209473
7.980756759643555
11.46378231048584
8.093060493469238
9.521681785583496
6.227016925811768
8.569214820861816
8.454265594482422
7.388335227966309
6.649340629577637
7.111802101135254
7.661591053009033
12.89154052734375
8.51455020904541
5.992252349853516
6.762502193450928
6.146595478057861
8.050479888916016
8.089849472045898
7.87835168838501
7.628716945648193
7.732893943786621
6.767331600189209
8.324596405029297
8.824007987976074
8.258061408996582
7.636016368865967
6.856623649597168
6.543000221252441
7.319474697113037
5.69791841506958
5.777251243591309
6.574363708496094
5.4569573402404785
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>5.4569573402404785</span></code></div></div></div><div id="NgbnsKwkis" class="myst-jp-nb-block relative group/block"><p>Better! How about:</p></div><div id="KdvpuJpiN7" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">train(x_all, y_all, lr=10, epochs=100)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="fVYUHzUxB50b5L-yUCfk_" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>19.74797821044922
38.09200668334961
46.314208984375
40.89773941040039
73.4378433227539
58.34128952026367
51.356876373291016
55.045501708984375
54.94329833984375
56.30958557128906
70.37184143066406
56.63667678833008
56.90707778930664
45.661888122558594
50.97232437133789
55.65245819091797
57.46098327636719
78.44556427001953
70.66008758544922
51.70524978637695
48.533164978027344
80.0494613647461
90.79659271240234
61.892642974853516
52.87863540649414
41.49900436401367
63.885189056396484
69.60615539550781
60.386714935302734
76.09366607666016
40.18576431274414
53.72964096069336
48.1932373046875
46.865108489990234
48.253753662109375
53.61216735839844
77.9145278930664
75.54542541503906
65.61190795898438
78.13446044921875
83.6716537475586
79.6883773803711
59.334083557128906
74.78559875488281
50.28561782836914
53.59624099731445
35.096195220947266
50.16322326660156
73.99742889404297
86.66049194335938
70.05807495117188
78.18916320800781
48.637943267822266
77.84318542480469
56.17559051513672
44.09672164916992
70.90714263916016
79.0201187133789
67.89301300048828
65.17256927490234
68.24624633789062
63.97649383544922
90.05917358398438
91.45114135742188
60.47791290283203
70.57051086425781
57.64970397949219
44.6708984375
54.10292053222656
60.48087692260742
59.21522903442383
51.96377944946289
53.79441452026367
63.579402923583984
65.1745376586914
54.898189544677734
50.91022872924805
55.830299377441406
47.503177642822266
56.56501770019531
46.5484504699707
43.91749954223633
50.70798110961914
48.224388122558594
69.06616973876953
62.38393020629883
53.78395080566406
61.84634780883789
55.61307907104492
48.13108825683594
55.1087532043457
62.52896499633789
52.36894226074219
52.819580078125
73.38019561767578
87.60235595703125
78.37958526611328
61.38961410522461
65.26103973388672
70.43557739257812
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>70.43557739257812</span></code></div></div></div><div id="c4MViynYrZ" class="myst-jp-nb-block relative group/block"><p>Haha, no thanks. So, we know that a satisfactory learning rate lies between <code>0.001</code> and <code>1</code>. To find it, we can lay these numbers out, exponentially separated:</p></div><div id="Q0iG6YtGt6" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">step = 1000
lre = torch.linspace(-3, 0, step)
lrs = 10**lre
lrs</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="WqpoDkYUiQjPVlB52c_Cm" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,
        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,
        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,
        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,
        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,
        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,
        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,
        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,
        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,
        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,
        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,
        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,
        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,
        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,
        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,
        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,
        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,
        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,
        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,
        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,
        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,
        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,
        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,
        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,
        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,
        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,
        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,
        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,
        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,
        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,
        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,
        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,
        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,
        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,
        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,
        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,
        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,
        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,
        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,
        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,
        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,
        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,
        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,
        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,
        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,
        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,
        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,
        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,
        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,
        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,
        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,
        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,
        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,
        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,
        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,
        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,
        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,
        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,
        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,
        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,
        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,
        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,
        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,
        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,
        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,
        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,
        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,
        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,
        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,
        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,
        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,
        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,
        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,
        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,
        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,
        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,
        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,
        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,
        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,
        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,
        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,
        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,
        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,
        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,
        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,
        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,
        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,
        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,
        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,
        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,
        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,
        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,
        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,
        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,
        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,
        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,
        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,
        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,
        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,
        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,
        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,
        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,
        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,
        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,
        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,
        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,
        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,
        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,
        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,
        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,
        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,
        1.0000])</span></code></div></div></div><div id="hwt9HYoya1" class="myst-jp-nb-block relative group/block"><p>then, we plot the loss for each learning rate:</p></div><div id="STWFk5VRKm" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">parameters = define_nn()
lrei = []
lossi = []
for p in parameters:
    p.requires_grad = True
for i in range(1000):
    batchix = torch.randint(0, x_all.shape[0], (batchsize,))
    bx, by = x_all[batchix], y_all[batchix]
    loss = forward_pass(bx, by)
    # print(loss.item())
    for p in parameters:
        p.grad = None
    loss.backward()
    lr = lrs[i]
    for p in parameters:
        p.data += -lr * p.grad
    lrei.append(lre[i])
    lossi.append(loss.item())
plt.figure()
plt.plot(lrei, lossi)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="jIdayz1ad4zOD63nLnRR2" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="q4RPsz7Mqd" class="myst-jp-nb-block relative group/block"><p>Here, we see the <strong>loss</strong> dropping as the exponent of the learning rate starts to increase, then, after a learning rate of around <code>0.5</code>, the <strong>loss</strong> starts to increase. A good rule of thumb is to pick a learning rate whose at a point around which the <strong>loss</strong> is the lowest and most stable, before any increasing tendency. Let’s pick the learning rate whose exponent corresponds to the lowest <strong>loss</strong>:</p></div><div id="GFktbJVGXz" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">lr = 10**lrei[lossi.index(min(lossi))]
lr</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="mDbLE9wgBnsaJ2vo1Nzo_" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(0.2309)</span></code></div></div></div><div id="xweJIh4M34" class="myst-jp-nb-block relative group/block"><p>Now we have some confidence that this is a fairly good learning rate. Now let’s train for many epochs using this new learning rate!</p></div><div id="Yo0PKazDRY" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">_ = train(x_all, y_all, lr=lr, epochs=10000, print_all_losses=False)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="6PmjsRMm4YQ50gDsWxG-5" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>2.5242083072662354
</span></code></pre></div></div></div></div><div id="dnO2x9f45B" class="myst-jp-nb-block relative group/block"><p>Nice. We got a much smaller <strong>loss</strong> after training. We have dramatically improved on the bigram language model, using this simple <strong>nn</strong> of only <!-- -->3481<!-- --> parameters. Now, there’s something we have to be careful with. Although our <strong>loss</strong> is the lowest so far, it is not <em>exactly</em> true to say that we now have a better model. The reason is that this is actually a very small model. Even though these kind of models can get much larger by adding more and more parameters, e.g. with <!-- -->10000<!-- -->, <!-- -->100000<!-- --> or a million parameters, as the capacity of the <strong>nn</strong> grows, it becomes more and more capable of overfitting your training set. What that means is that the <strong>loss</strong> on the training set (the data that you are training on), will become very very low. As low as <!-- -->0<!-- -->. But in such a case, all that the model is doing is memorizing your training set exactly, <strong>verbatim</strong>. So, if you were to take this model and it’s working very well, but you try to sample from it, you will only get examples, exactly as they are in the training set. You won’t get any new data. In addition to that, if you try to evaluate the <strong>loss</strong> on some withheld names or other input data (e.g. words), you will actually see that the <strong>loss</strong> of those will be very high. And so it is in practice basically not a very good model, since it doesn’t generalize. So, it is standard in the field to split up the dataset into <!-- -->3<!-- --> splits, as we call them: the <em>training</em> split (roughly <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>80</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">80\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">80%</span></span></span></span></span> of data), the <em>dev</em>/<em>validation</em> split (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">10\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">10%</span></span></span></span></span>) and the <em>test</em> split (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">10\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">10%</span></span></span></span></span>).</p></div><div id="mtdUuoU4rG" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># training split, dev/validation split, test split
# 80%, 10%, 10%</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="243VHiKytib0IWf0LKvkX" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="h7MkDyaVBf" class="myst-jp-nb-block relative group/block"><p>Now, the training split is used to optimize the parameters of the model (for training). The validation split is typically used for development and tuning over all the hyperparameters of the model, such as the learning rate, layer width, embedding size, regularization parameters, and other settings, in order to choose a combination that works best on this split. The test split is used to evaluate the performance of the model at the end (after training). So, we are only evaluating the <strong>loss</strong> on the test split very very sparingly and very few times. Because, every single time you evaluate your test <strong>loss</strong> and you learn something from it, you are basically trying to also train on the test split. So, you are only allowed to evaluate the <strong>loss</strong> on the test dataset very few times, otherwise you risk overfitting to it as well, as you experiment on your model. Now, let’s actually split our dataset into training, validation and test datasets. Then, we are going to train on the training dataset and only evaluate on the test dataset very very sparingly. Here we go:</p></div><div id="VrMtRvUkbs" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import random

random.seed(42)
random.shuffle(words)
lenwords = len(words)
n1 = int(0.8 * lenwords)
n2 = int(0.9 * lenwords)
print(f&quot;{lenwords=}&quot;)
print(f&quot;{n1} words in training set&quot;)
print(f&quot;{n2 - n1} words in validation set&quot;)
print(f&quot;{lenwords - n2} words in test&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="IN6LUMtjFyA1PLzYTtrN6" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>lenwords=32033
25626 words in training set
3203 words in validation set
3204 words in test
</span></code></pre></div></div></div></div><div id="uYKfD3Qn9Y" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">xtrain, ytrain = build_dataset(words[:n1])
xval, yval = build_dataset(words[n1:n2])
xtest, ytest = build_dataset(words[n2:])</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="4v90o9slz1-j3drqEmxmR" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>torch.Size([182580, 3]) torch.Size([182580])
torch.Size([22767, 3]) torch.Size([22767])
torch.Size([22799, 3]) torch.Size([22799])
</span></code></pre></div></div></div></div><div id="PdiHkK3X7h" class="myst-jp-nb-block relative group/block"><p>We now have the <!-- -->3<!-- --> split sets. Great! Let’s now re-define our parameters train, anew, on the training dataset:</p></div><div id="QuZnLLZcQP" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">loss_train = train(xtrain, ytrain, lr=lr, epochs=30000, print_all_losses=False)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="k4acDEXCP8kRH_Nc_Vkcy" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>2.083573579788208
</span></code></pre></div></div></div></div><div id="iHQTjNvYD1" class="myst-jp-nb-block relative group/block"><p>Awesome. Our <strong>nn</strong> has been trained and the final <strong>loss</strong> is actually surprisingly good. Let’s now evaluate the <strong>loss</strong> of the validation set (remember, this data was not in the training set on which it was trained):</p></div><div id="UbMn1Ncsgq" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">loss_val = forward_pass(xval, yval)
loss_val</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="kZPxtF5OBFGbL7XhtKnrr" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(2.4294, grad_fn=&lt;NllLossBackward0&gt;)</span></code></div></div></div><div id="z9xlKJ1WMR" class="myst-jp-nb-block relative group/block"><p>Not too bad! Now, as you can see, our <code>loss_train</code> and <code>loss_val</code> are pretty close. In fact, they are roughly equal. This means that we are <strong>not</strong> overfitting, but underfitting. It seems that this model is not powerful enough so as not to be purely memorizing the data. Basically, our <strong>nn</strong> is very tiny. But, we can expect to make performance improvements by scaling up the size of this <strong>nn</strong>. The easiest way to do this is to redefine our <strong>nn</strong> with more neurons in the hidden layer:</p></div><div id="oh2PA1BvGa" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">parameters = define_nn(l1out=300)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="smSndMH0NmgidO48ZFsDP" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="VXBmoYBN9t" class="myst-jp-nb-block relative group/block"><p>Then, let’s re-train and visualize the loss curve:</p></div><div id="srFuigAZpX" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">lossi = []
stepi = []
for p in parameters:
    p.requires_grad = True
for i in range(30000):
    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))
    bx, by = xtrain[batchix], ytrain[batchix]
    loss = forward_pass(bx, by)
    for p in parameters:
        p.grad = None
    loss.backward()
    for p in parameters:
        p.data += -lr * p.grad
    stepi.append(i)
    lossi.append(loss.log10().item())
plt.figure()
plt.plot(stepi, lossi)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="_0EKpw3oFBNCFukWCupx1" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="UBnTd5gXje" class="myst-jp-nb-block relative group/block"><p>As you can see, it is a bit noisy, but that is just because of the minibatches!</p></div><div id="b3Yb9wNR9J" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">loss_train = forward_pass(xtrain, ytrain)
loss_val = forward_pass(xval, yval)
print(loss_train)
print(loss_val)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="FvjJ1fb7aUlCUr1qrtxPE" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>tensor(2.4805, grad_fn=&lt;NllLossBackward0&gt;)
tensor(2.4808, grad_fn=&lt;NllLossBackward0&gt;)
</span></code></pre></div></div></div></div><div id="pd4DM2inou" class="myst-jp-nb-block relative group/block"><p>Awesome, the training loss is actually lower than before, whereas the validation loss is pretty much the same. So, increasing the size of the hidden layer gave us some benefit. Let’s experiment more to see if we can get even lower losses by increasing the embedding layer. First though, let’s visualize the character embeddings:</p></div><div id="oj4CZ1AfvK" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># visualize dimensions 0 and 1 of the embedding matrix C for all characters
plt.figure(figsize=(8, 8))
plt.scatter(C[:, 0].data, C[:, 1].data, s=200)
for i in range(C.shape[0]):
    plt.text(
        C[i, 0].item(),
        C[i, 1].item(),
        itoc[i],
        ha=&quot;center&quot;,
        va=&quot;center&quot;,
        color=&quot;white&quot;,
    )
plt.grid(&quot;minor&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="q_AxNi0CiVMg9y0EBuf6y" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="itmdKepvez" class="myst-jp-nb-block relative group/block"><p>The network has basically learned to separate out the characters and cluster them a little bit. For example, it has learned some characters are usually found more closer together than others. Let’s try to improve our model loss by choosing a greater embeddings layer size of <!-- -->10<!-- --> and by increasing the number of epochs to <!-- -->200000<!-- -->, also we’ll decay the learning rate after <!-- -->100000<!-- --> epochs:</p></div><div id="MWZS1TOulr" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">parameters = define_nn(l1out=200, embsize=10)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="L3rreTPMAssGU2U3OVpzJ" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="Slr5gCMDwG" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">lossi = []
stepi = []
for p in parameters:
    p.requires_grad = True
for i in range(200000):
    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))
    bx, by = xtrain[batchix], ytrain[batchix]
    loss = forward_pass(bx, by)
    for p in parameters:
        p.grad = None
    loss.backward()
    lr = 0.1 if i &lt; 100000 else 0.01
    for p in parameters:
        p.data += -lr * p.grad
    stepi.append(i)
    lossi.append(loss.log10().item())
plt.figure()
plt.plot(stepi, lossi);</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="Gzh5vANTfA1-NVq7hyJGq" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="MTWDKXTy7J" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">loss_train = forward_pass(xtrain, ytrain)
loss_val = forward_pass(xval, yval)
print(loss_train)
print(loss_val)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="Z8WVhsG3iaaq1yDuUpLR4" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>tensor(2.1189, grad_fn=&lt;NllLossBackward0&gt;)
tensor(2.1583, grad_fn=&lt;NllLossBackward0&gt;)
</span></code></pre></div></div></div></div><div id="KmnXOrbmMs" class="myst-jp-nb-block relative group/block"><p>Wow, we did it! Both train and validation losses are lower now. Can we go lower? Play around and find out! Now, before we end this lesson, let’s sample from our model:</p></div><div id="vqWyi8XMbu" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># sample from the model
g = torch.Generator().manual_seed(2147483647 + 10)
for _ in range(20):    
    out = []
    context = [0] * block_size # initialize with all ...
    while True:
        emb = C[torch.tensor([context])] # (1,block_size,d)
        h = torch.tanh(emb.view(1, -1) @ w1 + b1)
        logits = h @ w2 + b2
        probs = F.softmax(logits, dim=1)
        ix = torch.multinomial(probs, num_samples=1, generator=g).item()
        context = context[1:] + [ix]
        out.append(ix)
        if ix == 0:
            break
    print(&#x27;&#x27;.join(itoc[i] for i in out))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="N_OoGi4yfo7rqaulJCbU5" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>mona.
kayah.
see.
med.
ryla.
remmastendrael.
azeer.
melin.
shivonna.
keisen.
anaraelynn.
hotelin.
shaber.
shiriel.
kinze.
jenslenter.
fius.
kavder.
yaralyeha.
kayshayton.
</span></code></pre></div></div></div></div><div id="D06BRO6gKN" class="myst-jp-nb-block relative group/block"><h2 id="outro" class="relative group"><span class="heading-text">Outro</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#outro" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="dvV9Yb86gi" class="myst-jp-nb-block relative group/block"><p>As you can see, our model now is pretty decent and able to produce suprisingly name-like text, which is what we wanted all along! Next up, we will explore <strong>mlp</strong> internals and other such magic.</p></div><div class="myst-backmatter-parts"></div><div class="myst-footer-links flex pt-10 mb-10 space-x-4"><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-prev" href="/micrograduate/makemore1"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">microgra∇uate</div>2. makemore (part 1): implementing a bigram character-level language model</div></div></a><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-next" href="/micrograduate/makemore3"><div class="flex h-full align-middle"><div class="flex-grow"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">microgra∇uate</div>4. makemore (part 3): activations &amp; gradients, batchnorm</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,l)=>{if(!window.history.state||!window.history.state.key){let u=Math.random().toString(32).slice(2);window.history.replaceState({key:u},"")}try{let d=JSON.parse(sessionStorage.getItem(a)||"{}")[l||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(u){console.error(u),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-PCJPW7TK.js"/><link rel="modulepreload" href="/build/_shared/chunk-AQ2CODAG.js"/><link rel="modulepreload" href="/build/_shared/chunk-JJXTQVMA.js"/><link rel="modulepreload" href="/build/_shared/chunk-OZE3FFNP.js"/><link rel="modulepreload" href="/build/_shared/chunk-7UUHRSK3.js"/><link rel="modulepreload" href="/build/_shared/chunk-C4DFGG5C.js"/><link rel="modulepreload" href="/build/_shared/chunk-J7TUH54J.js"/><link rel="modulepreload" href="/build/_shared/chunk-FZ2S7OYD.js"/><link rel="modulepreload" href="/build/_shared/chunk-JEM6JXYA.js"/><link rel="modulepreload" href="/build/_shared/chunk-34XIY2DH.js"/><link rel="modulepreload" href="/build/_shared/chunk-KQM5FBHR.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-7HNKBP4B.js"/><link rel="modulepreload" href="/build/_shared/chunk-CUKUDK3R.js"/><link rel="modulepreload" href="/build/_shared/chunk-3EBOCCHJ.js"/><link rel="modulepreload" href="/build/_shared/chunk-O4VQNZ62.js"/><link rel="modulepreload" href="/build/_shared/chunk-4OEDG4JQ.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-EDJFWIEV.js"/><link rel="modulepreload" href="/build/_shared/chunk-ECLX7DIY.js"/><link rel="modulepreload" href="/build/routes/$-AD65NCUT.js"/><script>window.__remixContext = {"url":"/micrograduate/makemore2","state":{"loaderData":{"root":{"config":{"version":3,"myst":"1.8.0","options":{"favicon":"/build/logo-1a1c7119355e3dbc339d456e6b58f518.png","logo":"/build/logo-1a1c7119355e3dbc339d456e6b58f518.png","folders":true,"style":"/build/custom-bf65082cee648dc0442b77b0bcdfa840.css"},"nav":[],"actions":[],"projects":[{"title":"microgra∇uate","github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","toc":[{"file":"index.md"},{"file":"micrograduate/micrograd.ipynb"},{"file":"micrograduate/makemore1.ipynb"},{"file":"micrograduate/makemore2.ipynb"},{"file":"micrograduate/makemore3.ipynb"},{"file":"micrograduate/makemore4.ipynb"},{"file":"micrograduate/makemore5.ipynb"},{"file":"micrograduate/picogpt.ipynb"}],"thumbnail":"/build/heading-2992afcd5615ae46f403e88bcb8569f4.png","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"micrograduate.micrograd","title":"1. micrograd: implementing an autograd engine","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore1","title":"2. makemore (part 1): implementing a bigram character-level language model","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore2","title":"3. makemore (part 2): mlp","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore3","title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore4","title":"5. makemore (part 4): becoming a backprop ninja","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore5","title":"6. makemore (part 5): building a WaveNet","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.picogpt","title":"7. picoGPT: implementing a tiny GPT from scratch","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static"},"routes/$":{"config":{"version":3,"myst":"1.8.0","options":{"favicon":"/build/logo-1a1c7119355e3dbc339d456e6b58f518.png","logo":"/build/logo-1a1c7119355e3dbc339d456e6b58f518.png","folders":true,"style":"/build/custom-bf65082cee648dc0442b77b0bcdfa840.css"},"nav":[],"actions":[],"projects":[{"title":"microgra∇uate","github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","toc":[{"file":"index.md"},{"file":"micrograduate/micrograd.ipynb"},{"file":"micrograduate/makemore1.ipynb"},{"file":"micrograduate/makemore2.ipynb"},{"file":"micrograduate/makemore3.ipynb"},{"file":"micrograduate/makemore4.ipynb"},{"file":"micrograduate/makemore5.ipynb"},{"file":"micrograduate/picogpt.ipynb"}],"thumbnail":"/build/heading-2992afcd5615ae46f403e88bcb8569f4.png","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"micrograduate.micrograd","title":"1. micrograd: implementing an autograd engine","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore1","title":"2. makemore (part 1): implementing a bigram character-level language model","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore2","title":"3. makemore (part 2): mlp","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore3","title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore4","title":"5. makemore (part 4): becoming a backprop ninja","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore5","title":"6. makemore (part 5): building a WaveNet","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.picogpt","title":"7. picoGPT: implementing a tiny GPT from scratch","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"page":{"version":3,"kind":"Notebook","sha256":"368ea204c15533451af249185ed405308b9b8c5bc6868922427213442e93bb46","slug":"micrograduate.makemore2","location":"/micrograduate/makemore2.ipynb","dependencies":[],"frontmatter":{"title":"3. makemore (part 2): mlp","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"micrograduate-env","language":"python"},"github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","source_url":"https://github.com/ckaraneen/micrograduate/blob/main/micrograduate/makemore2.ipynb","edit_url":"https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore2.ipynb","exports":[{"format":"ipynb","filename":"makemore2.ipynb","url":"/build/makemore2-d9fba20841ab4f395cab1ee28afbdad9.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git \u003e /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt","key":"DyGtrwCzzy"},{"type":"outputs","id":"LN_YwL7eySP1qrZkvoshL","children":[],"key":"tzHBAUMCkK"}],"key":"bPtSIx8P4Q"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Intro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zOhxEHktOx"}],"identifier":"intro","label":"Intro","html_id":"intro","implicit":true,"key":"Wnkw0AMFkK"}],"key":"gGtxN08mDO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Time to make more out of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cTAlESXXVZ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AUEgvneCEX"}],"key":"hPCGicaJri"},{"type":"text","value":"! In the last lesson, we implemented the bigram language model, both using counts and a super simple, 1-linear-layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ceuj9A9BeI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jSViPDrxhU"}],"key":"pNujLBowl0"},{"type":"text","value":". How we approach training is that we looked only at the single previous character and we predicted a distribution for the character coming next in the sequence. We did that by taking counts and normalizing them into probabilities so that each row in the count matrix sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nVnrnHmGHq"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kMTJIp5tKV"},{"type":"text","value":". This method is great if you only have one character of previous context. The problem with that model though is that predictions are not very good. Another problem, if we are to take more context into account, is that the counts in the matrix grow exponentially as we increase the length of the context. For just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oeaAp2MDgg"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ka6AbvgJUh"},{"type":"text","value":" character of context we have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jOmhU8vQJZ"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mSIPmKvZhH"},{"type":"text","value":" rows, each representing the next possible character. For ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mse6ZBiLA7"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P946L4aM0V"},{"type":"text","value":" characters, the number of rows would grow to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VW1rOKl60j"},{"type":"inlineMath","value":"27 \\cdot 27 = 729","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e729\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e27 \\cdot 27 = 729\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e729\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"GEB2bsXYRN"},{"type":"text","value":". Whereas for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vJFB315FTP"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tJzqMcNUNW"},{"type":"text","value":" characters, it would explode to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jvRCoCtFMw"},{"type":"inlineMath","value":"27 \\cdot 27 \\cdot 27 = 19683","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e19683\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e27 \\cdot 27 \\cdot 27 = 19683\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e19683\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"KBcqEO0jcd"},{"type":"text","value":", and so on. This solution simply doesn’t scale well and explodes. That is why we are going to move on and instead implement an ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xbPiEhPsEb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G3muXDVvOQ"}],"key":"RkB6ib6zJi"},{"type":"text","value":" model to predict the next character in a sequence.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EIsZP1OiKc"}],"key":"Yi5Tq11MvD"}],"key":"btveSTaexH"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Building a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dajC32rQhp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"afZwV4boAp"}],"key":"bliN0RBFLq"},{"type":"text","value":" language model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dGnKaheYJP"}],"identifier":"building-a-mlp-language-model","label":"Building a mlp language model","html_id":"building-a-mlp-language-model","implicit":true,"key":"KLMEIWl18O"}],"key":"AaeuZnKoG7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The modeling approach we are going to adopt follows ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JFOaCuJ3HK"},{"type":"link","url":"https://doi.org/10.5555/944919.944966","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Bengio et al. 2003","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zb3xgxpUBk"}],"urlSource":"https://dl.acm.org/doi/10.5555/944919.944966","data":{"doi":"10.5555/944919.944966"},"internal":false,"protocol":"doi","key":"tBCCECKPvy"},{"type":"text","value":", an important paper that we are going to implement. Although they implement a word-level language model, we are going to stick to our character-level language model, but follow the same approach. The authors propose associating each and every word (out of e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d1gYHNhu3S"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZrDPkO58t5"},{"type":"text","value":") with a feature vector (e.g. of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iitxomdPIp"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z7IAsdst1Z"},{"type":"text","value":" dimensions). In other words, every word is a point that is embedded into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"szsISigFmu"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tE2lAgHs38"},{"type":"text","value":"-dimensional space. You can think of it this way. We have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q5ikPgBJUn"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sQIhUyjUt1"},{"type":"text","value":" point-vectors in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"glmQNiZLZC"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zcp2MSLYD6"},{"type":"text","value":"-dimensional space. As you can imagine, that is very crowded, that’s lots of points for a very small space. Now, in the beginning, these words are initialized completely randomly: they are spread out at random. But, then we are going to tune these embeddings of these words using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gtKpG1NJJQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nvq3tlGwxa"}],"key":"v9lAomSguZ"},{"type":"text","value":". So during the course of training of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qGRLmthNbr"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cp4cDbB56x"}],"key":"OT4qMYCvLZ"},{"type":"text","value":", these point-vectors are going to basically be moved around in this space. And you might imagine that, for example, words that have very similar meanings or that are indeed synonyms of each other might end up in a very similar part of the vector space, and, conversely, words with very different meanings will go somewhere else in that space. Now, their modeling approach otherwise is identical to what ours has been so far. They are using a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JjWMwxrbi0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PVioUFyuxn"}],"key":"K48uddLubS"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MAlYBxIFIT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vvt0Ivj1gF"}],"key":"FiyKNXJo8B"},{"type":"text","value":" to predict the next word, given the previous words and to train the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uOCk1axEJB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wGlFUUBSQZ"}],"key":"uZ5qunmxIH"},{"type":"text","value":" they are maximizing the log-likehood of the training data, just like we did. Here, is their example of this intuition: suppose the exact phrase ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iZMEDk3weL"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a dog was running in a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XM3gR7fvqg"}],"key":"QgRsEbb5Iv"},{"type":"text","value":" has never occured and at test time we want our model to complete the sentence by predicting the word that might follow it (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UqroPOJuPS"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"room","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y9ZzvkyHdt"}],"key":"m2Ri6PiMZb"},{"type":"text","value":"). Because the model has never encountered this exact phrase in the training set, it is out of distribution, as we say. Meaning, you don’t have fundamentally any reason to suspect what might come next. However, the approach we are following allows you to get around such suspicion. Maybe we haven’t seen the exact phrase, but maybe we have seen similar phrases like: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vYo5zFeZjt"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"the dog was running in a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MaP1PzRyZ9"}],"key":"U4lgaHinh7"},{"type":"text","value":" and maybe your ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kpOjoMCkfY"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UO1Ot354gE"}],"key":"oPpkU5SoC2"},{"type":"text","value":" has learned that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wELX6baRyl"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bmURmiOe4W"}],"key":"dCCnBVxO4B"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"la2uEkMD8Q"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"the","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"veN7SmhrMR"}],"key":"KmG3R6RYdk"},{"type":"text","value":" are frequently interchangeble with each other. So maybe our model took the embeddings for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UCVdoAKWwn"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lcPMwmWCTR"}],"key":"tTUbfiZkwq"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FGSR9CfGqY"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"the","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q3WXTuGzE2"}],"key":"A21kd4MSUP"},{"type":"text","value":" and it actually put them nearby eachother in the vector space. Thus, you can transfer knowledge through such an embedding and generalize in that way. Similarly, it can do the same with other similar words such that a phrase such as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pT1Yc6WwUy"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"The cat is walking in the bedroom","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ObWUYbcDuV"}],"key":"cnZxADOz6P"},{"type":"text","value":" can help us generalize to a diserable or at least valid sentence like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CPQJfRFGzo"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a dog was running in a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zFSEXN1xWn"}],"key":"YMy0Cb5xA4"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"igJ9BpCIge"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"room","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZF9n6qoNut"}],"key":"kMFcn6vobz"},{"type":"text","value":" by merit of the magic of feature vector similarity after training! To put it more simply, manipulating the embedding space allows us to transfer knowledge, predict and generalize to novel scenarios even when fed inputs like the sequence of words mentioned that we have not trained on. If you scroll down the paper, you will see the following diagram:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tJzUO2haVe"}],"key":"bnRBoeGLnd"}],"key":"ODhf1ipLk8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\ndisplay(Image(filename='bengio2003nn.jpeg'))","key":"sBd9iQIOsU"},{"type":"outputs","id":"KdrQgPgAnxLA3D6HiJQ7X","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"a21abcc7498c74c85d4a3cd5f51b3817","path":"/build/a21abcc7498c74c85d4a3cd5f51b3817.jpeg"},"text/plain":{"content":"\u003cIPython.core.display.Image object\u003e","content_type":"text/plain"}}},"children":[],"key":"XoDKK5pbiB"}],"key":"Z4yIvYZ599"}],"key":"eRhgkNQy9Z"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This is the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LiJ9KU7u4j"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tVXuLGeYHz"}],"key":"bjKD2x5GVg"},{"type":"text","value":" where we are taking e.g. three previous words and we are trying to predict the fourth word in a sequence, where ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PLHRlQejUg"},{"type":"inlineMath","value":"w_{t-3}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t-3}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"za0IBOq3PK"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZXM9aPpSh9"},{"type":"inlineMath","value":"w_{t-2}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t-2}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"MInigiAs3Y"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DpJ6nbXH70"},{"type":"inlineMath","value":"w_{t-1}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t-1}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"FF9v0Bx2l8"},{"type":"text","value":" are the indeces of each incoming word. Since there are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Teg5EoYngN"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X5PApPfR1H"},{"type":"text","value":" possible words, these indeces are integers between ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bUrjhNXUwE"},{"type":"inlineMath","value":"0-16999","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e16999\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e0-16999\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e16999\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"cozJ03dfnU"},{"type":"text","value":". There’s also a lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"twTuIDTm1I"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"YLTfAYWOjG"},{"type":"text","value":", a matrix that has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bd1AKLktFs"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jht0EEVmmZ"},{"type":"text","value":" rows (one for each word embedding) and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rrCdhgCdIM"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kq9QnhHk1v"},{"type":"text","value":" columns (one for each feature vector/embedding dimension). Every index basically plucks out a row of this embedding matrix so that each index is converted to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yrPJNgZ3L7"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uxhKWu1aVc"},{"type":"text","value":"-dimensional embedding vector corresponding to that word. Therefore, each word index corresponds to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xWvt5Jxyur"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WD7pwLjA3I"},{"type":"text","value":" neuron activations exiting the first layer: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qcvvW4u2pW"},{"type":"inlineMath","value":"w_{t-3} \\rightarrow C(w_{t-3})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e→\u003c/mo\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t-3} \\rightarrow C(w_{t-3})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e→\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"tCcns9NMVc"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oLs4LEC1Mv"},{"type":"inlineMath","value":"w_{t-2} \\rightarrow C(w_{t-2})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e→\u003c/mo\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t-2} \\rightarrow C(w_{t-2})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e→\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"l4nuwNd1HC"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X9gtSYIFf6"},{"type":"inlineMath","value":"w_{t-1} \\rightarrow C(w_{t-1})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e→\u003c/mo\u003e\u003cmi\u003eC\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t-1} \\rightarrow C(w_{t-1})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e→\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"iZohh0UwOI"},{"type":"text","value":". Thus, the first layer contains ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KWojv3Ssju"},{"type":"text","value":"90","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OZEDJAx31S"},{"type":"text","value":" neurons in total. Notice how the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"avCRSiwn3O"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"dJ5rZgMIRE"},{"type":"text","value":" matrix is shared, which means that we are indexing the same matrix over and over. Next up is the hidden layer of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mNsO9dY0WT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cExTdAWsMX"}],"key":"WmkyIL1f2R"},{"type":"text","value":" whose size is a hyperparameter, meaning that it is up to the choice of the designer how wide, aka how many neurons, it is going to have. For example it could have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f4GWeTRJHg"},{"type":"text","value":"100","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zNwh90i4NP"},{"type":"text","value":" or any other number that endows the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pU8MNVcJWu"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i5gAL6ZiSk"}],"key":"avZfgCLRZb"},{"type":"text","value":" with the best performance, after evaluation. This hidden layer is fully connected to the input layer of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xEqRKbJhjl"},{"type":"text","value":"90","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lQsUD7pPmP"},{"type":"text","value":" neurons, meaning each neuron is connected to each one of this layer’s neurons. Then there’s a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X2TVoLK478"},{"type":"inlineMath","value":"\\tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003etanh\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\tanh\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003etanh\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"jnN3IlRKrk"},{"type":"text","value":" non-linearity, and then there’s an output layer. And because of course we want the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UvRY5CSBTg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OXvhYz78lR"}],"key":"akZCTj8Juo"},{"type":"text","value":" to give us the next word, the output layer has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BRntMRtaZp"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gbJFTCnwsp"},{"type":"text","value":" neurons that are also fully connected to the previous (hidden) layer’s neurons. So, there’s a lot of parameters, as there are a lot of words, so most computation happens in the output layer. Each of this layer’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tSfUF40Amp"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dtLdcnYaXL"},{"type":"text","value":" logits is passed through a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r5St9liC3t"},{"type":"inlineMath","value":"softmax","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ef\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003esoftmax\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eso\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ema\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"MNy8A0aj1p"},{"type":"text","value":" function, meaning they are all exponentiated and then everything is normalized to sum to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WbKqTyLnGP"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BXjPTcVGb0"},{"type":"text","value":", so that we have a nice probability distribution ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u2TPBd9fyQ"},{"type":"inlineMath","value":"P(w_{t}=i\\ |\\ context)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eP\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmtext\u003e \u003c/mtext\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eP(w_{t}=i\\ |\\ context)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eP\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ei\u003c/span\u003e\u003cspan class=\"mspace\"\u003e \u003c/span\u003e\u003cspan class=\"mord\"\u003e∣\u003c/span\u003e\u003cspan class=\"mspace\"\u003e \u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eco\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"Am8BPRCM2J"},{"type":"text","value":" for the next word ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X8Sk4ocWnP"},{"type":"inlineMath","value":"w_{t}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_{t}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"nG9kqQhGWV"},{"type":"text","value":" in the sequence. During training of course, we have the label or target index: the index of the next word in the sequence which we use to pluck out the probability of that word from that distribution. The point of training is to maximize the probability of that word ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iIXWnePrVP"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"w.r.t.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"onjeujBrKz"}],"key":"VnEawBHaZB"},{"type":"text","value":" the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"erGE5hVe0b"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oWNjFkkBJX"}],"key":"CY4SVjpOfi"},{"type":"text","value":" parameters, meaning the weights and biases of the output layer, of the hidden layer and of the embedding lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gV9AXdPFwz"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eC\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eC\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"GRjNCeSqJm"},{"type":"text","value":". All of these parameters are optimized using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xVVlXzeaBB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wWcsORRUi8"}],"key":"zvqWM5JzdM"},{"type":"text","value":". Ignore the green dashed arrows in the diagram, they represent a variation of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zLFL8tifDE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DbkjqQQa1H"}],"key":"mZ1i8joH9B"},{"type":"text","value":" we are not going to explore in this lesson. So, what we  described is the setup. Now, let’s implement it!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XoGgnjYREo"}],"key":"EWdW8G9Opv"}],"key":"Lc6JDEg7fe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl","key":"iOFrggSUCs"},{"type":"outputs","id":"cOrRKrMTFtAuzWqgU13DB","children":[],"key":"prOy3gLA1r"}],"key":"ETFPBhrgU0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nwords[:8]","key":"SbXo4NWB6v"},{"type":"outputs","id":"AhKg5PbAZjWmRswIsXuGs","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']","content_type":"text/plain"}}},"children":[],"key":"Pz5YuQ4fWb"}],"key":"z8rprfNZEL"}],"key":"UYBXSNv2ct"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(words)","key":"J9nYLxS8dV"},{"type":"outputs","id":"gw5wMOeyZcC_l3QjoDxny","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":5,"metadata":{},"data":{"text/plain":{"content":"32033","content_type":"text/plain"}}},"children":[],"key":"BK1CIHXz97"}],"key":"EJtyfSUCuH"}],"key":"nLuzFoFPcl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {c: i + 1 for i, c in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: c for c, i in ctoi.items()}\nprint(itoc)","key":"UVbZCIiHF8"},{"type":"outputs","id":"ZhDmFVsNXCDSYd8pwSg09","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"},"children":[],"key":"J50I9D6O3p"}],"key":"ylvazAVhrV"}],"key":"d5ozLWF0YK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, we are reading ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pNkO9hgxjK"},{"type":"inlineCode","value":"32033","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E9bOepkFLS"},{"type":"text","value":" words into a list and we are creating character-to/from-index mappings. From here, the first thing we want to do is compile the dataset for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v5EDtZ4vOk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M22pDeeszU"}],"key":"okxmVRSk1j"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DCs6gQoKsk"}],"key":"NaVTb2UkXs"}],"key":"scGWdAvH9f"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# context length: how many characters do we take to predict the next one?\nblock_size = 3\n\n\ndef build_dataset(words, verbose=False):\n    x, y = [], []\n    for w in words:\n        if verbose:\n            print(w)\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            if verbose:\n                print(\"\".join(itoc[i] for i in context), \"---\u003e\", itoc[ix])\n            context = context[1:] + [ix]  # crop and append\n            x.append(context)\n            y.append(ix)\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(f\"{x.shape=}, {y.shape=}\")\n    print(f\"{x.dtype=}, {y.dtype=}\")\n    return x, y\n\n\nx, y = build_dataset(words[:5], verbose=True)","key":"xq4G5FC18p"},{"type":"outputs","id":"8d3HW0n4sIuIOFYTFpGF9","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"emma\n... ---\u003e e\n..e ---\u003e m\n.em ---\u003e m\nemm ---\u003e a\nmma ---\u003e .\nolivia\n... ---\u003e o\n..o ---\u003e l\n.ol ---\u003e i\noli ---\u003e v\nliv ---\u003e i\nivi ---\u003e a\nvia ---\u003e .\nava\n... ---\u003e a\n..a ---\u003e v\n.av ---\u003e a\nava ---\u003e .\nisabella\n... ---\u003e i\n..i ---\u003e s\n.is ---\u003e a\nisa ---\u003e b\nsab ---\u003e e\nabe ---\u003e l\nbel ---\u003e l\nell ---\u003e a\nlla ---\u003e .\nsophia\n... ---\u003e s\n..s ---\u003e o\n.so ---\u003e p\nsop ---\u003e h\noph ---\u003e i\nphi ---\u003e a\nhia ---\u003e .\nx.shape=torch.Size([32, 3]), y.shape=torch.Size([32])\nx.dtype=torch.int64, y.dtype=torch.int64\n"},"children":[],"key":"CZlzyeN5xd"}],"key":"opUEaJEiFF"}],"key":"LI23rjbATX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We first define the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KvOTYyvTyJ"},{"type":"inlineCode","value":"block_size","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q9b0ZJuXTb"},{"type":"text","value":" which is how many characters we need to predict the next one. In the example I just described, we used ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lhQRMCd84U"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jBoSvLfHTI"},{"type":"text","value":" words to predict the next one. Here, we also use a block size of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lqsguxPZwW"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JadqFs02zZ"},{"type":"text","value":" and do the same thing, but remember, instead of words we expect characters as inputs and predictions. After defining the block size, we construct ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hlywnkiNqi"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PUpyHWwXaP"},{"type":"text","value":": a feature list of word index triplets (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DC7n4v9v6e"},{"type":"inlineCode","value":"[[ 0,  0,  5], [ 0,  5, 13], ...]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mpqD7d1ZGh"},{"type":"text","value":") that represent the context inputs, and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BvWFH8l96G"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VdLhMChpYv"},{"type":"text","value":": a list of corresponding target word indeces (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ifXK19DwEq"},{"type":"inlineCode","value":"[ 5, 13, ...]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"htDYF2UcvY"},{"type":"text","value":"). In the printout above, you can see for each word’s context character triplet, the corresponding target character. E.g. for an input of  ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pOxHhKyJm7"},{"type":"inlineCode","value":"...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kUPI3Qp10F"},{"type":"text","value":" the target character is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pNjg1WEU62"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J3FhN02dRk"},{"type":"text","value":", for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BVaiuhBSBZ"},{"type":"inlineCode","value":"..e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n7SW24ow0W"},{"type":"text","value":" it’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KW7Mh6IbHI"},{"type":"inlineCode","value":"m","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tzeStfIooJ"},{"type":"text","value":", and so on! Change the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WSM15bODCn"},{"type":"inlineCode","value":"block_size","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lLGzfCdwHx"},{"type":"text","value":" and see the print out for yourself. Notice how we are using dots as padding. After building the dataset, inputs and targets look like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PxbySUg2nT"}],"key":"Q1V3K0KXva"}],"key":"lajPHVQePY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"x, y","key":"FQbn7rp6kh"},{"type":"outputs","id":"7va8qFY39_OPECVxOyehP","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":8,"metadata":{},"data":{"text/plain":{"content":"(tensor([[ 0,  0,  5],\n         [ 0,  5, 13],\n         [ 5, 13, 13],\n         [13, 13,  1],\n         [13,  1,  0],\n         [ 0,  0, 15],\n         [ 0, 15, 12],\n         [15, 12,  9],\n         [12,  9, 22],\n         [ 9, 22,  9],\n         [22,  9,  1],\n         [ 9,  1,  0],\n         [ 0,  0,  1],\n         [ 0,  1, 22],\n         [ 1, 22,  1],\n         [22,  1,  0],\n         [ 0,  0,  9],\n         [ 0,  9, 19],\n         [ 9, 19,  1],\n         [19,  1,  2],\n         [ 1,  2,  5],\n         [ 2,  5, 12],\n         [ 5, 12, 12],\n         [12, 12,  1],\n         [12,  1,  0],\n         [ 0,  0, 19],\n         [ 0, 19, 15],\n         [19, 15, 16],\n         [15, 16,  8],\n         [16,  8,  9],\n         [ 8,  9,  1],\n         [ 9,  1,  0]]),\n tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n          1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))","content_type":"text/plain"}}},"children":[],"key":"WHVZVFP0N2"}],"key":"iw30TxE0ui"}],"key":"qazYGLebGh"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Given these, let’s write a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lXnJOuZDHX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ONuSENyomu"}],"key":"UpyOOsMtMP"},{"type":"text","value":" that takes the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ISzja0dMn3"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EVohITsP6T"},{"type":"text","value":" and predicts ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bFsmdKvkTh"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y9QMcMrd7M"},{"type":"text","value":". First, let’s build the embedding lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FuaZkHldF5"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ynpr8508SV"},{"type":"text","value":". In the paper, they have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zi6vlcoxVF"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f6HgFnQ2e6"},{"type":"text","value":" words and embed them in spaces as low-dimensional as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K97X2V1MhK"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n0lEs4IxqG"},{"type":"text","value":", so they cram ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g2CvZw9KKf"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yKlsG9sDdS"},{"type":"text","value":" into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Kl06rQmZFS"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M4ytGnleWQ"},{"type":"text","value":"-dimensional space. In our case, we have only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S9NNcY5FK3"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nk6CNNzPrz"},{"type":"text","value":" possible characters, so let’s cram them into as small as -let’s say- a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ShEeHmtlmW"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wv80qGwNUq"},{"type":"text","value":"-dimensional space:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f4tpzYtFYH"}],"key":"hVKDN8RkeM"}],"key":"jafkSctMCJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"SEED = 2147483647\ng = torch.Generator().manual_seed(SEED)\nC = torch.randn((27, 2), generator=g)\nC","key":"IO8fBOydEE"},{"type":"outputs","id":"8wvEOtdci-XePangG9v5Q","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":9,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 1.5674, -0.2373],\n        [-0.0274, -1.1008],\n        [ 0.2859, -0.0296],\n        [-1.5471,  0.6049],\n        [ 0.0791,  0.9046],\n        [-0.4713,  0.7868],\n        [-0.3284, -0.4330],\n        [ 1.3729,  2.9334],\n        [ 1.5618, -1.6261],\n        [ 0.6772, -0.8404],\n        [ 0.9849, -0.1484],\n        [-1.4795,  0.4483],\n        [-0.0707,  2.4968],\n        [ 2.4448, -0.6701],\n        [-1.2199,  0.3031],\n        [-1.0725,  0.7276],\n        [ 0.0511,  1.3095],\n        [-0.8022, -0.8504],\n        [-1.8068,  1.2523],\n        [ 0.1476, -1.0006],\n        [-0.5030, -1.0660],\n        [ 0.8480,  2.0275],\n        [-0.1158, -1.2078],\n        [-1.0406, -1.5367],\n        [-0.5132,  0.2961],\n        [-1.4904, -0.2838],\n        [ 0.2569,  0.2130]])","content_type":"text/plain"}}},"children":[],"key":"U2psjbPFsF"}],"key":"pQf4ARvaxg"}],"key":"kq2y94hw6R"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Each of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dQg9F6uEAJ"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZuaB79x5Gd"},{"type":"text","value":" characters will have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NYUxYamqRW"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dmUlz39orG"},{"type":"text","value":"-dimensional embedding. Therefore, our table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a5BmSoQUQx"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v6xKXZC96l"},{"type":"text","value":" will have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aCi9rjOdDa"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JOhvCUCuYL"},{"type":"text","value":" rows (one for each character) and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d5b8mdkiCB"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gO55MYwEUR"},{"type":"text","value":" columns (number of dimensions per character embedding). Before we embed all the integers inside input ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tIHcoo7JDE"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cuhxeUsHfG"},{"type":"text","value":" using this lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nPSI2vN881"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VlRhWD5k24"},{"type":"text","value":", let’s first embed a single, individual character, let’s say, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YR1M1yfqMz"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O2GEI7HI03"},{"type":"text","value":", so we get a sense of how this works. One way to do it is to simply index the table using the character index:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ws1dGUFuq4"}],"key":"Qt0jiRjoX8"}],"key":"bU1cVi989R"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"C[5]","key":"HUcxFhg27t"},{"type":"outputs","id":"nQGVf2xIkyLo1qNcT8Pwc","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":10,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.4713,  0.7868])","content_type":"text/plain"}}},"children":[],"key":"iufgk4pky8"}],"key":"iM24Bc8gRJ"}],"key":"KWyyW5bXNN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Another way, as we saw in the previous lesson, is to one-hot encode the character:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D5ra6IZmqW"}],"key":"NG4VAiCeqX"}],"key":"kGdMwMvHSh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ohv = F.one_hot(torch.tensor(5), num_classes=27)\nprint(ohv)\nprint(ohv.shape)","key":"kz8PcLl2hW"},{"type":"outputs","id":"hFLS6LBceEGMarsRuPJWo","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0])\ntorch.Size([27])\n"},"children":[],"key":"BrN21Nh1gB"}],"key":"wnDgY7FYgq"}],"key":"H5eCMIiyBz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"With this way we get a one-hot encoded representation whose ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KSddyQTuSS"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RxHjXcNXE8"},{"type":"text","value":"-th element is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VEuOcQFNQs"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ap4S4TExGE"},{"type":"text","value":" and all the rest are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GntVINjbND"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fd01zWb8ZU"},{"type":"text","value":". Now, notice how, just as we previously alluded to in the previous lesson, if we take this one-hot vector and we multiply it by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nN0sdRNv2w"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P4gEKIZvl8"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IbzKgn7e34"}],"key":"zc5hZEZolQ"}],"key":"fk7MHm3eF4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ohv_matmul_C = ohv.float() @ C\nohv_matmul_C","key":"PWa1NaOppH"},{"type":"outputs","id":"X1pTzQ9ANNYLwOJDbK9QB","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":12,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.4713,  0.7868])","content_type":"text/plain"}}},"children":[],"key":"UXa2Osf8lf"}],"key":"tJaH889woM"}],"key":"VP74CPbJhs"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"as you can see, they are identical:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C8SsnyZ8rY"}],"key":"MQtw5o1XMY"}],"key":"Qn0Eq4osGe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert C[5].equal(ohv_matmul_C)","key":"gtPS2LTEhY"},{"type":"outputs","id":"fYXt0C9UdP14V5-95o9pZ","children":[],"key":"W8pCX9CiZn"}],"key":"SOUCHcVwNy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Multiplying a one-hot encoded vector and an appropriate matrix acts like indexing that matrix with the index of the vector that points to element ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c1ZBXB1p0q"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OuK6NrVkkP"},{"type":"text","value":"! And so, we actually arrive at the same result. This is interesting since it points out how the first layer of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Xz8rLvV7Kx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JfpHL9HcCa"}],"key":"qpN1SVfyYJ"},{"type":"text","value":" (see diagram above) can be thought of as a set of neurons whose weight matrix is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PWaXaFWm2L"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HrlHz5GpnK"},{"type":"text","value":", when the inputs (the integer character indeces) are one-hot encoded. Note aside, in order to embed a character, we are just going to simply index the table as it’s much faster:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nfqRvZUTV2"}],"key":"BU2q6AiVrY"}],"key":"uoeKDHvLU6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"C[5]","key":"KQAXBvnCCJ"},{"type":"outputs","id":"mmxq93uqsw_3dkKW0soO7","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.4713,  0.7868])","content_type":"text/plain"}}},"children":[],"key":"IIC0ZUBLiT"}],"key":"ODjB6e8tqs"}],"key":"LRZ3GcqBFN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"which is easy for a single character. But what if we want to index more simultaneously? That’s also easy:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z3hLQONORk"}],"key":"Ae7XEfi2uv"}],"key":"qwumtg2yGL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"C[[5, 6, 7]]","key":"hHx9Py2LDR"},{"type":"outputs","id":"eCQMv22CzFrR-NNPo9XAf","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":15,"metadata":{},"data":{"text/plain":{"content":"tensor([[-0.4713,  0.7868],\n        [-0.3284, -0.4330],\n        [ 1.3729,  2.9334]])","content_type":"text/plain"}}},"children":[],"key":"IaypeXbUTa"}],"key":"F77nCIMDLl"}],"key":"JOxxlZskcQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Cool! You can actually index a PyTorch tensor with a list or another tensor. Therefore, to easily get all character embeddings, we can simply do:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SrxyJ5WvMu"}],"key":"WtGwBU52LI"}],"key":"x6RTgc4PHq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"emb = C[x]\nprint(emb)\nprint(emb.shape)","key":"xuzQA0OivB"},{"type":"outputs","id":"xW2SvnClekUIVu7f9-6ac","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([[[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [-0.4713,  0.7868]],\n\n        [[ 1.5674, -0.2373],\n         [-0.4713,  0.7868],\n         [ 2.4448, -0.6701]],\n\n        [[-0.4713,  0.7868],\n         [ 2.4448, -0.6701],\n         [ 2.4448, -0.6701]],\n\n        [[ 2.4448, -0.6701],\n         [ 2.4448, -0.6701],\n         [-0.0274, -1.1008]],\n\n        [[ 2.4448, -0.6701],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [-1.0725,  0.7276]],\n\n        [[ 1.5674, -0.2373],\n         [-1.0725,  0.7276],\n         [-0.0707,  2.4968]],\n\n        [[-1.0725,  0.7276],\n         [-0.0707,  2.4968],\n         [ 0.6772, -0.8404]],\n\n        [[-0.0707,  2.4968],\n         [ 0.6772, -0.8404],\n         [-0.1158, -1.2078]],\n\n        [[ 0.6772, -0.8404],\n         [-0.1158, -1.2078],\n         [ 0.6772, -0.8404]],\n\n        [[-0.1158, -1.2078],\n         [ 0.6772, -0.8404],\n         [-0.0274, -1.1008]],\n\n        [[ 0.6772, -0.8404],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [-0.0274, -1.1008]],\n\n        [[ 1.5674, -0.2373],\n         [-0.0274, -1.1008],\n         [-0.1158, -1.2078]],\n\n        [[-0.0274, -1.1008],\n         [-0.1158, -1.2078],\n         [-0.0274, -1.1008]],\n\n        [[-0.1158, -1.2078],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [ 0.6772, -0.8404]],\n\n        [[ 1.5674, -0.2373],\n         [ 0.6772, -0.8404],\n         [ 0.1476, -1.0006]],\n\n        [[ 0.6772, -0.8404],\n         [ 0.1476, -1.0006],\n         [-0.0274, -1.1008]],\n\n        [[ 0.1476, -1.0006],\n         [-0.0274, -1.1008],\n         [ 0.2859, -0.0296]],\n\n        [[-0.0274, -1.1008],\n         [ 0.2859, -0.0296],\n         [-0.4713,  0.7868]],\n\n        [[ 0.2859, -0.0296],\n         [-0.4713,  0.7868],\n         [-0.0707,  2.4968]],\n\n        [[-0.4713,  0.7868],\n         [-0.0707,  2.4968],\n         [-0.0707,  2.4968]],\n\n        [[-0.0707,  2.4968],\n         [-0.0707,  2.4968],\n         [-0.0274, -1.1008]],\n\n        [[-0.0707,  2.4968],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [ 0.1476, -1.0006]],\n\n        [[ 1.5674, -0.2373],\n         [ 0.1476, -1.0006],\n         [-1.0725,  0.7276]],\n\n        [[ 0.1476, -1.0006],\n         [-1.0725,  0.7276],\n         [ 0.0511,  1.3095]],\n\n        [[-1.0725,  0.7276],\n         [ 0.0511,  1.3095],\n         [ 1.5618, -1.6261]],\n\n        [[ 0.0511,  1.3095],\n         [ 1.5618, -1.6261],\n         [ 0.6772, -0.8404]],\n\n        [[ 1.5618, -1.6261],\n         [ 0.6772, -0.8404],\n         [-0.0274, -1.1008]],\n\n        [[ 0.6772, -0.8404],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]]])\ntorch.Size([32, 3, 2])\n"},"children":[],"key":"L6XWOVMcFO"}],"key":"wwCl2rh3oj"}],"key":"twHdRLijpm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Notice the shape: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y0gw0f4qN9"},{"type":"inlineCode","value":"[\u003cnumber of character input sets\u003e, \u003cinput size\u003e, \u003cnumber of character embedding dimensions\u003e]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CcZigVloiF"},{"type":"text","value":". Indexing as following, we can assert that both ways of representation are valid:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IzDaZz1sf2"}],"key":"h95JHBWYsc"}],"key":"aJrCkwskFE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert emb[13, 2].equal(C[x[13, 2]])","key":"iJ7ddO0fKY"},{"type":"outputs","id":"x22gPDmiq5AlcKB8X5pD0","children":[],"key":"VtsiO59X2m"}],"key":"LegP6QLpFg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Long story short, PyTorch indexing is awesome and tensors such as embedding tables can be indexed by other tensors, e.g. inputs. One last thing, as far as the first layer is concerned. Since each embedding of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XqtJ7jqswU"},{"type":"inlineCode","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D0Su8C4n5n"},{"type":"text","value":" inputs has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l9LVa5TFwv"},{"type":"inlineCode","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dHOBaQXTxH"},{"type":"text","value":" dimensions, the output dimension of our first layer is basically ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qlrFSQBR2Q"},{"type":"inlineMath","value":"3 \\cdot 2 = 6","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e3\u003c/mn\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e6\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e3 \\cdot 2 = 6\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e3\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e2\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e6\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"aRdqcVsauh"},{"type":"text","value":". Usually, a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QpKt3Q0XMT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MGhGLrIfXs"}],"key":"V8VW3eW850"},{"type":"text","value":" layer is described by a pair of input and output dimensions. The input dimension of our first, embeddings layer is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F6tkSb2JWr"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XxUgsk4F7I"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G6m2D5I2rk"},{"type":"inlineCode","value":"\u003cnumber of character inputs\u003e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"igUF3aIJKu"},{"type":"text","value":"). To get the output dimension we have to concatenate the following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"by3pxIthLs"},{"type":"inlineCode","value":"\u003cinputs size\u003e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U9whbaAajw"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w4279ZLSRv"},{"type":"inlineCode","value":"\u003cnumber of character embedding dimensions\u003e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UVgpoPxd2v"},{"type":"text","value":" tensor dimensions into one dimension:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eeEBH8UBGZ"}],"key":"EHSCsJnqU8"}],"key":"ouRcIN80pn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"last_dims = emb.shape[1:]  # get dimensions after the first one\nlast_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\nemb_proper = emb.view(-1, last_dims_product)\nemb_proper.shape  # tada!","key":"A4PQbpt9OM"},{"type":"outputs","id":"HR8YbC5YG7utPtJcltWdz","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":18,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 6])","content_type":"text/plain"}}},"children":[],"key":"nmFgvZ7wLT"}],"key":"gcxlc9B8wc"}],"key":"wR7Ud9N0Dw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We have prepared the dimensionality of the first layer. Now, let’s implement the hidden, second layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oBtSUdPEt5"}],"key":"PeWRKiEOK6"}],"key":"RFRylnRvrR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"l0out = emb_proper.shape[-1]\nl1in = l0out\nl1out = 100  # neurons of hidden layer\nw1 = torch.randn(l1in, l1out, generator=g)\nb1 = torch.randn(l1out, generator=g)\nprint(w1.shape)\nprint(b1.shape)","key":"cxGz96UDUs"},{"type":"outputs","id":"4lDy1CbRKXk0z1yZzfTsw","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([6, 100])\ntorch.Size([100])\n"},"children":[],"key":"ZxipA0LMjR"}],"key":"gqLpfiVXuS"}],"key":"drjhghWXeE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"h = torch.tanh(emb_proper @ w1 + b1)\nh","key":"ubvdkNzhjP"},{"type":"outputs","id":"HuSazjmXOgO6_GwZjrsql","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":20,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0.2797,  0.9997,  0.7675,  ...,  0.9929,  0.9992,  0.9981],\n        [-0.9960,  1.0000, -0.8694,  ..., -0.5159, -1.0000, -0.0069],\n        [-0.9968,  1.0000,  0.9878,  ...,  0.4976, -0.9297, -0.8616],\n        ...,\n        [-0.9043,  1.0000,  0.9868,  ..., -0.7859, -0.4819,  0.9981],\n        [-0.9048,  1.0000,  0.9553,  ...,  0.9866,  1.0000,  0.9907],\n        [-0.9868,  1.0000,  0.5264,  ...,  0.9843,  0.0223, -0.1655]])","content_type":"text/plain"}}},"children":[],"key":"bk9dwd92dd"}],"key":"TId8NnpaY5"}],"key":"DrvSOLvtG5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"h.shape","key":"tnr74hZBF4"},{"type":"outputs","id":"f2JVa5EdVWJshintv_WzL","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":21,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 100])","content_type":"text/plain"}}},"children":[],"key":"se6ulfkXel"}],"key":"WNG9Pg3Pn1"}],"key":"HQtDBFabD7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Done! And now, to create the output layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z3Vn7xXxYW"}],"key":"xJw1WbucUt"}],"key":"hiqXvo8nsA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"l2in = l1out\nl2out = 27  # number of characters\nw2 = torch.randn(l2in, l2out, generator=g)\nb2 = torch.randn(l2out, generator=g)\nprint(w2.shape)\nprint(b2.shape)","key":"zo7FxSEn6T"},{"type":"outputs","id":"ql9vlG_s560N3pVs4lGss","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([100, 27])\ntorch.Size([27])\n"},"children":[],"key":"VVhQwLaPMZ"}],"key":"w7QmieGppb"}],"key":"SRHd0LFJ8x"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits = h @ w2 + b2\nlogits.shape","key":"dSRwPEjFV1"},{"type":"outputs","id":"h_t6TEGRCsV_EWSgQaodI","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":23,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 27])","content_type":"text/plain"}}},"children":[],"key":"AuzOXMMmZf"}],"key":"xcQOoywOSc"}],"key":"JEzKbuU68S"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exactly as we saw in the previous lesson, we want to take these logits and we want to first exponentiate them to get our fake counts. Then, we want to normalize them to get the probabilities of how likely it is for each character to come next:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CoZZN7GScQ"}],"key":"Q2eyAqLtZA"}],"key":"iO8btmFnYG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"counts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nprob.shape","key":"Hk8RHITO3l"},{"type":"outputs","id":"NKtLyPh22gSeWOMNauhRr","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":24,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 27])","content_type":"text/plain"}}},"children":[],"key":"J2hoF8gFBD"}],"key":"YuQXO9t29s"}],"key":"xsXPa9yCqa"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Remember, we also have the the target values ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TaKbpblNyM"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HSHqq7ap0O"},{"type":"text","value":", the actual characters that come next that we would like our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D3tSoYdwzJ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XFMPLFuzHh"}],"key":"q4NO9MLt4V"},{"type":"text","value":" to be able to predict:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ndewMzCu1Y"}],"key":"Gfa69gg6iy"}],"key":"uB2MEHQ7OY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"y","key":"aLOKJkB1AA"},{"type":"outputs","id":"sEorjMtEYBngjmd82hD7s","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":25,"metadata":{},"data":{"text/plain":{"content":"tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])","content_type":"text/plain"}}},"children":[],"key":"AAxsEBMO6b"}],"key":"kBTdAgnEHY"}],"key":"bGjLZZ0ami"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, what we would like to do now is index into the rows of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iJj3xcixmk"},{"type":"inlineCode","value":"prob","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A1Lv8mVcQ7"},{"type":"text","value":" and for each row to pluck out the probability given to the correct character:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rMgRQ2OBeq"}],"key":"eAtfsXmzXC"}],"key":"Wm5Q9BxB6p"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"prob[range(len(y)), y]","key":"AdbhqndFQB"},{"type":"outputs","id":"AUfXOmEasFi-6EqtxMwQK","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":26,"metadata":{},"data":{"text/plain":{"content":"tensor([9.9994e-12, 1.9647e-08, 4.0322e-07, 3.0845e-09, 4.6517e-11, 7.4238e-12,\n        2.0297e-09, 9.9179e-01, 1.7138e-02, 3.2410e-03, 2.8552e-06, 1.0565e-06,\n        2.6391e-09, 4.1804e-06, 3.5843e-08, 7.7737e-07, 3.5022e-02, 2.7425e-10,\n        1.7086e-08, 6.3572e-02, 1.1315e-08, 1.6961e-09, 2.1885e-11, 1.5201e-10,\n        1.0528e-03, 3.6704e-08, 9.5847e-02, 3.1954e-12, 8.5695e-17, 2.5576e-03,\n        9.1782e-12, 1.0565e-06])","content_type":"text/plain"}}},"children":[],"key":"bubuJkfWDX"}],"key":"NasbgPDEqh"}],"key":"hVCAydHj04"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This gives the current probabilities for these specific correct, target characters that come next after each character sequence, given the current ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zcAt0IeETf"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FnqOqGXAk9"}],"key":"CziMMNrDaj"},{"type":"text","value":" configuration (weights and biases). Currently these probabilities are pretty bad and most characters are pretty unlikely to occur next. Of course, we haven’t trained the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"svXNHlENCK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tDuR2vOUX0"}],"key":"FfGU7edD8G"},{"type":"text","value":" yet. So, we want to train it so that each probability approximates ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aZrTjNR6Wr"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SlzakKkh3y"},{"type":"text","value":". As we saw previously, to do so, we have to define the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JBC50s56mU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qsIV49OiUo"}],"key":"tmibXT1c2E"},{"type":"text","value":" and then minimize it:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EzcwrAtlzN"}],"key":"WB6Xkpuios"}],"key":"MeHQywhQ2X"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss = -prob[range(len(y)), y].log().mean()\nloss","key":"hZmUcx9QFX"},{"type":"outputs","id":"7wtdiT-9OIiMP8SA6tPOG","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":27,"metadata":{},"data":{"text/plain":{"content":"tensor(16.0342)","content_type":"text/plain"}}},"children":[],"key":"us8PXV5OlE"}],"key":"WOk300ba27"}],"key":"GG0AYr5QuG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Pretty big loss! Haha. Now we will minimize it so our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rpCs4gQl1Z"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jB2fQcZDRt"}],"key":"lsJ0WKHtz2"},{"type":"text","value":" able to predict the next character in each sequence correctly. To do so, we have to optimize the parameters. Let’s define a function that defines them and collects them all into a list just so we have easy access:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YYTyevcQen"}],"key":"MCRIH09xpd"}],"key":"E7GEdfU36u"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import random\n\n# context length: how many characters do we take to predict the next one?\nblock_size = 3\n\n\n# build the dataset\ndef build_dataset(words):\n    X, Y = [], []\n    for w in words:\n        # print(w)\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            X.append(context)\n            Y.append(ix)\n            # print(''.join(itos[i] for i in context), '---\u003e', itos[ix])\n            context = context[1:] + [ix]  # crop and append\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\n\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\n\ndef define_nn(l1out=100, embsize=2):\n    global C, w1, b1, w2, b2\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((27, embsize), generator=g)\n    l1in = embsize * block_size\n    # l1out: neurons of hidden layer\n    w1 = torch.randn(l1in, l1out, generator=g)\n    b1 = torch.randn(l1out, generator=g)\n    l2in = l1out\n    l2out = 27  # neurons of output layer, number of characters\n    w2 = torch.randn(l2in, l2out, generator=g)\n    b2 = torch.randn(l2out, generator=g)\n    parameters = [C, w1, b1, w2, b2]\n    return parameters\n\n\nparameters = define_nn()\nsum(p.nelement() for p in parameters)","key":"L03YL6Jiyb"},{"type":"outputs","id":"bennTUAXbm82fKJ2LiCZT","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n"},"children":[],"key":"hgtZ6Ytwsu"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":28,"metadata":{},"data":{"text/plain":{"content":"3481","content_type":"text/plain"}}},"children":[],"key":"WewMRTCRWe"}],"key":"EXyFVEGSVS"}],"key":"x0aOSjL7zz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To recap the forward pass:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EvsEYl6AHu"}],"key":"S0k0MdVXE6"}],"key":"BRdKoI2ouc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"emb = C[x]  # [32, 3, 2]\nlast_dims = emb.shape[1:]  # get dimensions after the first one\nlast_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\nemb_proper = emb.view(-1, last_dims_product)  # [32, 6]\nh = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]\nlogits = h @ w2 + b2  # [32, 27]\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nloss = -prob[range(len(y)), y].log().mean()\nloss","key":"oyzQumS73v"},{"type":"outputs","id":"52Sl6R8je95adS719PPWI","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":29,"metadata":{},"data":{"text/plain":{"content":"tensor(16.0342)","content_type":"text/plain"}}},"children":[],"key":"cIvxXCtdf8"}],"key":"TTal7zuhXD"}],"key":"RNZoPdHcLE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"A better and more efficient way to calculate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dTao8M4gUj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RBvXMnW56x"}],"key":"q4NhF7LGS5"},{"type":"text","value":" from logits and targets is through the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eZVSVq3nyZ"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"cross entropy loss function","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A5tTyDGwpH"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html","key":"m5EBOIcutP"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mhT0xnpSNj"}],"key":"wFMkXGIOtb"}],"key":"U5niBFPaSL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"F.cross_entropy(logits, y)","key":"ewxDpONyA2"},{"type":"outputs","id":"VaD0XuY2FCHZWCZOvEbHG","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":30,"metadata":{},"data":{"text/plain":{"content":"tensor(16.0342)","content_type":"text/plain"}}},"children":[],"key":"zMn87LKJ3m"}],"key":"cKxvTzJ4a1"}],"key":"WWSfaFjfz6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s tidy up the forward pass:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OxE58j23Q6"}],"key":"uV57KfaAf4"}],"key":"GzdY2sJuhd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward_pass(x, y):\n    emb = C[x]  # [32, 3, 2]\n    last_dims = emb.shape[1:]  # get dimensions after the first one\n    last_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\n    emb_proper = emb.view(-1, last_dims_product)  # [32, 6]\n    h = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]\n    logits = h @ w2 + b2  # [32, 27]\n    loss = F.cross_entropy(logits, y)\n    return loss","key":"kZp3uZgq3B"},{"type":"outputs","id":"UekCWVq_qgaFlGbuPG2Nj","children":[],"key":"oINpE92Qvu"}],"key":"mg8GfbNqmF"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Training the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cwfq9ZBxPX"}],"identifier":"training-the-model","label":"Training the model","html_id":"training-the-model","implicit":true,"key":"OObKQLzyM9"}],"key":"PEMzM9JzdT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And now, let’s train our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MRvqIxE3e5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SE3xuFEA8l"}],"key":"QNBpYHgwdm"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OGpTxZlehV"}],"key":"lWOq2bigfC"}],"key":"W0FREMMmtM"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def train(x, y, epochs=10):\n    parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for _ in range(epochs):\n        loss = forward_pass(x, y)\n        print(loss.item())\n        for p in parameters:\n            p.grad = None\n        loss.backward()\n        for p in parameters:\n            p.data += -0.1 * p.grad","visibility":"show","key":"vARSyVIcED"},{"type":"outputs","id":"NLK9ZxQMl7KGnTyIvz_zT","children":[],"visibility":"show","key":"K6Mo5QuJ9F"}],"visibility":"show","key":"FH2ida8OOh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x, y)","key":"oLbDiNYUKJ"},{"type":"outputs","id":"-1jo6gUeE7K9BFTKMU9yx","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"16.034189224243164\n11.740133285522461\n9.195296287536621\n7.302927017211914\n5.805147647857666\n4.850736618041992\n4.184849739074707\n3.644319534301758\n3.207334518432617\n2.843700885772705\n"},"children":[],"key":"KptnyIVvEB"}],"key":"mMQyV1mYYW"}],"key":"tvXprFRBif"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BbBfCcuczf"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"juDTsMp8P6"}],"key":"fhws115zFG"},{"type":"text","value":" keeps decreasing, which means that the training process is working! Now, since we are only training using a dataset of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Obbbe42acW"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q7KaTnVnNZ"},{"type":"text","value":" words, and since our parameters are many more than the samples we are training on, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nLZ1nUi81p"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rjleiyEwa6"}],"key":"H24M1bq6it"},{"type":"text","value":" is probably overfitting. What we have to do now, is train on the whole dataset.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dSMZZ9URHm"}],"key":"wIzXv384qE"}],"key":"KNGS3e1rp1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"x_all, y_all = build_dataset(words)","key":"Cdp8gY0tyw"},{"type":"outputs","id":"Zo4YYQNFXS22HZvgVgfMH","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([228146, 3]) torch.Size([228146])\n"},"children":[],"key":"QwCztZLEwH"}],"key":"EHESb3hXoE"}],"key":"LJK7nCWT5k"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all)","key":"o1QedHHFYP"},{"type":"outputs","id":"mcY3SA64SeQtHkcejs5na","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"19.505229949951172\n17.084487915039062\n15.776533126831055\n14.83334732055664\n14.002612113952637\n13.253267288208008\n12.579923629760742\n11.983108520507812\n11.470502853393555\n11.05186653137207\n"},"children":[],"key":"LrMO3lbRHK"}],"key":"msPe3wbSKa"}],"key":"ICPEn1m4XJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Same, the loss for all input samples also keeps decreasing. But, you’ll notice that training takes longer now. This is happening because we are doing a lot of work, forward and backward passing on ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RWzbDytzur"},{"type":"text","value":"228146","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NZAby7vJdg"},{"type":"text","value":" examples. That’s way too much work! In practice, what people usually do in such cases is they train on minibatches of the whole dataset. So, what we want to do, is we want to randomly select some portion of the dataset, and that’s a minibatch! And then, only forward, backward and update on that minibatch, likewise iterate and train on those minibatches. A simple way to implement minibatching is to set a batch size, e.g.:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XooDXvGEas"}],"key":"NnuJFxFSiF"}],"key":"PphZRw1Tjo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"batchsize = 32","key":"tFCjeyosWm"},{"type":"outputs","id":"r7HqiA56Qif3gK9aD1sSv","children":[],"key":"goAySB70LM"}],"key":"FcTCZBF3YY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"and then to randomly select ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WSoEWZ0RWS"},{"type":"inlineCode","value":"batchsize","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DWq0yJf9eW"},{"type":"text","value":" number of indeces referencing the subset of input data to be used for minibatch training. To get the indeces you can do something like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JoR5yhNC7a"}],"key":"c70yKybCYR"}],"key":"QaDnOsbXjK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"batchix = torch.randint(0, x_all.shape[0], (batchsize,))\nprint(batchix)","key":"eGq3O6OOXA"},{"type":"outputs","id":"FQXxp4r1yTOo4eAZttgQz","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([ 74679, 122216,  57092, 133769, 226181,  38045, 126099,  23446, 218663,\n         17662, 225764, 199486, 185049,  64041, 217855, 198821, 192633,  84825,\n         44722,  46171, 182390,  99196, 102624,    409, 168159, 182770, 142590,\n        173184,  86521,   1596, 158516, 206175])\n"},"children":[],"key":"FQQxwnppCS"}],"key":"dmgVhmdfX4"}],"key":"V5elJDmAjP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then, to actually get a minibatch per epoch, just create a new, random set of indeces and index the samples and targets from the dataset before each forward pass. Like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lv53EPxBbR"}],"key":"z359akxCqB"}],"key":"tW6UPgg6R2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(x, y, lr=0.1, epochs=10, print_all_losses=True):\n    parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for _ in range(epochs):\n        batchix = torch.randint(0, x.shape[0], (batchsize,))\n        bx, by = x[batchix], y[batchix]\n        loss = forward_pass(bx, by)\n        if print_all_losses:\n            print(loss.item())\n        for p in parameters:\n            p.grad = None\n        loss.backward()\n        for p in parameters:\n            p.data += -lr * p.grad\n    if not print_all_losses:\n        print(loss.item())\n    return loss.item()","key":"Zbpxv5Rkhy"},{"type":"outputs","id":"ULNAEPPF3XzvddUmibdRM","children":[],"key":"wWpAV75gYn"}],"key":"D3aBbGeoYu"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, if we train using minibatches...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gWS2L13jS1"}],"key":"EwDAB3kBSN"}],"key":"L9pSETl5IX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all)","key":"t6ENcPPLjt"},{"type":"outputs","id":"ZZUH2CeV0MtRZwt65zsn8","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"17.94289779663086\n15.889695167541504\n15.060649871826172\n13.832050323486328\n16.023155212402344\n14.010979652404785\n16.336170196533203\n13.788375854492188\n11.292967796325684\n13.045702934265137\n"},"children":[],"key":"IvyIQkVc4X"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"13.045702934265137","content_type":"text/plain"}}},"children":[],"key":"VYczmdWWIZ"}],"key":"mFFxR4AK6l"}],"key":"qqx3PtIUbo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"training is much much faster, almost instant! However, since we are dealing with minibatches, the quality of our gradient is lower, so the direction is not as reliable. It’s not the actual exact gradient direction, but the gradient direction is good enough even though it’s being estimated for only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yo7YPjdp3R"},{"type":"inlineCode","value":"batchsize","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VaU16d9wRk"},{"type":"text","value":" (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vKD2S2PBEq"},{"type":"inlineCode","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BraLcI4JOl"},{"type":"text","value":") examples. In general, it is better to have an approximate gradient and just make more steps than it is to compute the exact gradient and take fewer steps. And that is why in practice, minibatching works quite well.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HrY46hA32f"}],"key":"FuOtDNueRv"}],"key":"CFbUBJ7qV7"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Finding a good learning rate","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e3JbDVWSOV"}],"identifier":"finding-a-good-learning-rate","label":"Finding a good learning rate","html_id":"finding-a-good-learning-rate","implicit":true,"key":"oUAIto5QrE"}],"key":"DiZS8oJVFU"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, one issue that has popped up as you may have noticed, is that during minibatch training the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O4kABJALum"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"srgHS0NBxg"}],"key":"gHEcV9OurW"},{"type":"text","value":" seems to fluctuate. For some epochs it decreases, but then it increases again, and vice versa. That question that arises from this observation is this: are we stepping too slow or too fast? Meaning, are we updating the parameters by a fraction of their gradients that is too small or too large? Such magnitude is determined by the step size, aka the learning rate. Therefore, the overarching question is: how do you determine this learning rate? How do we gain confidence that we are stepping with the right speed? Let’s see one way to determine the learning rate. We basically want to find a reasonable search range, if you will. What people usually do is they pick different learning rate values until they find a satisfactory one. Let’s try to find one that is better. We see for example if it is very small:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IlFkvYVn6K"}],"key":"car6GXtmAq"}],"key":"tq0dkgTUPt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=0.0001, epochs=100)","key":"bUX5DhERnW"},{"type":"outputs","id":"bGrpeSeWIW9_MYzgFATlR","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"21.48345375061035\n17.693952560424805\n20.993595123291016\n19.23766326904297\n17.807458877563477\n19.426368713378906\n20.208740234375\n21.426673889160156\n20.107091903686523\n16.44301986694336\n16.98691749572754\n16.693296432495117\n16.654979705810547\n18.207632064819336\n20.281587600708008\n19.277870178222656\n19.782976150512695\n20.056819915771484\n19.198200225830078\n15.863028526306152\n18.550344467163086\n19.435653686523438\n19.682138442993164\n17.305164337158203\n21.181236267089844\n19.23027992248535\n18.67540168762207\n18.95297622680664\n21.35270881652832\n21.46781349182129\n21.018564224243164\n20.318994522094727\n21.243608474731445\n19.62767791748047\n19.476289749145508\n17.74656867980957\n20.23328399658203\n20.085819244384766\n16.801542282104492\n18.122915267944336\n19.09043312072754\n19.84799575805664\n20.199235916137695\n16.658361434936523\n19.510778427124023\n19.398319244384766\n18.517004013061523\n19.53419303894043\n22.490541458129883\n20.45920753479004\n17.721420288085938\n18.58787727355957\n20.76034927368164\n20.696556091308594\n18.54053497314453\n19.546337127685547\n15.577354431152344\n18.100522994995117\n15.600821495056152\n21.15610122680664\n20.79819107055664\n18.512712478637695\n17.394367218017578\n15.756057739257812\n21.389039993286133\n16.85922622680664\n13.484357833862305\n19.010683059692383\n18.83637046813965\n19.841796875\n18.28095054626465\n20.777664184570312\n19.818172454833984\n18.778358459472656\n20.82563591003418\n19.217248916625977\n18.208587646484375\n19.463356018066406\n16.181228637695312\n16.927345275878906\n18.849687576293945\n19.017803192138672\n18.24212074279785\n20.15293312072754\n19.38414764404297\n19.442598342895508\n22.70920181274414\n19.071269989013672\n17.25360679626465\n16.035856246948242\n19.327434539794922\n20.848506927490234\n19.198562622070312\n18.62538719177246\n20.031288146972656\n22.616220474243164\n18.733247756958008\n20.26487159729004\n18.593149185180664\n24.60611343383789\n"},"children":[],"key":"pvM8T4743L"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":40,"metadata":{},"data":{"text/plain":{"content":"24.60611343383789","content_type":"text/plain"}}},"children":[],"key":"gyZArApV60"}],"key":"oYKY6obT7y"}],"key":"l9lgrdQOXD"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The loss barely decreases. So this value is too low. Let’s try something bigger, e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qDASRiiE8Y"}],"key":"palx15TaWk"}],"key":"BdBc7qdd1x"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=0.001, epochs=100)","key":"VSJSIAtvBV"},{"type":"outputs","id":"YwanthjGC6gI_9N-gDBWb","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"20.843355178833008\n21.689167022705078\n19.067970275878906\n17.899921417236328\n20.831283569335938\n21.015792846679688\n21.317489624023438\n18.584075927734375\n19.30166244506836\n17.16819190979004\n20.15867805480957\n18.009313583374023\n22.017822265625\n18.148967742919922\n17.53749656677246\n19.633005142211914\n17.75607681274414\n17.64638900756836\n18.511669158935547\n20.743165969848633\n18.751705169677734\n18.893756866455078\n19.241785049438477\n19.387001037597656\n18.413681030273438\n20.803842544555664\n18.513309478759766\n20.150976181030273\n19.927082061767578\n20.02385711669922\n19.09459686279297\n16.731922149658203\n19.16921615600586\n19.426868438720703\n17.71548843383789\n17.51811408996582\n18.333765029907227\n20.96685028076172\n19.99691390991211\n19.345508575439453\n19.923913955688477\n16.887836456298828\n17.372751235961914\n18.805681228637695\n18.897857666015625\n16.86487579345703\n17.781234741210938\n20.2587833404541\n17.451217651367188\n18.460481643676758\n17.9292049407959\n20.8989315032959\n20.129817962646484\n17.018564224243164\n19.071075439453125\n15.609376907348633\n18.350452423095703\n14.199233055114746\n18.659013748168945\n17.954235076904297\n17.826528549194336\n18.58924102783203\n17.669662475585938\n16.46000862121582\n15.66697883605957\n17.6021785736084\n17.65107536315918\n16.883989334106445\n14.59417724609375\n16.17646026611328\n18.381986618041992\n19.10284423828125\n15.856918334960938\n18.458749771118164\n18.598033905029297\n17.683555603027344\n17.749269485473633\n17.12112808227539\n20.2098445892334\n18.316301345825195\n16.487417221069336\n14.472514152526855\n16.50566864013672\n19.501144409179688\n18.444271087646484\n17.818748474121094\n12.876835823059082\n17.16472816467285\n15.761727333068848\n18.426593780517578\n17.760990142822266\n18.603355407714844\n16.690837860107422\n16.553945541381836\n15.75294303894043\n17.358163833618164\n16.67896842956543\n17.08140754699707\n18.213592529296875\n17.534658432006836\n"},"children":[],"key":"qvBaQQtYUU"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/plain":{"content":"17.534658432006836","content_type":"text/plain"}}},"children":[],"key":"pZWlxSNs6T"}],"key":"GZGi8cQQG4"}],"key":"IJWJ6gYaqz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This is ok, but still not good enough. The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yo1WsmEQPd"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cTzTOEKHps"}],"key":"ZqWC6fHvkD"},{"type":"text","value":" value decreases but not steadily and fluctuates a lot. For an even bigger value:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CSKv7zO2ps"}],"key":"wJ3y2hsQlK"}],"key":"Mpj9z4qqNv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=1, epochs=100)","key":"ijZRhzp9ep"},{"type":"outputs","id":"nSJFHxxNF210vvmpktqjo","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"18.0602970123291\n15.639816284179688\n14.974983215332031\n16.521020889282227\n10.994373321533203\n19.934722900390625\n13.230535507202148\n10.555554389953613\n11.227700233459473\n8.685490608215332\n11.373151779174805\n9.253009796142578\n8.64297866821289\n9.108452796936035\n7.178742408752441\n9.831550598144531\n5.762266635894775\n9.50097370147705\n10.957695960998535\n12.328742980957031\n8.328937530517578\n8.131488800048828\n11.364215850830078\n9.899446487426758\n7.836635589599609\n8.942032814025879\n11.561779022216797\n9.97337532043457\n8.2869291305542\n9.61953067779541\n9.630435943603516\n14.119514465332031\n12.620526313781738\n8.802383422851562\n8.957738876342773\n11.694437980651855\n16.162137985229492\n10.493476867675781\n7.7210798263549805\n10.860843658447266\n8.748751640319824\n13.449786186218262\n10.955209732055664\n8.923118591308594\n6.181601047515869\n8.725625991821289\n6.119848251342773\n11.221086502075195\n8.663549423217773\n9.03221607208252\n8.159632682800293\n11.553065299987793\n7.1041059494018555\n6.436527729034424\n9.19931697845459\n6.504988670349121\n8.564536094665527\n6.59806489944458\n8.718829154968262\n7.369975566864014\n11.306722640991211\n10.493293762207031\n7.680598735809326\n8.20093059539795\n7.427743911743164\n7.3400983810424805\n8.856118202209473\n7.980756759643555\n11.46378231048584\n8.093060493469238\n9.521681785583496\n6.227016925811768\n8.569214820861816\n8.454265594482422\n7.388335227966309\n6.649340629577637\n7.111802101135254\n7.661591053009033\n12.89154052734375\n8.51455020904541\n5.992252349853516\n6.762502193450928\n6.146595478057861\n8.050479888916016\n8.089849472045898\n7.87835168838501\n7.628716945648193\n7.732893943786621\n6.767331600189209\n8.324596405029297\n8.824007987976074\n8.258061408996582\n7.636016368865967\n6.856623649597168\n6.543000221252441\n7.319474697113037\n5.69791841506958\n5.777251243591309\n6.574363708496094\n5.4569573402404785\n"},"children":[],"key":"fVmVtAy2ie"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":42,"metadata":{},"data":{"text/plain":{"content":"5.4569573402404785","content_type":"text/plain"}}},"children":[],"key":"ufYkYgxxic"}],"key":"isfbKDcJ9y"}],"key":"LEHMHhHROR"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Better! How about:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XFeqMSRyKL"}],"key":"a3JDGhDBQF"}],"key":"NgbnsKwkis"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=10, epochs=100)","key":"cZKVBT7cRZ"},{"type":"outputs","id":"fVYUHzUxB50b5L-yUCfk_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"19.74797821044922\n38.09200668334961\n46.314208984375\n40.89773941040039\n73.4378433227539\n58.34128952026367\n51.356876373291016\n55.045501708984375\n54.94329833984375\n56.30958557128906\n70.37184143066406\n56.63667678833008\n56.90707778930664\n45.661888122558594\n50.97232437133789\n55.65245819091797\n57.46098327636719\n78.44556427001953\n70.66008758544922\n51.70524978637695\n48.533164978027344\n80.0494613647461\n90.79659271240234\n61.892642974853516\n52.87863540649414\n41.49900436401367\n63.885189056396484\n69.60615539550781\n60.386714935302734\n76.09366607666016\n40.18576431274414\n53.72964096069336\n48.1932373046875\n46.865108489990234\n48.253753662109375\n53.61216735839844\n77.9145278930664\n75.54542541503906\n65.61190795898438\n78.13446044921875\n83.6716537475586\n79.6883773803711\n59.334083557128906\n74.78559875488281\n50.28561782836914\n53.59624099731445\n35.096195220947266\n50.16322326660156\n73.99742889404297\n86.66049194335938\n70.05807495117188\n78.18916320800781\n48.637943267822266\n77.84318542480469\n56.17559051513672\n44.09672164916992\n70.90714263916016\n79.0201187133789\n67.89301300048828\n65.17256927490234\n68.24624633789062\n63.97649383544922\n90.05917358398438\n91.45114135742188\n60.47791290283203\n70.57051086425781\n57.64970397949219\n44.6708984375\n54.10292053222656\n60.48087692260742\n59.21522903442383\n51.96377944946289\n53.79441452026367\n63.579402923583984\n65.1745376586914\n54.898189544677734\n50.91022872924805\n55.830299377441406\n47.503177642822266\n56.56501770019531\n46.5484504699707\n43.91749954223633\n50.70798110961914\n48.224388122558594\n69.06616973876953\n62.38393020629883\n53.78395080566406\n61.84634780883789\n55.61307907104492\n48.13108825683594\n55.1087532043457\n62.52896499633789\n52.36894226074219\n52.819580078125\n73.38019561767578\n87.60235595703125\n78.37958526611328\n61.38961410522461\n65.26103973388672\n70.43557739257812\n"},"children":[],"key":"jlHvFCRKTJ"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":43,"metadata":{},"data":{"text/plain":{"content":"70.43557739257812","content_type":"text/plain"}}},"children":[],"key":"jIAbyfpLUJ"}],"key":"kbkIMm5ODO"}],"key":"KdvpuJpiN7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Haha, no thanks. So, we know that a satisfactory learning rate lies between ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kQN4mq7V7K"},{"type":"inlineCode","value":"0.001","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KhgXJv5nCT"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q4J0w2KL47"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uqsuLYS50J"},{"type":"text","value":". To find it, we can lay these numbers out, exponentially separated:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vk3rkczZnE"}],"key":"m3fvAahoz4"}],"key":"c4MViynYrZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"step = 1000\nlre = torch.linspace(-3, 0, step)\nlrs = 10**lre\nlrs","key":"WFZhTmvOx6"},{"type":"outputs","id":"WqpoDkYUiQjPVlB52c_Cm","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":44,"metadata":{},"data":{"text/plain":{"content":"tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n        1.0000])","content_type":"text/plain"}}},"children":[],"key":"SadiJHxPIW"}],"key":"HISPHEPQVM"}],"key":"Q0iG6YtGt6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"then, we plot the loss for each learning rate:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uf9ImtZq7U"}],"key":"FbaaFQPpuR"}],"key":"hwt9HYoya1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn()\nlrei = []\nlossi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(1000):\n    batchix = torch.randint(0, x_all.shape[0], (batchsize,))\n    bx, by = x_all[batchix], y_all[batchix]\n    loss = forward_pass(bx, by)\n    # print(loss.item())\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    lr = lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n    lrei.append(lre[i])\n    lossi.append(loss.item())\nplt.figure()\nplt.plot(lrei, lossi)","key":"PMWTjupg3P"},{"type":"outputs","id":"jIdayz1ad4zOD63nLnRR2","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"e9e22b74e1844fd4b84c5f82be7c853c\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"cf4563592fce8c1c14bc482165ba7665","path":"/build/cf4563592fce8c1c14bc482165ba7665.png"},"text/html":{"content_type":"text/html","hash":"9812705dc3b5ead7acfa74e727626910","path":"/build/9812705dc3b5ead7acfa74e727626910.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"U68N1PCEhG"}],"key":"dgYFFlgUJm"}],"key":"STWFk5VRKm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here, we see the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SlXoAoZX7E"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t2e4PK3cPk"}],"key":"Gp1EgBPceA"},{"type":"text","value":" dropping as the exponent of the learning rate starts to increase, then, after a learning rate of around ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QPSqw4TqQ7"},{"type":"inlineCode","value":"0.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e49gVp1ukA"},{"type":"text","value":", the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zje6lNuxnj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oSLLDgFJht"}],"key":"REmsHJDYMC"},{"type":"text","value":" starts to increase. A good rule of thumb is to pick a learning rate whose at a point around which the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mDBm41lJ06"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OWygXvQb8A"}],"key":"Lp8fSwog9o"},{"type":"text","value":" is the lowest and most stable, before any increasing tendency. Let’s pick the learning rate whose exponent corresponds to the lowest ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pzG668AZwt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lNCcm9uuT8"}],"key":"t7xWgO8gkv"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BRY2gmth1s"}],"key":"nRfMmRNdok"}],"key":"q4RPsz7Mqd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lr = 10**lrei[lossi.index(min(lossi))]\nlr","key":"zi6f6IwmuY"},{"type":"outputs","id":"mDbLE9wgBnsaJ2vo1Nzo_","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":46,"metadata":{},"data":{"text/plain":{"content":"tensor(0.2309)","content_type":"text/plain"}}},"children":[],"key":"zZV6sD2WZ6"}],"key":"yJOt9PaUUN"}],"key":"GFktbJVGXz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now we have some confidence that this is a fairly good learning rate. Now let’s train for many epochs using this new learning rate!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l4hpDeslC8"}],"key":"MM0QZGMVqL"}],"key":"xweJIh4M34"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"_ = train(x_all, y_all, lr=lr, epochs=10000, print_all_losses=False)","key":"t7Y9EJ0qUS"},{"type":"outputs","id":"6PmjsRMm4YQ50gDsWxG-5","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"2.5242083072662354\n"},"children":[],"key":"SYEaVsexnc"}],"key":"ngDVEsuMxx"}],"key":"Yo0PKazDRY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Nice. We got a much smaller ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"trJABuGmtA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ty5d5i2D96"}],"key":"Ti1dgIgEYA"},{"type":"text","value":" after training. We have dramatically improved on the bigram language model, using this simple ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o3lcvCji6f"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pfUgDY9poH"}],"key":"bybC3nz28m"},{"type":"text","value":" of only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mhnLIaQ0ni"},{"type":"text","value":"3481","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UPONjbrbd7"},{"type":"text","value":" parameters. Now, there’s something we have to be careful with. Although our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jycHBLehK7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kPh41Rjd4I"}],"key":"NhPHNCMis5"},{"type":"text","value":" is the lowest so far, it is not ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xbt6Akx1eL"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"exactly","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FMlC5msbtH"}],"key":"UHwUDxdOtW"},{"type":"text","value":" true to say that we now have a better model. The reason is that this is actually a very small model. Even though these kind of models can get much larger by adding more and more parameters, e.g. with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Pdz4ET9WTi"},{"type":"text","value":"10000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"roy4WFrErE"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QmWHQIxEzQ"},{"type":"text","value":"100000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gyI3LfXlqO"},{"type":"text","value":" or a million parameters, as the capacity of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r7KeQfLENo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"imBvTIBoTv"}],"key":"bem8DVizDm"},{"type":"text","value":" grows, it becomes more and more capable of overfitting your training set. What that means is that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nzk9fJFKbp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tNFCB7LvTe"}],"key":"Lb2uPguiaN"},{"type":"text","value":" on the training set (the data that you are training on), will become very very low. As low as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SLGonUWRTU"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gKQp6vYfZD"},{"type":"text","value":". But in such a case, all that the model is doing is memorizing your training set exactly, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pmpGdwu8OK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"verbatim","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SUvCnp0605"}],"key":"oXj2PdUlrA"},{"type":"text","value":". So, if you were to take this model and it’s working very well, but you try to sample from it, you will only get examples, exactly as they are in the training set. You won’t get any new data. In addition to that, if you try to evaluate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wd4lkFdhum"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oWUPqg1t3B"}],"key":"p2V4Xbuv81"},{"type":"text","value":" on some withheld names or other input data (e.g. words), you will actually see that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vWm1mQ9vKR"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nTmXaE3FXn"}],"key":"iSVFatW2AJ"},{"type":"text","value":" of those will be very high. And so it is in practice basically not a very good model, since it doesn’t generalize. So, it is standard in the field to split up the dataset into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sQgVkWbArs"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aOUrWpHTut"},{"type":"text","value":" splits, as we call them: the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pRZ57OHSx1"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pW2SbdFE4U"}],"key":"JdX9tmCCzq"},{"type":"text","value":" split (roughly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iBc57HjhIg"},{"type":"inlineMath","value":"80\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e80\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e80\\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e80%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"qDTlTHJZlf"},{"type":"text","value":" of data), the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FKoPmgYIIh"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dev","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I8mBktt1wA"}],"key":"ybvczTeUek"},{"type":"text","value":"/","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QXf3QhFabW"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"validation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uaH9TvCbPr"}],"key":"cmlUlzGLP6"},{"type":"text","value":" split (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JnwkLXasB3"},{"type":"inlineMath","value":"10\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e10\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e10\\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e10%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"mw0kG0J3CJ"},{"type":"text","value":") and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LcsBaoEqmd"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"test","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L80o8AK7I6"}],"key":"WMkzlxwPiF"},{"type":"text","value":" split (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Neg2Zt1qpZ"},{"type":"inlineMath","value":"10\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e10\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e10\\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e10%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"yPccRTbmtM"},{"type":"text","value":").","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oXGbElW3OQ"}],"key":"hv3mk0OABR"}],"key":"dnO2x9f45B"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# training split, dev/validation split, test split\n# 80%, 10%, 10%","key":"f11mblDUBb"},{"type":"outputs","id":"243VHiKytib0IWf0LKvkX","children":[],"key":"Rt5BfesTKf"}],"key":"mtdUuoU4rG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, the training split is used to optimize the parameters of the model (for training). The validation split is typically used for development and tuning over all the hyperparameters of the model, such as the learning rate, layer width, embedding size, regularization parameters, and other settings, in order to choose a combination that works best on this split. The test split is used to evaluate the performance of the model at the end (after training). So, we are only evaluating the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hFouX7zM6q"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DcSnyZdq5l"}],"key":"TLsvpWM3XR"},{"type":"text","value":" on the test split very very sparingly and very few times. Because, every single time you evaluate your test ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oth4ih0LQ3"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v6OgUqEZaD"}],"key":"Py9N3pK64n"},{"type":"text","value":" and you learn something from it, you are basically trying to also train on the test split. So, you are only allowed to evaluate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yZkQyMD7LH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I6PgFY7mBx"}],"key":"cuLkjiOwSz"},{"type":"text","value":" on the test dataset very few times, otherwise you risk overfitting to it as well, as you experiment on your model. Now, let’s actually split our dataset into training, validation and test datasets. Then, we are going to train on the training dataset and only evaluate on the test dataset very very sparingly. Here we go:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sq2gQ6L2Gn"}],"key":"zBme7EJdYO"}],"key":"h7MkDyaVBf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import random\n\nrandom.seed(42)\nrandom.shuffle(words)\nlenwords = len(words)\nn1 = int(0.8 * lenwords)\nn2 = int(0.9 * lenwords)\nprint(f\"{lenwords=}\")\nprint(f\"{n1} words in training set\")\nprint(f\"{n2 - n1} words in validation set\")\nprint(f\"{lenwords - n2} words in test\")","key":"T1VlSNLa89"},{"type":"outputs","id":"IN6LUMtjFyA1PLzYTtrN6","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"lenwords=32033\n25626 words in training set\n3203 words in validation set\n3204 words in test\n"},"children":[],"key":"EUu0OR8gI5"}],"key":"Czc47ty9iM"}],"key":"VrMtRvUkbs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xtrain, ytrain = build_dataset(words[:n1])\nxval, yval = build_dataset(words[n1:n2])\nxtest, ytest = build_dataset(words[n2:])","key":"pBqjaccXlv"},{"type":"outputs","id":"4v90o9slz1-j3drqEmxmR","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182580, 3]) torch.Size([182580])\ntorch.Size([22767, 3]) torch.Size([22767])\ntorch.Size([22799, 3]) torch.Size([22799])\n"},"children":[],"key":"ioEL72H1aD"}],"key":"zpntnW3VPH"}],"key":"uYKfD3Qn9Y"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We now have the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kSXaGKeJlQ"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o2bda1DdQf"},{"type":"text","value":" split sets. Great! Let’s now re-define our parameters train, anew, on the training dataset:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZUXHPad2tE"}],"key":"FdACMzHQ29"}],"key":"PdiHkK3X7h"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_train = train(xtrain, ytrain, lr=lr, epochs=30000, print_all_losses=False)","key":"SCjtmdlfLF"},{"type":"outputs","id":"k4acDEXCP8kRH_Nc_Vkcy","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"2.083573579788208\n"},"children":[],"key":"Ytnc7VWI32"}],"key":"yxCkbWThph"}],"key":"QuZnLLZcQP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome. Our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j5nam7CS42"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LjgcfIBZeY"}],"key":"V5IndJsSqS"},{"type":"text","value":" has been trained and the final ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U1rid6OWuz"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qBe6QXgu2d"}],"key":"nRQNbb8FgT"},{"type":"text","value":" is actually surprisingly good. Let’s now evaluate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RMVDG9RNqI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vVwB4oZPqO"}],"key":"QZhDlE2qRq"},{"type":"text","value":" of the validation set (remember, this data was not in the training set on which it was trained):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x7Sqm15t96"}],"key":"yh0vbdAxbv"}],"key":"iHQTjNvYD1"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_val = forward_pass(xval, yval)\nloss_val","key":"kaSA7hFrkO"},{"type":"outputs","id":"kZPxtF5OBFGbL7XhtKnrr","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":52,"metadata":{},"data":{"text/plain":{"content":"tensor(2.4294, grad_fn=\u003cNllLossBackward0\u003e)","content_type":"text/plain"}}},"children":[],"key":"RRvKMq60QZ"}],"key":"dbTpnuC6uA"}],"key":"UbMn1Ncsgq"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Not too bad! Now, as you can see, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j8DFJlZjQC"},{"type":"inlineCode","value":"loss_train","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WalUIWoD9g"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mdXqqEIrnt"},{"type":"inlineCode","value":"loss_val","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SJkgWRr46B"},{"type":"text","value":" are pretty close. In fact, they are roughly equal. This means that we are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V7jBhmlRny"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"not","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qjYznKWlOW"}],"key":"oa57AEtxAL"},{"type":"text","value":" overfitting, but underfitting. It seems that this model is not powerful enough so as not to be purely memorizing the data. Basically, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GO11b1cm53"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D1eMBlKdbO"}],"key":"DlTjrr3A5S"},{"type":"text","value":" is very tiny. But, we can expect to make performance improvements by scaling up the size of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xUOLsHUkCT"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j25gFCScTx"}],"key":"ZaRNjSflfh"},{"type":"text","value":". The easiest way to do this is to redefine our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yxLQZbS8NQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WaPxFHrOP5"}],"key":"XaTiUJeWk0"},{"type":"text","value":" with more neurons in the hidden layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fP40utLFPW"}],"key":"AyBRFSs254"}],"key":"z9xlKJ1WMR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(l1out=300)","key":"u7GvA0RUQs"},{"type":"outputs","id":"smSndMH0NmgidO48ZFsDP","children":[],"key":"KNWm5yJUrF"}],"key":"oh2PA1BvGa"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then, let’s re-train and visualize the loss curve:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T7CYlL5FYA"}],"key":"QmFOG3NbM8"}],"key":"VXBmoYBN9t"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi = []\nstepi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(30000):\n    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))\n    bx, by = xtrain[batchix], ytrain[batchix]\n    loss = forward_pass(bx, by)\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    for p in parameters:\n        p.data += -lr * p.grad\n    stepi.append(i)\n    lossi.append(loss.log10().item())\nplt.figure()\nplt.plot(stepi, lossi)","key":"cSAtFpMuqq"},{"type":"outputs","id":"_0EKpw3oFBNCFukWCupx1","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"7435f13fbd754dadbafc577840424b04\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"2cbad2c6c621b032a7b473cbccdde177","path":"/build/2cbad2c6c621b032a7b473cbccdde177.png"},"text/html":{"content_type":"text/html","hash":"d90e7262aaa68b89d50c492cd5d117be","path":"/build/d90e7262aaa68b89d50c492cd5d117be.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"TxZ9M1hcQ1"}],"key":"vYWTvtia9y"}],"key":"srFuigAZpX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, it is a bit noisy, but that is just because of the minibatches!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GaDvtZ75lm"}],"key":"kbRDiFaLEi"}],"key":"UBnTd5gXje"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_train = forward_pass(xtrain, ytrain)\nloss_val = forward_pass(xval, yval)\nprint(loss_train)\nprint(loss_val)","key":"viG9d0op7Y"},{"type":"outputs","id":"FvjJ1fb7aUlCUr1qrtxPE","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(2.4805, grad_fn=\u003cNllLossBackward0\u003e)\ntensor(2.4808, grad_fn=\u003cNllLossBackward0\u003e)\n"},"children":[],"key":"sbRGIPeAYz"}],"key":"Mw8ULcLLep"}],"key":"b3Yb9wNR9J"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome, the training loss is actually lower than before, whereas the validation loss is pretty much the same. So, increasing the size of the hidden layer gave us some benefit. Let’s experiment more to see if we can get even lower losses by increasing the embedding layer. First though, let’s visualize the character embeddings:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pRrGwMkrvR"}],"key":"eg79YS2CWB"}],"key":"pd4DM2inou"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# visualize dimensions 0 and 1 of the embedding matrix C for all characters\nplt.figure(figsize=(8, 8))\nplt.scatter(C[:, 0].data, C[:, 1].data, s=200)\nfor i in range(C.shape[0]):\n    plt.text(\n        C[i, 0].item(),\n        C[i, 1].item(),\n        itoc[i],\n        ha=\"center\",\n        va=\"center\",\n        color=\"white\",\n    )\nplt.grid(\"minor\")","key":"KaHXj1TSHS"},{"type":"outputs","id":"q_AxNi0CiVMg9y0EBuf6y","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9e5b59f480324492897003bc6557de7b\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"8c426aae1269dc28fa53e703a7eaae9f","path":"/build/8c426aae1269dc28fa53e703a7eaae9f.png"},"text/html":{"content_type":"text/html","hash":"ac1d5d2a7f7cd24ad610bfc9c41fdf7f","path":"/build/ac1d5d2a7f7cd24ad610bfc9c41fdf7f.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"aQse7Yp4aQ"}],"key":"V4dSpBReEc"}],"key":"oj4CZ1AfvK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The network has basically learned to separate out the characters and cluster them a little bit. For example, it has learned some characters are usually found more closer together than others. Let’s try to improve our model loss by choosing a greater embeddings layer size of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XO7WwLK2Au"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DXVsVK3h8L"},{"type":"text","value":" and by increasing the number of epochs to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jkLLdPGXzj"},{"type":"text","value":"200000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kuOrnp5UBz"},{"type":"text","value":", also we’ll decay the learning rate after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Bjzd3j6fur"},{"type":"text","value":"100000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u6OF7mgxcZ"},{"type":"text","value":" epochs:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uTNzz3HyH4"}],"key":"nCr0NJYG04"}],"key":"itmdKepvez"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(l1out=200, embsize=10)","key":"GnNbnX3pBM"},{"type":"outputs","id":"L3rreTPMAssGU2U3OVpzJ","children":[],"key":"uAk2kVTA5u"}],"key":"MWZS1TOulr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi = []\nstepi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(200000):\n    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))\n    bx, by = xtrain[batchix], ytrain[batchix]\n    loss = forward_pass(bx, by)\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    lr = 0.1 if i \u003c 100000 else 0.01\n    for p in parameters:\n        p.data += -lr * p.grad\n    stepi.append(i)\n    lossi.append(loss.log10().item())\nplt.figure()\nplt.plot(stepi, lossi);","key":"zJK8zYpxqd"},{"type":"outputs","id":"Gzh5vANTfA1-NVq7hyJGq","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8278e481a5184d788204c09648462766\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"1a716447d3751b1ef4e3d97328f400b4","path":"/build/1a716447d3751b1ef4e3d97328f400b4.png"},"text/html":{"content_type":"text/html","hash":"69e928816945882ac57b7cf5a3ff305e","path":"/build/69e928816945882ac57b7cf5a3ff305e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"UintKxyD50"}],"key":"LX4suwgRX8"}],"key":"Slr5gCMDwG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_train = forward_pass(xtrain, ytrain)\nloss_val = forward_pass(xval, yval)\nprint(loss_train)\nprint(loss_val)","key":"C3aoqZZviu"},{"type":"outputs","id":"Z8WVhsG3iaaq1yDuUpLR4","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(2.1189, grad_fn=\u003cNllLossBackward0\u003e)\ntensor(2.1583, grad_fn=\u003cNllLossBackward0\u003e)\n"},"children":[],"key":"L9o690daug"}],"key":"pEMaFWKPPG"}],"key":"MTWDKXTy7J"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Wow, we did it! Both train and validation losses are lower now. Can we go lower? Play around and find out! Now, before we end this lesson, let’s sample from our model:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yBMHYaKXFR"}],"key":"ehVfVT7GEq"}],"key":"KmnXOrbmMs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\nfor _ in range(20):    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        emb = C[torch.tensor([context])] # (1,block_size,d)\n        h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n        logits = h @ w2 + b2\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join(itoc[i] for i in out))","key":"FzzZ8QrOyD"},{"type":"outputs","id":"N_OoGi4yfo7rqaulJCbU5","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"mona.\nkayah.\nsee.\nmed.\nryla.\nremmastendrael.\nazeer.\nmelin.\nshivonna.\nkeisen.\nanaraelynn.\nhotelin.\nshaber.\nshiriel.\nkinze.\njenslenter.\nfius.\nkavder.\nyaralyeha.\nkayshayton.\n"},"children":[],"key":"Zq5fh5zPgl"}],"key":"fU1jsP37Y6"}],"key":"vqWyi8XMbu"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Outro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y6RRDUnUkW"}],"identifier":"outro","label":"Outro","html_id":"outro","implicit":true,"key":"VoOc8ZCHkn"}],"key":"D06BRO6gKN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, our model now is pretty decent and able to produce suprisingly name-like text, which is what we wanted all along! Next up, we will explore ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ufMwzZW36j"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dCrIJA8JBe"}],"key":"Kdvjf0hxmP"},{"type":"text","value":" internals and other such magic.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nyz0mPF6hj"}],"key":"lEnVLmS9UX"}],"key":"dvV9Yb86gi"}],"key":"YAj5gvq0dn"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"2. makemore (part 1): implementing a bigram character-level language model","url":"/micrograduate/makemore1","group":"microgra∇uate"},"next":{"title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","url":"/micrograduate/makemore3","group":"microgra∇uate"}}},"domain":"http://localhost:3000"},"project":{"title":"microgra∇uate","github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","toc":[{"file":"index.md"},{"file":"micrograduate/micrograd.ipynb"},{"file":"micrograduate/makemore1.ipynb"},{"file":"micrograduate/makemore2.ipynb"},{"file":"micrograduate/makemore3.ipynb"},{"file":"micrograduate/makemore4.ipynb"},{"file":"micrograduate/makemore5.ipynb"},{"file":"micrograduate/picogpt.ipynb"}],"thumbnail":"/build/heading-2992afcd5615ae46f403e88bcb8569f4.png","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"micrograduate.micrograd","title":"1. micrograd: implementing an autograd engine","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore1","title":"2. makemore (part 1): implementing a bigram character-level language model","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore2","title":"3. makemore (part 2): mlp","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore3","title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore4","title":"5. makemore (part 4): becoming a backprop ninja","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore5","title":"6. makemore (part 5): building a WaveNet","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.picogpt","title":"7. picoGPT: implementing a tiny GPT from scratch","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-7B0967AD.js";
import * as route0 from "/build/root-EDJFWIEV.js";
import * as route1 from "/build/routes/$-AD65NCUT.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/build/entry.client-PCJPW7TK.js");</script></body></html>