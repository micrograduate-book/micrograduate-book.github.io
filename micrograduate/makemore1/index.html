<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>2. makemore (part 1): implementing a bigram character-level language model - microgra∇uate</title><meta property="og:title" content="2. makemore (part 1): implementing a bigram character-level language model - microgra∇uate"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><meta name="image" content="/build/heading-2d149a320da7264b9eda93edc721b9e5.png"/><meta property="og:image" content="/build/heading-2d149a320da7264b9eda93edc721b9e5.png"/><link rel="stylesheet" href="/build/_assets/app-OIHP3NGU.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="myst-skip-to-article fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article content</a></div><dialog id="myst-no-css" style="position:fixed;left:0px;top:0px;width:100vw;height:100vh;font-size:4rem;padding:1rem;color:black;background:white"><strong>Site not loading correctly?</strong><p>This may be due to an incorrect <code>BASE_URL</code> configuration. See<!-- --> <a href="https://mystmd.org/guide/deployment#deploy-base-url">the MyST Documentation</a> <!-- -->for reference.</p><script>
    (() => {
            // Test for has-styling variable set by the MyST stylesheet
            const node = document.currentScript.parentNode;
            const hasCSS = window.getComputedStyle(node).getPropertyValue("--has-styling");
            if (hasCSS === ""){
                    node.showModal();
            }

    })()
</script></dialog><div class="myst-top-nav bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="myst-top-nav-bar flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="myst-top-nav-menu-button flex items-center justify-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100 w-10 h-10"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="myst-home-link flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><div class="myst-home-link-logo mr-3 flex items-center dark:bg-white dark:rounded px-1"><img src="/build/logo-1eba642f581799c20c3dfbf015138027.png" class="h-9" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5 sr-only">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R75cp:" data-state="closed" class="myst-search-bar flex items-center h-10 aspect-square sm:w-64 text-left text-gray-600 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 myst-search-bar-disabled hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="myst-search-text-placeholder hidden sm:block grow">Search</span><div aria-hidden="true" class="myst-search-shortcut items-center hidden mx-1 font-mono text-sm text-gray-600 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="myst-theme-button theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-10 h-10 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-moon-icon h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-sun-icon h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="myst-primary-sidebar fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="myst-primary-sidebar-pointer pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="myst-primary-sidebar-nav flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="myst-primary-sidebar-topnav overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="myst-primary-sidebar-toc flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="myst-toc w-full px-1 dark:text-white"><a title="microgra∇uate" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/">microgra∇uate</a><a title="1. micrograd: implementing an autograd engine" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/micrograd">1. micrograd: implementing an autograd engine</a><a title="2. makemore (part 1): implementing a bigram character-level language model" aria-current="page" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg myst-toc-item-exact bg-blue-300/30 active" href="/micrograduate/makemore1">2. makemore (part 1): implementing a bigram character-level language model</a><a title="3. makemore (part 2): mlp" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore2">3. makemore (part 2): mlp</a><a title="4. makemore (part 3): activations &amp; gradients, batchnorm" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore3">4. makemore (part 3): activations &amp; gradients, batchnorm</a><a title="5. makemore (part 4): becoming a backprop ninja" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore4">5. makemore (part 4): becoming a backprop ninja</a><a title="6. makemore (part 5): building a WaveNet" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/makemore5">6. makemore (part 5): building a WaveNet</a><a title="7. picoGPT: implementing a tiny GPT from scratch" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/micrograduate/picogpt">7. picoGPT: implementing a tiny GPT from scratch</a></div></nav></div><div class="myst-primary-sidebar-footer flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="myst-made-with-myst flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="myst-fm-block mb-8 pt-9"><div class="myst-fm-block-header flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><div class="myst-fm-block-badges"><a href="https://github.com/ckaraneen/micrograduate" title="GitHub Repository: ckaraneen/micrograduate" target="_blank" rel="noopener noreferrer" class="myst-fm-github-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="myst-fm-github-icon inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a></div><a href="https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore1.ipynb" title="Edit This Page" target="_blank" rel="noopener noreferrer" class="myst-fm-edit-link text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-edit-icon inline-block mr-1 opacity-60 hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="m16.862 4.487 1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L10.582 16.07a4.5 4.5 0 0 1-1.897 1.13L6 18l.8-2.685a4.5 4.5 0 0 1 1.13-1.897l8.932-8.931Zm0 0L19.5 7.125M18 14v4.75A2.25 2.25 0 0 1 15.75 21H5.25A2.25 2.25 0 0 1 3 18.75V8.25A2.25 2.25 0 0 1 5.25 6H10"></path></svg></a><div class="myst-fm-downloads-dropdown relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="myst-fm-downloads-button relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8ucp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-downloads-icon"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="myst-fm-block-title mb-0">2. makemore (part 1): implementing a bigram character-level language model</h1></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="PR0lQCo9X7" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import sys

IN_COLAB = &quot;google.colab&quot; in sys.modules
if IN_COLAB:
    print(&quot;Cloning repo...&quot;)
    !git clone --quiet https://github.com/ckaraneen/micrograduate.git &gt; /dev/null
    %cd micrograduate
    print(&quot;Installing requirements...&quot;)
    !pip install --quiet uv
    !uv pip install --system --quiet -r requirements.txt</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="IVFxpASLFlf6YAhrrmvsc" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="z3704UKTtT" class="myst-jp-nb-block relative group/block"><h2 id="intro" class="relative group"><span class="heading-text">Intro</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#intro" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="DbPN8qSVRn" class="myst-jp-nb-block relative group/block"><p>Just like <a class="link" href="/micrograduate/makemore1#1.-micrograd">micrograd</a> before it, here, <em>step-by-step</em> with everything <em>spelled-out</em>, we will build <strong>makemore</strong>: a bigram character-level language model. We’re going to build it out slowly and together! But what is <strong>makemore</strong>? As the name suggests, <strong>makemore</strong> <em>makes more</em> of things that you give it. <a class="link" href="/build/names-9e8297c91106d2ea4113125b84bb5653.txt">names.txt</a> is an example dataset. Specifically, it is a very large list of different names. If you train <strong>makemore</strong> on this dataset, it will learn to <em>make more</em> of name-like things, basically more unique names! So, maybe if you have a baby and you’re looking for a new, cool-sounding unique name, <strong>makemore</strong> might help you. Here are some examples of such names that the <strong>makemore</strong> will be able to generate:</p></div><div id="juPuo0MyU3" class="myst-jp-nb-block relative group/block"><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-text" style="white-space:pre">dontell
khylum
camatena
aeriline
najlah
sherrith
ryel
irmi
taislee
mortaz
akarli
maxfelynn
biolett
zendy
laisa
halliliana
goralynn
brodynn
romima
chiyomin
loghlyn
melichae
mahmed
irot
helicha
besdy
ebokun
lucianno</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div></div><div id="mjCxKF0y8o" class="myst-jp-nb-block relative group/block"><p><code>dontell</code>, <code>irot</code>, <code>zendy</code>, and so on, you name it! So under the hood, <strong>makemore</strong> is a character-level language model. That means that it’s treating every single line (i.e. name) of <a class="link" href="/build/names-9e8297c91106d2ea4113125b84bb5653.txt">its training dataset</a> as an example. And each example is treated as a sequence of individual characters. For instance, it treats the name <code>reese</code> as the sequence of characters: <code>r</code>, <code>e</code>, <code>e</code>, <code>s</code>, <code>e</code>. That is the level on which we are building out <strong>makemore</strong>. Basically, its purpose is this: given a character, it can predict the next character in the sequence based upon the names that it has seen so far. Now, we’re actually going to implement a large number of character-level language models, following a few key innovations:</p><ul><li><p>Bigram (one character predicts the next one with a lookup table of counts)</p></li><li><p>MLP, following <a target="_blank" rel="noreferrer" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" class="link">Bengio et al. 2003<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a></p></li><li><p>CNN, following <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1609.03499" class="link">DeepMind WaveNet 2016<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a> (in progress...)</p></li><li><p>RNN, following <a target="_blank" rel="noreferrer" href="https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" class="link">Mikolov et al. 2010<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a></p></li><li><p>LSTM, following <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1308.0850" class="link">Graves et al. 2014<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a></p></li><li><p>GRU, following <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1409.1259" class="link">Kyunghyun Cho et al. 2014<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a></p></li><li><p>Transformer, following <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1706.03762" class="link">Vaswani et al. 2017<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a></p></li></ul><p>In fact, the transformer we are going to build will be the equivalent of <a href="https://en.wikipedia.org/wiki/GPT-2" class="hover-link" target="_blank" rel="noreferrer" data-state="closed">GPT-2</a>. Kind of a big deal, since it’s a modern network and by the end of this guide you’ll actually understand how it works at the level of characters. Later on, we will probably spend some time on the word level, so we can generate documents of words, not just segments of characters. And then we’re probably going to go into image and image-text networks such as <a href="https://en.wikipedia.org/wiki/DALL-E" class="hover-link" target="_blank" rel="noreferrer" data-state="closed">DALL-E</a>, <a href="https://en.wikipedia.org/wiki/Stable_Diffusion" class="hover-link" target="_blank" rel="noreferrer" data-state="closed">Stable Diffusion</a>, and so on. But first, let’s jump into character-level modeling.</p></div><div id="nrDG3HVjNT" class="myst-jp-nb-block relative group/block"><h2 id="building-a-bigram-language-model" class="relative group"><span class="heading-text">Building a bigram language model</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#building-a-bigram-language-model" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="l2joLahAwd" class="myst-jp-nb-block relative group/block"><p>Let’s start by reading all the names into a list:</p></div><div id="oUx8oR0ePr" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">words = open(&quot;names.txt&quot;).read().splitlines()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="GmT3ZU9IoarEKvGoSZMJn" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="pnDN0cs5lS" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">words[:10]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="KKCTerrwjmmaqRKCz8bNh" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>[&#x27;emma&#x27;,
 &#x27;olivia&#x27;,
 &#x27;ava&#x27;,
 &#x27;isabella&#x27;,
 &#x27;sophia&#x27;,
 &#x27;charlotte&#x27;,
 &#x27;mia&#x27;,
 &#x27;amelia&#x27;,
 &#x27;harper&#x27;,
 &#x27;evelyn&#x27;]</span></code></div></div></div><div id="pC22aCtNbs" class="myst-jp-nb-block relative group/block"><p>Now, we want to learn a bit more about this dataset.</p></div><div id="o2U4fjN9cW" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">len(words)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="jEqaHrXM0yjy2q2f5iOqf" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>32033</span></code></div></div></div><div id="pXrSh6Eh3z" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">len(min(words, key=len))  # shortest</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="KCjxlA5ykA37FzSUURpLC" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>2</span></code></div></div></div><div id="S6nEZRauVW" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">len(max(words, key=len))  # longest</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="lIZV2S6-5Ar7fgn44vsAX" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>15</span></code></div></div></div><div id="V2iUJ654av" class="myst-jp-nb-block relative group/block"><p>Let’s think through our very first language model. A character-level language model is predicting the next character in the sequence given already some concrete sequence of characters before it. What we have to realize here is that every single word like <code>isabella</code> is actually quite a few examples packed in that single word. Because, let’s think: what is a word telling us really? It’s saying that the character <code>i</code> is a very likely character to come first in the sequence that constitutes a name. The character <code>s</code> is likely to come after <code>i</code>, the character <code>a</code> is likely to come after <code>is</code>, the character <code>b</code> is likely to come after <code>isa</code>, and so on all the way to <code>a</code> following <code>isabell</code>. And then there’s one more important piece of information in here. And that is that after <code>isabella</code>, the word is very likely to end. So, time to build our first network: a bigram language model. In these, we are working with two characters at a time. So, we are only looking for one character we are given and we are trying to predict the next character in a sequence. For example, in the name <code>charlotte</code>, we ask: what characters are likely to follow <code>r</code>?  In the name <code>sophia</code>: we ask what characters are likely to follow <code>p</code>? And so on. This mean we are just modeling that local structure. Meaning, we only look at the previous character, even though there might be a lot of useful information before it. This is a very simple model, which is why it’s a great place to start! We can learn about the statistics of which characters are likely to follow which other characters by counting. So by iterating over all names, we can count how often each consecutive pair (bigram) of characters appears.</p></div><div id="ZGaVgUUwgs" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">b = {}
for w in words:
    chs = [&quot;.&quot;] + list(w) + [&quot;.&quot;]
    for ch1, ch2 in zip(chs, chs[1:]):
        bigram = (ch1, ch2)
        b[bigram] = b.get(bigram, 0) + 1</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="GJlLPJZOYq6tKfguRa8Dz" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="F1isFITMRr" class="myst-jp-nb-block relative group/block"><p>Notice that we have also added the character <code>.</code> to signify the start and end of each word. And obviously, the variable <code>b</code> now holds the statistics of the entire dataset.</p></div><div id="LIqJCNeuxw" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">sorted(b.items(), key=lambda tup: tup[1], reverse=True)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="BDxWUmSVI12Iktj1vk-pB" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>[((&#x27;n&#x27;, &#x27;.&#x27;), 6763),
 ((&#x27;a&#x27;, &#x27;.&#x27;), 6640),
 ((&#x27;a&#x27;, &#x27;n&#x27;), 5438),
 ((&#x27;.&#x27;, &#x27;a&#x27;), 4410),
 ((&#x27;e&#x27;, &#x27;.&#x27;), 3983),
 ((&#x27;a&#x27;, &#x27;r&#x27;), 3264),
 ((&#x27;e&#x27;, &#x27;l&#x27;), 3248),
 ((&#x27;r&#x27;, &#x27;i&#x27;), 3033),
 ((&#x27;n&#x27;, &#x27;a&#x27;), 2977),
 ((&#x27;.&#x27;, &#x27;k&#x27;), 2963),
 ((&#x27;l&#x27;, &#x27;e&#x27;), 2921),
 ((&#x27;e&#x27;, &#x27;n&#x27;), 2675),
 ((&#x27;l&#x27;, &#x27;a&#x27;), 2623),
 ((&#x27;m&#x27;, &#x27;a&#x27;), 2590),
 ((&#x27;.&#x27;, &#x27;m&#x27;), 2538),
 ((&#x27;a&#x27;, &#x27;l&#x27;), 2528),
 ((&#x27;i&#x27;, &#x27;.&#x27;), 2489),
 ((&#x27;l&#x27;, &#x27;i&#x27;), 2480),
 ((&#x27;i&#x27;, &#x27;a&#x27;), 2445),
 ((&#x27;.&#x27;, &#x27;j&#x27;), 2422),
 ((&#x27;o&#x27;, &#x27;n&#x27;), 2411),
 ((&#x27;h&#x27;, &#x27;.&#x27;), 2409),
 ((&#x27;r&#x27;, &#x27;a&#x27;), 2356),
 ((&#x27;a&#x27;, &#x27;h&#x27;), 2332),
 ((&#x27;h&#x27;, &#x27;a&#x27;), 2244),
 ((&#x27;y&#x27;, &#x27;a&#x27;), 2143),
 ((&#x27;i&#x27;, &#x27;n&#x27;), 2126),
 ((&#x27;.&#x27;, &#x27;s&#x27;), 2055),
 ((&#x27;a&#x27;, &#x27;y&#x27;), 2050),
 ((&#x27;y&#x27;, &#x27;.&#x27;), 2007),
 ((&#x27;e&#x27;, &#x27;r&#x27;), 1958),
 ((&#x27;n&#x27;, &#x27;n&#x27;), 1906),
 ((&#x27;y&#x27;, &#x27;n&#x27;), 1826),
 ((&#x27;k&#x27;, &#x27;a&#x27;), 1731),
 ((&#x27;n&#x27;, &#x27;i&#x27;), 1725),
 ((&#x27;r&#x27;, &#x27;e&#x27;), 1697),
 ((&#x27;.&#x27;, &#x27;d&#x27;), 1690),
 ((&#x27;i&#x27;, &#x27;e&#x27;), 1653),
 ((&#x27;a&#x27;, &#x27;i&#x27;), 1650),
 ((&#x27;.&#x27;, &#x27;r&#x27;), 1639),
 ((&#x27;a&#x27;, &#x27;m&#x27;), 1634),
 ((&#x27;l&#x27;, &#x27;y&#x27;), 1588),
 ((&#x27;.&#x27;, &#x27;l&#x27;), 1572),
 ((&#x27;.&#x27;, &#x27;c&#x27;), 1542),
 ((&#x27;.&#x27;, &#x27;e&#x27;), 1531),
 ((&#x27;j&#x27;, &#x27;a&#x27;), 1473),
 ((&#x27;r&#x27;, &#x27;.&#x27;), 1377),
 ((&#x27;n&#x27;, &#x27;e&#x27;), 1359),
 ((&#x27;l&#x27;, &#x27;l&#x27;), 1345),
 ((&#x27;i&#x27;, &#x27;l&#x27;), 1345),
 ((&#x27;i&#x27;, &#x27;s&#x27;), 1316),
 ((&#x27;l&#x27;, &#x27;.&#x27;), 1314),
 ((&#x27;.&#x27;, &#x27;t&#x27;), 1308),
 ((&#x27;.&#x27;, &#x27;b&#x27;), 1306),
 ((&#x27;d&#x27;, &#x27;a&#x27;), 1303),
 ((&#x27;s&#x27;, &#x27;h&#x27;), 1285),
 ((&#x27;d&#x27;, &#x27;e&#x27;), 1283),
 ((&#x27;e&#x27;, &#x27;e&#x27;), 1271),
 ((&#x27;m&#x27;, &#x27;i&#x27;), 1256),
 ((&#x27;s&#x27;, &#x27;a&#x27;), 1201),
 ((&#x27;s&#x27;, &#x27;.&#x27;), 1169),
 ((&#x27;.&#x27;, &#x27;n&#x27;), 1146),
 ((&#x27;a&#x27;, &#x27;s&#x27;), 1118),
 ((&#x27;y&#x27;, &#x27;l&#x27;), 1104),
 ((&#x27;e&#x27;, &#x27;y&#x27;), 1070),
 ((&#x27;o&#x27;, &#x27;r&#x27;), 1059),
 ((&#x27;a&#x27;, &#x27;d&#x27;), 1042),
 ((&#x27;t&#x27;, &#x27;a&#x27;), 1027),
 ((&#x27;.&#x27;, &#x27;z&#x27;), 929),
 ((&#x27;v&#x27;, &#x27;i&#x27;), 911),
 ((&#x27;k&#x27;, &#x27;e&#x27;), 895),
 ((&#x27;s&#x27;, &#x27;e&#x27;), 884),
 ((&#x27;.&#x27;, &#x27;h&#x27;), 874),
 ((&#x27;r&#x27;, &#x27;o&#x27;), 869),
 ((&#x27;e&#x27;, &#x27;s&#x27;), 861),
 ((&#x27;z&#x27;, &#x27;a&#x27;), 860),
 ((&#x27;o&#x27;, &#x27;.&#x27;), 855),
 ((&#x27;i&#x27;, &#x27;r&#x27;), 849),
 ((&#x27;b&#x27;, &#x27;r&#x27;), 842),
 ((&#x27;a&#x27;, &#x27;v&#x27;), 834),
 ((&#x27;m&#x27;, &#x27;e&#x27;), 818),
 ((&#x27;e&#x27;, &#x27;i&#x27;), 818),
 ((&#x27;c&#x27;, &#x27;a&#x27;), 815),
 ((&#x27;i&#x27;, &#x27;y&#x27;), 779),
 ((&#x27;r&#x27;, &#x27;y&#x27;), 773),
 ((&#x27;e&#x27;, &#x27;m&#x27;), 769),
 ((&#x27;s&#x27;, &#x27;t&#x27;), 765),
 ((&#x27;h&#x27;, &#x27;i&#x27;), 729),
 ((&#x27;t&#x27;, &#x27;e&#x27;), 716),
 ((&#x27;n&#x27;, &#x27;d&#x27;), 704),
 ((&#x27;l&#x27;, &#x27;o&#x27;), 692),
 ((&#x27;a&#x27;, &#x27;e&#x27;), 692),
 ((&#x27;a&#x27;, &#x27;t&#x27;), 687),
 ((&#x27;s&#x27;, &#x27;i&#x27;), 684),
 ((&#x27;e&#x27;, &#x27;a&#x27;), 679),
 ((&#x27;d&#x27;, &#x27;i&#x27;), 674),
 ((&#x27;h&#x27;, &#x27;e&#x27;), 674),
 ((&#x27;.&#x27;, &#x27;g&#x27;), 669),
 ((&#x27;t&#x27;, &#x27;o&#x27;), 667),
 ((&#x27;c&#x27;, &#x27;h&#x27;), 664),
 ((&#x27;b&#x27;, &#x27;e&#x27;), 655),
 ((&#x27;t&#x27;, &#x27;h&#x27;), 647),
 ((&#x27;v&#x27;, &#x27;a&#x27;), 642),
 ((&#x27;o&#x27;, &#x27;l&#x27;), 619),
 ((&#x27;.&#x27;, &#x27;i&#x27;), 591),
 ((&#x27;i&#x27;, &#x27;o&#x27;), 588),
 ((&#x27;e&#x27;, &#x27;t&#x27;), 580),
 ((&#x27;v&#x27;, &#x27;e&#x27;), 568),
 ((&#x27;a&#x27;, &#x27;k&#x27;), 568),
 ((&#x27;a&#x27;, &#x27;a&#x27;), 556),
 ((&#x27;c&#x27;, &#x27;e&#x27;), 551),
 ((&#x27;a&#x27;, &#x27;b&#x27;), 541),
 ((&#x27;i&#x27;, &#x27;t&#x27;), 541),
 ((&#x27;.&#x27;, &#x27;y&#x27;), 535),
 ((&#x27;t&#x27;, &#x27;i&#x27;), 532),
 ((&#x27;s&#x27;, &#x27;o&#x27;), 531),
 ((&#x27;m&#x27;, &#x27;.&#x27;), 516),
 ((&#x27;d&#x27;, &#x27;.&#x27;), 516),
 ((&#x27;.&#x27;, &#x27;p&#x27;), 515),
 ((&#x27;i&#x27;, &#x27;c&#x27;), 509),
 ((&#x27;k&#x27;, &#x27;i&#x27;), 509),
 ((&#x27;o&#x27;, &#x27;s&#x27;), 504),
 ((&#x27;n&#x27;, &#x27;o&#x27;), 496),
 ((&#x27;t&#x27;, &#x27;.&#x27;), 483),
 ((&#x27;j&#x27;, &#x27;o&#x27;), 479),
 ((&#x27;u&#x27;, &#x27;s&#x27;), 474),
 ((&#x27;a&#x27;, &#x27;c&#x27;), 470),
 ((&#x27;n&#x27;, &#x27;y&#x27;), 465),
 ((&#x27;e&#x27;, &#x27;v&#x27;), 463),
 ((&#x27;s&#x27;, &#x27;s&#x27;), 461),
 ((&#x27;m&#x27;, &#x27;o&#x27;), 452),
 ((&#x27;i&#x27;, &#x27;k&#x27;), 445),
 ((&#x27;n&#x27;, &#x27;t&#x27;), 443),
 ((&#x27;i&#x27;, &#x27;d&#x27;), 440),
 ((&#x27;j&#x27;, &#x27;e&#x27;), 440),
 ((&#x27;a&#x27;, &#x27;z&#x27;), 435),
 ((&#x27;i&#x27;, &#x27;g&#x27;), 428),
 ((&#x27;i&#x27;, &#x27;m&#x27;), 427),
 ((&#x27;r&#x27;, &#x27;r&#x27;), 425),
 ((&#x27;d&#x27;, &#x27;r&#x27;), 424),
 ((&#x27;.&#x27;, &#x27;f&#x27;), 417),
 ((&#x27;u&#x27;, &#x27;r&#x27;), 414),
 ((&#x27;r&#x27;, &#x27;l&#x27;), 413),
 ((&#x27;y&#x27;, &#x27;s&#x27;), 401),
 ((&#x27;.&#x27;, &#x27;o&#x27;), 394),
 ((&#x27;e&#x27;, &#x27;d&#x27;), 384),
 ((&#x27;a&#x27;, &#x27;u&#x27;), 381),
 ((&#x27;c&#x27;, &#x27;o&#x27;), 380),
 ((&#x27;k&#x27;, &#x27;y&#x27;), 379),
 ((&#x27;d&#x27;, &#x27;o&#x27;), 378),
 ((&#x27;.&#x27;, &#x27;v&#x27;), 376),
 ((&#x27;t&#x27;, &#x27;t&#x27;), 374),
 ((&#x27;z&#x27;, &#x27;e&#x27;), 373),
 ((&#x27;z&#x27;, &#x27;i&#x27;), 364),
 ((&#x27;k&#x27;, &#x27;.&#x27;), 363),
 ((&#x27;g&#x27;, &#x27;h&#x27;), 360),
 ((&#x27;t&#x27;, &#x27;r&#x27;), 352),
 ((&#x27;k&#x27;, &#x27;o&#x27;), 344),
 ((&#x27;t&#x27;, &#x27;y&#x27;), 341),
 ((&#x27;g&#x27;, &#x27;e&#x27;), 334),
 ((&#x27;g&#x27;, &#x27;a&#x27;), 330),
 ((&#x27;l&#x27;, &#x27;u&#x27;), 324),
 ((&#x27;b&#x27;, &#x27;a&#x27;), 321),
 ((&#x27;d&#x27;, &#x27;y&#x27;), 317),
 ((&#x27;c&#x27;, &#x27;k&#x27;), 316),
 ((&#x27;.&#x27;, &#x27;w&#x27;), 307),
 ((&#x27;k&#x27;, &#x27;h&#x27;), 307),
 ((&#x27;u&#x27;, &#x27;l&#x27;), 301),
 ((&#x27;y&#x27;, &#x27;e&#x27;), 301),
 ((&#x27;y&#x27;, &#x27;r&#x27;), 291),
 ((&#x27;m&#x27;, &#x27;y&#x27;), 287),
 ((&#x27;h&#x27;, &#x27;o&#x27;), 287),
 ((&#x27;w&#x27;, &#x27;a&#x27;), 280),
 ((&#x27;s&#x27;, &#x27;l&#x27;), 279),
 ((&#x27;n&#x27;, &#x27;s&#x27;), 278),
 ((&#x27;i&#x27;, &#x27;z&#x27;), 277),
 ((&#x27;u&#x27;, &#x27;n&#x27;), 275),
 ((&#x27;o&#x27;, &#x27;u&#x27;), 275),
 ((&#x27;n&#x27;, &#x27;g&#x27;), 273),
 ((&#x27;y&#x27;, &#x27;d&#x27;), 272),
 ((&#x27;c&#x27;, &#x27;i&#x27;), 271),
 ((&#x27;y&#x27;, &#x27;o&#x27;), 271),
 ((&#x27;i&#x27;, &#x27;v&#x27;), 269),
 ((&#x27;e&#x27;, &#x27;o&#x27;), 269),
 ((&#x27;o&#x27;, &#x27;m&#x27;), 261),
 ((&#x27;r&#x27;, &#x27;u&#x27;), 252),
 ((&#x27;f&#x27;, &#x27;a&#x27;), 242),
 ((&#x27;b&#x27;, &#x27;i&#x27;), 217),
 ((&#x27;s&#x27;, &#x27;y&#x27;), 215),
 ((&#x27;n&#x27;, &#x27;c&#x27;), 213),
 ((&#x27;h&#x27;, &#x27;y&#x27;), 213),
 ((&#x27;p&#x27;, &#x27;a&#x27;), 209),
 ((&#x27;r&#x27;, &#x27;t&#x27;), 208),
 ((&#x27;q&#x27;, &#x27;u&#x27;), 206),
 ((&#x27;p&#x27;, &#x27;h&#x27;), 204),
 ((&#x27;h&#x27;, &#x27;r&#x27;), 204),
 ((&#x27;j&#x27;, &#x27;u&#x27;), 202),
 ((&#x27;g&#x27;, &#x27;r&#x27;), 201),
 ((&#x27;p&#x27;, &#x27;e&#x27;), 197),
 ((&#x27;n&#x27;, &#x27;l&#x27;), 195),
 ((&#x27;y&#x27;, &#x27;i&#x27;), 192),
 ((&#x27;g&#x27;, &#x27;i&#x27;), 190),
 ((&#x27;o&#x27;, &#x27;d&#x27;), 190),
 ((&#x27;r&#x27;, &#x27;s&#x27;), 190),
 ((&#x27;r&#x27;, &#x27;d&#x27;), 187),
 ((&#x27;h&#x27;, &#x27;l&#x27;), 185),
 ((&#x27;s&#x27;, &#x27;u&#x27;), 185),
 ((&#x27;a&#x27;, &#x27;x&#x27;), 182),
 ((&#x27;e&#x27;, &#x27;z&#x27;), 181),
 ((&#x27;e&#x27;, &#x27;k&#x27;), 178),
 ((&#x27;o&#x27;, &#x27;v&#x27;), 176),
 ((&#x27;a&#x27;, &#x27;j&#x27;), 175),
 ((&#x27;o&#x27;, &#x27;h&#x27;), 171),
 ((&#x27;u&#x27;, &#x27;e&#x27;), 169),
 ((&#x27;m&#x27;, &#x27;m&#x27;), 168),
 ((&#x27;a&#x27;, &#x27;g&#x27;), 168),
 ((&#x27;h&#x27;, &#x27;u&#x27;), 166),
 ((&#x27;x&#x27;, &#x27;.&#x27;), 164),
 ((&#x27;u&#x27;, &#x27;a&#x27;), 163),
 ((&#x27;r&#x27;, &#x27;m&#x27;), 162),
 ((&#x27;a&#x27;, &#x27;w&#x27;), 161),
 ((&#x27;f&#x27;, &#x27;i&#x27;), 160),
 ((&#x27;z&#x27;, &#x27;.&#x27;), 160),
 ((&#x27;u&#x27;, &#x27;.&#x27;), 155),
 ((&#x27;u&#x27;, &#x27;m&#x27;), 154),
 ((&#x27;e&#x27;, &#x27;c&#x27;), 153),
 ((&#x27;v&#x27;, &#x27;o&#x27;), 153),
 ((&#x27;e&#x27;, &#x27;h&#x27;), 152),
 ((&#x27;p&#x27;, &#x27;r&#x27;), 151),
 ((&#x27;d&#x27;, &#x27;d&#x27;), 149),
 ((&#x27;o&#x27;, &#x27;a&#x27;), 149),
 ((&#x27;w&#x27;, &#x27;e&#x27;), 149),
 ((&#x27;w&#x27;, &#x27;i&#x27;), 148),
 ((&#x27;y&#x27;, &#x27;m&#x27;), 148),
 ((&#x27;z&#x27;, &#x27;y&#x27;), 147),
 ((&#x27;n&#x27;, &#x27;z&#x27;), 145),
 ((&#x27;y&#x27;, &#x27;u&#x27;), 141),
 ((&#x27;r&#x27;, &#x27;n&#x27;), 140),
 ((&#x27;o&#x27;, &#x27;b&#x27;), 140),
 ((&#x27;k&#x27;, &#x27;l&#x27;), 139),
 ((&#x27;m&#x27;, &#x27;u&#x27;), 139),
 ((&#x27;l&#x27;, &#x27;d&#x27;), 138),
 ((&#x27;h&#x27;, &#x27;n&#x27;), 138),
 ((&#x27;u&#x27;, &#x27;d&#x27;), 136),
 ((&#x27;.&#x27;, &#x27;x&#x27;), 134),
 ((&#x27;t&#x27;, &#x27;l&#x27;), 134),
 ((&#x27;a&#x27;, &#x27;f&#x27;), 134),
 ((&#x27;o&#x27;, &#x27;e&#x27;), 132),
 ((&#x27;e&#x27;, &#x27;x&#x27;), 132),
 ((&#x27;e&#x27;, &#x27;g&#x27;), 125),
 ((&#x27;f&#x27;, &#x27;e&#x27;), 123),
 ((&#x27;z&#x27;, &#x27;l&#x27;), 123),
 ((&#x27;u&#x27;, &#x27;i&#x27;), 121),
 ((&#x27;v&#x27;, &#x27;y&#x27;), 121),
 ((&#x27;e&#x27;, &#x27;b&#x27;), 121),
 ((&#x27;r&#x27;, &#x27;h&#x27;), 121),
 ((&#x27;j&#x27;, &#x27;i&#x27;), 119),
 ((&#x27;o&#x27;, &#x27;t&#x27;), 118),
 ((&#x27;d&#x27;, &#x27;h&#x27;), 118),
 ((&#x27;h&#x27;, &#x27;m&#x27;), 117),
 ((&#x27;c&#x27;, &#x27;l&#x27;), 116),
 ((&#x27;o&#x27;, &#x27;o&#x27;), 115),
 ((&#x27;y&#x27;, &#x27;c&#x27;), 115),
 ((&#x27;o&#x27;, &#x27;w&#x27;), 114),
 ((&#x27;o&#x27;, &#x27;c&#x27;), 114),
 ((&#x27;f&#x27;, &#x27;r&#x27;), 114),
 ((&#x27;b&#x27;, &#x27;.&#x27;), 114),
 ((&#x27;m&#x27;, &#x27;b&#x27;), 112),
 ((&#x27;z&#x27;, &#x27;o&#x27;), 110),
 ((&#x27;i&#x27;, &#x27;b&#x27;), 110),
 ((&#x27;i&#x27;, &#x27;u&#x27;), 109),
 ((&#x27;k&#x27;, &#x27;r&#x27;), 109),
 ((&#x27;g&#x27;, &#x27;.&#x27;), 108),
 ((&#x27;y&#x27;, &#x27;v&#x27;), 106),
 ((&#x27;t&#x27;, &#x27;z&#x27;), 105),
 ((&#x27;b&#x27;, &#x27;o&#x27;), 105),
 ((&#x27;c&#x27;, &#x27;y&#x27;), 104),
 ((&#x27;y&#x27;, &#x27;t&#x27;), 104),
 ((&#x27;u&#x27;, &#x27;b&#x27;), 103),
 ((&#x27;u&#x27;, &#x27;c&#x27;), 103),
 ((&#x27;x&#x27;, &#x27;a&#x27;), 103),
 ((&#x27;b&#x27;, &#x27;l&#x27;), 103),
 ((&#x27;o&#x27;, &#x27;y&#x27;), 103),
 ((&#x27;x&#x27;, &#x27;i&#x27;), 102),
 ((&#x27;i&#x27;, &#x27;f&#x27;), 101),
 ((&#x27;r&#x27;, &#x27;c&#x27;), 99),
 ((&#x27;c&#x27;, &#x27;.&#x27;), 97),
 ((&#x27;m&#x27;, &#x27;r&#x27;), 97),
 ((&#x27;n&#x27;, &#x27;u&#x27;), 96),
 ((&#x27;o&#x27;, &#x27;p&#x27;), 95),
 ((&#x27;i&#x27;, &#x27;h&#x27;), 95),
 ((&#x27;k&#x27;, &#x27;s&#x27;), 95),
 ((&#x27;l&#x27;, &#x27;s&#x27;), 94),
 ((&#x27;u&#x27;, &#x27;k&#x27;), 93),
 ((&#x27;.&#x27;, &#x27;q&#x27;), 92),
 ((&#x27;d&#x27;, &#x27;u&#x27;), 92),
 ((&#x27;s&#x27;, &#x27;m&#x27;), 90),
 ((&#x27;r&#x27;, &#x27;k&#x27;), 90),
 ((&#x27;i&#x27;, &#x27;x&#x27;), 89),
 ((&#x27;v&#x27;, &#x27;.&#x27;), 88),
 ((&#x27;y&#x27;, &#x27;k&#x27;), 86),
 ((&#x27;u&#x27;, &#x27;w&#x27;), 86),
 ((&#x27;g&#x27;, &#x27;u&#x27;), 85),
 ((&#x27;b&#x27;, &#x27;y&#x27;), 83),
 ((&#x27;e&#x27;, &#x27;p&#x27;), 83),
 ((&#x27;g&#x27;, &#x27;o&#x27;), 83),
 ((&#x27;s&#x27;, &#x27;k&#x27;), 82),
 ((&#x27;u&#x27;, &#x27;t&#x27;), 82),
 ((&#x27;a&#x27;, &#x27;p&#x27;), 82),
 ((&#x27;e&#x27;, &#x27;f&#x27;), 82),
 ((&#x27;i&#x27;, &#x27;i&#x27;), 82),
 ((&#x27;r&#x27;, &#x27;v&#x27;), 80),
 ((&#x27;f&#x27;, &#x27;.&#x27;), 80),
 ((&#x27;t&#x27;, &#x27;u&#x27;), 78),
 ((&#x27;y&#x27;, &#x27;z&#x27;), 78),
 ((&#x27;.&#x27;, &#x27;u&#x27;), 78),
 ((&#x27;l&#x27;, &#x27;t&#x27;), 77),
 ((&#x27;r&#x27;, &#x27;g&#x27;), 76),
 ((&#x27;c&#x27;, &#x27;r&#x27;), 76),
 ((&#x27;i&#x27;, &#x27;j&#x27;), 76),
 ((&#x27;w&#x27;, &#x27;y&#x27;), 73),
 ((&#x27;z&#x27;, &#x27;u&#x27;), 73),
 ((&#x27;l&#x27;, &#x27;v&#x27;), 72),
 ((&#x27;h&#x27;, &#x27;t&#x27;), 71),
 ((&#x27;j&#x27;, &#x27;.&#x27;), 71),
 ((&#x27;x&#x27;, &#x27;t&#x27;), 70),
 ((&#x27;o&#x27;, &#x27;i&#x27;), 69),
 ((&#x27;e&#x27;, &#x27;u&#x27;), 69),
 ((&#x27;o&#x27;, &#x27;k&#x27;), 68),
 ((&#x27;b&#x27;, &#x27;d&#x27;), 65),
 ((&#x27;a&#x27;, &#x27;o&#x27;), 63),
 ((&#x27;p&#x27;, &#x27;i&#x27;), 61),
 ((&#x27;s&#x27;, &#x27;c&#x27;), 60),
 ((&#x27;d&#x27;, &#x27;l&#x27;), 60),
 ((&#x27;l&#x27;, &#x27;m&#x27;), 60),
 ((&#x27;a&#x27;, &#x27;q&#x27;), 60),
 ((&#x27;f&#x27;, &#x27;o&#x27;), 60),
 ((&#x27;p&#x27;, &#x27;o&#x27;), 59),
 ((&#x27;n&#x27;, &#x27;k&#x27;), 58),
 ((&#x27;w&#x27;, &#x27;n&#x27;), 58),
 ((&#x27;u&#x27;, &#x27;h&#x27;), 58),
 ((&#x27;e&#x27;, &#x27;j&#x27;), 55),
 ((&#x27;n&#x27;, &#x27;v&#x27;), 55),
 ((&#x27;s&#x27;, &#x27;r&#x27;), 55),
 ((&#x27;o&#x27;, &#x27;z&#x27;), 54),
 ((&#x27;i&#x27;, &#x27;p&#x27;), 53),
 ((&#x27;l&#x27;, &#x27;b&#x27;), 52),
 ((&#x27;i&#x27;, &#x27;q&#x27;), 52),
 ((&#x27;w&#x27;, &#x27;.&#x27;), 51),
 ((&#x27;m&#x27;, &#x27;c&#x27;), 51),
 ((&#x27;s&#x27;, &#x27;p&#x27;), 51),
 ((&#x27;e&#x27;, &#x27;w&#x27;), 50),
 ((&#x27;k&#x27;, &#x27;u&#x27;), 50),
 ((&#x27;v&#x27;, &#x27;r&#x27;), 48),
 ((&#x27;u&#x27;, &#x27;g&#x27;), 47),
 ((&#x27;o&#x27;, &#x27;x&#x27;), 45),
 ((&#x27;u&#x27;, &#x27;z&#x27;), 45),
 ((&#x27;z&#x27;, &#x27;z&#x27;), 45),
 ((&#x27;j&#x27;, &#x27;h&#x27;), 45),
 ((&#x27;b&#x27;, &#x27;u&#x27;), 45),
 ((&#x27;o&#x27;, &#x27;g&#x27;), 44),
 ((&#x27;n&#x27;, &#x27;r&#x27;), 44),
 ((&#x27;f&#x27;, &#x27;f&#x27;), 44),
 ((&#x27;n&#x27;, &#x27;j&#x27;), 44),
 ((&#x27;z&#x27;, &#x27;h&#x27;), 43),
 ((&#x27;c&#x27;, &#x27;c&#x27;), 42),
 ((&#x27;r&#x27;, &#x27;b&#x27;), 41),
 ((&#x27;x&#x27;, &#x27;o&#x27;), 41),
 ((&#x27;b&#x27;, &#x27;h&#x27;), 41),
 ((&#x27;p&#x27;, &#x27;p&#x27;), 39),
 ((&#x27;x&#x27;, &#x27;l&#x27;), 39),
 ((&#x27;h&#x27;, &#x27;v&#x27;), 39),
 ((&#x27;b&#x27;, &#x27;b&#x27;), 38),
 ((&#x27;m&#x27;, &#x27;p&#x27;), 38),
 ((&#x27;x&#x27;, &#x27;x&#x27;), 38),
 ((&#x27;u&#x27;, &#x27;v&#x27;), 37),
 ((&#x27;x&#x27;, &#x27;e&#x27;), 36),
 ((&#x27;w&#x27;, &#x27;o&#x27;), 36),
 ((&#x27;c&#x27;, &#x27;t&#x27;), 35),
 ((&#x27;z&#x27;, &#x27;m&#x27;), 35),
 ((&#x27;t&#x27;, &#x27;s&#x27;), 35),
 ((&#x27;m&#x27;, &#x27;s&#x27;), 35),
 ((&#x27;c&#x27;, &#x27;u&#x27;), 35),
 ((&#x27;o&#x27;, &#x27;f&#x27;), 34),
 ((&#x27;u&#x27;, &#x27;x&#x27;), 34),
 ((&#x27;k&#x27;, &#x27;w&#x27;), 34),
 ((&#x27;p&#x27;, &#x27;.&#x27;), 33),
 ((&#x27;g&#x27;, &#x27;l&#x27;), 32),
 ((&#x27;z&#x27;, &#x27;r&#x27;), 32),
 ((&#x27;d&#x27;, &#x27;n&#x27;), 31),
 ((&#x27;g&#x27;, &#x27;t&#x27;), 31),
 ((&#x27;g&#x27;, &#x27;y&#x27;), 31),
 ((&#x27;h&#x27;, &#x27;s&#x27;), 31),
 ((&#x27;x&#x27;, &#x27;s&#x27;), 31),
 ((&#x27;g&#x27;, &#x27;s&#x27;), 30),
 ((&#x27;x&#x27;, &#x27;y&#x27;), 30),
 ((&#x27;y&#x27;, &#x27;g&#x27;), 30),
 ((&#x27;d&#x27;, &#x27;m&#x27;), 30),
 ((&#x27;d&#x27;, &#x27;s&#x27;), 29),
 ((&#x27;h&#x27;, &#x27;k&#x27;), 29),
 ((&#x27;y&#x27;, &#x27;x&#x27;), 28),
 ((&#x27;q&#x27;, &#x27;.&#x27;), 28),
 ((&#x27;g&#x27;, &#x27;n&#x27;), 27),
 ((&#x27;y&#x27;, &#x27;b&#x27;), 27),
 ((&#x27;g&#x27;, &#x27;w&#x27;), 26),
 ((&#x27;n&#x27;, &#x27;h&#x27;), 26),
 ((&#x27;k&#x27;, &#x27;n&#x27;), 26),
 ((&#x27;g&#x27;, &#x27;g&#x27;), 25),
 ((&#x27;d&#x27;, &#x27;g&#x27;), 25),
 ((&#x27;l&#x27;, &#x27;c&#x27;), 25),
 ((&#x27;r&#x27;, &#x27;j&#x27;), 25),
 ((&#x27;w&#x27;, &#x27;u&#x27;), 25),
 ((&#x27;l&#x27;, &#x27;k&#x27;), 24),
 ((&#x27;m&#x27;, &#x27;d&#x27;), 24),
 ((&#x27;s&#x27;, &#x27;w&#x27;), 24),
 ((&#x27;s&#x27;, &#x27;n&#x27;), 24),
 ((&#x27;h&#x27;, &#x27;d&#x27;), 24),
 ((&#x27;w&#x27;, &#x27;h&#x27;), 23),
 ((&#x27;y&#x27;, &#x27;j&#x27;), 23),
 ((&#x27;y&#x27;, &#x27;y&#x27;), 23),
 ((&#x27;r&#x27;, &#x27;z&#x27;), 23),
 ((&#x27;d&#x27;, &#x27;w&#x27;), 23),
 ((&#x27;w&#x27;, &#x27;r&#x27;), 22),
 ((&#x27;t&#x27;, &#x27;n&#x27;), 22),
 ((&#x27;l&#x27;, &#x27;f&#x27;), 22),
 ((&#x27;y&#x27;, &#x27;h&#x27;), 22),
 ((&#x27;r&#x27;, &#x27;w&#x27;), 21),
 ((&#x27;s&#x27;, &#x27;b&#x27;), 21),
 ((&#x27;m&#x27;, &#x27;n&#x27;), 20),
 ((&#x27;f&#x27;, &#x27;l&#x27;), 20),
 ((&#x27;w&#x27;, &#x27;s&#x27;), 20),
 ((&#x27;k&#x27;, &#x27;k&#x27;), 20),
 ((&#x27;h&#x27;, &#x27;z&#x27;), 20),
 ((&#x27;g&#x27;, &#x27;d&#x27;), 19),
 ((&#x27;l&#x27;, &#x27;h&#x27;), 19),
 ((&#x27;n&#x27;, &#x27;m&#x27;), 19),
 ((&#x27;x&#x27;, &#x27;z&#x27;), 19),
 ((&#x27;u&#x27;, &#x27;f&#x27;), 19),
 ((&#x27;f&#x27;, &#x27;t&#x27;), 18),
 ((&#x27;l&#x27;, &#x27;r&#x27;), 18),
 ((&#x27;p&#x27;, &#x27;t&#x27;), 17),
 ((&#x27;t&#x27;, &#x27;c&#x27;), 17),
 ((&#x27;k&#x27;, &#x27;t&#x27;), 17),
 ((&#x27;d&#x27;, &#x27;v&#x27;), 17),
 ((&#x27;u&#x27;, &#x27;p&#x27;), 16),
 ((&#x27;p&#x27;, &#x27;l&#x27;), 16),
 ((&#x27;l&#x27;, &#x27;w&#x27;), 16),
 ((&#x27;p&#x27;, &#x27;s&#x27;), 16),
 ((&#x27;o&#x27;, &#x27;j&#x27;), 16),
 ((&#x27;r&#x27;, &#x27;q&#x27;), 16),
 ((&#x27;y&#x27;, &#x27;p&#x27;), 15),
 ((&#x27;l&#x27;, &#x27;p&#x27;), 15),
 ((&#x27;t&#x27;, &#x27;v&#x27;), 15),
 ((&#x27;r&#x27;, &#x27;p&#x27;), 14),
 ((&#x27;l&#x27;, &#x27;n&#x27;), 14),
 ((&#x27;e&#x27;, &#x27;q&#x27;), 14),
 ((&#x27;f&#x27;, &#x27;y&#x27;), 14),
 ((&#x27;s&#x27;, &#x27;v&#x27;), 14),
 ((&#x27;u&#x27;, &#x27;j&#x27;), 14),
 ((&#x27;v&#x27;, &#x27;l&#x27;), 14),
 ((&#x27;q&#x27;, &#x27;a&#x27;), 13),
 ((&#x27;u&#x27;, &#x27;y&#x27;), 13),
 ((&#x27;q&#x27;, &#x27;i&#x27;), 13),
 ((&#x27;w&#x27;, &#x27;l&#x27;), 13),
 ((&#x27;p&#x27;, &#x27;y&#x27;), 12),
 ((&#x27;y&#x27;, &#x27;f&#x27;), 12),
 ((&#x27;c&#x27;, &#x27;q&#x27;), 11),
 ((&#x27;j&#x27;, &#x27;r&#x27;), 11),
 ((&#x27;n&#x27;, &#x27;w&#x27;), 11),
 ((&#x27;n&#x27;, &#x27;f&#x27;), 11),
 ((&#x27;t&#x27;, &#x27;w&#x27;), 11),
 ((&#x27;m&#x27;, &#x27;z&#x27;), 11),
 ((&#x27;u&#x27;, &#x27;o&#x27;), 10),
 ((&#x27;f&#x27;, &#x27;u&#x27;), 10),
 ((&#x27;l&#x27;, &#x27;z&#x27;), 10),
 ((&#x27;h&#x27;, &#x27;w&#x27;), 10),
 ((&#x27;u&#x27;, &#x27;q&#x27;), 10),
 ((&#x27;j&#x27;, &#x27;y&#x27;), 10),
 ((&#x27;s&#x27;, &#x27;z&#x27;), 10),
 ((&#x27;s&#x27;, &#x27;d&#x27;), 9),
 ((&#x27;j&#x27;, &#x27;l&#x27;), 9),
 ((&#x27;d&#x27;, &#x27;j&#x27;), 9),
 ((&#x27;k&#x27;, &#x27;m&#x27;), 9),
 ((&#x27;r&#x27;, &#x27;f&#x27;), 9),
 ((&#x27;h&#x27;, &#x27;j&#x27;), 9),
 ((&#x27;v&#x27;, &#x27;n&#x27;), 8),
 ((&#x27;n&#x27;, &#x27;b&#x27;), 8),
 ((&#x27;i&#x27;, &#x27;w&#x27;), 8),
 ((&#x27;h&#x27;, &#x27;b&#x27;), 8),
 ((&#x27;b&#x27;, &#x27;s&#x27;), 8),
 ((&#x27;w&#x27;, &#x27;t&#x27;), 8),
 ((&#x27;w&#x27;, &#x27;d&#x27;), 8),
 ((&#x27;v&#x27;, &#x27;v&#x27;), 7),
 ((&#x27;v&#x27;, &#x27;u&#x27;), 7),
 ((&#x27;j&#x27;, &#x27;s&#x27;), 7),
 ((&#x27;m&#x27;, &#x27;j&#x27;), 7),
 ((&#x27;f&#x27;, &#x27;s&#x27;), 6),
 ((&#x27;l&#x27;, &#x27;g&#x27;), 6),
 ((&#x27;l&#x27;, &#x27;j&#x27;), 6),
 ((&#x27;j&#x27;, &#x27;w&#x27;), 6),
 ((&#x27;n&#x27;, &#x27;x&#x27;), 6),
 ((&#x27;y&#x27;, &#x27;q&#x27;), 6),
 ((&#x27;w&#x27;, &#x27;k&#x27;), 6),
 ((&#x27;g&#x27;, &#x27;m&#x27;), 6),
 ((&#x27;x&#x27;, &#x27;u&#x27;), 5),
 ((&#x27;m&#x27;, &#x27;h&#x27;), 5),
 ((&#x27;m&#x27;, &#x27;l&#x27;), 5),
 ((&#x27;j&#x27;, &#x27;m&#x27;), 5),
 ((&#x27;c&#x27;, &#x27;s&#x27;), 5),
 ((&#x27;j&#x27;, &#x27;v&#x27;), 5),
 ((&#x27;n&#x27;, &#x27;p&#x27;), 5),
 ((&#x27;d&#x27;, &#x27;f&#x27;), 5),
 ((&#x27;x&#x27;, &#x27;d&#x27;), 5),
 ((&#x27;z&#x27;, &#x27;b&#x27;), 4),
 ((&#x27;f&#x27;, &#x27;n&#x27;), 4),
 ((&#x27;x&#x27;, &#x27;c&#x27;), 4),
 ((&#x27;m&#x27;, &#x27;t&#x27;), 4),
 ((&#x27;t&#x27;, &#x27;m&#x27;), 4),
 ((&#x27;z&#x27;, &#x27;n&#x27;), 4),
 ((&#x27;z&#x27;, &#x27;t&#x27;), 4),
 ((&#x27;p&#x27;, &#x27;u&#x27;), 4),
 ((&#x27;c&#x27;, &#x27;z&#x27;), 4),
 ((&#x27;b&#x27;, &#x27;n&#x27;), 4),
 ((&#x27;z&#x27;, &#x27;s&#x27;), 4),
 ((&#x27;f&#x27;, &#x27;w&#x27;), 4),
 ((&#x27;d&#x27;, &#x27;t&#x27;), 4),
 ((&#x27;j&#x27;, &#x27;d&#x27;), 4),
 ((&#x27;j&#x27;, &#x27;c&#x27;), 4),
 ((&#x27;y&#x27;, &#x27;w&#x27;), 4),
 ((&#x27;v&#x27;, &#x27;k&#x27;), 3),
 ((&#x27;x&#x27;, &#x27;w&#x27;), 3),
 ((&#x27;t&#x27;, &#x27;j&#x27;), 3),
 ((&#x27;c&#x27;, &#x27;j&#x27;), 3),
 ((&#x27;q&#x27;, &#x27;w&#x27;), 3),
 ((&#x27;g&#x27;, &#x27;b&#x27;), 3),
 ((&#x27;o&#x27;, &#x27;q&#x27;), 3),
 ((&#x27;r&#x27;, &#x27;x&#x27;), 3),
 ((&#x27;d&#x27;, &#x27;c&#x27;), 3),
 ((&#x27;g&#x27;, &#x27;j&#x27;), 3),
 ((&#x27;x&#x27;, &#x27;f&#x27;), 3),
 ((&#x27;z&#x27;, &#x27;w&#x27;), 3),
 ((&#x27;d&#x27;, &#x27;k&#x27;), 3),
 ((&#x27;u&#x27;, &#x27;u&#x27;), 3),
 ((&#x27;m&#x27;, &#x27;v&#x27;), 3),
 ((&#x27;c&#x27;, &#x27;x&#x27;), 3),
 ((&#x27;l&#x27;, &#x27;q&#x27;), 3),
 ((&#x27;p&#x27;, &#x27;b&#x27;), 2),
 ((&#x27;t&#x27;, &#x27;g&#x27;), 2),
 ((&#x27;q&#x27;, &#x27;s&#x27;), 2),
 ((&#x27;t&#x27;, &#x27;x&#x27;), 2),
 ((&#x27;f&#x27;, &#x27;k&#x27;), 2),
 ((&#x27;b&#x27;, &#x27;t&#x27;), 2),
 ((&#x27;j&#x27;, &#x27;n&#x27;), 2),
 ((&#x27;k&#x27;, &#x27;c&#x27;), 2),
 ((&#x27;z&#x27;, &#x27;k&#x27;), 2),
 ((&#x27;s&#x27;, &#x27;j&#x27;), 2),
 ((&#x27;s&#x27;, &#x27;f&#x27;), 2),
 ((&#x27;z&#x27;, &#x27;j&#x27;), 2),
 ((&#x27;n&#x27;, &#x27;q&#x27;), 2),
 ((&#x27;f&#x27;, &#x27;z&#x27;), 2),
 ((&#x27;h&#x27;, &#x27;g&#x27;), 2),
 ((&#x27;w&#x27;, &#x27;w&#x27;), 2),
 ((&#x27;k&#x27;, &#x27;j&#x27;), 2),
 ((&#x27;j&#x27;, &#x27;k&#x27;), 2),
 ((&#x27;w&#x27;, &#x27;m&#x27;), 2),
 ((&#x27;z&#x27;, &#x27;c&#x27;), 2),
 ((&#x27;z&#x27;, &#x27;v&#x27;), 2),
 ((&#x27;w&#x27;, &#x27;f&#x27;), 2),
 ((&#x27;q&#x27;, &#x27;m&#x27;), 2),
 ((&#x27;k&#x27;, &#x27;z&#x27;), 2),
 ((&#x27;j&#x27;, &#x27;j&#x27;), 2),
 ((&#x27;z&#x27;, &#x27;p&#x27;), 2),
 ((&#x27;j&#x27;, &#x27;t&#x27;), 2),
 ((&#x27;k&#x27;, &#x27;b&#x27;), 2),
 ((&#x27;m&#x27;, &#x27;w&#x27;), 2),
 ((&#x27;h&#x27;, &#x27;f&#x27;), 2),
 ((&#x27;c&#x27;, &#x27;g&#x27;), 2),
 ((&#x27;t&#x27;, &#x27;f&#x27;), 2),
 ((&#x27;h&#x27;, &#x27;c&#x27;), 2),
 ((&#x27;q&#x27;, &#x27;o&#x27;), 2),
 ((&#x27;k&#x27;, &#x27;d&#x27;), 2),
 ((&#x27;k&#x27;, &#x27;v&#x27;), 2),
 ((&#x27;s&#x27;, &#x27;g&#x27;), 2),
 ((&#x27;z&#x27;, &#x27;d&#x27;), 2),
 ((&#x27;q&#x27;, &#x27;r&#x27;), 1),
 ((&#x27;d&#x27;, &#x27;z&#x27;), 1),
 ((&#x27;p&#x27;, &#x27;j&#x27;), 1),
 ((&#x27;q&#x27;, &#x27;l&#x27;), 1),
 ((&#x27;p&#x27;, &#x27;f&#x27;), 1),
 ((&#x27;q&#x27;, &#x27;e&#x27;), 1),
 ((&#x27;b&#x27;, &#x27;c&#x27;), 1),
 ((&#x27;c&#x27;, &#x27;d&#x27;), 1),
 ((&#x27;m&#x27;, &#x27;f&#x27;), 1),
 ((&#x27;p&#x27;, &#x27;n&#x27;), 1),
 ((&#x27;w&#x27;, &#x27;b&#x27;), 1),
 ((&#x27;p&#x27;, &#x27;c&#x27;), 1),
 ((&#x27;h&#x27;, &#x27;p&#x27;), 1),
 ((&#x27;f&#x27;, &#x27;h&#x27;), 1),
 ((&#x27;b&#x27;, &#x27;j&#x27;), 1),
 ((&#x27;f&#x27;, &#x27;g&#x27;), 1),
 ((&#x27;z&#x27;, &#x27;g&#x27;), 1),
 ((&#x27;c&#x27;, &#x27;p&#x27;), 1),
 ((&#x27;p&#x27;, &#x27;k&#x27;), 1),
 ((&#x27;p&#x27;, &#x27;m&#x27;), 1),
 ((&#x27;x&#x27;, &#x27;n&#x27;), 1),
 ((&#x27;s&#x27;, &#x27;q&#x27;), 1),
 ((&#x27;k&#x27;, &#x27;f&#x27;), 1),
 ((&#x27;m&#x27;, &#x27;k&#x27;), 1),
 ((&#x27;x&#x27;, &#x27;h&#x27;), 1),
 ((&#x27;g&#x27;, &#x27;f&#x27;), 1),
 ((&#x27;v&#x27;, &#x27;b&#x27;), 1),
 ((&#x27;j&#x27;, &#x27;p&#x27;), 1),
 ((&#x27;g&#x27;, &#x27;z&#x27;), 1),
 ((&#x27;v&#x27;, &#x27;d&#x27;), 1),
 ((&#x27;d&#x27;, &#x27;b&#x27;), 1),
 ((&#x27;v&#x27;, &#x27;h&#x27;), 1),
 ((&#x27;h&#x27;, &#x27;h&#x27;), 1),
 ((&#x27;g&#x27;, &#x27;v&#x27;), 1),
 ((&#x27;d&#x27;, &#x27;q&#x27;), 1),
 ((&#x27;x&#x27;, &#x27;b&#x27;), 1),
 ((&#x27;w&#x27;, &#x27;z&#x27;), 1),
 ((&#x27;h&#x27;, &#x27;q&#x27;), 1),
 ((&#x27;j&#x27;, &#x27;b&#x27;), 1),
 ((&#x27;x&#x27;, &#x27;m&#x27;), 1),
 ((&#x27;w&#x27;, &#x27;g&#x27;), 1),
 ((&#x27;t&#x27;, &#x27;b&#x27;), 1),
 ((&#x27;z&#x27;, &#x27;x&#x27;), 1)]</span></code></div></div></div><div id="iwg0wD6k5J" class="myst-jp-nb-block relative group/block"><p>And this is the sorted list of counts of the individual bigrams across all the words in the dataset! Now let’s convert our current bigram-to-occurence-frequency map into a bigram counts array, where every row index represents the first character and every column index represents the second character of each bigram. Before doing so, we must first find a way to convert each character into a unique integer index:</p></div><div id="SBkywr6NuJ" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">chars = [&quot;.&quot;] + sorted(list(set(&quot;&quot;.join(words))))
ctoi = {c: i for i, c in enumerate(chars)}
print(ctoi)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="bEC3yk0ZgM3jojMwS94JA" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>{&#x27;.&#x27;: 0, &#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3, &#x27;d&#x27;: 4, &#x27;e&#x27;: 5, &#x27;f&#x27;: 6, &#x27;g&#x27;: 7, &#x27;h&#x27;: 8, &#x27;i&#x27;: 9, &#x27;j&#x27;: 10, &#x27;k&#x27;: 11, &#x27;l&#x27;: 12, &#x27;m&#x27;: 13, &#x27;n&#x27;: 14, &#x27;o&#x27;: 15, &#x27;p&#x27;: 16, &#x27;q&#x27;: 17, &#x27;r&#x27;: 18, &#x27;s&#x27;: 19, &#x27;t&#x27;: 20, &#x27;u&#x27;: 21, &#x27;v&#x27;: 22, &#x27;w&#x27;: 23, &#x27;x&#x27;: 24, &#x27;y&#x27;: 25, &#x27;z&#x27;: 26}
</span></code></pre></div></div></div></div><div id="kjRexkBFpN" class="myst-jp-nb-block relative group/block"><p>Now that we have a character-to-index map, we may construct our bigram counts array <code>N</code>:</p></div><div id="bkAhqF4Usm" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import torch

nchars = len(chars)
N = torch.zeros(nchars, nchars, dtype=torch.int32)
for w in words:
    chs = [&quot;.&quot;] + list(w) + [&quot;.&quot;]
    for ch1, ch2 in zip(chs, chs[1:]):
        N[ctoi[ch1], ctoi[ch2]] += 1</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="PZ2xV6115Z9sYvdrmstCV" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="RcTpvWIF61" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">N</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="sNUaEnEE299YXAterswNy" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
          134,  535,  929],
        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,
         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,
          182, 2050,  435],
        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,
          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,
            0,   83,    0],
        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,
          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,
            3,  104,    4],
        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,
           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,
            0,  317,    1],
        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,
         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,
          132, 1070,  181],
        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,
           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,
            0,   14,    2],
        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,
           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,
            0,   31,    1],
        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,
          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,
            0,  213,   20],
        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,
         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,
           89,  779,  277],
        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,
            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,
            0,   10,    0],
        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,
          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,
            0,  379,    2],
        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,
         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,
            0, 1588,   10],
        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,
            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,
            0,  287,   11],
        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,
          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,
            6,  465,  145],
        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,
          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,
           45,  103,   54],
        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,
           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,
            0,   12,    0],
        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,
            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,
            0,    0,    0],
        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,
          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,
            3,  773,   23],
        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,
          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,
            0,  215,   10],
        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,
          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,
            2,  341,  105],
        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,
          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,
           34,   13,   45],
        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,
           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,
            0,  121,    0],
        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,
           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,
            0,   73,    1],
        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,
           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,
           38,   30,   19],
        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,
         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,
           28,   23,   78],
        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,
          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,
            1,  147,   45]], dtype=torch.int32)</span></code></div></div></div><div id="AHXvgxAfij" class="myst-jp-nb-block relative group/block"><p>Done! Of course, this looks like a mess. So let’s visualize it better.</p></div><div id="YK01SUa9BZ" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import matplotlib.pyplot as plt
if IN_COLAB:
    %matplotlib inline
else:
    %matplotlib ipympl

itoc = {i: c for c, i in ctoi.items()}
plt.figure(figsize=(16, 16))
plt.imshow(N, cmap=&quot;Blues&quot;)
for i in range(27):
    for j in range(27):
        chstr = itoc[i] + itoc[j]
        plt.text(j, i, chstr, ha=&quot;center&quot;, va=&quot;bottom&quot;, color=&quot;gray&quot;)
        plt.text(j, i, N[i, j].item(), ha=&quot;center&quot;, va=&quot;top&quot;, color=&quot;gray&quot;)
plt.axis(&quot;off&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="rhxzW687hWKtMDJmYlOg9" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>(np.float64(-0.5), np.float64(26.5), np.float64(26.5), np.float64(-0.5))</span></code></div><div><div class="p-2.5">Loading...</div></div></div></div><div id="YU1a6BlxuT" class="myst-jp-nb-block relative group/block"><p>The color-graded bigram counts array! Looks good. This array actually has all the necessary information for us to start sampling from this bigram character language model. Let’s just start by sampling the start character (of course) of each name: the <code>.</code> character. The first row tells us how often each other character follows it. In other words, the first row tells us how often each character is the first character of a word:</p></div><div id="FxNpZmVNrj" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">N[0]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="5Nd_GMww4aEFnlHFPT2gO" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
         134,  535,  929], dtype=torch.int32)</span></code></div></div></div><div id="MOj9uNhcON" class="myst-jp-nb-block relative group/block"><p>To get the probability of each of character being the first:</p></div><div id="BNexBuZjZx" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">p = N[0].float()
p = p / p.sum()
p</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="safrIvINTwfbhA_sMyiia" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,
        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,
        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])</span></code></div></div></div><div id="TIrrQPwunH" class="myst-jp-nb-block relative group/block"><p>Each value of this probability distribution corresponds simply to the probability of the corresponding character being the first character of a word. And of course it sums to <code>1</code>:</p></div><div id="Pc5h9ZDw48" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">assert p.sum() == 1</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="gcGJ0W2Wst1QpQX-ePUwm" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="UDyNsuBMGu" class="myst-jp-nb-block relative group/block"><p>Now, we’ll sample numbers according to this probability distribution using <a target="_blank" rel="noreferrer" href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial" class="link"><code>torch.multinomial</code><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="external-link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>. And to do so deterministically we are going to use a generator. So, let’s take a brief detour and test out how to sample. First we create a probability distribution:</p></div><div id="dY3Uq41C8X" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">SEED = 2147483647
g = torch.Generator().manual_seed(SEED)
ptest = torch.rand(3, generator=g)
ptest = ptest / ptest.sum()
ptest</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="nXWY9zmIX31dMoe9VnAiO" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([0.6064, 0.3033, 0.0903])</span></code></div></div></div><div id="VRBJFjpae8" class="myst-jp-nb-block relative group/block"><p>Then, we sample from this distribution:</p></div><div id="K16ofsen4Q" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">s = torch.multinomial(ptest, num_samples=100, replacement=True, generator=g)
s</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="QvhawxL-NM12TJNDRppfV" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,
        0, 1, 1, 1])</span></code></div></div></div><div id="JYKcvu1ptE" class="myst-jp-nb-block relative group/block"><p>Simple. Now, notice that it outputs the same tensor however many times you run the cells. That’s because we have set a fixed seed and passed the generator object to the functions. Now, notice the output of <code>torch.multinomial</code>. What we expect is that around <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>60.64</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">60.64\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">60.64%</span></span></span></span></span> of the numbers to be <code>0</code>, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>30.33</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">30.33\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">30.33%</span></span></span></span></span> to be <code>1</code> and <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9.03</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">9.03\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">9.03%</span></span></span></span></span> to be <code>2</code>:</p></div><div id="xMqtTyrqYA" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">sbc = torch.bincount(s)
for i in [0, 1, 2]:
    print(f&quot;Ratio of {i}: {sbc[i]/sbc.sum()}&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="dMgvFLJTnHttEx2pxxSpj" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Ratio of 0: 0.6100000143051147
Ratio of 1: 0.33000001311302185
Ratio of 2: 0.05999999865889549
</span></code></pre></div></div></div></div><div id="TWtQ2M4Rli" class="myst-jp-nb-block relative group/block"><p>Not too far away from what we expected! But, if we increase the number of samples, we will get much closer to the probabilities of our distribution. Try it out! The more samples we take, the more the actual occurence ratios match the probabilities of the distribution the numbers were sampled from. Now, it’s time to sample from our initial character probability distribution:</p></div><div id="CmPA9C016O" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">g = torch.Generator().manual_seed(SEED)
idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
itoc[idx]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="KcztTb-NzjfDcQLLi7lUc" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>&#x27;j&#x27;</span></code></div></div></div><div id="hwnK83eU4j" class="myst-jp-nb-block relative group/block"><p>We are now ready to write out our name generator.</p></div><div id="ofxrbFh6AT" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">g = torch.Generator().manual_seed(SEED)
P = N.float()
P = P / P.sum(
    1, keepdim=True
)  # sum over the column dimension and keep column dimension
for i in range(20):
    out = []
    idx = 0
    while True:
        p = P[idx]
        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        out.append(itoc[idx])
        if idx == 0:
            break
    print(&quot;&quot;.join(out))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="TIA3SuDZS5nxmR1EgZs1V" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>junide.
janasah.
p.
cony.
a.
nn.
kohin.
tolian.
juee.
ksahnaauranilevias.
dedainrwieta.
ssonielylarte.
faveumerifontume.
phynslenaruani.
core.
yaenon.
ka.
jabdinerimikimaynin.
anaasn.
ssorionsush.
</span></code></pre></div></div></div></div><div id="OF3xSJ0VB3" class="myst-jp-nb-block relative group/block"><p>It works! It yields names. Well, kinda. Some look name-like enough but most are just terrible. Lol. This is a bigrams model for you! To recap, we trained a bigrams language model essentially just by counting how frequently any pairing of characters occurs and then normalizing so that we get a nice probability distribution. Really, the elements of array <code>P</code> are the parameters of our model that summarize the statistics of these bigrams. We train the model and iteratively sample the next character and feed it in each time and get the next character. But how do we evaluate our model? We can do so, by looking at the probability of each bigram.</p></div><div id="rwJo08kQvl" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">for w in words[:3]:
    chs = [&quot;.&quot;] + list(w) + [&quot;.&quot;]
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = ctoi[ch1]
        ix2 = ctoi[ch2]
        prob = P[ix1, ix2]
        print(f&quot;{ch1}{ch2}: {prob:.4f}&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="Xy1LN9QVGQ_WJS4D09MMu" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>.e: 0.0478
em: 0.0377
mm: 0.0253
ma: 0.3899
a.: 0.1960
.o: 0.0123
ol: 0.0780
li: 0.1777
iv: 0.0152
vi: 0.3541
ia: 0.1381
a.: 0.1960
.a: 0.1377
av: 0.0246
va: 0.2495
a.: 0.1960
</span></code></pre></div></div></div></div><div id="RwFX6tXQen" class="myst-jp-nb-block relative group/block"><p>Here we are looking at the probabilities that the model assigns to every bigram in the dataset. Just keep in mind that we have <!-- -->27<!-- --> characters, so if everything was equally likely we would expect all probabilities to be:</p></div><div id="LhKkuUgLNX" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">1/27</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="xOlkWrH4x2jBWEuuSLxiW" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>0.037037037037037035</span></code></div></div></div><div id="CaK7gpSrWM" class="myst-jp-nb-block relative group/block"><p>Since they are not and we have mostly higher probabilities, it means that our model has learned something useful. In an ideal case, we would expect the bigram probabilities to be near <!-- -->1.0<!-- --> (perfect prediction probability). Now, when you look at the literature of <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" class="hover-link" target="_blank" rel="noreferrer" data-state="closed">maximum likelihood estimation</a>, statistical modelling and so on, you’ll see that what’s typically used here is something called the likelihood: the product of all the above probabilities. This gives us the probability of the entire dataset assigned by the model that you made. But, because the product of these probabilities is an unwieldly, very tiny number to work with (think <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.0478</mn><mo>×</mo><mn>0.0377</mn><mo>×</mo><mn>0.0253</mn><mo>×</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">0.0478 \times 0.0377 \times 0.0253 \times ...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.0478</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.0377</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.0253</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.1056em;"></span><span class="mord">...</span></span></span></span></span>), for convenience, what people usually work with is not the likelihood, but the log-likelihood. The log, as you can see:</p></div><div id="wT0lVuhQQd" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import numpy as np

x = np.arange(0.01, 1.0, 0.01)
y = np.log(x)
plt.figure()
plt.plot(x, y)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="IiOOAksKRThh5jLzfAm0b" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="XJV7i9u5gX" class="myst-jp-nb-block relative group/block"><p>is a monotonic transformation of the probability, where if you pass in probability <!-- -->1.0<!-- --> you get log-probability of <!-- -->0<!-- -->, and as the probabilities you pass in decrease, the log-probability decreases all the way to <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mord">∞</span></span></span></span></span> as the probability approaches <!-- -->0<!-- -->. Therefore, let’s also add the log probability in our loop to see what that looks like:</p></div><div id="J7WjOtHded" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def test_model(iterable, print_probs=True, calc_ll=False, print_nll=False):
    if print_nll:
        calc_ll = True
    log_likelihood = 0.0
    n = 0
    for w in iterable:
        chs = [&quot;.&quot;] + list(w) + [&quot;.&quot;]
        for ch1, ch2 in zip(chs, chs[1:]):
            prob = P[ctoi[ch1], ctoi[ch2]]
            logprob = torch.log(prob)
            if calc_ll:
                log_likelihood += logprob.item()
                n += 1
            if print_probs:
                print(f&quot;{ch1}{ch2}: {prob:.4f} {logprob:.4f}&quot;)
    if calc_ll:
        print(f&quot;{log_likelihood=}&quot;)
    if print_nll:
        nll = -log_likelihood
        print(f&quot;{nll=}&quot;)
        print(f&quot;loss={nll/n}&quot;)
    return log_likelihood


_ = test_model(words[:3])</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="vxakhtC_k3D2pJWMKzviA" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
</span></code></pre></div></div></div></div><div id="If1dIupOyK" class="myst-jp-nb-block relative group/block"><p>As you can see, for higher probabilities we get closer and closer to <!-- -->0<!-- -->, but lower probabilities gives us a more negative number. And so to calculate the log-likelihood, we just sum up all the log probabilities:</p></div><div id="wGkLUp2A8B" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">log_likelihood = test_model(words[:3], calc_ll=True)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="zNNNoawZC1nVuEOxNPfTS" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
log_likelihood=-38.785636603832245
</span></code></pre></div></div></div></div><div id="xqf4lisGb7" class="myst-jp-nb-block relative group/block"><p>Now, how high can log-likelihood get? As high as <!-- -->0<!-- -->! So, when all the probabilities are <!-- -->1.0<!-- -->, it will be <!-- -->0<!-- -->. But the further away from <!-- -->1.0<!-- --> they are, the more negative the log-likehood will get. Now, we don’t actually like this because we are looking to define here is a <strong>loss</strong> function, that has the semantics where high is bad and low is good, since we are trying to minimize it. Any ideas? Well, we actually just need to invert the log-likelihood, aka take the negative log-likelihood (<strong>nll</strong>):</p></div><div id="a4RSV9NeyQ" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">nll = -log_likelihood
print(f&#x27;{nll=}&#x27;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="WGuh5nb9JuWXeVCKEysRY" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>nll=38.785636603832245
</span></code></pre></div></div></div></div><div id="DZlQppRNND" class="myst-jp-nb-block relative group/block"><p><strong>nll</strong> is a very nice loss function because the lowest it can get is zero and the higher it is the worse off the predictions are that we are making. People also usually like to see the average of the <strong>nll</strong> instead of just the sum:</p></div><div id="Pwrd83hnef" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">test_model(words[:3], print_probs=False, calc_ll=True, print_nll=True);</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="ImxkUYViUtrxykDwmjOwA" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>log_likelihood=-38.785636603832245
nll=38.785636603832245
loss=2.4241022877395153
</span></code></pre></div></div></div></div><div id="ZWOG66GNcO" class="myst-jp-nb-block relative group/block"><p>Our <strong>loss</strong> function for the training set assigned by the model yields a <strong>loss</strong> of <!-- -->2.424<!-- -->. The lower it is, the better off we are. The higher it is, the worse off we are. So, the job of training is produce a high-quality model, by finding the parameters that minimize the <strong>loss</strong>. In this case, ones that minimize the <strong>nll</strong> <strong>loss</strong>. To summarize, our <strong>goal</strong> is to maximize likelihood of the data <strong>w.r.t.</strong> model parameters (in our statistical modeling case these are the bigram probabilities), which is:</p><ul><li><p>equivalent to maximizing the log-likelihood (because the <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\log</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span></span></span></span></span> function is monotonic)</p></li><li><p>equivalent to minimizing the <strong>nll</strong></p></li><li><p>equivalent to minimizing the average <strong>nll</strong></p></li></ul><p>The lower the <strong>nll</strong> <strong>loss</strong> the better, since that would mean assigning high probabilities. Remember: <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>a</mi><mo>⋅</mo><mi>b</mi><mo>⋅</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log(a \cdot b \cdot c) = \log(a) + \log(b) + \log(c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mclose">)</span></span></span></span></span>. Also, keep in mind that here we store the probabilities in a table format. But in what’s coming up, these numbers will not be kept explicitly but they will be calculated by a <strong>nn</strong> and we will change its parameters to maximize the likelihood of these probabilities. Let’s now test out our model with a random name:</p></div><div id="ceiEIbe6QF" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">test_model(iterable=[&#x27;christosqj&#x27;], calc_ll=True, print_nll=True);</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="d_EOo1_uWykZeH2AAXIOx" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>.c: 0.0481 -3.0337
ch: 0.1880 -1.6713
hr: 0.0268 -3.6199
ri: 0.2388 -1.4320
is: 0.0743 -2.5990
st: 0.0944 -2.3605
to: 0.1197 -2.1224
os: 0.0635 -2.7563
sq: 0.0001 -9.0004
qj: 0.0000 -inf
j.: 0.0245 -3.7098
log_likelihood=-inf
nll=inf
loss=inf
</span></code></pre></div></div></div></div><div id="lbQmvPvQAx" class="myst-jp-nb-block relative group/block"><p>As you can see, the probability of the bigram <code>sq</code> is super low. Whereas the probability for <code>qj</code>, since it is never encountered in our training data (see our bigram count table!), is <!-- -->0<!-- -->, which predictably yields a log-probability of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mord">∞</span></span></span></span></span>, which in turn causes the <strong>loss</strong> to be <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mord">∞</span></span></span></span></span>. What this means is that this model is exactly <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0 \%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0%</span></span></span></span></span> likely to predict this name (infinite <strong>loss</strong>). If you look up the table you see that <code>q</code> is followed by <code>j</code> zero times. This kind of behavior people don’t usually like too much, so there is a simple trick to alleviate it: model smoothing. It involves adding some fake counts to the bigram counts array so that never is there a bigram with <code>0</code> counts (and therefore <code>0</code> probability). This ensures that there are no zeros in our bigram counts matrix. E.g.</p></div><div id="RmMtUqkwYU" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">P = (N + 1).float()
P = P / P.sum(
    1, keepdim=True
)  # sum over the column dimension and keep column dimension
test_model(iterable=[&quot;christosqj&quot;], calc_ll=True, print_nll=True)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="qCEZov87KqBmycM2ai-_g" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>.c: 0.0481 -3.0339
ch: 0.1869 -1.6774
hr: 0.0268 -3.6185
ri: 0.2384 -1.4338
is: 0.0743 -2.5998
st: 0.0942 -2.3625
to: 0.1193 -2.1257
os: 0.0634 -2.7578
sq: 0.0002 -8.3105
qj: 0.0033 -5.7004
j.: 0.0246 -3.7051
log_likelihood=-37.32549834251404
nll=37.32549834251404
loss=3.393227122046731
</span></code></pre></div></div><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>-37.32549834251404</span></code></div></div></div><div id="q8CfqCEKiY" class="myst-jp-nb-block relative group/block"><p>Now, we avoid getting a loss of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mord">∞</span></span></span></span></span>. Cool! So we’ve now trained a respectable bigram character-level language model. We trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions. We saw that we can also then use those parameters of this model to perform sampling of new words (sample new names according to these distributions) and evaluate the quality of this model which is summarized by a single number: the <strong>nll</strong>. And the lower this number is, the better the model is because it is giving high probabilities to the actual mixed characters of all the bigrams in our training set. Great! We basically, counted and then normalized those counts, which is sensible enough.</p></div><div id="JFGm7UpeXF" class="myst-jp-nb-block relative group/block"><h2 id="casting-the-model-as-a-nn" class="relative group"><span class="heading-text">Casting the model as a <strong>nn</strong></span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#casting-the-model-as-a-nn" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="h4k2DHSx9u" class="myst-jp-nb-block relative group/block"><p>Let’s now try a different approach by casting such a bigram language model into a <strong>nn</strong> framework to achieve the same goal. Our <strong>nn</strong> is still going to be a bigram character-level language model. It will receive a single character as an input that will pass through a bunch of weighted neurons and then output the probability distribution over the next character in the sequence. It’s going to make guesses about what character is going to follow the input character. In addition, we’ll be able to evaluate any setting of the parameters of the <strong>nn</strong>, since we have a <strong>loss</strong> function. Basically, we’re going to take a look at the probabilities distributions our model assigns for our next character and find the <strong>loss</strong> between those and the labels (which are the character that we expect to come next in the bigram). By doing so, we can use gradient-based optimization to tune the weights of our <strong>nn</strong> that give us the output probabilities. Let’s begin this alternative approach by first constructing our dataset:</p></div><div id="EWs1FqdsdA" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Create training dataset of bigrams (x, y)
xs, ys = [], []
for w in words[:1]:
    chs = [&quot;.&quot;] + list(w) + [&quot;.&quot;]
    for ch1, ch2 in zip(chs, chs[1:]):
        print(ch1, ch2)
        xs.append(ctoi[ch1])
        ys.append(ctoi[ch2])
# Convert to pytorch tensor (https://pytorch.org/docs/stable/generated/torch.tensor.html)
xs = torch.tensor(xs)
ys = torch.tensor(ys)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="D92V6Kl9bhutTIDoPXAHU" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>. e
e m
m m
m a
a .
</span></code></pre></div></div></div></div><div id="r8I5VQD0Kt" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">xs</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="7yMn6v-E0g9WJjYyYWjhm" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([ 0,  5, 13, 13,  1])</span></code></div></div></div><div id="PoU8E3rg0W" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ys</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="rtPMj6j4dfX0sk-xgU2Y3" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([ 5, 13, 13,  1,  0])</span></code></div></div></div><div id="yhCZmfjnk0" class="myst-jp-nb-block relative group/block"><p>Now, how do we pass each character into the <strong>nn</strong>? <a href="https://en.wikipedia.org/wiki/One-hot" class="hover-link" target="_blank" rel="noreferrer" data-state="closed">One-hot encoding</a>! With this encoding, each integer is encoded with bits.</p></div><div id="iT4AaZG1qT" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">import torch.nn.functional as F

xenc = F.one_hot(xs, num_classes=27).float()
xenc</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="NHcGEVDTUTRxdZjJ9lBCO" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.]])</span></code></div></div></div><div id="HS82G5dQh7" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">xenc.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="WWaQLBHPxSNhFNjOR65kr" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([5, 27])</span></code></div></div></div><div id="TTXzfLgPN2" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">plt.figure()
plt.imshow(xenc);</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="U5qlKRPUHfzDXbHwbo75U" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><div class="p-2.5">Loading...</div></div></div></div><div id="cjpYsMnBwT" class="myst-jp-nb-block relative group/block"><p>Let’s create our neuron:</p></div><div id="hdolWYSuX3" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W = torch.randn((27, 1))
xenc @ W</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="wCiGIx7ZTtOdardahkhv8" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[-0.3270],
        [-1.4539],
        [-0.8739],
        [-0.8739],
        [ 1.4181]])</span></code></div></div></div><div id="sOW2G0dUPt" class="myst-jp-nb-block relative group/block"><p>Our neuron receives one character of size <!-- -->27<!-- --> and spits out <!-- -->1<!-- --> output value. However, as you can see, since PyTorch supports matrix multiplication, our neuron can receive <!-- -->5<!-- --> characters of size <!-- -->27<!-- --> in parallel and output each character’s output in a <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">5 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span> matrix (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>5</mn><mo>×</mo><mn>27</mn><mo stretchy="false">]</mo><mo>⋅</mo><mo stretchy="false">[</mo><mn>27</mn><mo>×</mo><mn>1</mn><mo stretchy="false">]</mo><mo>→</mo><mo stretchy="false">[</mo><mn>5</mn><mo>×</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[5 \times 27] \cdot [27 \times 1] \rightarrow [5 \times 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">27</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">27</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>). Now, let’s pass our <!-- -->5<!-- --> characters as inputs through <!-- -->27<!-- --> neurons instead of just <!-- -->1<!-- --> neuron:</p></div><div id="GNws3rZv4j" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W = torch.randn((27, 27))
xenc @ W</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="plfxfI47oysBQSY1rFJ12" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[ 0.9979,  0.2825,  1.1355,  0.3798, -0.2801,  0.0672, -1.1496,  2.1393,
         -0.2687, -1.4350,  1.1158,  0.4346, -0.4915, -0.1916,  1.4139, -0.4590,
         -0.5869,  1.6688,  0.8819,  0.8542, -0.0366, -0.6968,  0.1041,  0.8881,
          0.7592, -0.5573,  0.9596],
        [-0.1725, -1.5476,  1.5005,  1.4560,  0.9079, -1.2025,  0.1265,  0.1533,
         -0.2189, -1.3150,  1.6275,  0.3342,  1.4620, -0.3458, -0.2391,  0.5896,
          1.7679,  1.1726, -0.6278, -0.1539, -0.6117, -0.0106,  0.7131,  2.0526,
          1.2183,  1.6270, -1.3764],
        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,
          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,
          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,
          0.1524,  1.5829,  0.3142],
        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,
          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,
          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,
          0.1524,  1.5829,  0.3142],
        [ 0.2948,  0.0746, -0.4187,  0.4092, -0.6537,  1.1562,  0.6917, -1.2596,
         -0.1424, -0.5520, -1.1731, -0.4088, -0.6465, -0.2629, -0.3580,  0.8126,
         -1.7589,  1.7377, -0.5665,  1.9188, -0.6135, -1.2176,  0.0166,  0.1594,
         -0.8806,  0.6167, -0.9173]])</span></code></div></div></div><div id="VOOwPpBUwb" class="myst-jp-nb-block relative group/block"><p>Predictably, we get <!-- -->5<!-- --> arrays (one per input/character) of <!-- -->27<!-- --> outputs (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>5</mn><mo>×</mo><mn>27</mn><mo stretchy="false">]</mo><mo>⋅</mo><mo stretchy="false">[</mo><mn>27</mn><mo>×</mo><mn>27</mn><mo stretchy="false">]</mo><mo>→</mo><mo stretchy="false">[</mo><mn>5</mn><mo>×</mo><mn>27</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[5 \times 27] \cdot [27 \times 27] \rightarrow [5 \times 27]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">27</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">27</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">27</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">27</span><span class="mclose">]</span></span></span></span></span>). Each output number represents each neuron’s firing rate of a specific input. For example, the following is the firing rate of the 13th neuron of the 3rd input:</p></div><div id="RJeXQmWIgl" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">(xenc @ W)[3, 13]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="gkBMvidg3RHgFSBAKrEH9" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(-1.2633)</span></code></div></div></div><div id="XvkG219tuw" class="myst-jp-nb-block relative group/block"><p>What PyTorch allows is matrix multiplication that enables parallel dot products of many inputs in a batch with the weights of neurons of a <strong>nn</strong>. For example, this is how to multiply the inputs that represent the 3rd character with the weights of the 13th neuron:</p></div><div id="mgn4oMb2vS" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">xenc[3]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="UfDYOv8K0HvPjft-6Fqw6" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])</span></code></div></div></div><div id="HJBrX1qyZT" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W[:, 13]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="lBc6H9tAbGYThFEtP7iOL" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([-0.1916, -0.2629, -1.1183,  0.9108,  0.7797, -0.3458, -1.2783, -0.7899,
        -0.3221, -0.4800,  0.3307,  0.2826, -0.5372, -1.2633,  0.3663,  0.1210,
         0.0446, -0.1690, -0.3741, -0.0798, -0.5883, -0.9373, -0.1367, -0.2475,
        -0.4424, -2.0253, -0.1943])</span></code></div></div></div><div id="mwgODAo6DV" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">(xenc[3] * W[:, 13]).sum()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="Vh4lCnKuF06u-CmArwjAF" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(-1.2633)</span></code></div></div></div><div id="E1ipKixV5c" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">(xenc @ W)[3, 13]  # same as above</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="giRO1Gf1J_vFz-YJQilUn" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(-1.2633)</span></code></div></div></div><div id="wZdiiSatgl" class="myst-jp-nb-block relative group/block"><p>Ok, so what did is we fed our <!-- -->27<!-- -->-dimensional inputs into the first layer of a <strong>nn</strong> that has 27 neurons. These neurons perform <code>W * x</code>. They don’t have a bias and they don’t have a non-linearity like <code>tanh</code>. We are going to leave our network as is: a 1-layer <em>linear</em> <strong>nn</strong>. That’s it. Basically, the dumbest, smallest, simplest <strong>nn</strong>. Remember, what we trying to produce is a probability distribution for a next character in a sequence. And there’s <!-- -->27<!-- --> of them. But we have to come up with exact semantics as to how we are going to interpret these <!-- -->27<!-- --> numbers that these neurons take on. Intuitively, as we can see in the <code>xenc @ W</code> output, some of these outputs numbers are positive and some negative. That’s because they come out of a <strong>nn</strong> layer with weights are initialized from the normal <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> distribution. But, what we want however is something like a bigram count table that we previously produced, where each row told us the counts which we then normalized to get the probabilities. So, we want something similar to come out of our <strong>nn</strong>. But, what we have right now, are some negative and positive numbers. Now, we therefore want these numbers to represent the probabilities for the next character with their unique characteristics. For example, probabilities are positive numbers and they sum to 1. Also, they obviously have to be probabilities. They can’t be counts because counts are positive integers; not a great output from a <strong>nn</strong>. Instead, what the <strong>nn</strong> is going to output and how we are going to interpret these <!-- -->27<!-- --> output numbers is as log counts. One way to accomplish this is by exponentiating each output number so that the result is always positive. Specifically, exponentiating a  negative number yields a result that is a positive value <em>less</em> than <!-- -->1<!-- -->. Whereas, exponentiating a positive number yields a result whose value is between greater than <!-- -->1<!-- --> and <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord">∞</span></span></span></span></span>.</p></div><div id="KvoPSGbIjF" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">(xenc @ W).exp()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="BNVhuaEyYbbfr8YGOYCyR" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[2.7125, 1.3265, 3.1128, 1.4619, 0.7557, 1.0695, 0.3168, 8.4932, 0.7644,
         0.2381, 3.0519, 1.5444, 0.6117, 0.8256, 4.1119, 0.6319, 0.5561, 5.3059,
         2.4156, 2.3495, 0.9641, 0.4982, 1.1098, 2.4304, 2.1365, 0.5727, 2.6108],
        [0.8415, 0.2128, 4.4841, 4.2888, 2.4792, 0.3004, 1.1348, 1.1657, 0.8034,
         0.2685, 5.0910, 1.3968, 4.3146, 0.7077, 0.7873, 1.8032, 5.8586, 3.2305,
         0.5338, 0.8574, 0.5424, 0.9895, 2.0402, 7.7883, 3.3814, 5.0888, 0.2525],
        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,
         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,
         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],
        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,
         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,
         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],
        [1.3429, 1.0774, 0.6579, 1.5056, 0.5201, 3.1778, 1.9972, 0.2838, 0.8673,
         0.5758, 0.3094, 0.6644, 0.5239, 0.7688, 0.6990, 2.2538, 0.1722, 5.6841,
         0.5675, 6.8131, 0.5415, 0.2959, 1.0168, 1.1728, 0.4145, 1.8528, 0.3996]])</span></code></div></div></div><div id="nPE5YWHOtD" class="myst-jp-nb-block relative group/block"><p>Such exponentiation is a great way to make the <strong>nn</strong> predict counts. Which are positive numbers that can take on various values depending on the setting of <code>W</code>. Let’s break it down more:</p></div><div id="TcNFfdZZrn" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">logits = xenc @ W  # log-counts
counts = logits.exp()  # equivalent to the N bigram counts array
probs = counts / counts.sum(1, keepdims=True)
probs</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="95bDUYjNJyNAvoLcHKPt7" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,
         0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,
         0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502],
        [0.0139, 0.0035, 0.0739, 0.0707, 0.0409, 0.0050, 0.0187, 0.0192, 0.0132,
         0.0044, 0.0840, 0.0230, 0.0711, 0.0117, 0.0130, 0.0297, 0.0966, 0.0533,
         0.0088, 0.0141, 0.0089, 0.0163, 0.0336, 0.1284, 0.0558, 0.0839, 0.0042],
        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,
         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,
         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],
        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,
         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,
         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],
        [0.0371, 0.0298, 0.0182, 0.0416, 0.0144, 0.0879, 0.0552, 0.0078, 0.0240,
         0.0159, 0.0086, 0.0184, 0.0145, 0.0213, 0.0193, 0.0623, 0.0048, 0.1572,
         0.0157, 0.1884, 0.0150, 0.0082, 0.0281, 0.0324, 0.0115, 0.0512, 0.0111]])</span></code></div></div></div><div id="LXE1ajgvLU" class="myst-jp-nb-block relative group/block"><p>Therefore, we have a way to get the probabilities, where each row sums to <!-- -->1<!-- --> (since they are normalized), e.g.</p></div><div id="mkbENN2dqf" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">probs[0].sum().item()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="01eSQy8USN2YHgUn0FEQH" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>1.0</span></code></div></div></div><div id="eitVXny4Ka" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">probs.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="g9OB2TgdH9gQw1_2Aqvqi" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([5, 27])</span></code></div></div></div><div id="Fi965KywYW" class="myst-jp-nb-block relative group/block"><p>What we have achieved is that for every one of our <!-- -->5<!-- --> examples, we now have a row that came out of our <strong>nn</strong>. And because of the transformations here, we made sure that this output of the <strong>nn</strong> can be interpreted as probabilities. In other words, what we have done is that we took inputs, applied differentiable operations on them (e.g. <code>@</code>, <code>exp()</code>) that we can <strong>backprop</strong> through and we are getting out probability distributions. Take the first input character that was fed in as an example:</p></div><div id="u59ZXHtBWB" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">xenc[0]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="P5I_RrzqnQ814i85OIPdg" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])</span></code></div></div></div><div id="heNi2CYoXS" class="myst-jp-nb-block relative group/block"><p>that corresponds to the <code>.</code> symbol from the name:</p></div><div id="JGcUvm1I66" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">words[0]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="r4p5XPAJ5YfIzKceluPKA" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>&#x27;emma&#x27;</span></code></div></div></div><div id="RoP05uXllN" class="myst-jp-nb-block relative group/block"><p>The way we fed this character into the neural network is that we first got its index, then we one-hot encoded it, then it went into the <strong>nn</strong> and out came this distribution of probabilities:</p></div><div id="hnfPlE8Pot" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">probs[0]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="rqhewhPanyO6Tu9lQbpke" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,
        0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,
        0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502])</span></code></div></div></div><div id="dkkr7wktGR" class="myst-jp-nb-block relative group/block"><p>with a shape of:</p></div><div id="lYxWP6i10T" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">probs[0].shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="Mlccv61QalHhx86nWmQTA" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([27])</span></code></div></div></div><div id="iMpQzWvUck" class="myst-jp-nb-block relative group/block"><p>27<!-- --> numbers. We interpret these numbers of <code>probs[0]</code> as the probability or <em>how likely it is</em> for each of the corresponding characters to come next. As we train the <strong>nn</strong> by tuning the weights <code>W</code>, we are of course going to be getting different probabilities out for every character that you input. So, the question is: can we tune <code>W</code> such that the probabilities coming out are <em>pretty good</em>? The way we measure <em>pretty good</em> is by the <strong>loss</strong> function. Below you can see what have done in a simple summary:</p></div><div id="jxE9GVWbqf" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># SUMMARY ------------------------------&gt;&gt;&gt;&gt;
xs  # inputs</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="AXgTsBbnLa5y6go5eAekC" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([ 0,  5, 13, 13,  1])</span></code></div></div></div><div id="SeCIwxrD6S" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">ys  # targets</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="R3YGz7cu4tzOa-Grzny7q" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([ 5, 13, 13,  1,  0])</span></code></div></div></div><div id="HRnsl0m47b" class="myst-jp-nb-block relative group/block"><p>Both <code>xs</code> and <code>ys</code> constitute the dataset. They are integers representing characters of a sequence/word.</p></div><div id="eXAnCk1G9s" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Use a generator for reproducability and randomly initialize 27 neurons&#x27; weights. Each neuron receives 27 inputs.
g = torch.Generator().manual_seed(SEED)
W = torch.randn((27, 27), generator=g)  # 27 incoming weights for 27 neurons
# Encode the inputs into one-hot representations
xenc = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding
# Pass encoded inputs through first layer to get logits
logits = xenc @ W  # predict log-counts
# Exponentiate the logits to get fake counts
counts = logits.exp()  # counts, equivalent to N
# Normalize these counts to get probabilities
probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character
# NOTE: the 2 lines above constitute what is called a &#x27;softmax&#x27;
probs.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="ak97ARUDsUDfjWCMSCoRo" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>torch.Size([5, 27])</span></code></div></div></div><div id="pSMfPJZ712" class="myst-jp-nb-block relative group/block"><p>Softmax is a very-often-used <strong>loss</strong> function in <strong>nn</strong>s. It takes in logits, exponentiates them, then divides and normalizes. It’s a way of taking outputs of a linear layer that might be positive or negative and it outputs numbers that are only positive and always sum to <!-- -->1<!-- -->, adhering to the properties of probability distributions. It can be viewed as a normalization function if you want to think of it that way.</p></div><div id="GlxE1H8Rks" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">from IPython.display import Image, display
display(Image(filename=&#x27;softmax.jpeg&#x27;))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="9cjzDwW9GOYyPfi8BFs7Z" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-image"><img src="/build/16dc2c955b6f9f81a6cf3b8b72fe56f3.jpeg" alt="&lt;IPython.core.display.Image object&gt;"/></div></div></div><div id="upw7me1H5P" class="myst-jp-nb-block relative group/block"><p>Now, since every operation in the forward pass is differentiable, we can <strong>backprop</strong> through. Below, we iterate over every input character and describe what is going on:</p></div><div id="ZIuIEryCyJ" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">nlls = torch.zeros(5)
for i in range(5):
    # i-th bigram:
    x = xs[i].item()  # input character index
    y = ys[i].item()  # label character index
    print(&quot;--------&quot;)
    print(f&quot;bigram example {i+1}: {itoc[x]}{itoc[y]} (indexes {x},{y})&quot;)
    print(&quot;input to the nn:&quot;, x)
    print(&quot;output probabilities from the nn:&quot;, probs[i])
    print(&quot;label (actual next character):&quot;, y)
    p = probs[i, y]
    print(&quot;probability assigned by the nn to the correct next character:&quot;, p.item())
    logp = torch.log(p)
    print(&quot;log likelihood:&quot;, logp.item())
    nll = -logp
    print(&quot;negative log likelihood:&quot;, nll.item())
    nlls[i] = nll
loss = nlls.mean()
print(&quot;=========&quot;)
print(&quot;average negative log likelihood, i.e. loss =&quot;, loss.item())</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="y8w1A6wIG9H-njLNNmO_N" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>--------
bigram example 1: .e (indexes 0,5)
input to the nn: 0
output probabilities from the nn: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,
        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,
        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])
label (actual next character): 5
probability assigned by the nn to the correct next character: 0.01228625513613224
log likelihood: -4.399273872375488
negative log likelihood: 4.399273872375488
--------
bigram example 2: em (indexes 5,13)
input to the nn: 5
output probabilities from the nn: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,
        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,
        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])
label (actual next character): 13
probability assigned by the nn to the correct next character: 0.018050700426101685
log likelihood: -4.014570713043213
negative log likelihood: 4.014570713043213
--------
bigram example 3: mm (indexes 13,13)
input to the nn: 13
output probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,
        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,
        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])
label (actual next character): 13
probability assigned by the nn to the correct next character: 0.026691533625125885
log likelihood: -3.623408794403076
negative log likelihood: 3.623408794403076
--------
bigram example 4: ma (indexes 13,1)
input to the nn: 13
output probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,
        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,
        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])
label (actual next character): 1
probability assigned by the nn to the correct next character: 0.07367686182260513
log likelihood: -2.6080665588378906
negative log likelihood: 2.6080665588378906
--------
bigram example 5: a. (indexes 1,0)
input to the nn: 1
output probabilities from the nn: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,
        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,
        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])
label (actual next character): 0
probability assigned by the nn to the correct next character: 0.014977526850998402
log likelihood: -4.201204299926758
negative log likelihood: 4.201204299926758
=========
average negative log likelihood, i.e. loss = 3.7693049907684326
</span></code></pre></div></div></div></div><div id="Yn3VvooxGH" class="myst-jp-nb-block relative group/block"><p>As you can see, the probabilities assigned by the <strong>nn</strong> to the correct next character are bad (pretty low). See for example the probability predicted by the network of <code>m</code> following <code>e</code> (<code>em</code> example): the <strong>nll</strong> value is very high (e.g. <!-- -->4.0145<!-- -->). And in general, for the whole word, the <strong>loss</strong> (the average <strong>nll</strong>) is high! This means that this is not a favorable setting of weights and we can do better. One easy way to do better is to reinitialize <code>W</code> using a different seed for example and pray to god that the loss is smaller or repeat until it is. But that is what amateurs do. We are professionals or, at least, we want to be! And what professionals do is they start with random weights, like we did, and then they optimize those weights in order to minimize the loss. We do so by some gradient-based optimization (e.g. gradient descent) which entails first doing backprop in order to compute the gradients of that weight <strong>w.r.t.</strong> to those weights and then changing the weights by some such gradient amount in order to optimize them and minimize the loss. As we did with <strong>micrograd</strong>, we will write an optimization loop for doing the backward pass. But instead of mean-squared error, we are using the <strong>nll</strong> as a loss function, since we are dealing with a classification task and not a regression one.</p></div><div id="JpWu136b9U" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">g = torch.Generator().manual_seed(SEED)
W = torch.randn(
    (27, 27), generator=g, requires_grad=True
)  # 27 incoming weights for 27 neurons


def forward_pass(regularize=False):
    num = xs.nelement()
    xenc = F.one_hot(
        xs, num_classes=27
    ).float()  # input to the network: one-hot encoding
    logits = xenc @ W  # predict log-counts
    counts = logits.exp()  # counts, equivalent to N
    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character
    loss = -probs[torch.arange(num), ys].log().mean()
    return W, loss


W, loss = forward_pass()
# backward pass
W.grad = None  # set to zero
loss.backward()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="tJoj_9Poi_JFFPkGUE6QN" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="MEdzSN9vgI" class="myst-jp-nb-block relative group/block"><p>Now, something magical happened when <code>backward</code> ran. Like <strong>micrograd</strong>, PyTorch, during the forward pass, keeps track of all the operations under the hood and builds a full computational graph. So, it knows all the dependencies and all the mathematical operations of everything. Therefore, calling <code>backward</code> on the <strong>loss</strong> fills in the gradients of all the intermediate nodes, all the way back to the <code>W</code> value nodes. Take a look:</p></div><div id="JZbfYEyXHx" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W.grad</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="LrRsz42PQ7Zx6JIf5fhoc" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,
          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,
          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,
          0.0024,  0.0307,  0.0292],
        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,
          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,
          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,
          0.0131,  0.0101,  0.0018],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,
          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,
          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,
          0.0024,  0.0004,  0.0094],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,
          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,
          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,
          0.0482,  0.0187,  0.0051],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000]])</span></code></div></div></div><div id="bWJWum2ak6" class="myst-jp-nb-block relative group/block"><p>And obviously:</p></div><div id="AssIupMaUf" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">assert W.shape == W.grad.shape</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="TDGdc6V6B6xFbxmD-zOHV" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="JMw0uNlfLF" class="myst-jp-nb-block relative group/block"><p>What a gradient value is telling us, e.g.</p></div><div id="PGVrj8x9mm" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W.grad[1][4].item()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="synq_C8-5B6RYzaFekMb6" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>0.012119228951632977</span></code></div></div></div><div id="Qgg8ickvMc" class="myst-jp-nb-block relative group/block"><p>is that nudging the specific corresponding weight by a small <code>h</code> value, would nudge the <strong>loss</strong> by that gradient amount. Since we want to decrease the <strong>loss</strong>, we simply need to change the weights by a small negative fraction of the gradients in order to move them in the direction that locally most steeply decreases the <strong>loss</strong> value:</p></div><div id="j34x5jSgoG" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W.data += -0.1 * W.grad</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="aTmVc3xzr9JrtCKU2JuRC" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="sd9HRxQm9i" class="myst-jp-nb-block relative group/block"><p>We just did a single gradient descent optimization step, which means that if we re-calculate the loss, it will be lower:</p></div><div id="hKkLXaoYVc" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">W, loss = forward_pass()
loss.item()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="5QyOZklysF6v3jUrojlik" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>3.7492127418518066</span></code></div></div></div><div id="jGSKiTbTIJ" class="myst-jp-nb-block relative group/block"><p>Tada! All we have to do now is put everything together and stick the single step into a loop so that we can do multi-step gradient descent optimization. This time, for all the words in our dataset, not just <code>emma</code>!</p></div><div id="gQGLApDVbl" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># create the dataset
xs, ys = [], []
for w in words:
    chs = [&quot;.&quot;] + list(w) + [&quot;.&quot;]
    for ch1, ch2 in zip(chs, chs[1:]):
        xs.append(ctoi[ch1])
        ys.append(ctoi[ch2])
xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print(&quot;number of examples (bigrams): &quot;, num)
# initialize the &#x27;network&#x27;
g = torch.Generator().manual_seed(SEED)
W = torch.randn((27, 27), generator=g, requires_grad=True)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="yCPQd-vUpDYdLroGnBfAa" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>number of examples (bigrams):  228146
</span></code></pre></div></div></div></div><div id="jLHRIq9a80" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># gradient descent
for k in range(100):
    W, loss = forward_pass()
    print(loss.item())
    # backward pass
    W.grad = None  # set to zero the gradient
    loss.backward()
    # update
    W.data += -50 * W.grad</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="c8hUF76nQOlMXBjB0d1z_" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>3.758953809738159
3.371098756790161
3.1540417671203613
3.020373821258545
2.9277119636535645
2.860402822494507
2.8097293376922607
2.7701027393341064
2.7380733489990234
2.711496591567993
2.6890034675598145
2.6696884632110596
2.6529300212860107
2.638277292251587
2.6253881454467773
2.6139907836914062
2.603863477706909
2.5948219299316406
2.586712121963501
2.57940411567688
2.572789192199707
2.5667762756347656
2.5612881183624268
2.5562589168548584
2.551633596420288
2.547365665435791
2.5434155464172363
2.539748430252075
2.5363364219665527
2.5331544876098633
2.5301806926727295
2.5273969173431396
2.5247862339019775
2.522334575653076
2.520029067993164
2.517857789993286
2.515810966491699
2.513878345489502
2.512052059173584
2.510324001312256
2.5086867809295654
2.5071346759796143
2.5056610107421875
2.5042612552642822
2.502929210662842
2.5016613006591797
2.5004522800445557
2.4992990493774414
2.498197317123413
2.497144937515259
2.496137857437134
2.495173692703247
2.4942495822906494
2.493363380432129
2.4925124645233154
2.4916954040527344
2.4909095764160156
2.4901540279388428
2.4894261360168457
2.488725185394287
2.4880495071411133
2.4873974323272705
2.4867680072784424
2.4861605167388916
2.4855728149414062
2.4850049018859863
2.484455108642578
2.4839231967926025
2.483408212661743
2.4829084873199463
2.482424020767212
2.481955051422119
2.481499195098877
2.4810571670532227
2.4806275367736816
2.480210304260254
2.479804754257202
2.479410171508789
2.4790265560150146
2.4786536693573
2.478290557861328
2.4779367446899414
2.477592706680298
2.477257251739502
2.4769301414489746
2.476611852645874
2.4763011932373047
2.4759981632232666
2.4757025241851807
2.475414276123047
2.475132703781128
2.474858045578003
2.4745893478393555
2.474327802658081
2.474071741104126
2.4738216400146484
2.4735770225524902
2.4733383655548096
2.47310471534729
2.47287654876709
</span></code></pre></div></div></div></div><div id="QBn9aHNJEm" class="myst-jp-nb-block relative group/block"><p>Awesome! What we least expect is that our <strong>loss</strong>, by using such gradient-based optimization, becomes as small as the <strong>loss</strong> we got by our more primitive bigram-count-matrix way that we previously employed for optimizing. So, basically, before, we achieved roughly the same <strong>loss</strong> just by counting, whereas now we used gradient descent. It just happens that the explicit, counting approach nicely optimizes the model without the need for any gradient-based optimization because the setup for bigram language models is so straightforward and simple that we can afford to just directly estimate the probabilities and keep them in a table. However, the <strong>nn</strong> approach is much more flexible and scalable! And we have actually gained a lot. What we can do from hereon is expand and complexify our approach. Meaning, that instead of just taking a single character and predicting the next one in an extremely simple <strong>nn</strong>, as we have done so far, we will be taking multiple previous characters and we will be feeding them into increasingly more complex <strong>nn</strong>s. But, fundamentally, we will still be just calculating logits that will be going through exactly the same transformation by passing them through a softmax and doing the same gradient-based optimization process we just did. But before we do that, remember the smoothing we did by adding fake counts to our bigram count matrix? Turns out, we can do equivalent smoothing in our <strong>nn</strong> too! In particular, just incentivizing the weights to be zero for example leads to the probabilities being uniform, which is a form of smoothing. Such incentivization can be accomplished through regularization. It involves just adding a term like this:</p></div><div id="Sffka4DTkN" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">(W**2).mean()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="dwL5szk8vTMvg6qOYo5Bd" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-text" class="font-mono text-sm whitespace-pre-wrap myst-jp-safe-output-text"><code><span>tensor(1.6880, grad_fn=&lt;MeanBackward0&gt;)</span></code></div></div></div><div id="FvtENKpU7T" class="myst-jp-nb-block relative group/block"><p>to the <strong>loss</strong> as such:</p><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-stone-200/10"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div></div><div id="HxnH77qSOC" class="myst-jp-nb-block relative group/block"><p>where <code>0.01</code> represents the strength of the regulatization term. Optimizing with this term included in the loss would smoothen the model. Yay! Lastly, let’s sample from our <strong>nn</strong>:</p></div><div id="U5rWEA30zP" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">g = torch.Generator().manual_seed(SEED)
for i in range(20):
    out = []
    ix = 0
    while True:
        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()
        logits = xenc @ W  # predict log-counts
        counts = logits.exp()  # counts, equivalent to N
        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character
        # sample from probabilities distribution
        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()
        out.append(itoc[ix])
        if ix == 0:
            break
    print(&#x27;&#x27;.join(out))</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" data-mdast-node-id="-BpYlI7peSKlN4thlpO4L" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>junide.
janasah.
p.
cfay.
a.
nn.
kohin.
tolian.
juwe.
kilanaauranilevias.
dedainrwieta.
ssonielylarte.
faveumerifontume.
phynslenaruani.
core.
yaenon.
ka.
jabi.
werimikimaynin.
anaasn.
</span></code></pre></div></div></div></div><div id="Hkby9iVCoO" class="myst-jp-nb-block relative group/block"><p>We are getting kind of the same results as we previously did with our counting method! Not unpredictable at all, since our <strong>loss</strong> values are close enough. If we trained our <strong>nn</strong> more and the <strong>loss</strong> values became the same, it would means that the two models are identical. Meaning that given the same inputs, they would spit out the same outputs.</p></div><div id="VMotik9hXS" class="myst-jp-nb-block relative group/block"><h2 id="summary" class="relative group"><span class="heading-text">Summary</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#summary" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="tGAi4SGEQC" class="myst-jp-nb-block relative group/block"><p>All in all, we have actually covered lots of ground. To sum up, we introduced the bigram character language model, we saw how we can train the model, how we can sample from the model and how we can evaluate the quality of the model using the <strong>nll</strong> <strong>loss</strong>. We actually trained the model in two completely different ways that actually give or can give (with adequate training) the same result. In the first way, we just counted up the frequency of all the bigrams and normalized. Whereas, in the second way, we used the <strong>nll</strong> <strong>loss</strong> as a guide to optimizing the counts matrix or the counts array, so that the <strong>loss</strong> is minimized in a gradient-based framework. Despite our <strong>nn</strong> being super simple (single linear layer), it is the more flexible approach.</p></div><div id="oS9wq8iqfG" class="myst-jp-nb-block relative group/block"><h2 id="outro" class="relative group"><span class="heading-text">Outro</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#outro" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="c0dmOTN3TS" class="myst-jp-nb-block relative group/block"><p>In the follow-up lessons, we are going to complexify by taking more and more of these characters and we are going to be feeding them into a new <strong>nn</strong> that does more exciting stuff. Buckle up!</p></div><div class="myst-backmatter-parts"></div><div class="myst-footer-links flex pt-10 mb-10 space-x-4"><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-prev" href="/micrograduate/micrograd"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">microgra∇uate</div>1. micrograd: implementing an autograd engine</div></div></a><a class="myst-footer-link flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700 myst-footer-link-next" href="/micrograduate/makemore2"><div class="flex h-full align-middle"><div class="flex-grow"><div class="myst-footer-link-group text-xs text-gray-500 dark:text-gray-400">microgra∇uate</div>3. makemore (part 2): mlp</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="myst-footer-link-icon self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></article></main><script>((a,l)=>{if(!window.history.state||!window.history.state.key){let u=Math.random().toString(32).slice(2);window.history.replaceState({key:u},"")}try{let d=JSON.parse(sessionStorage.getItem(a)||"{}")[l||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(u){console.error(u),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-PCJPW7TK.js"/><link rel="modulepreload" href="/build/_shared/chunk-AQ2CODAG.js"/><link rel="modulepreload" href="/build/_shared/chunk-JJXTQVMA.js"/><link rel="modulepreload" href="/build/_shared/chunk-OZE3FFNP.js"/><link rel="modulepreload" href="/build/_shared/chunk-7UUHRSK3.js"/><link rel="modulepreload" href="/build/_shared/chunk-C4DFGG5C.js"/><link rel="modulepreload" href="/build/_shared/chunk-J7TUH54J.js"/><link rel="modulepreload" href="/build/_shared/chunk-FZ2S7OYD.js"/><link rel="modulepreload" href="/build/_shared/chunk-JEM6JXYA.js"/><link rel="modulepreload" href="/build/_shared/chunk-34XIY2DH.js"/><link rel="modulepreload" href="/build/_shared/chunk-KQM5FBHR.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-7HNKBP4B.js"/><link rel="modulepreload" href="/build/_shared/chunk-CUKUDK3R.js"/><link rel="modulepreload" href="/build/_shared/chunk-3EBOCCHJ.js"/><link rel="modulepreload" href="/build/_shared/chunk-O4VQNZ62.js"/><link rel="modulepreload" href="/build/_shared/chunk-4OEDG4JQ.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-EDJFWIEV.js"/><link rel="modulepreload" href="/build/_shared/chunk-ECLX7DIY.js"/><link rel="modulepreload" href="/build/routes/$-AD65NCUT.js"/><script>window.__remixContext = {"url":"/micrograduate/makemore1","state":{"loaderData":{"root":{"config":{"version":3,"myst":"1.8.0","options":{"favicon":"/build/logo-1eba642f581799c20c3dfbf015138027.png","logo":"/build/logo-1eba642f581799c20c3dfbf015138027.png","folders":true,"style":"/build/custom-954e955dcebf36f153cd28d4c80155cc.css"},"nav":[],"actions":[],"projects":[{"title":"microgra∇uate","github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","toc":[{"file":"index.md"},{"file":"micrograduate/micrograd.ipynb"},{"file":"micrograduate/makemore1.ipynb"},{"file":"micrograduate/makemore2.ipynb"},{"file":"micrograduate/makemore3.ipynb"},{"file":"micrograduate/makemore4.ipynb"},{"file":"micrograduate/makemore5.ipynb"},{"file":"micrograduate/picogpt.ipynb"}],"thumbnail":"/build/heading-2d149a320da7264b9eda93edc721b9e5.png","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"micrograduate.micrograd","title":"1. micrograd: implementing an autograd engine","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore1","title":"2. makemore (part 1): implementing a bigram character-level language model","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore2","title":"3. makemore (part 2): mlp","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore3","title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore4","title":"5. makemore (part 4): becoming a backprop ninja","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore5","title":"6. makemore (part 5): building a WaveNet","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.picogpt","title":"7. picoGPT: implementing a tiny GPT from scratch","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static"},"routes/$":{"config":{"version":3,"myst":"1.8.0","options":{"favicon":"/build/logo-1eba642f581799c20c3dfbf015138027.png","logo":"/build/logo-1eba642f581799c20c3dfbf015138027.png","folders":true,"style":"/build/custom-954e955dcebf36f153cd28d4c80155cc.css"},"nav":[],"actions":[],"projects":[{"title":"microgra∇uate","github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","toc":[{"file":"index.md"},{"file":"micrograduate/micrograd.ipynb"},{"file":"micrograduate/makemore1.ipynb"},{"file":"micrograduate/makemore2.ipynb"},{"file":"micrograduate/makemore3.ipynb"},{"file":"micrograduate/makemore4.ipynb"},{"file":"micrograduate/makemore5.ipynb"},{"file":"micrograduate/picogpt.ipynb"}],"thumbnail":"/build/heading-2d149a320da7264b9eda93edc721b9e5.png","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"micrograduate.micrograd","title":"1. micrograd: implementing an autograd engine","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore1","title":"2. makemore (part 1): implementing a bigram character-level language model","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore2","title":"3. makemore (part 2): mlp","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore3","title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore4","title":"5. makemore (part 4): becoming a backprop ninja","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore5","title":"6. makemore (part 5): building a WaveNet","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.picogpt","title":"7. picoGPT: implementing a tiny GPT from scratch","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"page":{"version":3,"kind":"Notebook","sha256":"719e54f7ba077e9cbf8e75e8ccc6e9b666dd3a22e73cf9647a47054701f780ce","slug":"micrograduate.makemore1","location":"/micrograduate/makemore1.ipynb","dependencies":[],"frontmatter":{"title":"2. makemore (part 1): implementing a bigram character-level language model","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"micrograduate-env","language":"python"},"github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","source_url":"https://github.com/ckaraneen/micrograduate/blob/main/micrograduate/makemore1.ipynb","edit_url":"https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore1.ipynb","exports":[{"format":"ipynb","filename":"makemore1.ipynb","url":"/build/makemore1-4bef9955aa3f9760fe91c61d5ffc3f98.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git \u003e /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt","key":"G2sZV7GbyB"},{"type":"outputs","id":"IVFxpASLFlf6YAhrrmvsc","children":[],"key":"RpXRwFntDS"}],"key":"PR0lQCo9X7"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Intro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SEIbWn4QWz"}],"identifier":"intro","label":"Intro","html_id":"intro","implicit":true,"key":"sveogJrx3n"}],"key":"z3704UKTtT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Just like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LWOBXIiaHF"},{"type":"link","url":"#1.-micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nMDZU4oR3R"}],"urlSource":"#1.-micrograd","key":"QsB15sVDoE"},{"type":"text","value":" before it, here, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fJTiWd2lMd"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"step-by-step","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QDA6gKypn1"}],"key":"kSStQNAM9R"},{"type":"text","value":" with everything ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ps6Uq0bWEu"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"spelled-out","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KNhTUGZj36"}],"key":"VCwFGcMUsK"},{"type":"text","value":", we will build ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"edaBwlnpfG"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Kwn4rFVGdg"}],"key":"cPQazmtirv"},{"type":"text","value":": a bigram character-level language model. We’re going to build it out slowly and together! But what is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CG8qHAMTF2"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RA9LUMdz5i"}],"key":"r5XagSx7Li"},{"type":"text","value":"? As the name suggests, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IjTtEy7vCt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BZ86qBv3sS"}],"key":"PcSAthwb4B"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vtcBJlfs0B"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makes more","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qfj2Sr708Y"}],"key":"jJa6x4Oeof"},{"type":"text","value":" of things that you give it. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lU2IXex526"},{"type":"link","url":"/build/names-9e8297c91106d2ea4113125b84bb5653.txt","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"names.txt","key":"zmQyQyKRGY"}],"urlSource":"names.txt","static":true,"protocol":"file","key":"HZappjtsBW"},{"type":"text","value":" is an example dataset. Specifically, it is a very large list of different names. If you train ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GuhUqtQ5tf"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CaOCxC69U4"}],"key":"gtek4MN4wA"},{"type":"text","value":" on this dataset, it will learn to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ecOjSXkMMo"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"make more","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A1GxS94n0G"}],"key":"RUXbUeTToZ"},{"type":"text","value":" of name-like things, basically more unique names! So, maybe if you have a baby and you’re looking for a new, cool-sounding unique name, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KTn0AQXI8Z"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gobaqef9JW"}],"key":"jT0INz54ML"},{"type":"text","value":" might help you. Here are some examples of such names that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gQ8E1qenXa"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d2eAUoT2hR"}],"key":"BaH1PO53bm"},{"type":"text","value":" will be able to generate:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aVpFgausA0"}],"key":"lO8ni3Esx3"}],"key":"DbPN8qSVRn"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"text","value":"dontell\nkhylum\ncamatena\naeriline\nnajlah\nsherrith\nryel\nirmi\ntaislee\nmortaz\nakarli\nmaxfelynn\nbiolett\nzendy\nlaisa\nhalliliana\ngoralynn\nbrodynn\nromima\nchiyomin\nloghlyn\nmelichae\nmahmed\nirot\nhelicha\nbesdy\nebokun\nlucianno","position":{"start":{"line":1,"column":1},"end":{"line":30,"column":1}},"key":"O4JX5Adkma"}],"key":"juPuo0MyU3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"dontell","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W0bO4SfsIZ"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jb8TlGpGWw"},{"type":"inlineCode","value":"irot","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WzeEits7bP"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a7J9OvvBF9"},{"type":"inlineCode","value":"zendy","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zUkg0BrWxK"},{"type":"text","value":", and so on, you name it! So under the hood, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d9kIRsyIch"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YQ44OJX5B6"}],"key":"FI7qe13i9J"},{"type":"text","value":" is a character-level language model. That means that it’s treating every single line (i.e. name) of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EJmzGaefsj"},{"type":"link","url":"/build/names-9e8297c91106d2ea4113125b84bb5653.txt","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"its training dataset","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sj2U3k6Vnx"}],"urlSource":"names.txt","static":true,"protocol":"file","key":"CPqsA2xD4i"},{"type":"text","value":" as an example. And each example is treated as a sequence of individual characters. For instance, it treats the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I3vTjM84r3"},{"type":"inlineCode","value":"reese","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QM6l6R5vl7"},{"type":"text","value":" as the sequence of characters: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NrvKb6ogW3"},{"type":"inlineCode","value":"r","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vW3pT1zxFT"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uxgpJNdZj2"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yLYVw1wIMB"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iI7Rvy54B9"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xFipTJX9Dk"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o1Ljzy15pX"},{"type":"inlineCode","value":"s","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qfGSP17Hhx"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MHRKb76fYU"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uL8G3HyxhE"},{"type":"text","value":". That is the level on which we are building out ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZI5JJdZhLI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"it4af2tRso"}],"key":"MtcRHVLrya"},{"type":"text","value":". Basically, its purpose is this: given a character, it can predict the next character in the sequence based upon the names that it has seen so far. Now, we’re actually going to implement a large number of character-level language models, following a few key innovations:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"idDFohJH29"}],"key":"lXtrUL84bR"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Bigram (one character predicts the next one with a lookup table of counts)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"zBXNVp9Cby"}],"key":"Vhsm39nFCB"}],"key":"c1bzl727rF"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"MLP, following ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"oWJwSbmc4L"},{"type":"link","url":"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Bengio et al. 2003","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"tpuXMsDJiy"}],"urlSource":"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf","key":"MdiPYFPTOy"}],"key":"MKZI847Ih8"}],"key":"KkvzcC7RYh"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"CNN, following ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"qauOKm1wbk"},{"type":"link","url":"https://arxiv.org/abs/1609.03499","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"DeepMind WaveNet 2016","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"PdspNHPQ03"}],"urlSource":"https://arxiv.org/abs/1609.03499","key":"HmxXAe4on4"},{"type":"text","value":" (in progress...)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"bqB465DZIe"}],"key":"l3fFz8QKY1"}],"key":"Rejugf6Sgg"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RNN, following ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"noOgjkvLdT"},{"type":"link","url":"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Mikolov et al. 2010","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"AIqx1uG0eZ"}],"urlSource":"https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf","key":"KlCu9A1sRb"}],"key":"WzSc634iAT"}],"key":"VaFAs4a5MY"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"LSTM, following ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"bRAQFffEMx"},{"type":"link","url":"https://arxiv.org/abs/1308.0850","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Graves et al. 2014","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Idmo6lePyn"}],"urlSource":"https://arxiv.org/abs/1308.0850","key":"QfIkkUDJuv"}],"key":"DPfhmkAGdA"}],"key":"xUeLaXTDdL"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"GRU, following ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"W1JrwhWhCB"},{"type":"link","url":"https://arxiv.org/abs/1409.1259","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Kyunghyun Cho et al. 2014","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"JQpQwFKflk"}],"urlSource":"https://arxiv.org/abs/1409.1259","key":"IajMZlFgJ0"}],"key":"ZZ3KUmgtqn"}],"key":"CbsRHKjy3h"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Transformer, following ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"GEepJVnI2y"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Vaswani et al. 2017","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ONkmzebMDV"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"b1WCM2N5Y7"}],"key":"jLLnYqBaJo"}],"key":"a8XNmId7V4"}],"key":"rUmigqwLfz"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"In fact, the transformer we are going to build will be the equivalent of ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"QFpsXhaDFA"},{"type":"link","url":"https://en.wikipedia.org/wiki/GPT-2","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"GPT-2","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"U8gl5S1rod"}],"urlSource":"https://en.wikipedia.org/wiki/GPT-2","data":{"page":"GPT-2","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"Bo5fjmH9Y3"},{"type":"text","value":". Kind of a big deal, since it’s a modern network and by the end of this guide you’ll actually understand how it works at the level of characters. Later on, we will probably spend some time on the word level, so we can generate documents of words, not just segments of characters. And then we’re probably going to go into image and image-text networks such as ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"KmUtcPrUi2"},{"type":"link","url":"https://en.wikipedia.org/wiki/DALL-E","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"DALL-E","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"QbiUwoWa3U"}],"urlSource":"https://en.wikipedia.org/wiki/DALL-E","data":{"page":"DALL-E","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"ptzn4zAtn5"},{"type":"text","value":", ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"TXcvPumZjk"},{"type":"link","url":"https://en.wikipedia.org/wiki/Stable_Diffusion","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Stable Diffusion","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"z2FyWNAPVW"}],"urlSource":"https://en.wikipedia.org/wiki/Stable_Diffusion","data":{"page":"Stable_Diffusion","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"VxDml7rqPP"},{"type":"text","value":", and so on. But first, let’s jump into character-level modeling.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"DdhvY6hZcc"}],"key":"bitpBzN5gN"}],"key":"mjCxKF0y8o"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Building a bigram language model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sciPfBnh2F"}],"identifier":"building-a-bigram-language-model","label":"Building a bigram language model","html_id":"building-a-bigram-language-model","implicit":true,"key":"Ef3AoeTCdX"}],"key":"nrDG3HVjNT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s start by reading all the names into a list:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hDmHxcHB5U"}],"key":"rSitBQuhI4"}],"key":"l2joLahAwd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words = open(\"names.txt\").read().splitlines()","key":"TzaBXcxygv"},{"type":"outputs","id":"GmT3ZU9IoarEKvGoSZMJn","children":[],"key":"h5LFSrVy8e"}],"key":"oUx8oR0ePr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words[:10]","key":"jQCVscW5jy"},{"type":"outputs","id":"KKCTerrwjmmaqRKCz8bNh","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"['emma',\n 'olivia',\n 'ava',\n 'isabella',\n 'sophia',\n 'charlotte',\n 'mia',\n 'amelia',\n 'harper',\n 'evelyn']","content_type":"text/plain"}}},"children":[],"key":"zEylI8OQWR"}],"key":"Ge8Da3rxFi"}],"key":"pnDN0cs5lS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we want to learn a bit more about this dataset.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"INwr630SlJ"}],"key":"i90MxiUgLn"}],"key":"pC22aCtNbs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(words)","key":"BPMPnFcIfu"},{"type":"outputs","id":"jEqaHrXM0yjy2q2f5iOqf","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"32033","content_type":"text/plain"}}},"children":[],"key":"Pxnn2bUbgc"}],"key":"e1FJCL1nCL"}],"key":"o2U4fjN9cW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(min(words, key=len))  # shortest","key":"n9KktvuWgn"},{"type":"outputs","id":"KCjxlA5ykA37FzSUURpLC","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":5,"metadata":{},"data":{"text/plain":{"content":"2","content_type":"text/plain"}}},"children":[],"key":"KWHfLPWhff"}],"key":"TANvie4mLO"}],"key":"pXrSh6Eh3z"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(max(words, key=len))  # longest","key":"oKAMvEhKRc"},{"type":"outputs","id":"lIZV2S6-5Ar7fgn44vsAX","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":6,"metadata":{},"data":{"text/plain":{"content":"15","content_type":"text/plain"}}},"children":[],"key":"tsdTDbvZ0O"}],"key":"eCt4D3bGfL"}],"key":"S6nEZRauVW"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s think through our very first language model. A character-level language model is predicting the next character in the sequence given already some concrete sequence of characters before it. What we have to realize here is that every single word like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MBqr5zQvUh"},{"type":"inlineCode","value":"isabella","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sl4ibyDAaM"},{"type":"text","value":" is actually quite a few examples packed in that single word. Because, let’s think: what is a word telling us really? It’s saying that the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HrcHAdQ7k1"},{"type":"inlineCode","value":"i","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nGPkQdnYhi"},{"type":"text","value":" is a very likely character to come first in the sequence that constitutes a name. The character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OFDtnz5lTz"},{"type":"inlineCode","value":"s","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pjZaPXrPQN"},{"type":"text","value":" is likely to come after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YH5eX7g4V4"},{"type":"inlineCode","value":"i","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LFz6PHDLZv"},{"type":"text","value":", the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WOTFWCzP7s"},{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QpfsY35bsU"},{"type":"text","value":" is likely to come after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xwvDNuNJUX"},{"type":"inlineCode","value":"is","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jMwiIcMh1C"},{"type":"text","value":", the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XiRkP1cIiq"},{"type":"inlineCode","value":"b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uOkoltkUwx"},{"type":"text","value":" is likely to come after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lEtvZLtpQZ"},{"type":"inlineCode","value":"isa","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qwYDYjpboA"},{"type":"text","value":", and so on all the way to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VlOwLunyUk"},{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XhsrewlDTR"},{"type":"text","value":" following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sQaVBMxzji"},{"type":"inlineCode","value":"isabell","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HQeWZehhHP"},{"type":"text","value":". And then there’s one more important piece of information in here. And that is that after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g5aG2sqTIY"},{"type":"inlineCode","value":"isabella","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WIYURGuqSJ"},{"type":"text","value":", the word is very likely to end. So, time to build our first network: a bigram language model. In these, we are working with two characters at a time. So, we are only looking for one character we are given and we are trying to predict the next character in a sequence. For example, in the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZR5jLdCWB4"},{"type":"inlineCode","value":"charlotte","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v2kAm1o1DZ"},{"type":"text","value":", we ask: what characters are likely to follow ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PNDfWxTNwX"},{"type":"inlineCode","value":"r","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nZkA7zboge"},{"type":"text","value":"?  In the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QnhNPLXEo6"},{"type":"inlineCode","value":"sophia","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C6KRH97d2X"},{"type":"text","value":": we ask what characters are likely to follow ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I39CjJPmQf"},{"type":"inlineCode","value":"p","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VJJMuUnXhN"},{"type":"text","value":"? And so on. This mean we are just modeling that local structure. Meaning, we only look at the previous character, even though there might be a lot of useful information before it. This is a very simple model, which is why it’s a great place to start! We can learn about the statistics of which characters are likely to follow which other characters by counting. So by iterating over all names, we can count how often each consecutive pair (bigram) of characters appears.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WBppTCb5Ey"}],"key":"kubMydJP0S"}],"key":"V2iUJ654av"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"b = {}\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        b[bigram] = b.get(bigram, 0) + 1","key":"uj75xC3tCt"},{"type":"outputs","id":"GJlLPJZOYq6tKfguRa8Dz","children":[],"key":"bC4J3ONqVp"}],"key":"ZGaVgUUwgs"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Notice that we have also added the character ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BgLj2yI9jn"},{"type":"inlineCode","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RPOzS36lH5"},{"type":"text","value":" to signify the start and end of each word. And obviously, the variable ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sCLaLYlLjM"},{"type":"inlineCode","value":"b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gpq5oB3Qlv"},{"type":"text","value":" now holds the statistics of the entire dataset.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"E7mHF2sRiT"}],"key":"eCCPaNg4iL"}],"key":"F1isFITMRr"},{"type":"block","kind":"notebook-code","data":{"scrolled":true,"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"sorted(b.items(), key=lambda tup: tup[1], reverse=True)","visibility":"show","key":"Gz1FGwVmj6"},{"type":"outputs","id":"BDxWUmSVI12Iktj1vk-pB","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":8,"metadata":{},"data":{"text/plain":{"content":"[(('n', '.'), 6763),\n (('a', '.'), 6640),\n (('a', 'n'), 5438),\n (('.', 'a'), 4410),\n (('e', '.'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('.', 'k'), 2963),\n (('l', 'e'), 2921),\n (('e', 'n'), 2675),\n (('l', 'a'), 2623),\n (('m', 'a'), 2590),\n (('.', 'm'), 2538),\n (('a', 'l'), 2528),\n (('i', '.'), 2489),\n (('l', 'i'), 2480),\n (('i', 'a'), 2445),\n (('.', 'j'), 2422),\n (('o', 'n'), 2411),\n (('h', '.'), 2409),\n (('r', 'a'), 2356),\n (('a', 'h'), 2332),\n (('h', 'a'), 2244),\n (('y', 'a'), 2143),\n (('i', 'n'), 2126),\n (('.', 's'), 2055),\n (('a', 'y'), 2050),\n (('y', '.'), 2007),\n (('e', 'r'), 1958),\n (('n', 'n'), 1906),\n (('y', 'n'), 1826),\n (('k', 'a'), 1731),\n (('n', 'i'), 1725),\n (('r', 'e'), 1697),\n (('.', 'd'), 1690),\n (('i', 'e'), 1653),\n (('a', 'i'), 1650),\n (('.', 'r'), 1639),\n (('a', 'm'), 1634),\n (('l', 'y'), 1588),\n (('.', 'l'), 1572),\n (('.', 'c'), 1542),\n (('.', 'e'), 1531),\n (('j', 'a'), 1473),\n (('r', '.'), 1377),\n (('n', 'e'), 1359),\n (('l', 'l'), 1345),\n (('i', 'l'), 1345),\n (('i', 's'), 1316),\n (('l', '.'), 1314),\n (('.', 't'), 1308),\n (('.', 'b'), 1306),\n (('d', 'a'), 1303),\n (('s', 'h'), 1285),\n (('d', 'e'), 1283),\n (('e', 'e'), 1271),\n (('m', 'i'), 1256),\n (('s', 'a'), 1201),\n (('s', '.'), 1169),\n (('.', 'n'), 1146),\n (('a', 's'), 1118),\n (('y', 'l'), 1104),\n (('e', 'y'), 1070),\n (('o', 'r'), 1059),\n (('a', 'd'), 1042),\n (('t', 'a'), 1027),\n (('.', 'z'), 929),\n (('v', 'i'), 911),\n (('k', 'e'), 895),\n (('s', 'e'), 884),\n (('.', 'h'), 874),\n (('r', 'o'), 869),\n (('e', 's'), 861),\n (('z', 'a'), 860),\n (('o', '.'), 855),\n (('i', 'r'), 849),\n (('b', 'r'), 842),\n (('a', 'v'), 834),\n (('m', 'e'), 818),\n (('e', 'i'), 818),\n (('c', 'a'), 815),\n (('i', 'y'), 779),\n (('r', 'y'), 773),\n (('e', 'm'), 769),\n (('s', 't'), 765),\n (('h', 'i'), 729),\n (('t', 'e'), 716),\n (('n', 'd'), 704),\n (('l', 'o'), 692),\n (('a', 'e'), 692),\n (('a', 't'), 687),\n (('s', 'i'), 684),\n (('e', 'a'), 679),\n (('d', 'i'), 674),\n (('h', 'e'), 674),\n (('.', 'g'), 669),\n (('t', 'o'), 667),\n (('c', 'h'), 664),\n (('b', 'e'), 655),\n (('t', 'h'), 647),\n (('v', 'a'), 642),\n (('o', 'l'), 619),\n (('.', 'i'), 591),\n (('i', 'o'), 588),\n (('e', 't'), 580),\n (('v', 'e'), 568),\n (('a', 'k'), 568),\n (('a', 'a'), 556),\n (('c', 'e'), 551),\n (('a', 'b'), 541),\n (('i', 't'), 541),\n (('.', 'y'), 535),\n (('t', 'i'), 532),\n (('s', 'o'), 531),\n (('m', '.'), 516),\n (('d', '.'), 516),\n (('.', 'p'), 515),\n (('i', 'c'), 509),\n (('k', 'i'), 509),\n (('o', 's'), 504),\n (('n', 'o'), 496),\n (('t', '.'), 483),\n (('j', 'o'), 479),\n (('u', 's'), 474),\n (('a', 'c'), 470),\n (('n', 'y'), 465),\n (('e', 'v'), 463),\n (('s', 's'), 461),\n (('m', 'o'), 452),\n (('i', 'k'), 445),\n (('n', 't'), 443),\n (('i', 'd'), 440),\n (('j', 'e'), 440),\n (('a', 'z'), 435),\n (('i', 'g'), 428),\n (('i', 'm'), 427),\n (('r', 'r'), 425),\n (('d', 'r'), 424),\n (('.', 'f'), 417),\n (('u', 'r'), 414),\n (('r', 'l'), 413),\n (('y', 's'), 401),\n (('.', 'o'), 394),\n (('e', 'd'), 384),\n (('a', 'u'), 381),\n (('c', 'o'), 380),\n (('k', 'y'), 379),\n (('d', 'o'), 378),\n (('.', 'v'), 376),\n (('t', 't'), 374),\n (('z', 'e'), 373),\n (('z', 'i'), 364),\n (('k', '.'), 363),\n (('g', 'h'), 360),\n (('t', 'r'), 352),\n (('k', 'o'), 344),\n (('t', 'y'), 341),\n (('g', 'e'), 334),\n (('g', 'a'), 330),\n (('l', 'u'), 324),\n (('b', 'a'), 321),\n (('d', 'y'), 317),\n (('c', 'k'), 316),\n (('.', 'w'), 307),\n (('k', 'h'), 307),\n (('u', 'l'), 301),\n (('y', 'e'), 301),\n (('y', 'r'), 291),\n (('m', 'y'), 287),\n (('h', 'o'), 287),\n (('w', 'a'), 280),\n (('s', 'l'), 279),\n (('n', 's'), 278),\n (('i', 'z'), 277),\n (('u', 'n'), 275),\n (('o', 'u'), 275),\n (('n', 'g'), 273),\n (('y', 'd'), 272),\n (('c', 'i'), 271),\n (('y', 'o'), 271),\n (('i', 'v'), 269),\n (('e', 'o'), 269),\n (('o', 'm'), 261),\n (('r', 'u'), 252),\n (('f', 'a'), 242),\n (('b', 'i'), 217),\n (('s', 'y'), 215),\n (('n', 'c'), 213),\n (('h', 'y'), 213),\n (('p', 'a'), 209),\n (('r', 't'), 208),\n (('q', 'u'), 206),\n (('p', 'h'), 204),\n (('h', 'r'), 204),\n (('j', 'u'), 202),\n (('g', 'r'), 201),\n (('p', 'e'), 197),\n (('n', 'l'), 195),\n (('y', 'i'), 192),\n (('g', 'i'), 190),\n (('o', 'd'), 190),\n (('r', 's'), 190),\n (('r', 'd'), 187),\n (('h', 'l'), 185),\n (('s', 'u'), 185),\n (('a', 'x'), 182),\n (('e', 'z'), 181),\n (('e', 'k'), 178),\n (('o', 'v'), 176),\n (('a', 'j'), 175),\n (('o', 'h'), 171),\n (('u', 'e'), 169),\n (('m', 'm'), 168),\n (('a', 'g'), 168),\n (('h', 'u'), 166),\n (('x', '.'), 164),\n (('u', 'a'), 163),\n (('r', 'm'), 162),\n (('a', 'w'), 161),\n (('f', 'i'), 160),\n (('z', '.'), 160),\n (('u', '.'), 155),\n (('u', 'm'), 154),\n (('e', 'c'), 153),\n (('v', 'o'), 153),\n (('e', 'h'), 152),\n (('p', 'r'), 151),\n (('d', 'd'), 149),\n (('o', 'a'), 149),\n (('w', 'e'), 149),\n (('w', 'i'), 148),\n (('y', 'm'), 148),\n (('z', 'y'), 147),\n (('n', 'z'), 145),\n (('y', 'u'), 141),\n (('r', 'n'), 140),\n (('o', 'b'), 140),\n (('k', 'l'), 139),\n (('m', 'u'), 139),\n (('l', 'd'), 138),\n (('h', 'n'), 138),\n (('u', 'd'), 136),\n (('.', 'x'), 134),\n (('t', 'l'), 134),\n (('a', 'f'), 134),\n (('o', 'e'), 132),\n (('e', 'x'), 132),\n (('e', 'g'), 125),\n (('f', 'e'), 123),\n (('z', 'l'), 123),\n (('u', 'i'), 121),\n (('v', 'y'), 121),\n (('e', 'b'), 121),\n (('r', 'h'), 121),\n (('j', 'i'), 119),\n (('o', 't'), 118),\n (('d', 'h'), 118),\n (('h', 'm'), 117),\n (('c', 'l'), 116),\n (('o', 'o'), 115),\n (('y', 'c'), 115),\n (('o', 'w'), 114),\n (('o', 'c'), 114),\n (('f', 'r'), 114),\n (('b', '.'), 114),\n (('m', 'b'), 112),\n (('z', 'o'), 110),\n (('i', 'b'), 110),\n (('i', 'u'), 109),\n (('k', 'r'), 109),\n (('g', '.'), 108),\n (('y', 'v'), 106),\n (('t', 'z'), 105),\n (('b', 'o'), 105),\n (('c', 'y'), 104),\n (('y', 't'), 104),\n (('u', 'b'), 103),\n (('u', 'c'), 103),\n (('x', 'a'), 103),\n (('b', 'l'), 103),\n (('o', 'y'), 103),\n (('x', 'i'), 102),\n (('i', 'f'), 101),\n (('r', 'c'), 99),\n (('c', '.'), 97),\n (('m', 'r'), 97),\n (('n', 'u'), 96),\n (('o', 'p'), 95),\n (('i', 'h'), 95),\n (('k', 's'), 95),\n (('l', 's'), 94),\n (('u', 'k'), 93),\n (('.', 'q'), 92),\n (('d', 'u'), 92),\n (('s', 'm'), 90),\n (('r', 'k'), 90),\n (('i', 'x'), 89),\n (('v', '.'), 88),\n (('y', 'k'), 86),\n (('u', 'w'), 86),\n (('g', 'u'), 85),\n (('b', 'y'), 83),\n (('e', 'p'), 83),\n (('g', 'o'), 83),\n (('s', 'k'), 82),\n (('u', 't'), 82),\n (('a', 'p'), 82),\n (('e', 'f'), 82),\n (('i', 'i'), 82),\n (('r', 'v'), 80),\n (('f', '.'), 80),\n (('t', 'u'), 78),\n (('y', 'z'), 78),\n (('.', 'u'), 78),\n (('l', 't'), 77),\n (('r', 'g'), 76),\n (('c', 'r'), 76),\n (('i', 'j'), 76),\n (('w', 'y'), 73),\n (('z', 'u'), 73),\n (('l', 'v'), 72),\n (('h', 't'), 71),\n (('j', '.'), 71),\n (('x', 't'), 70),\n (('o', 'i'), 69),\n (('e', 'u'), 69),\n (('o', 'k'), 68),\n (('b', 'd'), 65),\n (('a', 'o'), 63),\n (('p', 'i'), 61),\n (('s', 'c'), 60),\n (('d', 'l'), 60),\n (('l', 'm'), 60),\n (('a', 'q'), 60),\n (('f', 'o'), 60),\n (('p', 'o'), 59),\n (('n', 'k'), 58),\n (('w', 'n'), 58),\n (('u', 'h'), 58),\n (('e', 'j'), 55),\n (('n', 'v'), 55),\n (('s', 'r'), 55),\n (('o', 'z'), 54),\n (('i', 'p'), 53),\n (('l', 'b'), 52),\n (('i', 'q'), 52),\n (('w', '.'), 51),\n (('m', 'c'), 51),\n (('s', 'p'), 51),\n (('e', 'w'), 50),\n (('k', 'u'), 50),\n (('v', 'r'), 48),\n (('u', 'g'), 47),\n (('o', 'x'), 45),\n (('u', 'z'), 45),\n (('z', 'z'), 45),\n (('j', 'h'), 45),\n (('b', 'u'), 45),\n (('o', 'g'), 44),\n (('n', 'r'), 44),\n (('f', 'f'), 44),\n (('n', 'j'), 44),\n (('z', 'h'), 43),\n (('c', 'c'), 42),\n (('r', 'b'), 41),\n (('x', 'o'), 41),\n (('b', 'h'), 41),\n (('p', 'p'), 39),\n (('x', 'l'), 39),\n (('h', 'v'), 39),\n (('b', 'b'), 38),\n (('m', 'p'), 38),\n (('x', 'x'), 38),\n (('u', 'v'), 37),\n (('x', 'e'), 36),\n (('w', 'o'), 36),\n (('c', 't'), 35),\n (('z', 'm'), 35),\n (('t', 's'), 35),\n (('m', 's'), 35),\n (('c', 'u'), 35),\n (('o', 'f'), 34),\n (('u', 'x'), 34),\n (('k', 'w'), 34),\n (('p', '.'), 33),\n (('g', 'l'), 32),\n (('z', 'r'), 32),\n (('d', 'n'), 31),\n (('g', 't'), 31),\n (('g', 'y'), 31),\n (('h', 's'), 31),\n (('x', 's'), 31),\n (('g', 's'), 30),\n (('x', 'y'), 30),\n (('y', 'g'), 30),\n (('d', 'm'), 30),\n (('d', 's'), 29),\n (('h', 'k'), 29),\n (('y', 'x'), 28),\n (('q', '.'), 28),\n (('g', 'n'), 27),\n (('y', 'b'), 27),\n (('g', 'w'), 26),\n (('n', 'h'), 26),\n (('k', 'n'), 26),\n (('g', 'g'), 25),\n (('d', 'g'), 25),\n (('l', 'c'), 25),\n (('r', 'j'), 25),\n (('w', 'u'), 25),\n (('l', 'k'), 24),\n (('m', 'd'), 24),\n (('s', 'w'), 24),\n (('s', 'n'), 24),\n (('h', 'd'), 24),\n (('w', 'h'), 23),\n (('y', 'j'), 23),\n (('y', 'y'), 23),\n (('r', 'z'), 23),\n (('d', 'w'), 23),\n (('w', 'r'), 22),\n (('t', 'n'), 22),\n (('l', 'f'), 22),\n (('y', 'h'), 22),\n (('r', 'w'), 21),\n (('s', 'b'), 21),\n (('m', 'n'), 20),\n (('f', 'l'), 20),\n (('w', 's'), 20),\n (('k', 'k'), 20),\n (('h', 'z'), 20),\n (('g', 'd'), 19),\n (('l', 'h'), 19),\n (('n', 'm'), 19),\n (('x', 'z'), 19),\n (('u', 'f'), 19),\n (('f', 't'), 18),\n (('l', 'r'), 18),\n (('p', 't'), 17),\n (('t', 'c'), 17),\n (('k', 't'), 17),\n (('d', 'v'), 17),\n (('u', 'p'), 16),\n (('p', 'l'), 16),\n (('l', 'w'), 16),\n (('p', 's'), 16),\n (('o', 'j'), 16),\n (('r', 'q'), 16),\n (('y', 'p'), 15),\n (('l', 'p'), 15),\n (('t', 'v'), 15),\n (('r', 'p'), 14),\n (('l', 'n'), 14),\n (('e', 'q'), 14),\n (('f', 'y'), 14),\n (('s', 'v'), 14),\n (('u', 'j'), 14),\n (('v', 'l'), 14),\n (('q', 'a'), 13),\n (('u', 'y'), 13),\n (('q', 'i'), 13),\n (('w', 'l'), 13),\n (('p', 'y'), 12),\n (('y', 'f'), 12),\n (('c', 'q'), 11),\n (('j', 'r'), 11),\n (('n', 'w'), 11),\n (('n', 'f'), 11),\n (('t', 'w'), 11),\n (('m', 'z'), 11),\n (('u', 'o'), 10),\n (('f', 'u'), 10),\n (('l', 'z'), 10),\n (('h', 'w'), 10),\n (('u', 'q'), 10),\n (('j', 'y'), 10),\n (('s', 'z'), 10),\n (('s', 'd'), 9),\n (('j', 'l'), 9),\n (('d', 'j'), 9),\n (('k', 'm'), 9),\n (('r', 'f'), 9),\n (('h', 'j'), 9),\n (('v', 'n'), 8),\n (('n', 'b'), 8),\n (('i', 'w'), 8),\n (('h', 'b'), 8),\n (('b', 's'), 8),\n (('w', 't'), 8),\n (('w', 'd'), 8),\n (('v', 'v'), 7),\n (('v', 'u'), 7),\n (('j', 's'), 7),\n (('m', 'j'), 7),\n (('f', 's'), 6),\n (('l', 'g'), 6),\n (('l', 'j'), 6),\n (('j', 'w'), 6),\n (('n', 'x'), 6),\n (('y', 'q'), 6),\n (('w', 'k'), 6),\n (('g', 'm'), 6),\n (('x', 'u'), 5),\n (('m', 'h'), 5),\n (('m', 'l'), 5),\n (('j', 'm'), 5),\n (('c', 's'), 5),\n (('j', 'v'), 5),\n (('n', 'p'), 5),\n (('d', 'f'), 5),\n (('x', 'd'), 5),\n (('z', 'b'), 4),\n (('f', 'n'), 4),\n (('x', 'c'), 4),\n (('m', 't'), 4),\n (('t', 'm'), 4),\n (('z', 'n'), 4),\n (('z', 't'), 4),\n (('p', 'u'), 4),\n (('c', 'z'), 4),\n (('b', 'n'), 4),\n (('z', 's'), 4),\n (('f', 'w'), 4),\n (('d', 't'), 4),\n (('j', 'd'), 4),\n (('j', 'c'), 4),\n (('y', 'w'), 4),\n (('v', 'k'), 3),\n (('x', 'w'), 3),\n (('t', 'j'), 3),\n (('c', 'j'), 3),\n (('q', 'w'), 3),\n (('g', 'b'), 3),\n (('o', 'q'), 3),\n (('r', 'x'), 3),\n (('d', 'c'), 3),\n (('g', 'j'), 3),\n (('x', 'f'), 3),\n (('z', 'w'), 3),\n (('d', 'k'), 3),\n (('u', 'u'), 3),\n (('m', 'v'), 3),\n (('c', 'x'), 3),\n (('l', 'q'), 3),\n (('p', 'b'), 2),\n (('t', 'g'), 2),\n (('q', 's'), 2),\n (('t', 'x'), 2),\n (('f', 'k'), 2),\n (('b', 't'), 2),\n (('j', 'n'), 2),\n (('k', 'c'), 2),\n (('z', 'k'), 2),\n (('s', 'j'), 2),\n (('s', 'f'), 2),\n (('z', 'j'), 2),\n (('n', 'q'), 2),\n (('f', 'z'), 2),\n (('h', 'g'), 2),\n (('w', 'w'), 2),\n (('k', 'j'), 2),\n (('j', 'k'), 2),\n (('w', 'm'), 2),\n (('z', 'c'), 2),\n (('z', 'v'), 2),\n (('w', 'f'), 2),\n (('q', 'm'), 2),\n (('k', 'z'), 2),\n (('j', 'j'), 2),\n (('z', 'p'), 2),\n (('j', 't'), 2),\n (('k', 'b'), 2),\n (('m', 'w'), 2),\n (('h', 'f'), 2),\n (('c', 'g'), 2),\n (('t', 'f'), 2),\n (('h', 'c'), 2),\n (('q', 'o'), 2),\n (('k', 'd'), 2),\n (('k', 'v'), 2),\n (('s', 'g'), 2),\n (('z', 'd'), 2),\n (('q', 'r'), 1),\n (('d', 'z'), 1),\n (('p', 'j'), 1),\n (('q', 'l'), 1),\n (('p', 'f'), 1),\n (('q', 'e'), 1),\n (('b', 'c'), 1),\n (('c', 'd'), 1),\n (('m', 'f'), 1),\n (('p', 'n'), 1),\n (('w', 'b'), 1),\n (('p', 'c'), 1),\n (('h', 'p'), 1),\n (('f', 'h'), 1),\n (('b', 'j'), 1),\n (('f', 'g'), 1),\n (('z', 'g'), 1),\n (('c', 'p'), 1),\n (('p', 'k'), 1),\n (('p', 'm'), 1),\n (('x', 'n'), 1),\n (('s', 'q'), 1),\n (('k', 'f'), 1),\n (('m', 'k'), 1),\n (('x', 'h'), 1),\n (('g', 'f'), 1),\n (('v', 'b'), 1),\n (('j', 'p'), 1),\n (('g', 'z'), 1),\n (('v', 'd'), 1),\n (('d', 'b'), 1),\n (('v', 'h'), 1),\n (('h', 'h'), 1),\n (('g', 'v'), 1),\n (('d', 'q'), 1),\n (('x', 'b'), 1),\n (('w', 'z'), 1),\n (('h', 'q'), 1),\n (('j', 'b'), 1),\n (('x', 'm'), 1),\n (('w', 'g'), 1),\n (('t', 'b'), 1),\n (('z', 'x'), 1)]","content_type":"text/plain"}}},"children":[],"key":"p1cojaMlHw"}],"visibility":"show","key":"syWybFY25b"}],"visibility":"show","key":"LIqJCNeuxw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And this is the sorted list of counts of the individual bigrams across all the words in the dataset! Now let’s convert our current bigram-to-occurence-frequency map into a bigram counts array, where every row index represents the first character and every column index represents the second character of each bigram. Before doing so, we must first find a way to convert each character into a unique integer index:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vOiSmDiEcr"}],"key":"sKbu9NAsl0"}],"key":"iwg0wD6k5J"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"chars = [\".\"] + sorted(list(set(\"\".join(words))))\nctoi = {c: i for i, c in enumerate(chars)}\nprint(ctoi)","key":"zGKcttKA7z"},{"type":"outputs","id":"bEC3yk0ZgM3jojMwS94JA","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"},"children":[],"key":"XW5Cd69htA"}],"key":"DQAQJXB8IA"}],"key":"SBkywr6NuJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now that we have a character-to-index map, we may construct our bigram counts array ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WHom9MjKI0"},{"type":"inlineCode","value":"N","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dcMv9iKujS"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Pgy7A6vaoz"}],"key":"CyWU6M1wS0"}],"key":"kjRexkBFpN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\n\nnchars = len(chars)\nN = torch.zeros(nchars, nchars, dtype=torch.int32)\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        N[ctoi[ch1], ctoi[ch2]] += 1","key":"KAdq3KSHXW"},{"type":"outputs","id":"PZ2xV6115Z9sYvdrmstCV","children":[],"key":"eGYPyE4keV"}],"key":"bkAhqF4Usm"},{"type":"block","kind":"notebook-code","data":{"scrolled":true,"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"N","visibility":"show","key":"EhftWkms4b"},{"type":"outputs","id":"sNUaEnEE299YXAterswNy","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":11,"metadata":{},"data":{"text/plain":{"content":"tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n          134,  535,  929],\n        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n          182, 2050,  435],\n        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n            0,   83,    0],\n        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n            3,  104,    4],\n        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n            0,  317,    1],\n        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n          132, 1070,  181],\n        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n            0,   14,    2],\n        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n            0,   31,    1],\n        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n            0,  213,   20],\n        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n           89,  779,  277],\n        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n            0,   10,    0],\n        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n            0,  379,    2],\n        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n            0, 1588,   10],\n        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n            0,  287,   11],\n        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n            6,  465,  145],\n        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n           45,  103,   54],\n        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n            0,   12,    0],\n        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n            0,    0,    0],\n        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n            3,  773,   23],\n        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n            0,  215,   10],\n        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n            2,  341,  105],\n        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n           34,   13,   45],\n        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n            0,  121,    0],\n        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n            0,   73,    1],\n        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n           38,   30,   19],\n        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n           28,   23,   78],\n        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n            1,  147,   45]], dtype=torch.int32)","content_type":"text/plain"}}},"children":[],"key":"SLrQ03u1BM"}],"visibility":"show","key":"aHOXxF7WIO"}],"visibility":"show","key":"RcTpvWIF61"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Done! Of course, this looks like a mess. So let’s visualize it better.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vmv9HPTGdv"}],"key":"KB0Zwv6SXv"}],"key":"AHXvgxAfij"},{"type":"block","kind":"notebook-code","data":{"scrolled":true,"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\n\nitoc = {i: c for c, i in ctoi.items()}\nplt.figure(figsize=(16, 16))\nplt.imshow(N, cmap=\"Blues\")\nfor i in range(27):\n    for j in range(27):\n        chstr = itoc[i] + itoc[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\nplt.axis(\"off\")","visibility":"show","key":"Mo8mgaIRBM"},{"type":"outputs","id":"rhxzW687hWKtMDJmYlOg9","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":12,"metadata":{},"data":{"text/plain":{"content":"(np.float64(-0.5), np.float64(26.5), np.float64(26.5), np.float64(-0.5))","content_type":"text/plain"}}},"children":[],"key":"bitt7298UC"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"2e90789e8ce84917a2a5d630a82c41ef\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"b09bbce13ad8931ca3f31925d5c24351","path":"/build/b09bbce13ad8931ca3f31925d5c24351.png"},"text/html":{"content_type":"text/html","hash":"b7ac7932598006a180542adb878232b1","path":"/build/b7ac7932598006a180542adb878232b1.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"gG7QVLw7Il"}],"visibility":"show","key":"gE7zl6zL82"}],"visibility":"show","key":"YK01SUa9BZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The color-graded bigram counts array! Looks good. This array actually has all the necessary information for us to start sampling from this bigram character language model. Let’s just start by sampling the start character (of course) of each name: the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RDuEz53TDe"},{"type":"inlineCode","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"voPoTmD60t"},{"type":"text","value":" character. The first row tells us how often each other character follows it. In other words, the first row tells us how often each character is the first character of a word:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RNEQQjIbSe"}],"key":"KeGPrMOVbL"}],"key":"YU1a6BlxuT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"N[0]","key":"vZZMFVmYuX"},{"type":"outputs","id":"5Nd_GMww4aEFnlHFPT2gO","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":13,"metadata":{},"data":{"text/plain":{"content":"tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)","content_type":"text/plain"}}},"children":[],"key":"W6hdflvsqy"}],"key":"vsTSfTI8Qj"}],"key":"FxNpZmVNrj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To get the probability of each of character being the first:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FNimhn9cln"}],"key":"XD58oUfzYI"}],"key":"MOj9uNhcON"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"p = N[0].float()\np = p / p.sum()\np","key":"uwWOlTgNZC"},{"type":"outputs","id":"safrIvINTwfbhA_sMyiia","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/plain":{"content":"tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])","content_type":"text/plain"}}},"children":[],"key":"sXooIhjt3r"}],"key":"r9m7dzlHpH"}],"key":"BNexBuZjZx"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Each value of this probability distribution corresponds simply to the probability of the corresponding character being the first character of a word. And of course it sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nllxW3E7GJ"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dI2mDK1QtZ"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PXymCw0Tqa"}],"key":"SWF6Qg8QtI"}],"key":"TIrrQPwunH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert p.sum() == 1","key":"uBZDhp3M5x"},{"type":"outputs","id":"gcGJ0W2Wst1QpQX-ePUwm","children":[],"key":"Tez6Y3h3m4"}],"key":"Pc5h9ZDw48"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we’ll sample numbers according to this probability distribution using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W9ChRdY7b6"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.multinomial","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kvFrxbiWOh"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial","key":"POmFgbSnHH"},{"type":"text","value":". And to do so deterministically we are going to use a generator. So, let’s take a brief detour and test out how to sample. First we create a probability distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tLRkeydweh"}],"key":"biydCERIC8"}],"key":"UDyNsuBMGu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"SEED = 2147483647\ng = torch.Generator().manual_seed(SEED)\nptest = torch.rand(3, generator=g)\nptest = ptest / ptest.sum()\nptest","key":"B2mkJUsU7y"},{"type":"outputs","id":"nXWY9zmIX31dMoe9VnAiO","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":16,"metadata":{},"data":{"text/plain":{"content":"tensor([0.6064, 0.3033, 0.0903])","content_type":"text/plain"}}},"children":[],"key":"KgMj8lbaph"}],"key":"tbzyViGnej"}],"key":"dY3Uq41C8X"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then, we sample from this distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BT1TA73qGH"}],"key":"ABNfwmg08e"}],"key":"VRBJFjpae8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"s = torch.multinomial(ptest, num_samples=100, replacement=True, generator=g)\ns","key":"r2qB58pk17"},{"type":"outputs","id":"QvhawxL-NM12TJNDRppfV","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":17,"metadata":{},"data":{"text/plain":{"content":"tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,\n        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,\n        0, 1, 1, 1])","content_type":"text/plain"}}},"children":[],"key":"H5mvc8PpTT"}],"key":"WPQHCGNcze"}],"key":"K16ofsen4Q"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Simple. Now, notice that it outputs the same tensor however many times you run the cells. That’s because we have set a fixed seed and passed the generator object to the functions. Now, notice the output of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P0SHWLA9kj"},{"type":"inlineCode","value":"torch.multinomial","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"US96ybuMGC"},{"type":"text","value":". What we expect is that around ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VkN9omzFDO"},{"type":"inlineMath","value":"60.64\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e60.64\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e60.64\\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e60.64%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"qVdP82VYf1"},{"type":"text","value":" of the numbers to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"njfRzlR3TT"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LiLYg009j5"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WIGc5zlPD1"},{"type":"inlineMath","value":"30.33\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e30.33\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e30.33\\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e30.33%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"iwIsvFQcue"},{"type":"text","value":" to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"msNXQsvtvU"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JFOBZU6Rnq"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XHnBDoXtH5"},{"type":"inlineMath","value":"9.03\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e9.03\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e9.03\\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e9.03%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"XytnrWtILH"},{"type":"text","value":" to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AHiE3jXga7"},{"type":"inlineCode","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zS8IE4NIas"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L32R2vvNou"}],"key":"MnUfUB6Yfx"}],"key":"JYKcvu1ptE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sbc = torch.bincount(s)\nfor i in [0, 1, 2]:\n    print(f\"Ratio of {i}: {sbc[i]/sbc.sum()}\")","key":"f86KeiXVIt"},{"type":"outputs","id":"dMgvFLJTnHttEx2pxxSpj","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Ratio of 0: 0.6100000143051147\nRatio of 1: 0.33000001311302185\nRatio of 2: 0.05999999865889549\n"},"children":[],"key":"miSwU7yc3F"}],"key":"Xeyatt80lA"}],"key":"xMqtTyrqYA"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Not too far away from what we expected! But, if we increase the number of samples, we will get much closer to the probabilities of our distribution. Try it out! The more samples we take, the more the actual occurence ratios match the probabilities of the distribution the numbers were sampled from. Now, it’s time to sample from our initial character probability distribution:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CBrWmCz6WK"}],"key":"qwGrxkJ1Ic"}],"key":"TWtQ2M4Rli"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nidx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\nitoc[idx]","key":"uoIMwTuUZQ"},{"type":"outputs","id":"KcztTb-NzjfDcQLLi7lUc","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":19,"metadata":{},"data":{"text/plain":{"content":"'j'","content_type":"text/plain"}}},"children":[],"key":"lEJW3TlQrb"}],"key":"AiWoqgnBm6"}],"key":"CmPA9C016O"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We are now ready to write out our name generator.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mNAQO5g3X1"}],"key":"GSwtO61I1X"}],"key":"hwnK83eU4j"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nP = N.float()\nP = P / P.sum(\n    1, keepdim=True\n)  # sum over the column dimension and keep column dimension\nfor i in range(20):\n    out = []\n    idx = 0\n    while True:\n        p = P[idx]\n        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[idx])\n        if idx == 0:\n            break\n    print(\"\".join(out))","key":"taWHHPu9xW"},{"type":"outputs","id":"TIA3SuDZS5nxmR1EgZs1V","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"junide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\ndedainrwieta.\nssonielylarte.\nfaveumerifontume.\nphynslenaruani.\ncore.\nyaenon.\nka.\njabdinerimikimaynin.\nanaasn.\nssorionsush.\n"},"children":[],"key":"MzlBPBozRg"}],"key":"uUqxly6iBQ"}],"key":"ofxrbFh6AT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"It works! It yields names. Well, kinda. Some look name-like enough but most are just terrible. Lol. This is a bigrams model for you! To recap, we trained a bigrams language model essentially just by counting how frequently any pairing of characters occurs and then normalizing so that we get a nice probability distribution. Really, the elements of array ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KZiT4g4GZZ"},{"type":"inlineCode","value":"P","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nriJ9w1MdL"},{"type":"text","value":" are the parameters of our model that summarize the statistics of these bigrams. We train the model and iteratively sample the next character and feed it in each time and get the next character. But how do we evaluate our model? We can do so, by looking at the probability of each bigram.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pZGvOLskiH"}],"key":"uv16c1N56r"}],"key":"OF3xSJ0VB3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"for w in words[:3]:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = ctoi[ch1]\n        ix2 = ctoi[ch2]\n        prob = P[ix1, ix2]\n        print(f\"{ch1}{ch2}: {prob:.4f}\")","key":"poeyHNYVRD"},{"type":"outputs","id":"Xy1LN9QVGQ_WJS4D09MMu","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".e: 0.0478\nem: 0.0377\nmm: 0.0253\nma: 0.3899\na.: 0.1960\n.o: 0.0123\nol: 0.0780\nli: 0.1777\niv: 0.0152\nvi: 0.3541\nia: 0.1381\na.: 0.1960\n.a: 0.1377\nav: 0.0246\nva: 0.2495\na.: 0.1960\n"},"children":[],"key":"RLlkdRvMKA"}],"key":"T9gWbzVwm9"}],"key":"rwJo08kQvl"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here we are looking at the probabilities that the model assigns to every bigram in the dataset. Just keep in mind that we have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wIBKdSDke8"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cVvecAzb5R"},{"type":"text","value":" characters, so if everything was equally likely we would expect all probabilities to be:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AQMI4Kro2i"}],"key":"cF0hoDDi47"}],"key":"RwFX6tXQen"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"1/27","key":"JFlcHISFdR"},{"type":"outputs","id":"xOlkWrH4x2jBWEuuSLxiW","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":22,"metadata":{},"data":{"text/plain":{"content":"0.037037037037037035","content_type":"text/plain"}}},"children":[],"key":"ItPCyQgGKG"}],"key":"HNN3cm8i62"}],"key":"LhKkuUgLNX"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Since they are not and we have mostly higher probabilities, it means that our model has learned something useful. In an ideal case, we would expect the bigram probabilities to be near ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"widlu0F8KP"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ux4kjkD1Qr"},{"type":"text","value":" (perfect prediction probability). Now, when you look at the literature of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V4IXMs8Bvd"},{"type":"link","url":"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"maximum likelihood estimation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dcyjnsx2UL"}],"urlSource":"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation","data":{"page":"Maximum_likelihood_estimation","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"aOVtE5bXfS"},{"type":"text","value":", statistical modelling and so on, you’ll see that what’s typically used here is something called the likelihood: the product of all the above probabilities. This gives us the probability of the entire dataset assigned by the model that you made. But, because the product of these probabilities is an unwieldly, very tiny number to work with (think ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bIFJM1HBhH"},{"type":"inlineMath","value":"0.0478 \\times 0.0377 \\times 0.0253 \\times ...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e0.0478\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e0.0377\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e0.0253\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e0.0478 \\times 0.0377 \\times 0.0253 \\times ...\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0.0478\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0.0377\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0.0253\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.1056em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e...\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"mgkZ92Ajk9"},{"type":"text","value":"), for convenience, what people usually work with is not the likelihood, but the log-likelihood. The log, as you can see:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YIUoLggO4R"}],"key":"m7o8bMCuN7"}],"key":"CaK7gpSrWM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\n\nx = np.arange(0.01, 1.0, 0.01)\ny = np.log(x)\nplt.figure()\nplt.plot(x, y)","key":"hOqQfWwbmk"},{"type":"outputs","id":"IiOOAksKRThh5jLzfAm0b","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8cff68e9d3544a6aa5001375df6992e9\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"d97c0c2cfc87a8e517face2402198721","path":"/build/d97c0c2cfc87a8e517face2402198721.png"},"text/html":{"content":"\n            \u003cdiv style=\"display: inline-block;\"\u003e\n                \u003cdiv class=\"jupyter-widgets widget-label\" style=\"text-align: center;\"\u003e\n                    Figure\n                \u003c/div\u003e\n                \u003cimg src='data:image/png;base64,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' width=640.0/\u003e\n            \u003c/div\u003e\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"qaqL9WZqNo"}],"key":"xLX95dEHfQ"}],"key":"wT0lVuhQQd"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"is a monotonic transformation of the probability, where if you pass in probability ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EY2BocVFx2"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Djo6XftzG4"},{"type":"text","value":" you get log-probability of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qPfiRtGQ82"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iaWZMEcUZy"},{"type":"text","value":", and as the probabilities you pass in decrease, the log-probability decreases all the way to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MFtKBIQqxk"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∞\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e-\\infty\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e−\u003c/span\u003e\u003cspan class=\"mord\"\u003e∞\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"QbWBG4rsZt"},{"type":"text","value":" as the probability approaches ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vBi09yPSMD"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CJ43oBLinW"},{"type":"text","value":". Therefore, let’s also add the log probability in our loop to see what that looks like:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RBQU4md1rS"}],"key":"WEoEgst4NL"}],"key":"XJV7i9u5gX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def test_model(iterable, print_probs=True, calc_ll=False, print_nll=False):\n    if print_nll:\n        calc_ll = True\n    log_likelihood = 0.0\n    n = 0\n    for w in iterable:\n        chs = [\".\"] + list(w) + [\".\"]\n        for ch1, ch2 in zip(chs, chs[1:]):\n            prob = P[ctoi[ch1], ctoi[ch2]]\n            logprob = torch.log(prob)\n            if calc_ll:\n                log_likelihood += logprob.item()\n                n += 1\n            if print_probs:\n                print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n    if calc_ll:\n        print(f\"{log_likelihood=}\")\n    if print_nll:\n        nll = -log_likelihood\n        print(f\"{nll=}\")\n        print(f\"loss={nll/n}\")\n    return log_likelihood\n\n\n_ = test_model(words[:3])","key":"P2xOlu4kl4"},{"type":"outputs","id":"vxakhtC_k3D2pJWMKzviA","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\n"},"children":[],"key":"PPnxotfhJv"}],"key":"EMo3WaRhRv"}],"key":"J7WjOtHded"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, for higher probabilities we get closer and closer to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dzxRLwSmdt"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eFCe6MZ5hz"},{"type":"text","value":", but lower probabilities gives us a more negative number. And so to calculate the log-likelihood, we just sum up all the log probabilities:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jcMmFfXVAZ"}],"key":"WB1yz7OnES"}],"key":"If1dIupOyK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"log_likelihood = test_model(words[:3], calc_ll=True)","key":"GcbezXk6OZ"},{"type":"outputs","id":"zNNNoawZC1nVuEOxNPfTS","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".e: 0.0478 -3.0408\nem: 0.0377 -3.2793\nmm: 0.0253 -3.6772\nma: 0.3899 -0.9418\na.: 0.1960 -1.6299\n.o: 0.0123 -4.3982\nol: 0.0780 -2.5508\nli: 0.1777 -1.7278\niv: 0.0152 -4.1867\nvi: 0.3541 -1.0383\nia: 0.1381 -1.9796\na.: 0.1960 -1.6299\n.a: 0.1377 -1.9829\nav: 0.0246 -3.7045\nva: 0.2495 -1.3882\na.: 0.1960 -1.6299\nlog_likelihood=-38.785636603832245\n"},"children":[],"key":"rkdxhTMOab"}],"key":"M0U8HfNVVM"}],"key":"wGkLUp2A8B"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, how high can log-likelihood get? As high as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IVD8swA4QX"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s0meQ5F0Iz"},{"type":"text","value":"! So, when all the probabilities are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z3fKlBOdcJ"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eazKS6324c"},{"type":"text","value":", it will be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DRN4wq34S3"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VEqyDoPVzb"},{"type":"text","value":". But the further away from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eEw8ku8XzP"},{"type":"text","value":"1.0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"moaZE2WKt9"},{"type":"text","value":" they are, the more negative the log-likehood will get. Now, we don’t actually like this because we are looking to define here is a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d81FMWIPx4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uN0r5i0tm1"}],"key":"KFg6dhPemH"},{"type":"text","value":" function, that has the semantics where high is bad and low is good, since we are trying to minimize it. Any ideas? Well, we actually just need to invert the log-likelihood, aka take the negative log-likelihood (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UeUVzNgGA1"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O13TkMcmA6"}],"key":"ub9Gukl7bf"},{"type":"text","value":"):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WuK0h8vGco"}],"key":"Y65EGzIGnB"}],"key":"xqf4lisGb7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"nll = -log_likelihood\nprint(f'{nll=}')","key":"kjtfRlEbOf"},{"type":"outputs","id":"WGuh5nb9JuWXeVCKEysRY","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"nll=38.785636603832245\n"},"children":[],"key":"cx6ODECzlA"}],"key":"vc9rfdGlip"}],"key":"a4RSV9NeyQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ItDxH6dfzI"}],"key":"I02aK2MbAU"},{"type":"text","value":" is a very nice loss function because the lowest it can get is zero and the higher it is the worse off the predictions are that we are making. People also usually like to see the average of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wCV1n1mkzz"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ocdKcHUW7I"}],"key":"D9pQ3j2U5N"},{"type":"text","value":" instead of just the sum:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OEVRwoHt6O"}],"key":"GYWIklKMb2"}],"key":"DZlQppRNND"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"test_model(words[:3], print_probs=False, calc_ll=True, print_nll=True);","key":"c5vckDFANE"},{"type":"outputs","id":"ImxkUYViUtrxykDwmjOwA","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"log_likelihood=-38.785636603832245\nnll=38.785636603832245\nloss=2.4241022877395153\n"},"children":[],"key":"ZOmm76dr7J"}],"key":"kjuPpM3orD"}],"key":"Pwrd83hnef"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uuHEXS81KJ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e5FxdsMAdw"}],"key":"sAzH9USepx"},{"type":"text","value":" function for the training set assigned by the model yields a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cZlbVTEY3p"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p8Ti783ZTX"}],"key":"Am3q2U1BBj"},{"type":"text","value":" of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VmmpBHS6NV"},{"type":"text","value":"2.424","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HK6WMJPavh"},{"type":"text","value":". The lower it is, the better off we are. The higher it is, the worse off we are. So, the job of training is produce a high-quality model, by finding the parameters that minimize the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lCCpmDKvEt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iJLB82zgsT"}],"key":"KysFA8z15G"},{"type":"text","value":". In this case, ones that minimize the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vhHw5YQAzq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qw8jvKnnQA"}],"key":"fHS9LhHEme"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oWBNgowiUq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ot7TPA7vFQ"}],"key":"esYDCfc7vW"},{"type":"text","value":". To summarize, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Af0wUX0UoL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"goal","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CuOf3l5cBD"}],"key":"aYiJVgMWAG"},{"type":"text","value":" is to maximize likelihood of the data ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u0xUwwGcCx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"w.r.t.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ga6U4WPT4H"}],"key":"f7IMA3JeRQ"},{"type":"text","value":" model parameters (in our statistical modeling case these are the bigram probabilities), which is:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Wh9wHNj83t"}],"key":"owj0x5GhTT"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"equivalent to maximizing the log-likelihood (because the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jkDYOBkDhP"},{"type":"inlineMath","value":"\\log","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\log\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003elo\u003cspan style=\"margin-right:0.01389em;\"\u003eg\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"HUFGlXg3gz"},{"type":"text","value":" function is monotonic)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"CEAhFfRWkL"}],"key":"AQYS4u4rQR"}],"key":"zzljq12QPO"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"equivalent to minimizing the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ZnNMPK16Qu"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"fHY0vo4oAU"}],"key":"xg6gfCjutI"}],"key":"qTT26p036u"}],"key":"hYSxtvDWgZ"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"equivalent to minimizing the average ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"lqRv8Sw4Cw"},{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"SVoq6YqOaI"}],"key":"AS5MFiobqf"}],"key":"j667y9tW0E"}],"key":"sCoRxPi3Y0"}],"key":"mNH5b1gKP7"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"The lower the ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"G3y3rGe0QS"},{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"iJxBIRAozB"}],"key":"B1AUOjgzhG"},{"type":"text","value":" ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"F4AERQjGvG"},{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"rrln5L01qm"}],"key":"iYWcgT5XAl"},{"type":"text","value":" the better, since that would mean assigning high probabilities. Remember: ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"DyqI2UPTWt"},{"type":"inlineMath","value":"\\log(a \\cdot b \\cdot c) = \\log(a) + \\log(b) + \\log(c)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\log(a \\cdot b \\cdot c) = \\log(a) + \\log(b) + \\log(c)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003elo\u003cspan style=\"margin-right:0.01389em;\"\u003eg\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003elo\u003cspan style=\"margin-right:0.01389em;\"\u003eg\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003elo\u003cspan style=\"margin-right:0.01389em;\"\u003eg\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mop\"\u003elo\u003cspan style=\"margin-right:0.01389em;\"\u003eg\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ec\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"Jh3eXEnb7E"},{"type":"text","value":". Also, keep in mind that here we store the probabilities in a table format. But in what’s coming up, these numbers will not be kept explicitly but they will be calculated by a ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"XCIYfYnFyn"},{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"xBAii8D6kI"}],"key":"GyrdIdAgFz"},{"type":"text","value":" and we will change its parameters to maximize the likelihood of these probabilities. Let’s now test out our model with a random name:","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"NTaGT9Z2xy"}],"key":"em0OjuK6rZ"}],"key":"ZWOG66GNcO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"test_model(iterable=['christosqj'], calc_ll=True, print_nll=True);","key":"Lu2NhjAC33"},{"type":"outputs","id":"d_EOo1_uWykZeH2AAXIOx","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".c: 0.0481 -3.0337\nch: 0.1880 -1.6713\nhr: 0.0268 -3.6199\nri: 0.2388 -1.4320\nis: 0.0743 -2.5990\nst: 0.0944 -2.3605\nto: 0.1197 -2.1224\nos: 0.0635 -2.7563\nsq: 0.0001 -9.0004\nqj: 0.0000 -inf\nj.: 0.0245 -3.7098\nlog_likelihood=-inf\nnll=inf\nloss=inf\n"},"children":[],"key":"nyYmhNJHVd"}],"key":"SaBm0B7Q0j"}],"key":"ceiEIbe6QF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, the probability of the bigram ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I6rRdUhFbS"},{"type":"inlineCode","value":"sq","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XR0gyf1zNm"},{"type":"text","value":" is super low. Whereas the probability for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iiI7tX4Xm3"},{"type":"inlineCode","value":"qj","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lt0jsXXJN3"},{"type":"text","value":", since it is never encountered in our training data (see our bigram count table!), is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sm974lFKmh"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ikow0LDOGk"},{"type":"text","value":", which predictably yields a log-probability of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oW2q0hcKAO"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∞\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e-\\infty\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e−\u003c/span\u003e\u003cspan class=\"mord\"\u003e∞\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"fr72AIv8Ij"},{"type":"text","value":", which in turn causes the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QALPtA72qH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oQYXPXxUhj"}],"key":"T2gX23NpCk"},{"type":"text","value":" to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UdTCGk5e9w"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∞\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e-\\infty\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e−\u003c/span\u003e\u003cspan class=\"mord\"\u003e∞\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"L5oJcP3qOF"},{"type":"text","value":". What this means is that this model is exactly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ElMxRC02UF"},{"type":"inlineMath","value":"0 \\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e%\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e0 \\%\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0%\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"tB1Cf54qbu"},{"type":"text","value":" likely to predict this name (infinite ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VJsOnzrWxF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tZ2ybVpztu"}],"key":"g4VGA1Q7ra"},{"type":"text","value":"). If you look up the table you see that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WpLn9WNxR1"},{"type":"inlineCode","value":"q","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Svs2R84Div"},{"type":"text","value":" is followed by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wqVNCtcg7X"},{"type":"inlineCode","value":"j","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MHOljieoK5"},{"type":"text","value":" zero times. This kind of behavior people don’t usually like too much, so there is a simple trick to alleviate it: model smoothing. It involves adding some fake counts to the bigram counts array so that never is there a bigram with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Uy0bsM4PCp"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BR2if7Ycdq"},{"type":"text","value":" counts (and therefore ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O65k8b2ux8"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XHhp3S7fht"},{"type":"text","value":" probability). This ensures that there are no zeros in our bigram counts matrix. E.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MD6dg9nQ3V"}],"key":"lAm0M2iuTI"}],"key":"lbQmvPvQAx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"P = (N + 1).float()\nP = P / P.sum(\n    1, keepdim=True\n)  # sum over the column dimension and keep column dimension\ntest_model(iterable=[\"christosqj\"], calc_ll=True, print_nll=True)","key":"rcUSNj6gSh"},{"type":"outputs","id":"qCEZov87KqBmycM2ai-_g","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":".c: 0.0481 -3.0339\nch: 0.1869 -1.6774\nhr: 0.0268 -3.6185\nri: 0.2384 -1.4338\nis: 0.0743 -2.5998\nst: 0.0942 -2.3625\nto: 0.1193 -2.1257\nos: 0.0634 -2.7578\nsq: 0.0002 -8.3105\nqj: 0.0033 -5.7004\nj.: 0.0246 -3.7051\nlog_likelihood=-37.32549834251404\nnll=37.32549834251404\nloss=3.393227122046731\n"},"children":[],"key":"tCCAduIH32"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":29,"metadata":{},"data":{"text/plain":{"content":"-37.32549834251404","content_type":"text/plain"}}},"children":[],"key":"QJxN2fCzVK"}],"key":"WvY0Tfyn1v"}],"key":"RmMtUqkwYU"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, we avoid getting a loss of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vGZZnRmFND"},{"type":"inlineMath","value":"-\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∞\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e-\\infty\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e−\u003c/span\u003e\u003cspan class=\"mord\"\u003e∞\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"x2VzoLYDZA"},{"type":"text","value":". Cool! So we’ve now trained a respectable bigram character-level language model. We trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions. We saw that we can also then use those parameters of this model to perform sampling of new words (sample new names according to these distributions) and evaluate the quality of this model which is summarized by a single number: the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tp07NcZNdw"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aupQnrJRCA"}],"key":"myayTDvqQj"},{"type":"text","value":". And the lower this number is, the better the model is because it is giving high probabilities to the actual mixed characters of all the bigrams in our training set. Great! We basically, counted and then normalized those counts, which is sensible enough.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YNmMkRXOuG"}],"key":"pcMZnOyUu8"}],"key":"q8CfqCEKiY"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Casting the model as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gTYsgHzKzN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KCg0jRppGa"}],"key":"AGQwKa2vWZ"}],"identifier":"casting-the-model-as-a-nn","label":"Casting the model as a nn","html_id":"casting-the-model-as-a-nn","implicit":true,"key":"SojWUnOfk3"}],"key":"JFGm7UpeXF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s now try a different approach by casting such a bigram language model into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nhHt3lKGQE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IW1XlATFez"}],"key":"mehSu2i5GX"},{"type":"text","value":" framework to achieve the same goal. Our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Za87qX56o9"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ic4EN79eCU"}],"key":"J2XKpHDeZz"},{"type":"text","value":" is still going to be a bigram character-level language model. It will receive a single character as an input that will pass through a bunch of weighted neurons and then output the probability distribution over the next character in the sequence. It’s going to make guesses about what character is going to follow the input character. In addition, we’ll be able to evaluate any setting of the parameters of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O1LAQBFaRL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ECZbWpWi4W"}],"key":"K1N6gZqFzG"},{"type":"text","value":", since we have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zAoYDsouAk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TKPHLArMlg"}],"key":"ATO7O7GZbq"},{"type":"text","value":" function. Basically, we’re going to take a look at the probabilities distributions our model assigns for our next character and find the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Em9Rs8FbFI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jiDCTI8H6D"}],"key":"nUquvLwlSg"},{"type":"text","value":" between those and the labels (which are the character that we expect to come next in the bigram). By doing so, we can use gradient-based optimization to tune the weights of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jW6Fqi1wyN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T42Udfd0am"}],"key":"QDBcN6AV9N"},{"type":"text","value":" that give us the output probabilities. Let’s begin this alternative approach by first constructing our dataset:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MiWxW3Ap0W"}],"key":"ba5Msjq9qQ"}],"key":"h4k2DHSx9u"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create training dataset of bigrams (x, y)\nxs, ys = [], []\nfor w in words[:1]:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        print(ch1, ch2)\n        xs.append(ctoi[ch1])\n        ys.append(ctoi[ch2])\n# Convert to pytorch tensor (https://pytorch.org/docs/stable/generated/torch.tensor.html)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)","key":"nOTkjMvJDh"},{"type":"outputs","id":"D92V6Kl9bhutTIDoPXAHU","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":". e\ne m\nm m\nm a\na .\n"},"children":[],"key":"sRBsu0WTJ2"}],"key":"SpjzFx0pMo"}],"key":"EWs1FqdsdA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xs","key":"MpUCiz5Hc6"},{"type":"outputs","id":"7yMn6v-E0g9WJjYyYWjhm","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":31,"metadata":{},"data":{"text/plain":{"content":"tensor([ 0,  5, 13, 13,  1])","content_type":"text/plain"}}},"children":[],"key":"OiYn5NYwO8"}],"key":"UyTI1IIt8I"}],"key":"r8I5VQD0Kt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ys","key":"eiw16Rw8NP"},{"type":"outputs","id":"rtPMj6j4dfX0sk-xgU2Y3","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":32,"metadata":{},"data":{"text/plain":{"content":"tensor([ 5, 13, 13,  1,  0])","content_type":"text/plain"}}},"children":[],"key":"XDjhOsTpDJ"}],"key":"qaFBVuFOyL"}],"key":"PoU8E3rg0W"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, how do we pass each character into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yOX0HBnAWv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X21sdF9fdI"}],"key":"RgUYwKu8ME"},{"type":"text","value":"? ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k6arVHQ0cy"},{"type":"link","url":"https://en.wikipedia.org/wiki/One-hot","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"One-hot encoding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rGfrIp4wZA"}],"urlSource":"https://en.wikipedia.org/wiki/One-hot","data":{"page":"One-hot","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"sleURHuBjN"},{"type":"text","value":"! With this encoding, each integer is encoded with bits.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rx0qoPrGbG"}],"key":"MJNmI75fM4"}],"key":"yhCZmfjnk0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch.nn.functional as F\n\nxenc = F.one_hot(xs, num_classes=27).float()\nxenc","key":"epfzLZvldt"},{"type":"outputs","id":"NHcGEVDTUTRxdZjJ9lBCO","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":33,"metadata":{},"data":{"text/plain":{"content":"tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])","content_type":"text/plain"}}},"children":[],"key":"xGn9SNlSTA"}],"key":"DHAdE3XOfz"}],"key":"iT4AaZG1qT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xenc.shape","key":"SdY1j9n5uq"},{"type":"outputs","id":"WWaQLBHPxSNhFNjOR65kr","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":34,"metadata":{},"data":{"text/plain":{"content":"torch.Size([5, 27])","content_type":"text/plain"}}},"children":[],"key":"Nz9SfaoeJi"}],"key":"an0Bp0Tm51"}],"key":"HS82G5dQh7"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.imshow(xenc);","key":"poexYvSIbs"},{"type":"outputs","id":"U5qlKRPUHfzDXbHwbo75U","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"958200875f3d4e87b042cf5e9da17908\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"d18c6e0069b4dcc50d26f37021326cac","path":"/build/d18c6e0069b4dcc50d26f37021326cac.png"},"text/html":{"content":"\n            \u003cdiv style=\"display: inline-block;\"\u003e\n                \u003cdiv class=\"jupyter-widgets widget-label\" style=\"text-align: center;\"\u003e\n                    Figure\n                \u003c/div\u003e\n                \u003cimg src='data:image/png;base64,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' width=640.0/\u003e\n            \u003c/div\u003e\n        ","content_type":"text/html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"jGkeYFRQ8e"}],"key":"t37o8DDdJq"}],"key":"TTXzfLgPN2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s create our neuron:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oExV7l3do5"}],"key":"o5AafnTDY9"}],"key":"cjpYsMnBwT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W = torch.randn((27, 1))\nxenc @ W","key":"Fmau3W3Sp3"},{"type":"outputs","id":"wCiGIx7ZTtOdardahkhv8","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":36,"metadata":{},"data":{"text/plain":{"content":"tensor([[-0.3270],\n        [-1.4539],\n        [-0.8739],\n        [-0.8739],\n        [ 1.4181]])","content_type":"text/plain"}}},"children":[],"key":"VHAGA5MOWm"}],"key":"JZsrfz2jj6"}],"key":"hdolWYSuX3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Our neuron receives one character of size ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CkLRdwHf8H"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n4G6cJs8e2"},{"type":"text","value":" and spits out ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eYy87caoCD"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xn2NRx8RhC"},{"type":"text","value":" output value. However, as you can see, since PyTorch supports matrix multiplication, our neuron can receive ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Olw394IRNc"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GwfhGvQJFO"},{"type":"text","value":" characters of size ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CFQPuf5Dhm"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tM1CmMt2eQ"},{"type":"text","value":" in parallel and output each character’s output in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AohWNQ57FS"},{"type":"inlineMath","value":"5 \\times 1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmn\u003e5\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e5 \\times 1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e5\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"kUK0IIO0X3"},{"type":"text","value":" matrix (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OVoxXvGyMr"},{"type":"inlineMath","value":"[5 \\times 27] \\cdot [27 \\times 1] \\rightarrow [5 \\times 1]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e5\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo\u003e→\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e5\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e[5 \\times 27] \\cdot [27 \\times 1] \\rightarrow [5 \\times 1]\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e5\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e→\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e5\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"UrFy597ko5"},{"type":"text","value":"). Now, let’s pass our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mh0bSbL0xg"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pK4ej4Sjwo"},{"type":"text","value":" characters as inputs through ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xKsEGuMvup"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x439M0OXaa"},{"type":"text","value":" neurons instead of just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"toA6F5awpG"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eN2RA2i6MX"},{"type":"text","value":" neuron:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wQmPjAdHwS"}],"key":"wnSQg3AOMx"}],"key":"sOW2G0dUPt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W = torch.randn((27, 27))\nxenc @ W","key":"XzuKJ1TyjH"},{"type":"outputs","id":"plfxfI47oysBQSY1rFJ12","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":37,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0.9979,  0.2825,  1.1355,  0.3798, -0.2801,  0.0672, -1.1496,  2.1393,\n         -0.2687, -1.4350,  1.1158,  0.4346, -0.4915, -0.1916,  1.4139, -0.4590,\n         -0.5869,  1.6688,  0.8819,  0.8542, -0.0366, -0.6968,  0.1041,  0.8881,\n          0.7592, -0.5573,  0.9596],\n        [-0.1725, -1.5476,  1.5005,  1.4560,  0.9079, -1.2025,  0.1265,  0.1533,\n         -0.2189, -1.3150,  1.6275,  0.3342,  1.4620, -0.3458, -0.2391,  0.5896,\n          1.7679,  1.1726, -0.6278, -0.1539, -0.6117, -0.0106,  0.7131,  2.0526,\n          1.2183,  1.6270, -1.3764],\n        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,\n          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,\n          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,\n          0.1524,  1.5829,  0.3142],\n        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,\n          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,\n          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,\n          0.1524,  1.5829,  0.3142],\n        [ 0.2948,  0.0746, -0.4187,  0.4092, -0.6537,  1.1562,  0.6917, -1.2596,\n         -0.1424, -0.5520, -1.1731, -0.4088, -0.6465, -0.2629, -0.3580,  0.8126,\n         -1.7589,  1.7377, -0.5665,  1.9188, -0.6135, -1.2176,  0.0166,  0.1594,\n         -0.8806,  0.6167, -0.9173]])","content_type":"text/plain"}}},"children":[],"key":"bjzN6TstvS"}],"key":"oj5JMk8kLu"}],"key":"GNws3rZv4j"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Predictably, we get ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wekNyyLwyx"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ti4sscdoqI"},{"type":"text","value":" arrays (one per input/character) of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V5N2NzAUXO"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YOXKHW5sak"},{"type":"text","value":" outputs (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jkHqgX8v3M"},{"type":"inlineMath","value":"[5 \\times 27] \\cdot [27 \\times 27] \\rightarrow [5 \\times 27]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e5\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo\u003e→\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e5\u003c/mn\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmn\u003e27\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e[5 \\times 27] \\cdot [27 \\times 27] \\rightarrow [5 \\times 27]\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e5\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e⋅\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e→\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e5\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e27\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"qsyDlWQNgI"},{"type":"text","value":"). Each output number represents each neuron’s firing rate of a specific input. For example, the following is the firing rate of the 13th neuron of the 3rd input:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K7cNJjuhgU"}],"key":"vQ42lXf0vH"}],"key":"VOOwPpBUwb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc @ W)[3, 13]","key":"SLJE0cXtIx"},{"type":"outputs","id":"gkBMvidg3RHgFSBAKrEH9","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":38,"metadata":{},"data":{"text/plain":{"content":"tensor(-1.2633)","content_type":"text/plain"}}},"children":[],"key":"wNiZlCG70f"}],"key":"CAMroVs9M0"}],"key":"RJeXQmWIgl"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What PyTorch allows is matrix multiplication that enables parallel dot products of many inputs in a batch with the weights of neurons of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yuAIZERqR5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tiecCzoOmO"}],"key":"L9HZjfWjNG"},{"type":"text","value":". For example, this is how to multiply the inputs that represent the 3rd character with the weights of the 13th neuron:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fvMKIykTNz"}],"key":"GsyXOPBt69"}],"key":"XvkG219tuw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xenc[3]","key":"OVSJr24bdq"},{"type":"outputs","id":"UfDYOv8K0HvPjft-6Fqw6","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.])","content_type":"text/plain"}}},"children":[],"key":"OWZjmGatxg"}],"key":"QdjKBwg6S7"}],"key":"mgn4oMb2vS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W[:, 13]","key":"dP3sM1lXRC"},{"type":"outputs","id":"lBc6H9tAbGYThFEtP7iOL","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":40,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.1916, -0.2629, -1.1183,  0.9108,  0.7797, -0.3458, -1.2783, -0.7899,\n        -0.3221, -0.4800,  0.3307,  0.2826, -0.5372, -1.2633,  0.3663,  0.1210,\n         0.0446, -0.1690, -0.3741, -0.0798, -0.5883, -0.9373, -0.1367, -0.2475,\n        -0.4424, -2.0253, -0.1943])","content_type":"text/plain"}}},"children":[],"key":"tPxzTBzC7W"}],"key":"g31iebPQ5U"}],"key":"HJBrX1qyZT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc[3] * W[:, 13]).sum()","key":"VPapRIy2dP"},{"type":"outputs","id":"Vh4lCnKuF06u-CmArwjAF","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/plain":{"content":"tensor(-1.2633)","content_type":"text/plain"}}},"children":[],"key":"jUMbrAJmLO"}],"key":"TNSUXanRAu"}],"key":"mwgODAo6DV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc @ W)[3, 13]  # same as above","key":"dum31zsDWf"},{"type":"outputs","id":"giRO1Gf1J_vFz-YJQilUn","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":42,"metadata":{},"data":{"text/plain":{"content":"tensor(-1.2633)","content_type":"text/plain"}}},"children":[],"key":"nEIRSLNAVQ"}],"key":"Sa2ipHM9l9"}],"key":"E1ipKixV5c"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Ok, so what did is we fed our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MvSFMKceL9"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ES8CRXjAKn"},{"type":"text","value":"-dimensional inputs into the first layer of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hLVjutu9Hq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gy49IA5R6g"}],"key":"xLNjhzETOD"},{"type":"text","value":" that has 27 neurons. These neurons perform ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YbipM7OWSr"},{"type":"inlineCode","value":"W * x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vo8GPNSNoq"},{"type":"text","value":". They don’t have a bias and they don’t have a non-linearity like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MZKwBKHgxr"},{"type":"inlineCode","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a3aZnPJPy1"},{"type":"text","value":". We are going to leave our network as is: a 1-layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zmpneinb5E"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CO4LmiGdUT"}],"key":"TzOvlFZQ5Y"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fVSE6coVrp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HHkpoVLozq"}],"key":"vJfQh0C8H1"},{"type":"text","value":". That’s it. Basically, the dumbest, smallest, simplest ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qhnaCCyUmF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"reRgaOVTMl"}],"key":"hKlwRxSTIR"},{"type":"text","value":". Remember, what we trying to produce is a probability distribution for a next character in a sequence. And there’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i7KT7ANRB4"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XMuRCKEQkJ"},{"type":"text","value":" of them. But we have to come up with exact semantics as to how we are going to interpret these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MpE6R4nTdO"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kt1mz6Wtmq"},{"type":"text","value":" numbers that these neurons take on. Intuitively, as we can see in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FTNsxEHUpS"},{"type":"inlineCode","value":"xenc @ W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xMhAtzaXWo"},{"type":"text","value":" output, some of these outputs numbers are positive and some negative. That’s because they come out of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cuE2OgDE7w"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HfLvUWtXR4"}],"key":"pErrnPJwcu"},{"type":"text","value":" layer with weights are initialized from the normal ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VeMR9RnI4d"},{"type":"inlineMath","value":"[-1, 1]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e[-1, 1]\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e−\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"CYLJRhrUvF"},{"type":"text","value":" distribution. But, what we want however is something like a bigram count table that we previously produced, where each row told us the counts which we then normalized to get the probabilities. So, we want something similar to come out of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TKiDyWtxgx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EXk2sNKTht"}],"key":"dJgX7KAnyE"},{"type":"text","value":". But, what we have right now, are some negative and positive numbers. Now, we therefore want these numbers to represent the probabilities for the next character with their unique characteristics. For example, probabilities are positive numbers and they sum to 1. Also, they obviously have to be probabilities. They can’t be counts because counts are positive integers; not a great output from a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Cy9Ed2aSJ0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pitsDwILu5"}],"key":"jHPAgPFzIc"},{"type":"text","value":". Instead, what the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VdtEgfwZiH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CvQTJK9Ud1"}],"key":"bXsLTK5iVM"},{"type":"text","value":" is going to output and how we are going to interpret these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H0yqLZopKd"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MKlt5pihar"},{"type":"text","value":" output numbers is as log counts. One way to accomplish this is by exponentiating each output number so that the result is always positive. Specifically, exponentiating a  negative number yields a result that is a positive value ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gcSsSOUZAN"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"less","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gW4s4gfIqh"}],"key":"xfzv9NRkLp"},{"type":"text","value":" than ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"grVCM3df5R"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IKobPd0EBl"},{"type":"text","value":". Whereas, exponentiating a positive number yields a result whose value is between greater than ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ODSG1luAyg"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jmfY6UQLmZ"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DQBlGmRieC"},{"type":"inlineMath","value":"\\infty","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003e∞\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\infty\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∞\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"gKQXVGJorR"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZrqmjWolDC"}],"key":"l5FPvMcnf4"}],"key":"wZdiiSatgl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(xenc @ W).exp()","key":"NxyEs3rMdc"},{"type":"outputs","id":"BNVhuaEyYbbfr8YGOYCyR","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":43,"metadata":{},"data":{"text/plain":{"content":"tensor([[2.7125, 1.3265, 3.1128, 1.4619, 0.7557, 1.0695, 0.3168, 8.4932, 0.7644,\n         0.2381, 3.0519, 1.5444, 0.6117, 0.8256, 4.1119, 0.6319, 0.5561, 5.3059,\n         2.4156, 2.3495, 0.9641, 0.4982, 1.1098, 2.4304, 2.1365, 0.5727, 2.6108],\n        [0.8415, 0.2128, 4.4841, 4.2888, 2.4792, 0.3004, 1.1348, 1.1657, 0.8034,\n         0.2685, 5.0910, 1.3968, 4.3146, 0.7077, 0.7873, 1.8032, 5.8586, 3.2305,\n         0.5338, 0.8574, 0.5424, 0.9895, 2.0402, 7.7883, 3.3814, 5.0888, 0.2525],\n        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,\n         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,\n         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],\n        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,\n         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,\n         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],\n        [1.3429, 1.0774, 0.6579, 1.5056, 0.5201, 3.1778, 1.9972, 0.2838, 0.8673,\n         0.5758, 0.3094, 0.6644, 0.5239, 0.7688, 0.6990, 2.2538, 0.1722, 5.6841,\n         0.5675, 6.8131, 0.5415, 0.2959, 1.0168, 1.1728, 0.4145, 1.8528, 0.3996]])","content_type":"text/plain"}}},"children":[],"key":"cx7oXHdyRi"}],"key":"GB8z3R7uzy"}],"key":"KvoPSGbIjF"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Such exponentiation is a great way to make the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M955dCgdQj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"POmwy4QV0O"}],"key":"r44XhbCAKV"},{"type":"text","value":" predict counts. Which are positive numbers that can take on various values depending on the setting of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dYpMipvWtG"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i5IaR50BE7"},{"type":"text","value":". Let’s break it down more:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jm1yPsJC9D"}],"key":"HBSgr1xN5u"}],"key":"nPE5YWHOtD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits = xenc @ W  # log-counts\ncounts = logits.exp()  # equivalent to the N bigram counts array\nprobs = counts / counts.sum(1, keepdims=True)\nprobs","key":"CLaeacyqvK"},{"type":"outputs","id":"95bDUYjNJyNAvoLcHKPt7","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":44,"metadata":{},"data":{"text/plain":{"content":"tensor([[0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,\n         0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,\n         0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502],\n        [0.0139, 0.0035, 0.0739, 0.0707, 0.0409, 0.0050, 0.0187, 0.0192, 0.0132,\n         0.0044, 0.0840, 0.0230, 0.0711, 0.0117, 0.0130, 0.0297, 0.0966, 0.0533,\n         0.0088, 0.0141, 0.0089, 0.0163, 0.0336, 0.1284, 0.0558, 0.0839, 0.0042],\n        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,\n         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,\n         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],\n        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,\n         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,\n         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],\n        [0.0371, 0.0298, 0.0182, 0.0416, 0.0144, 0.0879, 0.0552, 0.0078, 0.0240,\n         0.0159, 0.0086, 0.0184, 0.0145, 0.0213, 0.0193, 0.0623, 0.0048, 0.1572,\n         0.0157, 0.1884, 0.0150, 0.0082, 0.0281, 0.0324, 0.0115, 0.0512, 0.0111]])","content_type":"text/plain"}}},"children":[],"key":"dL6uoE7FCx"}],"key":"sShycq9DBX"}],"key":"TcNFfdZZrn"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Therefore, we have a way to get the probabilities, where each row sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qm0bn3Lz0n"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O7BaH1E64p"},{"type":"text","value":" (since they are normalized), e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sf48JNuADd"}],"key":"K2XcF9Caw5"}],"key":"LXE1ajgvLU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs[0].sum().item()","key":"vtKNWhmDfp"},{"type":"outputs","id":"01eSQy8USN2YHgUn0FEQH","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":45,"metadata":{},"data":{"text/plain":{"content":"1.0","content_type":"text/plain"}}},"children":[],"key":"s8dpZxTvfN"}],"key":"tkl2TzESq6"}],"key":"mkbENN2dqf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs.shape","key":"aha4LYUQDe"},{"type":"outputs","id":"g9OB2TgdH9gQw1_2Aqvqi","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":46,"metadata":{},"data":{"text/plain":{"content":"torch.Size([5, 27])","content_type":"text/plain"}}},"children":[],"key":"gyULi6iLyq"}],"key":"CBUd0uXPAb"}],"key":"eitVXny4Ka"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What we have achieved is that for every one of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O9hyhk1uu7"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ugQExaYALO"},{"type":"text","value":" examples, we now have a row that came out of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PrCYka6sPM"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZnY2ayhKeR"}],"key":"UcMrxupDhP"},{"type":"text","value":". And because of the transformations here, we made sure that this output of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ar6LW5Ki0H"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gsXoVAn4CR"}],"key":"MSgtelfT5m"},{"type":"text","value":" can be interpreted as probabilities. In other words, what we have done is that we took inputs, applied differentiable operations on them (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oP7OCop1bG"},{"type":"inlineCode","value":"@","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KG2VGTZIW5"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"thImlxlErh"},{"type":"inlineCode","value":"exp()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jW8SppjlyL"},{"type":"text","value":") that we can ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZbSzA5WntJ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VIWgaeNnMK"}],"key":"g8XcEDMout"},{"type":"text","value":" through and we are getting out probability distributions. Take the first input character that was fed in as an example:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lvr6x2maqo"}],"key":"hfXf5YARPF"}],"key":"Fi965KywYW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xenc[0]","key":"ZUNt9ZnITL"},{"type":"outputs","id":"P5I_RrzqnQ814i85OIPdg","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":47,"metadata":{},"data":{"text/plain":{"content":"tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.])","content_type":"text/plain"}}},"children":[],"key":"QMV4iQoLbl"}],"key":"edauHdfe4l"}],"key":"u59ZXHtBWB"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"that corresponds to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oqWGH1m8cm"},{"type":"inlineCode","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EAaeq9UY0Z"},{"type":"text","value":" symbol from the name:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KQcyFBFpAr"}],"key":"YSgQCuqQG3"}],"key":"heNi2CYoXS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"words[0]","key":"b8BK9unZQr"},{"type":"outputs","id":"r4p5XPAJ5YfIzKceluPKA","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":48,"metadata":{},"data":{"text/plain":{"content":"'emma'","content_type":"text/plain"}}},"children":[],"key":"syBotqxPzF"}],"key":"vyu3VKfgMp"}],"key":"JGcUvm1I66"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The way we fed this character into the neural network is that we first got its index, then we one-hot encoded it, then it went into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oWImM70E8Y"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XMpe7Wqt3E"}],"key":"ZNm51TuT80"},{"type":"text","value":" and out came this distribution of probabilities:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ku821cVprB"}],"key":"OTqSv7awVC"}],"key":"RoP05uXllN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs[0]","key":"v8i68of48w"},{"type":"outputs","id":"rqhewhPanyO6Tu9lQbpke","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":49,"metadata":{},"data":{"text/plain":{"content":"tensor([0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,\n        0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,\n        0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502])","content_type":"text/plain"}}},"children":[],"key":"rURSjO0RhR"}],"key":"hG6WYvzH72"}],"key":"hnfPlE8Pot"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"with a shape of:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v83wPJT5MG"}],"key":"ChrBko9QZm"}],"key":"dkkr7wktGR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"probs[0].shape","key":"fkrUGQn59e"},{"type":"outputs","id":"Mlccv61QalHhx86nWmQTA","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":50,"metadata":{},"data":{"text/plain":{"content":"torch.Size([27])","content_type":"text/plain"}}},"children":[],"key":"NL3qr1YCmp"}],"key":"KAqDB3hcsx"}],"key":"lYxWP6i10T"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n8wlUadm6H"},{"type":"text","value":" numbers. We interpret these numbers of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OAmfxS4eCO"},{"type":"inlineCode","value":"probs[0]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dVVoedLLsw"},{"type":"text","value":" as the probability or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Wc69eyl4dA"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"how likely it is","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l5urdfBGQN"}],"key":"apFsrL9ZgX"},{"type":"text","value":" for each of the corresponding characters to come next. As we train the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pdDucDiSlR"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xxtvbfzSs3"}],"key":"DR3qXZ32uE"},{"type":"text","value":" by tuning the weights ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uVZIS7HcXx"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q7hIE2fkk0"},{"type":"text","value":", we are of course going to be getting different probabilities out for every character that you input. So, the question is: can we tune ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s4egVBtlCp"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OixNvZksfv"},{"type":"text","value":" such that the probabilities coming out are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FFoVoRUSgK"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"pretty good","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XXOVA6cOmp"}],"key":"KkV0zVflcP"},{"type":"text","value":"? The way we measure ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DNsvotOc8b"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"pretty good","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jbM5xxYBCx"}],"key":"gf7fHvMyoU"},{"type":"text","value":" is by the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ign0dhSkyn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oZzjIpaKgb"}],"key":"mAMPW2UGDY"},{"type":"text","value":" function. Below you can see what have done in a simple summary:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vF62ycHtwc"}],"key":"SFmenjMuOw"}],"key":"iMpQzWvUck"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# SUMMARY ------------------------------\u003e\u003e\u003e\u003e\nxs  # inputs","key":"m94ji8FRRm"},{"type":"outputs","id":"AXgTsBbnLa5y6go5eAekC","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":51,"metadata":{},"data":{"text/plain":{"content":"tensor([ 0,  5, 13, 13,  1])","content_type":"text/plain"}}},"children":[],"key":"lYOriTsTgg"}],"key":"ivXiXSc1AJ"}],"key":"jxE9GVWbqf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ys  # targets","key":"G7PfhvZ69o"},{"type":"outputs","id":"R3YGz7cu4tzOa-Grzny7q","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":52,"metadata":{},"data":{"text/plain":{"content":"tensor([ 5, 13, 13,  1,  0])","content_type":"text/plain"}}},"children":[],"key":"iAntPmn6Fb"}],"key":"jBInfo9Fbf"}],"key":"SeCIwxrD6S"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Both ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uRet2mUdVB"},{"type":"inlineCode","value":"xs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YfK3vr1Nq8"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NeAqfRpiQK"},{"type":"inlineCode","value":"ys","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t0825i0AjM"},{"type":"text","value":" constitute the dataset. They are integers representing characters of a sequence/word.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hXYYLkwng9"}],"key":"kpnTmJwRYL"}],"key":"HRnsl0m47b"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Use a generator for reproducability and randomly initialize 27 neurons' weights. Each neuron receives 27 inputs.\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn((27, 27), generator=g)  # 27 incoming weights for 27 neurons\n# Encode the inputs into one-hot representations\nxenc = F.one_hot(xs, num_classes=27).float()  # input to the network: one-hot encoding\n# Pass encoded inputs through first layer to get logits\nlogits = xenc @ W  # predict log-counts\n# Exponentiate the logits to get fake counts\ncounts = logits.exp()  # counts, equivalent to N\n# Normalize these counts to get probabilities\nprobs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n# NOTE: the 2 lines above constitute what is called a 'softmax'\nprobs.shape","key":"u80umWTYDM"},{"type":"outputs","id":"ak97ARUDsUDfjWCMSCoRo","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":53,"metadata":{},"data":{"text/plain":{"content":"torch.Size([5, 27])","content_type":"text/plain"}}},"children":[],"key":"ZwYDwPDIfx"}],"key":"jG7sYb3oo1"}],"key":"eXAnCk1G9s"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Softmax is a very-often-used ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P7jEizqxXS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KGnYorXRvy"}],"key":"ZXPNexCjUg"},{"type":"text","value":" function in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lWbuZo6QP5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AtbKlByKyk"}],"key":"uQC34A9b5B"},{"type":"text","value":"s. It takes in logits, exponentiates them, then divides and normalizes. It’s a way of taking outputs of a linear layer that might be positive or negative and it outputs numbers that are only positive and always sum to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YXQYfWfEUt"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fGR82jfnnr"},{"type":"text","value":", adhering to the properties of probability distributions. It can be viewed as a normalization function if you want to think of it that way.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QMQH9q0rG4"}],"key":"gMt0MRTS4g"}],"key":"pSMfPJZ712"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\ndisplay(Image(filename='softmax.jpeg'))","key":"Pl2TZOeevP"},{"type":"outputs","id":"9cjzDwW9GOYyPfi8BFs7Z","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"16dc2c955b6f9f81a6cf3b8b72fe56f3","path":"/build/16dc2c955b6f9f81a6cf3b8b72fe56f3.jpeg"},"text/plain":{"content":"\u003cIPython.core.display.Image object\u003e","content_type":"text/plain"}}},"children":[],"key":"ER2Pg5lnu0"}],"key":"AK8YTWtBwU"}],"key":"GlxE1H8Rks"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, since every operation in the forward pass is differentiable, we can ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rbmkfZ4RC7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dnex2WBfPf"}],"key":"JkUzUzEgk7"},{"type":"text","value":" through. Below, we iterate over every input character and describe what is going on:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hJdQrKQWhd"}],"key":"DuxhXVbwmJ"}],"key":"upw7me1H5P"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"nlls = torch.zeros(5)\nfor i in range(5):\n    # i-th bigram:\n    x = xs[i].item()  # input character index\n    y = ys[i].item()  # label character index\n    print(\"--------\")\n    print(f\"bigram example {i+1}: {itoc[x]}{itoc[y]} (indexes {x},{y})\")\n    print(\"input to the nn:\", x)\n    print(\"output probabilities from the nn:\", probs[i])\n    print(\"label (actual next character):\", y)\n    p = probs[i, y]\n    print(\"probability assigned by the nn to the correct next character:\", p.item())\n    logp = torch.log(p)\n    print(\"log likelihood:\", logp.item())\n    nll = -logp\n    print(\"negative log likelihood:\", nll.item())\n    nlls[i] = nll\nloss = nlls.mean()\nprint(\"=========\")\nprint(\"average negative log likelihood, i.e. loss =\", loss.item())","key":"JnWy6IiVdo"},{"type":"outputs","id":"y8w1A6wIG9H-njLNNmO_N","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"--------\nbigram example 1: .e (indexes 0,5)\ninput to the nn: 0\noutput probabilities from the nn: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\nlabel (actual next character): 5\nprobability assigned by the nn to the correct next character: 0.01228625513613224\nlog likelihood: -4.399273872375488\nnegative log likelihood: 4.399273872375488\n--------\nbigram example 2: em (indexes 5,13)\ninput to the nn: 5\noutput probabilities from the nn: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\nlabel (actual next character): 13\nprobability assigned by the nn to the correct next character: 0.018050700426101685\nlog likelihood: -4.014570713043213\nnegative log likelihood: 4.014570713043213\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the nn: 13\noutput probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 13\nprobability assigned by the nn to the correct next character: 0.026691533625125885\nlog likelihood: -3.623408794403076\nnegative log likelihood: 3.623408794403076\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the nn: 13\noutput probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 1\nprobability assigned by the nn to the correct next character: 0.07367686182260513\nlog likelihood: -2.6080665588378906\nnegative log likelihood: 2.6080665588378906\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the nn: 1\noutput probabilities from the nn: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\nlabel (actual next character): 0\nprobability assigned by the nn to the correct next character: 0.014977526850998402\nlog likelihood: -4.201204299926758\nnegative log likelihood: 4.201204299926758\n=========\naverage negative log likelihood, i.e. loss = 3.7693049907684326\n"},"children":[],"key":"z7gpNki0AR"}],"key":"A1aNjOMptj"}],"key":"ZIuIEryCyJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, the probabilities assigned by the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MjkJ15ATja"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VMXHdVnB7U"}],"key":"ciMX8l2c7X"},{"type":"text","value":" to the correct next character are bad (pretty low). See for example the probability predicted by the network of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Id1Xz6SrOB"},{"type":"inlineCode","value":"m","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AJnzgZKwWv"},{"type":"text","value":" following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fYeYMrTySo"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DKvXVO9c77"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OWcaDBq4N3"},{"type":"inlineCode","value":"em","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bnfsiupaFq"},{"type":"text","value":" example): the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gYCgcmnzmp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XjR66nWaj2"}],"key":"EE5tslKx8H"},{"type":"text","value":" value is very high (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L6GBI99sFC"},{"type":"text","value":"4.0145","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fhhLlpGhmC"},{"type":"text","value":"). And in general, for the whole word, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JCb60kc8Oq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GZ2bBn2ZCa"}],"key":"gdNbwNNNLp"},{"type":"text","value":" (the average ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jM6HtwGKDm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k4oFvLyJV9"}],"key":"yKcxxyH1up"},{"type":"text","value":") is high! This means that this is not a favorable setting of weights and we can do better. One easy way to do better is to reinitialize ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fya3N6ubpV"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xn5LAdaKdS"},{"type":"text","value":" using a different seed for example and pray to god that the loss is smaller or repeat until it is. But that is what amateurs do. We are professionals or, at least, we want to be! And what professionals do is they start with random weights, like we did, and then they optimize those weights in order to minimize the loss. We do so by some gradient-based optimization (e.g. gradient descent) which entails first doing backprop in order to compute the gradients of that weight ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gHCiQ9JE9i"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"w.r.t.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t2XhutzoOt"}],"key":"UnkG5AAeZB"},{"type":"text","value":" to those weights and then changing the weights by some such gradient amount in order to optimize them and minimize the loss. As we did with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wUbGjqjVup"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SJ12XRecDs"}],"key":"HHU678yRwY"},{"type":"text","value":", we will write an optimization loop for doing the backward pass. But instead of mean-squared error, we are using the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"haaQqUxNpp"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sx5z3yx4mU"}],"key":"JFQix4M25m"},{"type":"text","value":" as a loss function, since we are dealing with a classification task and not a regression one.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gCEbRzh4La"}],"key":"o79ttx3TgG"}],"key":"Yn3VvooxGH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nW = torch.randn(\n    (27, 27), generator=g, requires_grad=True\n)  # 27 incoming weights for 27 neurons\n\n\ndef forward_pass(regularize=False):\n    num = xs.nelement()\n    xenc = F.one_hot(\n        xs, num_classes=27\n    ).float()  # input to the network: one-hot encoding\n    logits = xenc @ W  # predict log-counts\n    counts = logits.exp()  # counts, equivalent to N\n    probs = counts / counts.sum(1, keepdims=True)  # probabilities for next character\n    loss = -probs[torch.arange(num), ys].log().mean()\n    return W, loss\n\n\nW, loss = forward_pass()\n# backward pass\nW.grad = None  # set to zero\nloss.backward()","key":"zX0Hw5PhNS"},{"type":"outputs","id":"tJoj_9Poi_JFFPkGUE6QN","children":[],"key":"Vf60wcyrIt"}],"key":"JpWu136b9U"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, something magical happened when ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W2qAA3WElD"},{"type":"inlineCode","value":"backward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"exIBOOW5xG"},{"type":"text","value":" ran. Like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TxTfbjm8n5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"micrograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"daL3A9bRO5"}],"key":"sgCVvwH9lN"},{"type":"text","value":", PyTorch, during the forward pass, keeps track of all the operations under the hood and builds a full computational graph. So, it knows all the dependencies and all the mathematical operations of everything. Therefore, calling ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xLlAuzACWr"},{"type":"inlineCode","value":"backward","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MMNnSWewTQ"},{"type":"text","value":" on the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iq1LMjXXMN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oJqr04tuF4"}],"key":"NkXj1kLE79"},{"type":"text","value":" fills in the gradients of all the intermediate nodes, all the way back to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zjjx0iufxH"},{"type":"inlineCode","value":"W","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iFQVprXn0e"},{"type":"text","value":" value nodes. Take a look:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dpVH769CAm"}],"key":"k6s2FbjPkp"}],"key":"MEdzSN9vgI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W.grad","key":"xCZj8u1lIP"},{"type":"outputs","id":"LrRsz42PQ7Zx6JIf5fhoc","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":57,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n          0.0024,  0.0307,  0.0292],\n        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n          0.0131,  0.0101,  0.0018],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n          0.0024,  0.0004,  0.0094],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n          0.0482,  0.0187,  0.0051],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000]])","content_type":"text/plain"}}},"children":[],"key":"riuYBFpxJJ"}],"key":"RRbf7mkvGm"}],"key":"JZbfYEyXHx"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And obviously:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uya1HFqNTz"}],"key":"xPQK2QYEDJ"}],"key":"bWJWum2ak6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert W.shape == W.grad.shape","key":"aOa61rJVKq"},{"type":"outputs","id":"TDGdc6V6B6xFbxmD-zOHV","children":[],"key":"WwKU8MOrkv"}],"key":"AssIupMaUf"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What a gradient value is telling us, e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"COpMH1jV4E"}],"key":"inleXAPmuV"}],"key":"JMw0uNlfLF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W.grad[1][4].item()","key":"bFK3x3EKtq"},{"type":"outputs","id":"synq_C8-5B6RYzaFekMb6","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":59,"metadata":{},"data":{"text/plain":{"content":"0.012119228951632977","content_type":"text/plain"}}},"children":[],"key":"am5Oxnhgdj"}],"key":"tacfaIu7PT"}],"key":"PGVrj8x9mm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"is that nudging the specific corresponding weight by a small ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IXVwNPEOiB"},{"type":"inlineCode","value":"h","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XMeokaobui"},{"type":"text","value":" value, would nudge the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zie7jWylh5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mfASgBAZtF"}],"key":"QeLnxLfgDz"},{"type":"text","value":" by that gradient amount. Since we want to decrease the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rvZEn6SFSe"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KFR1EmMUnt"}],"key":"dvAqgMQsUf"},{"type":"text","value":", we simply need to change the weights by a small negative fraction of the gradients in order to move them in the direction that locally most steeply decreases the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jTrHM19ooX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zlMWoasbH4"}],"key":"bGUBR0XLv1"},{"type":"text","value":" value:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jxlXMlpDND"}],"key":"Iv1UPAOaeA"}],"key":"Qgg8ickvMc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W.data += -0.1 * W.grad","key":"D6HKl1lcMb"},{"type":"outputs","id":"aTmVc3xzr9JrtCKU2JuRC","children":[],"key":"iAS7OvJ2r5"}],"key":"j34x5jSgoG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We just did a single gradient descent optimization step, which means that if we re-calculate the loss, it will be lower:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sbL4OvdR83"}],"key":"NphVsRP7yH"}],"key":"sd9HRxQm9i"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"W, loss = forward_pass()\nloss.item()","key":"Dj1CerZv1m"},{"type":"outputs","id":"5QyOZklysF6v3jUrojlik","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":61,"metadata":{},"data":{"text/plain":{"content":"3.7492127418518066","content_type":"text/plain"}}},"children":[],"key":"rUZRPW9YMI"}],"key":"MkNxdxojcG"}],"key":"hKkLXaoYVc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Tada! All we have to do now is put everything together and stick the single step into a loop so that we can do multi-step gradient descent optimization. This time, for all the words in our dataset, not just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CuhHbvLS8T"},{"type":"inlineCode","value":"emma","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FDXKJoDfg2"},{"type":"text","value":"!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c6DBlTMkmv"}],"key":"mztpVweEVj"}],"key":"jGSKiTbTIJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# create the dataset\nxs, ys = [], []\nfor w in words:\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        xs.append(ctoi[ch1])\n        ys.append(ctoi[ch2])\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint(\"number of examples (bigrams): \", num)\n# initialize the 'network'\ng = torch.Generator().manual_seed(SEED)\nW = torch.randn((27, 27), generator=g, requires_grad=True)","key":"rNTcaA1Dyf"},{"type":"outputs","id":"yCPQd-vUpDYdLroGnBfAa","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"number of examples (bigrams):  228146\n"},"children":[],"key":"VGKPJOVNaa"}],"key":"TgYaSJNM4l"}],"key":"gQGLApDVbl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# gradient descent\nfor k in range(100):\n    W, loss = forward_pass()\n    print(loss.item())\n    # backward pass\n    W.grad = None  # set to zero the gradient\n    loss.backward()\n    # update\n    W.data += -50 * W.grad","key":"fZ9Z2iDmIw"},{"type":"outputs","id":"c8hUF76nQOlMXBjB0d1z_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"3.758953809738159\n3.371098756790161\n3.1540417671203613\n3.020373821258545\n2.9277119636535645\n2.860402822494507\n2.8097293376922607\n2.7701027393341064\n2.7380733489990234\n2.711496591567993\n2.6890034675598145\n2.6696884632110596\n2.6529300212860107\n2.638277292251587\n2.6253881454467773\n2.6139907836914062\n2.603863477706909\n2.5948219299316406\n2.586712121963501\n2.57940411567688\n2.572789192199707\n2.5667762756347656\n2.5612881183624268\n2.5562589168548584\n2.551633596420288\n2.547365665435791\n2.5434155464172363\n2.539748430252075\n2.5363364219665527\n2.5331544876098633\n2.5301806926727295\n2.5273969173431396\n2.5247862339019775\n2.522334575653076\n2.520029067993164\n2.517857789993286\n2.515810966491699\n2.513878345489502\n2.512052059173584\n2.510324001312256\n2.5086867809295654\n2.5071346759796143\n2.5056610107421875\n2.5042612552642822\n2.502929210662842\n2.5016613006591797\n2.5004522800445557\n2.4992990493774414\n2.498197317123413\n2.497144937515259\n2.496137857437134\n2.495173692703247\n2.4942495822906494\n2.493363380432129\n2.4925124645233154\n2.4916954040527344\n2.4909095764160156\n2.4901540279388428\n2.4894261360168457\n2.488725185394287\n2.4880495071411133\n2.4873974323272705\n2.4867680072784424\n2.4861605167388916\n2.4855728149414062\n2.4850049018859863\n2.484455108642578\n2.4839231967926025\n2.483408212661743\n2.4829084873199463\n2.482424020767212\n2.481955051422119\n2.481499195098877\n2.4810571670532227\n2.4806275367736816\n2.480210304260254\n2.479804754257202\n2.479410171508789\n2.4790265560150146\n2.4786536693573\n2.478290557861328\n2.4779367446899414\n2.477592706680298\n2.477257251739502\n2.4769301414489746\n2.476611852645874\n2.4763011932373047\n2.4759981632232666\n2.4757025241851807\n2.475414276123047\n2.475132703781128\n2.474858045578003\n2.4745893478393555\n2.474327802658081\n2.474071741104126\n2.4738216400146484\n2.4735770225524902\n2.4733383655548096\n2.47310471534729\n2.47287654876709\n"},"children":[],"key":"OwkmVU1WVZ"}],"key":"kDeGeOu0vv"}],"key":"jLHRIq9a80"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome! What we least expect is that our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"liAC1f33Ds"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mptflGf6rk"}],"key":"NoyEZuTvEk"},{"type":"text","value":", by using such gradient-based optimization, becomes as small as the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TvOfd0oZPA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zcbnkBbVXo"}],"key":"dvGJQNEzTi"},{"type":"text","value":" we got by our more primitive bigram-count-matrix way that we previously employed for optimizing. So, basically, before, we achieved roughly the same ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t0TM39yz6c"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j5guSuX89J"}],"key":"qXAGVaE64g"},{"type":"text","value":" just by counting, whereas now we used gradient descent. It just happens that the explicit, counting approach nicely optimizes the model without the need for any gradient-based optimization because the setup for bigram language models is so straightforward and simple that we can afford to just directly estimate the probabilities and keep them in a table. However, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MfB9yviKZI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sGSKS4dPZ2"}],"key":"z0d8788G7y"},{"type":"text","value":" approach is much more flexible and scalable! And we have actually gained a lot. What we can do from hereon is expand and complexify our approach. Meaning, that instead of just taking a single character and predicting the next one in an extremely simple ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N40Hn1K5vg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O9HSTSH2xM"}],"key":"fUxRUPSJO8"},{"type":"text","value":", as we have done so far, we will be taking multiple previous characters and we will be feeding them into increasingly more complex ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xjAgzaeiiF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YqbCQ3Jc6w"}],"key":"OHgi8FH6ck"},{"type":"text","value":"s. But, fundamentally, we will still be just calculating logits that will be going through exactly the same transformation by passing them through a softmax and doing the same gradient-based optimization process we just did. But before we do that, remember the smoothing we did by adding fake counts to our bigram count matrix? Turns out, we can do equivalent smoothing in our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NF9TSySHaA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NfEgLaVVK6"}],"key":"nY4VnRDrvQ"},{"type":"text","value":" too! In particular, just incentivizing the weights to be zero for example leads to the probabilities being uniform, which is a form of smoothing. Such incentivization can be accomplished through regularization. It involves just adding a term like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XgVPUjmb83"}],"key":"ge5tKrwCGK"}],"key":"QBn9aHNJEm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(W**2).mean()","key":"ozIrnpnCZX"},{"type":"outputs","id":"dwL5szk8vTMvg6qOYo5Bd","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":64,"metadata":{},"data":{"text/plain":{"content":"tensor(1.6880, grad_fn=\u003cMeanBackward0\u003e)","content_type":"text/plain"}}},"children":[],"key":"DxXteyQ5wK"}],"key":"rqyHpfYAkP"}],"key":"Sffka4DTkN"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x0rBsyChLB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kn4xc8zq55"}],"key":"AVheCjbfv5"},{"type":"text","value":" as such:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YkrIXXPz51"}],"key":"rQsYKSenxg"},{"type":"code","lang":"python","value":"loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"key":"rFUe36s7Ef"}],"key":"FvtENKpU7T"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"where ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"olPHVMX36w"},{"type":"inlineCode","value":"0.01","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zr4W0yKhdi"},{"type":"text","value":" represents the strength of the regulatization term. Optimizing with this term included in the loss would smoothen the model. Yay! Lastly, let’s sample from our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D948OQZcIq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OVW65vaNVb"}],"key":"rJMkCiEuFS"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H0lvDrjSTH"}],"key":"Nx5gTl0H42"}],"key":"HxnH77qSOC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"g = torch.Generator().manual_seed(SEED)\nfor i in range(20):\n    out = []\n    ix = 0\n    while True:\n        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n        logits = xenc @ W  # predict log-counts\n        counts = logits.exp()  # counts, equivalent to N\n        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n        # sample from probabilities distribution\n        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[ix])\n        if ix == 0:\n            break\n    print(''.join(out))","key":"moWC0aPFNM"},{"type":"outputs","id":"-BpYlI7peSKlN4thlpO4L","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"junide.\njanasah.\np.\ncfay.\na.\nnn.\nkohin.\ntolian.\njuwe.\nkilanaauranilevias.\ndedainrwieta.\nssonielylarte.\nfaveumerifontume.\nphynslenaruani.\ncore.\nyaenon.\nka.\njabi.\nwerimikimaynin.\nanaasn.\n"},"children":[],"key":"rjG8y0nl5H"}],"key":"f7GnREDqJA"}],"key":"U5rWEA30zP"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We are getting kind of the same results as we previously did with our counting method! Not unpredictable at all, since our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GZYaZPmD3U"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NsYjCR4Rfn"}],"key":"gXUdjCWvWr"},{"type":"text","value":" values are close enough. If we trained our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cDzEjl5CFa"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qs0trlX5B1"}],"key":"jo0ee7GrP3"},{"type":"text","value":" more and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FbLnFGAHHv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IxvowBEXOG"}],"key":"XSb0JkT5Wc"},{"type":"text","value":" values became the same, it would means that the two models are identical. Meaning that given the same inputs, they would spit out the same outputs.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"whEtLG6qx1"}],"key":"mKkSIbQqAM"}],"key":"Hkby9iVCoO"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Summary","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ools4vP66h"}],"identifier":"summary","label":"Summary","html_id":"summary","implicit":true,"key":"VxlBeMXbyu"}],"key":"VMotik9hXS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"All in all, we have actually covered lots of ground. To sum up, we introduced the bigram character language model, we saw how we can train the model, how we can sample from the model and how we can evaluate the quality of the model using the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hxbuz33OBA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N6mHRG7kLj"}],"key":"GO0QM7kdMG"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CTcQNrqfXh"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eEssPNoeiu"}],"key":"iAlg8ScQj3"},{"type":"text","value":". We actually trained the model in two completely different ways that actually give or can give (with adequate training) the same result. In the first way, we just counted up the frequency of all the bigrams and normalized. Whereas, in the second way, we used the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DC7MlO87Xq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nll","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UmZ4r1fhEu"}],"key":"f7mdkKrvXS"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CVEA8hLmQs"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"evTLz67jbk"}],"key":"KQZIugSrOW"},{"type":"text","value":" as a guide to optimizing the counts matrix or the counts array, so that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qD4ivmHZas"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZvqZucEpSO"}],"key":"uzegGihTJt"},{"type":"text","value":" is minimized in a gradient-based framework. Despite our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lXywsF40AD"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VwVMplWLpV"}],"key":"YBQoMeJBxc"},{"type":"text","value":" being super simple (single linear layer), it is the more flexible approach.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ScWiPgVX2G"}],"key":"xklFKQHTcM"}],"key":"tGAi4SGEQC"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Outro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ifsdsbgEGe"}],"identifier":"outro","label":"Outro","html_id":"outro","implicit":true,"key":"lLx3jpHAjL"}],"key":"oS9wq8iqfG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In the follow-up lessons, we are going to complexify by taking more and more of these characters and we are going to be feeding them into a new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D7Fn1jaYc8"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d3N8YnIBs0"}],"key":"Mo4mD8fqBG"},{"type":"text","value":" that does more exciting stuff. Buckle up!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RMprF8VPBm"}],"key":"hNBfHdW3qZ"}],"key":"c0dmOTN3TS"}],"key":"U9nGLfom1U"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"1. micrograd: implementing an autograd engine","url":"/micrograduate/micrograd","group":"microgra∇uate"},"next":{"title":"3. makemore (part 2): mlp","url":"/micrograduate/makemore2","group":"microgra∇uate"}}},"domain":"http://localhost:3000"},"project":{"title":"microgra∇uate","github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","toc":[{"file":"index.md"},{"file":"micrograduate/micrograd.ipynb"},{"file":"micrograduate/makemore1.ipynb"},{"file":"micrograduate/makemore2.ipynb"},{"file":"micrograduate/makemore3.ipynb"},{"file":"micrograduate/makemore4.ipynb"},{"file":"micrograduate/makemore5.ipynb"},{"file":"micrograduate/picogpt.ipynb"}],"thumbnail":"/build/heading-2d149a320da7264b9eda93edc721b9e5.png","exports":[],"bibliography":[],"index":"index","pages":[{"slug":"micrograduate.micrograd","title":"1. micrograd: implementing an autograd engine","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore1","title":"2. makemore (part 1): implementing a bigram character-level language model","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore2","title":"3. makemore (part 2): mlp","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore3","title":"4. makemore (part 3): activations \u0026 gradients, batchnorm","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore4","title":"5. makemore (part 4): becoming a backprop ninja","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.makemore5","title":"6. makemore (part 5): building a WaveNet","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"micrograduate.picogpt","title":"7. picoGPT: implementing a tiny GPT from scratch","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-7B0967AD.js";
import * as route0 from "/build/root-EDJFWIEV.js";
import * as route1 from "/build/routes/$-AD65NCUT.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/build/entry.client-PCJPW7TK.js");</script></body></html>