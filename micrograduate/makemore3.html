
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. makemore (part 3): activations &amp; gradients, batchnorm &#8212; microgra∇uate</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'micrograduate/makemore3';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. makemore (part 4): becoming a backprop ninja" href="makemore4.html" />
    <link rel="prev" title="3. makemore (part 2): mlp" href="makemore2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="microgra∇uate - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="microgra∇uate - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    microgra∇uate   
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="micrograd.html">1. <strong>micrograd</strong>: implementing an autograd engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore1.html">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore2.html">3. <strong>makemore</strong> (part 2): mlp</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore4.html">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore5.html">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="picogpt.html">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate/issues/new?title=Issue%20on%20page%20%2Fmicrograduate/makemore3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/micrograduate/makemore3.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>4. makemore (part 3): activations & gradients, batchnorm</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rebuilding-mlp">Rebuilding <strong>mlp</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-bad-weights">Dealing with bad weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-dead-neurons">Dealing with dead neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-to-set-the-factors">Learning to set the factors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batchnorm">Batchnorm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">ResNet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-summary">Interim summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchification"><strong>torch</strong>ification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-activations-and-gradients">Visualizing activations and gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-visualization">Further visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="makemore-part-3-activations-gradients-batchnorm">
<h1>4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm<a class="headerlink" href="#makemore-part-3-activations-gradients-batchnorm" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cloning repo...&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>--quiet<span class="w"> </span>https://github.com/ckarageorgkaneen/micrograduate.git<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
    <span class="o">%</span><span class="k">cd</span> micrograduate
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Installing requirements...&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>uv
    <span class="o">!</span>uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--system<span class="w"> </span>--quiet<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</div>
</div>
<section id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Link to this heading">#</a></h2>
<p>Here, we will continue our implementation of <strong>makemore</strong>. In the previous lesson, we implemented an character-level language model using a <strong>mlp</strong> along the lines of <a class="reference external" href="https://dl.acm.org/doi/10.5555/944919.944966">Bengio et al. 2003</a>. The model took as inputs a few past characters and predicted the next character in the sequence. What we would like to do is move on to more complex and larger <strong>nn</strong>s, like</p>
<ul class="simple">
<li><p>RNN, following <a class="reference external" href="https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Mikolov et al. 2010</a></p></li>
<li><p>LSTM, following <a class="reference external" href="https://arxiv.org/abs/1308.0850">Graves et al. 2014</a></p></li>
<li><p>GRU, following <a class="reference external" href="https://arxiv.org/abs/1409.1259">Kyunghyun Cho et al. 2014</a></p></li>
<li><p>CNN, following <a class="reference external" href="https://arxiv.org/abs/1609.03499">Oord et al., 2016</a></p></li>
<li><p>Transformer, following <a class="reference external" href="https://arxiv.org/abs/1706.03762">Vaswani et al. 2017</a></p></li>
</ul>
<p>But before we do so, let’s stick around at the level of the <strong>mlp</strong> for a little longer in order to develop an intuitive understanding of the activations during training, and especially the gradients flowing backwards: how they behave and how they look like. This is important for understanding the history of the development of newer architectures. Because, RNNs, as we’ll see, for example, although they are very expressive, are universal function approximators and can in principle implement all algorithms, we will see that they are not that easily optimizable with the first-order gradient-based techniques that we have available to us and that we use all the time. The key to understanding why they are not easily optimizable, is to understand the activations and the gradients and how they behave during training. What we’ll also see is that a lot of variants since RNNs, have tried to improve upon this situation. And so, that’s the path that we have to take.</p>
</section>
<section id="rebuilding-mlp">
<h2>Rebuilding <strong>mlp</strong><a class="headerlink" href="#rebuilding-mlp" title="Link to this heading">#</a></h2>
<p>So, let’s get started by first building on the code from the previous lesson.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> inline
<span class="k">else</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> ipympl
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2147483647</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># read in all the words</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;names.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="n">words</span><span class="p">[:</span><span class="mi">8</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;, &#39;charlotte&#39;, &#39;mia&#39;, &#39;amelia&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32033
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build the vocabulary of characters and mappings to/from integers</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
<span class="n">ctoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ctoi</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">itoc</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ctoi</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">itoc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">itoc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}
27
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>


<span class="k">def</span><span class="w"> </span><span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">:</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
            <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>
<span class="n">xval</span><span class="p">,</span> <span class="n">yval</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>
<span class="n">xtest</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">w1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w1_factor</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b1_factor</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w2_factor</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b2_factor</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">redefine_params</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">parameters</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">redefine_params</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
        <span class="n">bix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,))</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bix</span><span class="p">]</span>
        <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
        <span class="n">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">lossi</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># this decorator disables gradient tracking</span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/ 200000: 26.9154
  10000/ 200000: 3.1635
  20000/ 200000: 2.6757
  30000/ 200000: 2.0344
  40000/ 200000: 2.6388
  50000/ 200000: 2.0378
  60000/ 200000: 2.8707
  70000/ 200000: 2.1878
  80000/ 200000: 2.1205
  90000/ 200000: 2.1872
 100000/ 200000: 2.6178
 110000/ 200000: 2.4887
 120000/ 200000: 1.6539
 130000/ 200000: 2.2179
 140000/ 200000: 2.2891
 150000/ 200000: 1.9963
 160000/ 200000: 1.7527
 170000/ 200000: 1.7564
 180000/ 200000: 2.2431
 190000/ 200000: 2.2670
train 2.13726806640625
val 2.1725592613220215
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f8a75ea0c50&gt;]
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2f462738d693496997f9881e754a78b7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sample_from_model</span><span class="p">():</span>
    <span class="c1"># sample from the model</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>  <span class="c1"># initialize with all ...</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">])]</span>  <span class="c1"># (1,block_size,d)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_from_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>eria.
kayanniee.
med.
ryah.
rethrus.
jernee.
aderedielin.
shi.
jen.
eden.
estana.
selyn.
malyan.
nyshabergiagriel.
kinleeney.
panthuon.
ubz.
geder.
yarue.
elsy.
</pre></div>
</div>
</div>
</div>
<p>So that’s our starting point. Awesome!</p>
</section>
<section id="dealing-with-bad-weights">
<h2>Dealing with bad weights<a class="headerlink" href="#dealing-with-bad-weights" title="Link to this heading">#</a></h2>
<p>Now, the first thing to scrutinize is the initialization. An experienced person would tell you that our network is very improperly configured at initialization and there are multiple things wrong with it. Let’s start with the first one. If you notice the <strong>loss</strong> at iteration <code class="docutils literal notranslate"><span class="pre">0/200000</span></code>, it is rather high. This rapidly comes down to <span class="math notranslate nohighlight">\(2\)</span> or so in the following training iterations. But you can tell that initialization is all messed up just by an initial <strong>loss</strong> that is way too high. In the training of <strong>nn</strong>s, it is almost always the case that you’ll have a rough idea of what <strong>loss</strong> to expect at initialization. And that just depends on the <strong>loss</strong> function and the problem setup. In our case, we expect a number <strong>much lower</strong> than what we get. Let’s calculate it together. Basically, there’s <span class="math notranslate nohighlight">\(27\)</span> characters that can come next for any one training example. At initialization, we have no reason to believe that any characters to be much more likely than others. So, we’d expect that the probability distribution that comes out initially is a uniform distribution, assigning about-equal probability to all the <span class="math notranslate nohighlight">\(27\)</span> characters. This means that what we’d like the ideal probability we should record for any character coming next to be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ideal_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">27</span><span class="p">)</span>
<span class="n">ideal_p</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.03703703731298447
</pre></div>
</div>
</div>
</div>
<p>And then the <strong>loss</strong> we would expect is the negative log probability:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">expected_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ideal_p</span><span class="p">)</span>
<span class="n">expected_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.295836925506592
</pre></div>
</div>
</div>
</div>
<p>So what’s happening right now is that at initialization the network is creating probability distributions that are all messed up. Some characters are very confident and some characters are very not-confident. Basically, the network is very confidently wrong and that’s what makes it record a very high <strong>loss</strong>. For simplicity, let’s see a smaller, <span class="math notranslate nohighlight">\(4\)</span>-dimensional example of the issue, by assuming we only have <span class="math notranslate nohighlight">\(4\)</span> characters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">probs</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_4d</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.2500, 0.2500, 0.2500, 0.2500]), 1.3862943649291992)
</pre></div>
</div>
</div>
</div>
<p>Suppose we have logits that come out of an <strong>nn</strong> that are all <span class="math notranslate nohighlight">\(0\)</span>. Then, when we calculate the softmax of these logits and get probabilities that are a diffused distribution that sums to <span class="math notranslate nohighlight">\(1\)</span> and is exactly uniform. Whereas, the <strong>loss</strong> we get is the <strong>loss</strong> we would expect for a <span class="math notranslate nohighlight">\(4\)</span>-dimensional example with a uniform probability distribution. And so it doesn’t matter whether the index is <span class="math notranslate nohighlight">\(0\)</span>, <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span> or <span class="math notranslate nohighlight">\(3\)</span>. We’ll see of course that as we start to manipulate these logits, the loss changes. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="n">index</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.0066, 0.0066, 0.9802, 0.0066]), 0.020012274384498596)
</pre></div>
</div>
</div>
</div>
<p>Yields a very low <strong>loss</strong> since we are assigning the correct probability at initialization to the correct (3rd) label. Much more likely it is that some other dimension will have a high logit, e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="n">index</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.0066, 0.9802, 0.0066, 0.0066]), 5.020012378692627)
</pre></div>
</div>
</div>
</div>
<p>and then what happens is we start to record a much higher <strong>loss</strong>. So, what of course can happens is that the logits might take on extreme values and come out like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]),</span> <span class="n">index</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([3.1741e-04, 9.4620e-01, 6.3754e-03, 4.7108e-02]), 5.055302619934082)
</pre></div>
</div>
</div>
</div>
<p>which also leads to a very high <strong>loss</strong>. For example, if logits are be relatively close to <span class="math notranslate nohighlight">\(0\)</span>, the loss is not too big. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">randn_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">randn_logits</span><span class="p">)</span>
<span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">randn_logits</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.6490,  0.7479, -0.3871, -0.5356])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.3617, 0.3993, 0.1284, 0.1106]), 2.0529625415802)
</pre></div>
</div>
</div>
</div>
<p>However, if they are larger, it’s very unlikely that you are going to be guessing the correct bucket and so you’d be confidently wrong and usually record a very high <strong>loss</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">big_randn_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="nb">print</span><span class="p">(</span><span class="n">big_randn_logits</span><span class="p">)</span>
<span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">big_randn_logits</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([  9.4665,   7.1429,   2.0826, -18.9976])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([9.1030e-01, 8.9138e-02, 5.6545e-04, 3.9573e-13]), 28.55805015563965)
</pre></div>
</div>
</div>
</div>
<p>For even more extreme logits, you might get extreme loss values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">huge_randn_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="n">huge_randn_logits</span><span class="p">)</span>
<span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">huge_randn_logits</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([   8.3889, -112.5325,  -85.9192, -154.8166])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([1.0000e+00, 0.0000e+00, 1.1028e-41, 0.0000e+00]), inf)
</pre></div>
</div>
</div>
</div>
<p>Basically, such logits are not good and we want the logits to be roughly <span class="math notranslate nohighlight">\(0\)</span> when the network is initialized. In fact, the logits don’t need to be zero, they just have to be equal, e.g.:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_4d</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]),</span> <span class="n">index</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.2500, 0.2500, 0.2500, 0.2500]), 1.3862943649291992)
</pre></div>
</div>
</div>
</div>
<p>Because of the normalization inside softmax, this will actually come out ok. But, for symmetry, we don’t want it to be any arbitrary positive or negative number, just zero. So let’s now concretely see where things go wrong in our initial example. First, let’s reinitialize our network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
</pre></div>
</div>
</div>
</div>
<p>Then let’s train it only for one step:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/      1: 29.0502
</pre></div>
</div>
</div>
</div>
<p>If we print the logits, we’ll see that they take on quite extreme values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 1.8587e+01, -8.8322e+00, -4.3481e+00,  9.0512e+00,  2.1609e+00,
         5.8525e+00, -1.5959e+01,  1.7691e-01,  1.4306e+01,  5.5933e+00,
        -1.0145e+01, -2.8939e+00, -2.0576e+01,  1.0257e+01,  1.2336e+01,
        -1.0782e+01, -3.0768e+01, -4.1870e+00, -1.3147e-02,  2.1114e+01,
        -6.2802e+00, -5.8485e+00, -9.8297e-01,  2.2049e+01, -4.3106e+00,
         1.4430e+01, -6.5003e+00], grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>which is what is creating the fake confidence and why the <strong>loss</strong> is so high. Let’s now try to scale down the values of the some of our parameters (e.g. <code class="docutils literal notranslate"><span class="pre">w2</span></code>) and retrain for a step:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/      1: 4.3367
</pre></div>
</div>
</div>
</div>
<p>Aha! The loss is lower, which makes sense. Let’s try a smaller factor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/      1: 3.2853
</pre></div>
</div>
</div>
</div>
<p>The loss decreases further. Alright, so we’re getting closer and closer… So, you might ask, why not just initialize the weights to <span class="math notranslate nohighlight">\(0.0\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/      1: 3.2958
</pre></div>
</div>
</div>
</div>
<p>Besides, it does yield an acceptable initial <strong>loss</strong> value. Well, you don’t want to be setting the parameters of a <strong>nn</strong> exactly to <span class="math notranslate nohighlight">\(0\)</span>. You usually want it to be small numbers instead of exactly <span class="math notranslate nohighlight">\(0\)</span>. Let’s see soon where things might go wrong if we set the initial parameters to <span class="math notranslate nohighlight">\(0\)</span>. For now, let’s just consider the <span class="math notranslate nohighlight">\(0.01\)</span> factor, which yields a small-enough initial <strong>loss</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/      1: 3.3213
</pre></div>
</div>
</div>
</div>
<p>The logits are now coming out as closer to <span class="math notranslate nohighlight">\(0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0719,  0.0493, -0.2910,  0.0210,  0.2192,  0.0624,  0.2226,  0.2487,
         0.1420,  0.1322,  0.0790, -0.0102, -0.0382,  0.1264,  0.0133, -0.0155,
         0.0955, -0.1007,  0.0885,  0.0645,  0.0264,  0.1433,  0.0642, -0.1751,
        -0.0414, -0.1055, -0.1209], grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Cool. So, let’s now train the network completely, and see what losses we get.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/ 200000: 3.2738
  10000/ 200000: 2.2549
  20000/ 200000: 2.1941
  30000/ 200000: 2.0484
  40000/ 200000: 2.1231
  50000/ 200000: 2.3176
  60000/ 200000: 1.9067
  70000/ 200000: 2.3038
  80000/ 200000: 2.2954
  90000/ 200000: 2.3491
 100000/ 200000: 2.4612
 110000/ 200000: 1.8579
 120000/ 200000: 1.8499
 130000/ 200000: 2.0158
 140000/ 200000: 2.2136
 150000/ 200000: 1.8362
 160000/ 200000: 1.7483
 170000/ 200000: 1.9169
 180000/ 200000: 2.3354
 190000/ 200000: 2.1250
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f8a77193950&gt;]
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "25e7d03ead9b4bd28ce332c5de803f3a", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.068324327468872
val 2.128187656402588
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(2.1282)
</pre></div>
</div>
</div>
</div>
<p>The <strong>loss</strong> gets smaller after the first step. Now, notice that our <strong>loss</strong> plot does not have the previous <strong>loss</strong> plot’s hockey stick appearance. The reason is that that shape came from the optimization process basically squashing down the weights to a much smaller range than the initial one. But, now since we’ve already initialized the weights with small values, no such significant shrinking takes places, and thus no big <strong>loss</strong> drop happens between the first couple training steps. Therefore, we are not getting any easy gains, as we previously did in the beginning, but only just the hard gains from training. One important point to keep in mind is that the training and validation <strong>losses</strong> are now a bit better, since training now goes on for a bit longer, since the first epochs are no longer spent for squashing the parameters.</p>
</section>
<section id="dealing-with-dead-neurons">
<h2>Dealing with dead neurons<a class="headerlink" href="#dealing-with-dead-neurons" title="Link to this heading">#</a></h2>
<p>Now, time to deal with a second problem. Although our <strong>loss</strong> after initializing with smaller weights is low:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/      1: 3.3179
</pre></div>
</div>
</div>
</div>
<p>the activation variable contains many <span class="math notranslate nohighlight">\(1.0\)</span> and <span class="math notranslate nohighlight">\(-1.0\)</span> values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],
        [-1.0000,  0.4796, -0.9999,  ..., -0.9951,  0.9976,  1.0000],
        [-0.9999, -0.1726, -1.0000,  ..., -0.9927,  1.0000,  1.0000],
        ...,
        [ 0.0830, -0.9999,  0.9990,  ..., -0.7998,  0.9251,  1.0000],
        [-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],
        [-1.0000, -0.9665, -1.0000,  ..., -0.9994,  0.9962,  0.9919]],
       grad_fn=&lt;TanhBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Now, <code class="docutils literal notranslate"><span class="pre">h</span></code> is the result of the <span class="math notranslate nohighlight">\(tanh\)</span> activation function which is basically a squashing function that maps values within the <span class="math notranslate nohighlight">\([-1.0, 1.0]\)</span> range. To get an idea of the distribution of the values of <code class="docutils literal notranslate"><span class="pre">h</span></code>, let’s look at its histogram.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6b673d5ac3624cd0bdd0c980d7a72be2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>We clearly see that most of the values of <code class="docutils literal notranslate"><span class="pre">h</span></code> are either <span class="math notranslate nohighlight">\(-1.0\)</span> or <span class="math notranslate nohighlight">\(1.0\)</span>. So, this <span class="math notranslate nohighlight">\(tanh\)</span> is very very active. We can look at why that is by plotting the pre-activations that feed into the <span class="math notranslate nohighlight">\(tanh\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hpreact</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2ddab9572ce044578af8473046a73470", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>And we can see that the distribution of the preactivations is very very broad, with numbers between <span class="math notranslate nohighlight">\(-20\)</span> and around <span class="math notranslate nohighlight">\(20\)</span>. That is why in the <span class="math notranslate nohighlight">\(tanh\)</span> output values of <span class="math notranslate nohighlight">\(h\)</span>, everything is being squashed and capped to be in the <span class="math notranslate nohighlight">\([-1.0, 1.0]\)</span> range, with many extreme <span class="math notranslate nohighlight">\(-1.0\)</span> and <span class="math notranslate nohighlight">\(1.0\)</span> values. If you are new to <strong>nn</strong>s, you might not see this as an issue. But if you’re well-versed in the dark arts of <strong>backprop</strong> and have an intuitive sense of how these gradients flow through a <strong>nn</strong>, you are looking at how the <span class="math notranslate nohighlight">\(tanh\)</span> values are distributed and you are sweating! Either case, let’s see why this is an issue. First and foremost, we have to keep in mind that during <strong>backprop</strong>, we do a backward pass by starting at the <strong>loss</strong> and flowing through the network backwards. In particular, we get to a point where we <strong>backprop</strong> through the <span class="math notranslate nohighlight">\(tanh\)</span> function. If you scroll up to the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function, you’ll see that the layer we first <strong>backprop</strong> through is the hidden <strong>nn</strong> layer (with parameters <code class="docutils literal notranslate"><span class="pre">w2</span></code>, <code class="docutils literal notranslate"><span class="pre">b2</span></code>), with <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code> number of neurons, that implements an element-wise <span class="math notranslate nohighlight">\(tanh\)</span> non-linearity. Now, let’s look at what happens in <span class="math notranslate nohighlight">\(tanh\)</span> in the backward pass. We can actually go back to our very first <a class="reference internal" href="#1.-micrograd"><span class="xref myst"><strong>micrograd</strong></span></a> implementation, in the first notebook and see how we implement <span class="math notranslate nohighlight">\(tanh\)</span>. This is how the gradient of <span class="math notranslate nohighlight">\(tanh\)</span> is calculated: <span class="math notranslate nohighlight">\((1 - t^2) \cdot \dfrac{\partial L}{\partial out}\)</span>.  If the value of <span class="math notranslate nohighlight">\(t\)</span>, the output of <span class="math notranslate nohighlight">\(tanh\)</span> is <span class="math notranslate nohighlight">\(0\)</span>, then the <span class="math notranslate nohighlight">\(tanh\)</span> neuron is basically <em>inactive</em> and the gradient of the previous layer just passes through. Whereas, if <span class="math notranslate nohighlight">\(t\)</span> is <span class="math notranslate nohighlight">\(-1\)</span> or <span class="math notranslate nohighlight">\(+1\)</span>, then the gradient becomes <span class="math notranslate nohighlight">\(0\)</span>. This means that if most of the <code class="docutils literal notranslate"><span class="pre">h</span></code> values (outputs of <span class="math notranslate nohighlight">\(tanh\)</span>) are close to the flat <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(+1\)</span> regions of the <span class="math notranslate nohighlight">\(tanh\)</span> output value range, then the gradients that are flowing through the network are getting destroyed at this layer: an unwanted side-effect. Let’s further investigate the amount of <code class="docutils literal notranslate"><span class="pre">h</span></code> activation values that are concentrated at the flat regions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ac84eb5cfce4435399de6c8e27c3dfcc", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>What we see in this data display are each one of the <span class="math notranslate nohighlight">\(200\)</span> neurons (columns) per each of the <span class="math notranslate nohighlight">\(32\)</span> examples/batches (rows). A white pixel represents a neuron whose output is in the flat <span class="math notranslate nohighlight">\(tanh\)</span> region: either <span class="math notranslate nohighlight">\(-1\)</span> or <span class="math notranslate nohighlight">\(+1\)</span>. Whereas, a black pixel represents a neuron whose output is in-between those flat region values. In other words, the white neurons are all the maximum-valued neurons that block the flow of gradients during <strong>backprop</strong>. Of course, we would be in grave trouble if for all of these <span class="math notranslate nohighlight">\(200\)</span> neurons in each column (across all batches) were white. Because in that case we would have what we call a <em>dead neuron</em>. This would be a case wherein the initialization of weights and biases is such that no single example (batch) ever activates a neuron in the active region of the <span class="math notranslate nohighlight">\(tanh\)</span> value range, in between the flat, saturated regions. Since our display does not contain any column of all-whites, for each neuron of our <strong>nn</strong>, there are least one or a couple of neurons that activate in the active region, meaning that some gradients will flow through and each neuron will learn. Nevertheless, cases of <em>dead neurons</em> are possible and the way this manifests (e.g. for <span class="math notranslate nohighlight">\(tanh\)</span> neurons) is that no matter what inputs you plug in from your dataset, these <em>dead neurons</em> only fire either completely <span class="math notranslate nohighlight">\(+1\)</span> or completely <span class="math notranslate nohighlight">\(-1\)</span> and then these neurons will just not learn, because all the gradients will be zeroed out. These scenarios are not only true for <span class="math notranslate nohighlight">\(tanh\)</span>, but for many other non-linearities that people use in <strong>nn</strong>s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;activations.png&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3538df578a3fec7a6507216682a16a00170e30ce4fa16ec02ce432e04dad682f.png" src="../_images/3538df578a3fec7a6507216682a16a00170e30ce4fa16ec02ce432e04dad682f.png" />
</div>
</div>
<p>For example, the <span class="math notranslate nohighlight">\(sigmoid\)</span> activation function will have the exact same issues, as it is a similar squashing function. Now, consider <span class="math notranslate nohighlight">\(ReLU\)</span>, which has a completely flat region for negative input values. So, if you have a <span class="math notranslate nohighlight">\(ReLU\)</span> neuron, it is a pass-through if it is positive and if the pre-activation value is negative, it will just shut it off, giving an output value of <span class="math notranslate nohighlight">\(0\)</span>. Therefore, if a neuron with a <span class="math notranslate nohighlight">\(ReLU\)</span> non-linearity never activates, so for any inputs you feed it from the dataset it never turns on and remains always in its flat region, then this <span class="math notranslate nohighlight">\(ReLU\)</span> neuron is considered a <em>dead neuron</em>: its weights and bias will never receive a gradient and will thus never learn, simply because the neuron never activated. And this can sometimes happen at initialization, because the weights and biases just make it so that by chance some neurons are forever dead. But it can also happen during optimization. If you have too high of a learning rate for example, sometimes you have these neurons that get too much of a gradient and get knocked out of the data manifold, resulting in no example ever activating such a neuron. Consequently, one danger with large gradient is knocking off neurons and making them forever dead. Other non-linearities such as <span class="math notranslate nohighlight">\(leaky ReLU\)</span> will not suffer from this issue as much, because of the lack of flat tails, as they’ll almost always yield gradients. But, to return to our <span class="math notranslate nohighlight">\(tanh\)</span> issue, the problem is that our <span class="math notranslate nohighlight">\(tanh\)</span> pre-activation <code class="docutils literal notranslate"><span class="pre">hpreact</span></code> values are too far away from <span class="math notranslate nohighlight">\(0\)</span>, thus yielding a flat region activation distribution that is too saturated at the <code class="docutils literal notranslate"><span class="pre">tanh</span></code> flat regions, which leads to a suppression of learning for many neurons. How do we fix this? One easy way is to decrease the initial value of the <code class="docutils literal notranslate"><span class="pre">w1</span></code> and <code class="docutils literal notranslate"><span class="pre">b1</span></code> parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w1_factor</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.99</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/      1: 3.3174
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "dc6df1fdb2d04150aa45ce47e5e81eff", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now, our activations are not as saturated above 0.99 as they were before, with only a few white neurons. What is more, the activations are now more evenly distributed and the range of pre-activations is now significantly narrower:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hpreact</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9a39a55168e54b7287ab443f4a13fa59", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "5b8ce4511b554762a6b736293c96a9e3", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Since distributions look nicer now, perhaps this is what our initialization should be. Let’s now train a new network with this initialization setting and print the losses:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">w1_factor</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/ 200000: 3.3052
  10000/ 200000: 2.6664
  20000/ 200000: 2.5232
  30000/ 200000: 2.0007
  40000/ 200000: 1.8163
  50000/ 200000: 2.1677
  60000/ 200000: 2.2280
  70000/ 200000: 2.5228
  80000/ 200000: 2.1911
  90000/ 200000: 2.4983
 100000/ 200000: 2.1451
 110000/ 200000: 1.7719
 120000/ 200000: 1.9741
 130000/ 200000: 1.6981
 140000/ 200000: 1.8294
 150000/ 200000: 1.8194
 160000/ 200000: 2.0302
 170000/ 200000: 2.0787
 180000/ 200000: 1.9397
 190000/ 200000: 2.0921
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.0368359088897705
val 2.1042771339416504
</pre></div>
</div>
</div>
</div>
<p>The validation loss continues to improve! This exercise clarifies the effect of good initialization on performance and emphasizes being aware of <strong>nn</strong> internals like activations and gradients. Now, we’re working with a very small network which is basically just a 1-hidden layer <strong>mlp</strong>. Because the network is so shallow, the optimization problem is quite easy and very forgiving. So, even though our initialization in the beginning was terrible, the network still learned eventually. It just got a bit of a worse result. This is not the case in general though. Once we actually start working with much deeper networks that have say 50 layers, things can get much more complicated and these problems stack up, and it is often not surprising to get into a place where a network is basically not training at all, if your initialization is bad enough. Generally, the deeper and more complex a network is, the less forgiving it is to some of the aforementioned errors. But what has worked so far with our simple example is great! However, we have come up with a bunch of <em>magic</em> weight and bias factors (e.g. <code class="docutils literal notranslate"><span class="pre">w1_factor</span></code>). How did we come up with these? And how are we supposed to set these if we have a large <strong>nn</strong> with lots and lots of layers? As you might assume, no one sets these by hand. And there’s rather principled ways of setting these scales that I’d like to introduce to you now.</p>
</section>
<section id="learning-to-set-the-factors">
<h2>Learning to set the factors<a class="headerlink" href="#learning-to-set-the-factors" title="Link to this heading">#</a></h2>
<p>Let’s start with a short snippet, just to begin to motivate this discussion by defining an input tensor of many multi-dimensional examples and a weight tensor of a hidden layer, both drawn from a Gaussian distribution. We’ll calculate the mean and standard deviation of these inputs and the corresponding pre-activations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_x_y_distributions</span><span class="p">(</span><span class="n">n_inputs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">weight_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">)</span>  <span class="c1"># many examples of inputs</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight_factor</span>  <span class="c1"># weights of the hidden layer</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>  <span class="c1"># pre-activations</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_x_y_distributions</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.0020) tensor(1.0074)
tensor(0.0011) tensor(3.2319)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "5975360a86ad458cac35510b0ae38867", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>If you notice, the std of the pre-activations <code class="docutils literal notranslate"><span class="pre">y</span></code> has increased compared to that of <code class="docutils literal notranslate"><span class="pre">x</span></code>, as can also be seen by the widening of the Gaussian. The left Gaussian has basically undergone a stretching operation, resulting in the expanded right plot. We don’t want that. We want most of our <strong>nn</strong> to have relatively similar activations with a relatively uniform Gaussian throughout the <strong>nn</strong>. So the question becomes, how do we scale these weight vectors (e.g. <code class="docutils literal notranslate"><span class="pre">w</span></code>) to preserve the Gaussian distribution of the inputs (e.g. left)? Let’s do some experiments. If we scale <code class="docutils literal notranslate"><span class="pre">w</span></code> by a large number, e.g. <span class="math notranslate nohighlight">\(5\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_x_y_distributions</span><span class="p">(</span><span class="n">weight_factor</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.0091) tensor(0.9923)
tensor(-0.0045) tensor(15.6379)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "31a68d0130ad40e986412ddc5b6ca6ec", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>this Gaussian grows and grows in std, with the outputs in the x-axis taking on more and more extreme values (right plot). But if we scale the weights down, e.g. by <span class="math notranslate nohighlight">\(0.2\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_x_y_distributions</span><span class="p">(</span><span class="n">weight_factor</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-0.0002) tensor(1.0079)
tensor(-0.0013) tensor(0.6582)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7e28a2a6002a4958933032f4ee98a4f4", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>then, conversely, the Gaussian gets smaller and smaller and it’s shrinking. Notice the std of <code class="docutils literal notranslate"><span class="pre">y</span></code> now being smaller than that of <code class="docutils literal notranslate"><span class="pre">x</span></code>. The question then becomes: what is the appropriate factor to exactly preserve the std of the inputs? And it turns out that the correct answer, mathematically, (when you work out the variance of the <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">&#64;</span> <span class="pre">w</span></code> multiplication), is that you are supposed to divide by the square root of the fan-in. Meaning, the square root of the number of inputs. Therefore if the number of inputs is <span class="math notranslate nohighlight">\(10\)</span> then the appropriate factor for preserving the Gaussian distribution of the inputs is <span class="math notranslate nohighlight">\(10^{-1/2}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_x_y_distributions</span><span class="p">(</span><span class="n">n_inputs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">weight_factor</span><span class="o">=</span><span class="mi">10</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.0140) tensor(1.0000)
tensor(0.0028) tensor(1.0127)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "df97daf34e904125be75d0176356288b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now we see that the std remains roughly the same! Now, unsuprisingly, a number of papers have looked into how to best initialize <strong>nn</strong>s and in the case of <strong>mlp</strong>s, we can have these fairly deep networks that have these nonlinearities in between layers and we want to make sure that the activations are well-behaved and they don’t expand to infinity or shrink all the way to zero. And the question is, how do we initialize the weights so that they take on reasonable values throughout the network. In <a class="reference external" href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>, they study convolutional <strong>nn</strong>s (CNNs) and <span class="math notranslate nohighlight">\(ReLU\)</span> and <span class="math notranslate nohighlight">\(PReLU\)</span> non-linearities. But the analysis is very similar to the <span class="math notranslate nohighlight">\(tanh\)</span> non-linearity. As we saw previously, <span class="math notranslate nohighlight">\(ReLU\)</span> is somewhat of a squashing function where all the negative values are simply clamped to <span class="math notranslate nohighlight">\(0\)</span>. Because with <span class="math notranslate nohighlight">\(ReLU\)</span>s half of the distribution is thrown away, they find in their analysis of the forward activations of the <strong>nn</strong>, that you have to compensate for that with a gain. They find that to initialize their weights they have to do it with a zero-mean Gaussian whose std is <span class="math notranslate nohighlight">\(\sqrt{2/n_l}\)</span>. We just did the same, multiplying our weights by <span class="math notranslate nohighlight">\(\sqrt{1/10}\)</span> (the <span class="math notranslate nohighlight">\(2\)</span> has to do with the <span class="math notranslate nohighlight">\(ReLU\)</span> activation function they use). They also study the backward propagation case, finding that the backward pass is also approximately initialized up to a constant factor <span class="math notranslate nohighlight">\(c_2/d_L\)</span> that has to do with the number of hidden neurons in early and late layer. Now, this <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_">Kaiming initialization is also implemented in pytorch</a> and it is probably the most common way of initializing <strong>nn</strong>s now. This PyTorch method takes a mode and nonlinearity argument among others, with the latter determining the gain factor (e.g. <span class="math notranslate nohighlight">\(\sqrt{2}\)</span>). Why do we need a gain? For example, <span class="math notranslate nohighlight">\(ReLU\)</span> is a contractive transformation that squashes the output distribution by taking any negative value and clamping it to zero. <span class="math notranslate nohighlight">\(tanh\)</span> also squashes in some way, as it will squeeze values at the tails of its range. Therefore, in order to fight the squeezing-in of these activation functions, we have to boost the weights a little bit in order to counteract this effect and re-normalize everything back to standard unit deviation. So that’s why there’s a little bit of a gain that comes out. Now we’re actually intentionally skipping through this section quickly. The reason for that is the following. Around 2015, when this paper was written, you had to actually be extremely careful with the activations and the gradients, their ranges, their histograms, the precise setting of gains and the scrutinizing of the non-linearities and so on… So, everything was very finicky and very fragile and everything had to be very properly arranged in order to train a <strong>nn</strong>. But there are a number of modern innovations that made everything significantly more stable and well-behaved. And it has become less important to initialize these networks “exactly right”. Some of those innovations are for example: residual connections (which we will cover in the next notebooks), a number of normalization layers (e.g. batch normalization, layer normalization, group normalization) and of course much better optimizers: not just stochastic gradient descent (the simple optimizer we have been using), but slightly more complex optimizers such as RMSProp and especially Adam. All of these modern innovations make it less important for you to precisely calibrate the initialization of the <strong>nn</strong>. So, what do people do in practice? They usually initialize their weights with Kaiming-normally, like we did. Now notice how the following multiplier ends up being the std of Gaussian distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">multiplier</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.1984076201915741
</pre></div>
</div>
</div>
</div>
<p>But, according to the <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_">kaiming PyTorch docs</a>, we want an std of <span class="math notranslate nohighlight">\(\frac{gain}{\sqrt{fan\_in}}\)</span>. Therefore, for a <span class="math notranslate nohighlight">\(tanh\)</span> nonlinearity:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_embd</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">kaiming_w1_factor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s re-initialize and re-train our <strong>nn</strong> with this initilization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span>
    <span class="n">w1_factor</span><span class="o">=</span><span class="n">kaiming_w1_factor</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span>
<span class="p">)</span>
<span class="n">hpreact</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>11897
      0/ 200000: 3.3202
  10000/ 200000: 2.0289
  20000/ 200000: 2.2857
  30000/ 200000: 1.9499
  40000/ 200000: 1.8151
  50000/ 200000: 2.2310
  60000/ 200000: 2.1923
  70000/ 200000: 1.9824
  80000/ 200000: 2.1736
  90000/ 200000: 2.1285
 100000/ 200000: 2.0951
 110000/ 200000: 1.9936
 120000/ 200000: 1.8634
 130000/ 200000: 2.3813
 140000/ 200000: 1.7260
 150000/ 200000: 1.7054
 160000/ 200000: 1.8842
 170000/ 200000: 2.1857
 180000/ 200000: 1.7953
 190000/ 200000: 1.6770
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.040121078491211
val 2.1033377647399902
</pre></div>
</div>
</div>
</div>
<p>Of course, our <strong>loss</strong> is quite similar to before. The difference now though is that we don’t need to inspect histograms and introduce arbitrary factors. On the contrary, we now have a semi-principled way to initialize our weights that is also scalable to much larger networks which we can use as a guide. However, this precise weight initialization is not as important as we might think nowadays, due to some modern innovations.</p>
</section>
<section id="batchnorm">
<h2>Batchnorm<a class="headerlink" href="#batchnorm" title="Link to this heading">#</a></h2>
<p>Let’s now introduce one of them. Batch Normalization (<strong>batchnorm</strong>) came out in 2015 from a team at Google in <a class="reference external" href="https://arxiv.org/abs/1502.03167">an extremely impactful paper</a>, making it possible to train deep neural networks quite reliably. It basically just worked. Here’s what <strong>batchnorm</strong> does and how it’s implemented. Like we mentioned before, we don’t want the pre-activations (e.g. <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>) to <span class="math notranslate nohighlight">\(tanh\)</span> to be too small, nor too large because then the outputs will turn out either close to <span class="math notranslate nohighlight">\(0\)</span> or saturated. Instead, we want the pre-activations to be roughly Gaussian (with a zero mean and a std of <span class="math notranslate nohighlight">\(1\)</span>), at least at initialization. So, the insight from the <strong>batchnorm</strong> paper is: ok, we have these hidden pre-activation states/values that we’d like to be Gaussian, then why not take them and just normalize them to be Gaussian? Haha. I know, it sounds kinda crazy but you can just do that, because <em>standardizing</em> hidden states so that they become Gaussian is a perfectly differentiable operation. And so the gist of <strong>batchnorm</strong> is that if we want unit Gaussian hidden states in our network, then we can just normalize them to be so. Let’s see how that works. If you scroll up to our definition of the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function we can see the pre-activations <code class="docutils literal notranslate"><span class="pre">hpreact</span></code> before they are fed into the <span class="math notranslate nohighlight">\(tanh\)</span> function. Now, the idea, remember, is we are trying to make these roughly Gaussian. If the values are too small, the <span class="math notranslate nohighlight">\(tanh\)</span> is kind of inactive, whereas if they are very large, the <span class="math notranslate nohighlight">\(tanh\)</span> becomes very saturated and the gradients don’t flow. So, let’s learn how to standardize <code class="docutils literal notranslate"><span class="pre">hpreact</span></code> to be roughly Gaussian.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpreact</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 200])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">hpmean</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 200])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">hpstd</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 200])
</pre></div>
</div>
</div>
</div>
<p>After calculating the mean and std across batches of <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>, which in the paper are referred to the “mini-batch mean” and “mini-batch variance”, respectively, next, following along the paper, we are going to normalize or standardize the inputs (e.g. <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>) by subtracting the mean and dividing by the std. Basically:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpreact</span> <span class="o">=</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hpmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">hpstd</span>
</pre></div>
</div>
</div>
</div>
<p>What normalization does is that now every single neuron and its firing rate will be exactly unit Gaussian for each batch (which is why it’s called <strong>batchnorm</strong>). Now, we could in principle train using normalization. But we would not achieve a very good result. And the reason for that is that we want the pre-activations to be roughly Gaussian, but only at initialization. But we don’t want these to be forced to be Gaussian always. We’d like to allow the <strong>nn</strong> to move the distributions around, such as making them more diffuse, more sharp, perhaps to make some <code class="docutils literal notranslate"><span class="pre">tanh</span></code> neurons to be more trigger-happy or less trigger-happy. So we’d like this distribution to move around and we’d like the <strong>backprop</strong> to tell us how that distribution should move around. So in addition to standardization of any point in the network, we have to also introduce this additional component mentioned in the paper described as “scale and shift”. Basically, what we want to be doing is multiplying the normalized values by a gain <span class="math notranslate nohighlight">\(\gamma\)</span> and then addding a bias <span class="math notranslate nohighlight">\(\beta\)</span> to get a final output of each layer. Let’s define them:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>so that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hpmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">hpstd</span> <span class="o">+</span> <span class="n">bnbias</span>
</pre></div>
</div>
</div>
</div>
<p>Because the gain is initialized to <span class="math notranslate nohighlight">\(1\)</span> and the bias to <span class="math notranslate nohighlight">\(0\)</span>, at initialization, each neuron’s firing values in this batch will be exactly unit Gaussian and we’ll have nice numbers, regardless of what the distribution of the incoming (e.g. <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>) tensors are. That is roughly what we want, at least during initialization. And during optimization, we’ll be able to <strong>backprop</strong> and change the gain and the bias, so the network is given the full ability to do with this whatever it wants internally. In order to train these, we have to make sure to include these in the parameters of the <strong>nn</strong>. To do so, and by effect facilitate <strong>backprop</strong>, let’s update our <code class="docutils literal notranslate"><span class="pre">define_nn</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code> functions accordingly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">w1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w1_factor</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b1_factor</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w2_factor</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b2_factor</span>
    <span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># batchnorm</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">bngain</span>
        <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="o">/</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">bnbias</span>
    <span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>And now, let’s initialize our new <strong>nn</strong> and train!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span>
    <span class="n">w1_factor</span><span class="o">=</span><span class="n">kaiming_w1_factor</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12297
      0/ 200000: 3.3033
  10000/ 200000: 2.4509
  20000/ 200000: 2.2670
  30000/ 200000: 2.2647
  40000/ 200000: 2.2298
  50000/ 200000: 1.9236
  60000/ 200000: 2.1204
  70000/ 200000: 2.3262
  80000/ 200000: 2.1840
  90000/ 200000: 1.9809
 100000/ 200000: 2.2282
 110000/ 200000: 2.0057
 120000/ 200000: 2.0795
 130000/ 200000: 1.8743
 140000/ 200000: 2.2611
 150000/ 200000: 1.4639
 160000/ 200000: 2.2464
 170000/ 200000: 2.2788
 180000/ 200000: 2.3267
 190000/ 200000: 1.7778
train 2.0695457458496094
val 2.1072680950164795
</pre></div>
</div>
</div>
</div>
<p>We get a loss that is comparable to our previous results. Here’s a rough summary of our losses (re-running this notebook might yield slightly different values, but you get the point):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># loss log</span>

<span class="c1"># original:</span>
<span class="n">train</span> <span class="mf">2.127638339996338</span>
<span class="n">val</span> <span class="mf">2.171938180923462</span>

<span class="c1"># fix softmax confidently wrong</span>
<span class="n">train</span> <span class="mf">2.0707266330718994</span>
<span class="n">val</span> <span class="mf">2.1337196826934814</span>

<span class="c1"># fix tanh layer saturated at init</span>
<span class="n">train</span> <span class="mf">2.0373899936676025</span>
<span class="n">val</span> <span class="mf">2.1040639877319336</span>

<span class="c1"># use semi-principled kaiming initialization instead of hacky way:</span>
<span class="n">train</span> <span class="mf">2.038806438446045</span>
<span class="n">val</span> <span class="mf">2.108304977416992</span>

<span class="c1"># add a batchnorm layer</span>
<span class="n">train</span> <span class="mf">2.0688135623931885</span>
<span class="n">val</span> <span class="mf">2.10699462890625</span>
</pre></div>
</div>
<p>However, we should not actually be expecting an improvement in this case. And that’s because we are dealing with a very simple <strong>nn</strong> that has just a single hidden layer. In fact, in this very simple case of just one hidden layer, we were actually able to calculate what the scale of the weights should be to have the activations have a roughly Gaussian shape. So, <strong>batchnorm</strong> is not doing much here. But you might imagine that once you have a much deeper <strong>nn</strong>, that has lots of different types of operations and there’s also, for example, residual connections (which we’ll cover) and so on, it will become very very difficult to tune the scales of the weight matrices such that all the activations throughout the <strong>nn</strong> are roughly Gaussian: at scale, an intractable approach. Therefore, compared to that, it is much much easier to sprinkle <strong>batchnorm</strong> layers throughout the <strong>nn</strong>.  In particular, it’s common to look at every single linear layer like this one <code class="docutils literal notranslate"><span class="pre">hpreact</span> <span class="pre">=</span> <span class="pre">embcat</span> <span class="pre">&#64;</span> <span class="pre">w1</span> <span class="pre">+</span> <span class="pre">b1</span></code> (multiply by a weight matrix and add a bias), or for example convolutions that also perform matrix multiplication, (just in a more “structured” format) and append a <strong>batchnorm</strong> layer right after it to control the scale of these activations at every point in the <strong>nn</strong>. So, we’d be adding such normalization layers throughout the <strong>nn</strong> to control the scale of these activation (again, throughout the <strong>nn</strong>) without requiring us to do perfect mathematics in order to manually control individual activation distributions for any “building block” (e.g. layer) we might want to introduce into our <strong>nn</strong>. So, <strong>batchnorm</strong> significantly stabilizes training and that’s why these layers are quite popular. Beware though, the stability offered by <strong>batchnorm</strong> often comes at a terrible cost. If you think about it, what is happening at a <strong>batchnorm</strong> layer (e.g. <code class="docutils literal notranslate"><span class="pre">hpreact</span> <span class="pre">=</span> <span class="pre">bngain</span> <span class="pre">*</span> <span class="pre">(hpreact</span> <span class="pre">-</span> <span class="pre">hpreact.mean(0,</span> <span class="pre">keepdim=True))</span> <span class="pre">/</span> <span class="pre">hpreact.std(0,</span> <span class="pre">keepdim=True)</span> <span class="pre">+</span> <span class="pre">bnbias</span></code>) is something strange and terrible. Before introducing such a layer, it used to be the case that a single example was fed into the <strong>nn</strong> and then we calculated its activations and its logits in a deterministic manner in which a specific example yields specific logits. Then, for reasons of efficiency of learning, we started to use batches of examples. Those batches of examples were processed independently, but this was just an efficient thing. But now suddenly, with <strong>batchnorm</strong>, because of the normalization through the batch, we are mathematically coupling these examples in the forward pass and the backward pass of the <strong>nn</strong>. So with <strong>batchnorm</strong>, the hidden states (e.g. <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>) and the output states (e.g. logits), are not just a function of the inputs of a specific example, but they’re also a function of all the other examples that happen to come for a ride in that batch. Damn! So what’s happening is, if you see for example the activations <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">=</span> <span class="pre">torch.tanh(hpreact)</span></code>, for every different example/batch, the activations are going to actually change slightly, depending on what other examples there are in the batch. Thus depending on what examples there are, <code class="docutils literal notranslate"><span class="pre">h</span></code> is going to jitter if you sample from many examples, since the statistics of the mean and std are going to be impacted. So, you’ll get a jitter for the <code class="docutils literal notranslate"><span class="pre">h</span></code> and for the <code class="docutils literal notranslate"><span class="pre">logits</span></code> values. And you’d think that this would be a bug or something undesirable, but in a very strange way, this actually turns out to be good in <strong>nn</strong> training as a side effect. The reason for that is that you can think of <strong>barchnorm</strong> as some kind of regularizer. Because what is happening is the you have your input and your <code class="docutils literal notranslate"><span class="pre">h</span></code> and because of the other examples the value of <code class="docutils literal notranslate"><span class="pre">h</span></code> is jittering a bit. What that does is that is effectively padding-out any one of these input examples and it’s introducing a little bit of entropy. And because of the padding-out, the jittering effect is actually kind of like a form of data augmentation, making it harder for the <strong>nn</strong> to overfit for these concrete specific examples. So, by introducing all this noise, it actually pads out the examples and it regularizes the <strong>nn</strong>. And that is the reason why, deceivingly, as a second-order effect, this is acts like a regularizer, making it harder for the us as a community to remove or do without <strong>batchnorm</strong>. Because, basically, no one likes this property that the examples in a batch are coupled mathematically in the forward pass and it leads to all kinds of strange results, bugs and so on. Therefore, people do not like these side effects so many have advocated for deprecating the use of <strong>batchnorm</strong> and move to other normalization techniques that do not couple the examples of a batch. Examples are layer normalization, instance normalization, group normalization, and so on. But basically, long story short, <strong>batchnorm</strong> is the first kind of normalization layer to be introduced, it worked extremely well, it happened to have this regularizing effect, it stabilized training and people have been trying to remove it and move unto the other normalization techniques. But it’s been hard, because it just works quite well. And some of the reason it works quite well is because of this regularizing effect and because it is quite effective at controlling the activations and their distributions. So, that’s the brief story of <strong>batchnorm</strong>. But let’s see one of the other weird outcomes of this coupling. Basically, once we’ve trained a <strong>nn</strong>, we’d like to deploy it in some kind of setting and we’d like to feed in a single individual example and get a prediction out from our <strong>nn</strong>. But how can we do that when our <strong>nn</strong> now with <strong>batchnorm</strong> in a forward pass estimates the statistics of the mean and the std of a batch and not a single example? The <strong>nn</strong> expects batches as an input now. So how do we feed in a single example and get sensible results out? The proposal in the <strong>batchnorm</strong> paper is the following. What we would like to do is implement a step after training that calculates and sets the <strong>batchnorm</strong> mean and std a single time over the training dataset. Basically, calibrate the <strong>batchnorm</strong> statistics at the end of training as such. We are going to get the training dataset and get the pre-activations for every single training example, and then one single time estimate the mean and std over the entire training set: two fixed numbers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># disable gradient calculation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">infer_mean_and_std_over_trainset</span><span class="p">():</span>
    <span class="c1"># pass the entire training set through</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xtrain</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># measure the mean/std over the entire training set</span>
    <span class="n">bnmean_xtrain</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bnstd_xtrain</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bnmean_xtrain</span><span class="p">,</span> <span class="n">bnstd_xtrain</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnmean_xtrain</span><span class="p">,</span> <span class="n">bnstd_xtrain</span> <span class="o">=</span> <span class="n">infer_mean_and_std_over_trainset</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>And so after calculating these values, at test time we are going to clamp them to the <strong>batchnorm</strong> calculation. To do so, let’s extend the <code class="docutils literal notranslate"><span class="pre">forward</span></code> and <code class="docutils literal notranslate"><span class="pre">print_loss</span></code> functions as such:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># batchnorm</span>
    <span class="k">if</span> <span class="n">bnmean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bnstd</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">bnstd</span> <span class="o">+</span> <span class="n">bnbias</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># this decorator disables gradient tracking</span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="n">bnmean</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="n">bnstd</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>Now, if we do an inference with these mean and std values, instead of the batch-specific ones:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="n">bnmean_xtrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="n">bnstd_xtrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;val&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.0695457458496094
val 2.107233762741089
</pre></div>
</div>
</div>
</div>
<p>The losses we get may be more or less the same as our last losses before, but the benefit we have gained is that we can now forward a single example, because now the mean and std are fixed tensors. That said, because everyone is lazy, nobody wants to estimate the mean and std as a second stage after <strong>nn</strong> training. So, the <strong>batchnorm</strong> paper also introduced one more idea: that we can estimate these mean and std values in a running manner during the <strong>nn</strong> training phase. Let’s see what that would look like. First, we’ll define running value variables in the definition of our <strong>nn</strong>. Then we’ll modify the <code class="docutils literal notranslate"><span class="pre">train</span></code> function and calculate the running values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">w1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w1_factor</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b1_factor</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w2_factor</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b2_factor</span>
    <span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnmean_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnstd_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">bnmean_running</span><span class="p">,</span> <span class="n">bnstd_running</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">bnmean_running</span><span class="p">,</span> <span class="n">bnstd_running</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># batchnorm</span>
    <span class="k">if</span> <span class="n">bnmean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bnstd</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">bnstd</span> <span class="o">+</span> <span class="n">bnbias</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># disable gradient calculation</span>
        <span class="n">bnmean_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">bnmean_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">bnmean</span>
        <span class="n">bnstd_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">bnstd_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">bnstd</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">redefine_params</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">parameters</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">redefine_params</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
        <span class="n">bix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,))</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bix</span><span class="p">]</span>
        <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
        <span class="n">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">lossi</span>
</pre></div>
</div>
</div>
</div>
<p>Now if we train, we will be calculating the running values of <code class="docutils literal notranslate"><span class="pre">bnmean</span></code> and <code class="docutils literal notranslate"><span class="pre">bnstd</span></code> without requiring a second step after training. This also happens when using PyTorch <strong>batchnorm</strong> layers: running values are calculated and then are used during inference. Now, let’s re-define our <strong>nn</strong> and train it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnmean_running</span><span class="p">,</span> <span class="n">bnstd_running</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span>
    <span class="n">w1_factor</span><span class="o">=</span><span class="n">kaiming_w1_factor</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.0</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12297
      0/ 200000: 3.3186
  10000/ 200000: 2.5451
  20000/ 200000: 2.1227
  30000/ 200000: 2.3208
  40000/ 200000: 2.3080
  50000/ 200000: 2.2920
  60000/ 200000: 2.1421
  70000/ 200000: 2.2069
  80000/ 200000: 2.3143
  90000/ 200000: 1.6854
 100000/ 200000: 2.1594
 110000/ 200000: 2.1690
 120000/ 200000: 2.2234
 130000/ 200000: 2.2345
 140000/ 200000: 1.8121
 150000/ 200000: 2.3733
 160000/ 200000: 1.7584
 170000/ 200000: 2.7041
 180000/ 200000: 2.0920
 190000/ 200000: 1.8872
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnmean_xtrain</span><span class="p">,</span> <span class="n">bnstd_xtrain</span> <span class="o">=</span> <span class="n">infer_mean_and_std_over_trainset</span><span class="p">()</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="n">bnmean_xtrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="n">bnstd_xtrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.06760311126709
val 2.109143018722534
</pre></div>
</div>
</div>
</div>
<p>If we calculate the mean over the whole training set and compare it with the running mean, we notice they are quite similar:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">sci_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">bnmean_xtrain</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-2.6770, -0.1693, -0.6069,  0.4962,  0.7990,  0.6392,  2.3859, -1.3855,
          1.1074,  1.4398, -1.2298, -2.4216, -0.5599,  0.1668, -0.2828, -0.7317,
          1.0945, -1.9686, -1.1664,  0.6019, -0.1889, -0.8343, -0.6375,  0.6903,
          0.6620,  0.0190,  1.1514, -0.0258,  0.3537,  1.8418,  0.2683, -0.8331,
          0.5717, -0.5169, -0.0296, -1.6526,  0.7317, -0.2649, -0.1199,  0.3907,
         -0.2578, -1.1813, -0.3795,  0.0315,  0.6800, -0.7146,  1.4800, -1.1142,
          1.3800,  1.3333,  1.7228, -0.1832,  1.6916,  0.8562,  1.4595, -2.2924,
         -0.3804,  0.4507,  1.9066, -1.4633, -0.8475,  1.2721,  0.8002,  0.2051,
          2.0326,  1.2502, -1.0770,  1.3650, -0.9620,  0.4069,  0.3910,  0.6054,
          0.0738, -1.3951, -2.4795,  0.1532,  1.1407, -0.4320,  0.6385,  0.3845,
          0.3345,  0.9669,  1.5173,  0.5538,  0.8198, -0.2719, -0.7971, -0.3735,
          2.4336, -0.6536, -1.1261,  0.8431,  0.0744, -0.9863, -1.0063,  0.1658,
          0.4939, -1.2384, -0.7562, -0.8595, -0.2615,  0.1969, -1.7003,  1.0725,
          1.0187,  0.2188, -0.4287, -0.2156,  0.7122, -1.0895,  1.0372,  0.1750,
          0.0708,  1.3315,  2.8986,  1.5759,  1.1428, -0.4351,  0.4545, -0.2242,
         -1.2595, -1.5032,  0.3134,  1.1210, -0.5699, -0.1829,  1.0623, -1.5076,
         -1.3623, -0.6535,  2.5082, -0.4506,  0.7244,  1.3152,  0.9770,  0.9000,
         -0.8565,  1.5871,  0.7384,  0.3593,  1.2161,  0.8446,  1.6187,  0.0483,
          0.3879,  0.9822,  0.3694, -1.1177,  0.0051,  0.5489, -1.0130,  0.4538,
          1.4678,  2.0332,  0.7353, -0.3556,  1.6172, -1.8053, -0.2439,  0.9442,
          0.0504, -0.7963,  0.2883, -2.1548, -0.5377, -0.6621, -0.0440, -0.2134,
         -2.3616, -0.7478,  0.3349, -2.1589,  0.3803, -1.2049, -0.9475,  0.7523,
          1.8645, -0.7137,  1.1013, -1.0613,  1.6492,  1.3798,  0.7756, -0.9489,
         -0.1432, -0.2982, -0.4837,  0.3259,  2.6390,  0.8259,  0.2949,  1.7561,
         -0.7375, -0.1671,  0.7696,  1.0035,  1.2708, -0.7615, -0.1892,  1.1808]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnmean_running</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-2.6439, -0.1617, -0.6136,  0.4968,  0.7971,  0.6411,  2.3763, -1.3774,
          1.1354,  1.4372, -1.2115, -2.3877, -0.5565,  0.1727, -0.2845, -0.7350,
          1.1087, -1.9475, -1.1556,  0.6244, -0.1834, -0.8322, -0.6170,  0.6846,
          0.6547,  0.0314,  1.1314, -0.0138,  0.3604,  1.8411,  0.2521, -0.8311,
          0.5715, -0.5084, -0.0265, -1.6514,  0.7295, -0.2540, -0.1140,  0.4024,
         -0.2471, -1.1745, -0.3745,  0.0327,  0.6927, -0.7181,  1.4660, -1.1102,
          1.3809,  1.3116,  1.7163, -0.1866,  1.6785,  0.8492,  1.4503, -2.3023,
         -0.3868,  0.4478,  1.8868, -1.4559, -0.8400,  1.2690,  0.7847,  0.2059,
          2.0220,  1.2630, -1.0618,  1.3643, -0.9662,  0.3970,  0.3910,  0.5977,
          0.0795, -1.3865, -2.4648,  0.1534,  1.1447, -0.4265,  0.6496,  0.4061,
          0.3352,  0.9816,  1.5135,  0.5539,  0.8205, -0.2807, -0.8094, -0.3802,
          2.4052, -0.6548, -1.1304,  0.8636,  0.0803, -0.9660, -1.0143,  0.1948,
          0.5108, -1.2296, -0.7239, -0.8933, -0.2621,  0.2005, -1.6910,  1.0689,
          1.0074,  0.2274, -0.4184, -0.2276,  0.7187, -1.0911,  1.0455,  0.1610,
          0.0798,  1.3174,  2.9053,  1.5687,  1.1363, -0.4269,  0.4681, -0.2275,
         -1.2518, -1.5101,  0.3332,  1.1115, -0.5765, -0.1845,  1.0573, -1.5042,
         -1.3581, -0.6503,  2.4976, -0.4533,  0.7215,  1.3105,  0.9769,  0.8943,
         -0.8746,  1.5900,  0.7509,  0.3625,  1.2261,  0.8343,  1.6215,  0.0652,
          0.3875,  1.0001,  0.3721, -1.1022, -0.0122,  0.5340, -1.0139,  0.4521,
          1.4687,  2.0395,  0.7374, -0.3479,  1.6156, -1.7936, -0.2443,  0.9528,
          0.0623, -0.7957,  0.2997, -2.1441, -0.5244, -0.6628, -0.0482, -0.1997,
         -2.3657, -0.7569,  0.3328, -2.1407,  0.3706, -1.2124, -0.9397,  0.7629,
          1.8574, -0.7038,  1.1025, -1.0656,  1.6602,  1.3632,  0.7866, -0.9584,
         -0.1377, -0.2888, -0.4677,  0.3213,  2.6165,  0.8160,  0.2933,  1.7396,
         -0.7484, -0.1775,  0.7626,  1.0181,  1.2647, -0.7750, -0.1872,  1.1865]])
</pre></div>
</div>
</div>
</div>
<p>Similarly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnstd_xtrain</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[2.3470, 2.1151, 2.1772, 2.0503, 2.2926, 2.3693, 2.1253, 2.5290, 2.3184,
         2.1632, 2.2796, 2.2958, 2.1392, 2.3227, 2.0948, 2.6618, 2.3286, 1.9229,
         2.2178, 2.6903, 2.2699, 2.4215, 2.1179, 2.0361, 2.0093, 1.8318, 2.1781,
         2.3840, 2.2855, 2.4143, 1.6977, 1.7918, 2.0156, 2.0423, 1.9195, 1.7332,
         2.1424, 2.2936, 1.8010, 1.8278, 2.2029, 2.1118, 2.3197, 1.7458, 2.3419,
         1.9642, 2.1103, 2.4709, 2.0671, 2.4320, 2.0708, 1.6809, 2.0100, 1.8361,
         2.4507, 2.2494, 1.9286, 2.2677, 2.6837, 1.9560, 2.2003, 2.0271, 1.9214,
         2.2211, 2.3771, 2.3485, 1.9970, 2.1871, 2.1062, 2.1121, 1.9403, 2.0357,
         2.0631, 2.1141, 2.0321, 1.4313, 2.3739, 2.3750, 1.7508, 2.3588, 1.9391,
         2.0428, 1.9524, 2.1434, 2.4703, 2.3452, 2.1779, 2.3140, 2.5282, 2.6035,
         2.0373, 1.9570, 2.4558, 1.9520, 2.0133, 2.3092, 2.0963, 1.9307, 2.1936,
         2.0732, 2.2833, 1.9115, 2.1663, 2.0248, 1.7752, 2.3723, 2.0549, 2.2011,
         1.9060, 2.1103, 2.3679, 2.2174, 2.3703, 2.4740, 2.7726, 2.4209, 1.8527,
         1.9249, 1.9286, 2.1562, 2.1385, 2.1264, 2.0558, 2.0809, 1.9615, 2.0763,
         2.0409, 2.3690, 1.8694, 2.3763, 2.0429, 2.6295, 2.1289, 1.8621, 1.9486,
         2.1482, 2.2445, 3.0612, 1.9641, 1.9791, 2.0894, 1.7463, 2.1585, 1.9308,
         1.9275, 2.3182, 2.3112, 2.1799, 1.9871, 1.7467, 1.7394, 2.1327, 2.0271,
         2.2697, 2.1686, 2.1339, 1.9851, 1.8926, 1.8833, 1.9115, 2.2966, 1.9413,
         2.1535, 2.2445, 2.2070, 1.6808, 2.2534, 1.7394, 1.9822, 2.1503, 1.9299,
         2.2190, 2.2701, 2.1555, 2.3559, 2.0457, 2.2009, 2.0695, 2.2631, 1.9018,
         2.4972, 2.1612, 2.2842, 1.8935, 2.0535, 2.2222, 2.0146, 2.2677, 2.3287,
         2.1751, 2.2328, 2.1815, 2.0942, 1.8494, 2.1692, 2.1498, 2.0431, 2.6586,
         2.3651, 1.8138]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnstd_running</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[2.3397, 2.1127, 2.1607, 2.0195, 2.2593, 2.3516, 2.1103, 2.5075, 2.3009,
         2.1339, 2.2692, 2.2746, 2.1203, 2.3036, 2.0772, 2.6334, 2.3094, 1.9119,
         2.1908, 2.6739, 2.2396, 2.3972, 2.0972, 2.0194, 1.9913, 1.8106, 2.1576,
         2.3601, 2.2604, 2.4005, 1.6662, 1.7701, 1.9929, 2.0250, 1.9011, 1.7245,
         2.1089, 2.2855, 1.7875, 1.8095, 2.1857, 2.0851, 2.2970, 1.7297, 2.3121,
         1.9505, 2.0847, 2.4411, 2.0630, 2.4087, 2.0420, 1.6596, 1.9859, 1.8215,
         2.4230, 2.2367, 1.9207, 2.2545, 2.6714, 1.9398, 2.1691, 2.0143, 1.9043,
         2.1866, 2.3438, 2.3331, 1.9744, 2.1716, 2.0918, 2.0947, 1.9186, 2.0143,
         2.0387, 2.0704, 2.0176, 1.4192, 2.3597, 2.3436, 1.7193, 2.3276, 1.9210,
         2.0164, 1.9422, 2.1131, 2.4389, 2.3320, 2.1649, 2.2978, 2.5055, 2.5902,
         2.0084, 1.9485, 2.4278, 1.9296, 2.0035, 2.2858, 2.0765, 1.9142, 2.1631,
         2.0530, 2.2614, 1.9054, 2.1492, 2.0110, 1.7607, 2.3342, 2.0385, 2.1851,
         1.8868, 2.0890, 2.3435, 2.1972, 2.3476, 2.4317, 2.7641, 2.3959, 1.8332,
         1.9081, 1.9148, 2.1321, 2.1238, 2.1028, 2.0405, 2.0542, 1.9374, 2.0692,
         2.0270, 2.3360, 1.8488, 2.3442, 2.0212, 2.6016, 2.1188, 1.8529, 1.9363,
         2.1399, 2.2261, 3.0514, 1.9437, 1.9584, 2.0580, 1.7288, 2.1423, 1.8827,
         1.9192, 2.3113, 2.2884, 2.1547, 1.9679, 1.7264, 1.7157, 2.1096, 2.0085,
         2.2409, 2.1413, 2.1120, 1.9617, 1.8712, 1.8732, 1.8953, 2.2746, 1.9251,
         2.1248, 2.2261, 2.1831, 1.6496, 2.2212, 1.7222, 1.9535, 2.1321, 1.9156,
         2.1960, 2.2431, 2.1481, 2.3300, 2.0081, 2.1739, 2.0539, 2.2523, 1.8856,
         2.4582, 2.1390, 2.2654, 1.8744, 2.0369, 2.1935, 2.0023, 2.2502, 2.3119,
         2.1646, 2.2128, 2.1717, 2.0855, 1.8304, 2.1477, 2.1137, 2.0246, 2.6384,
         2.3380, 1.7943]])
</pre></div>
</div>
</div>
</div>
<p>Therefore, we can easily infer the loss using the running values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="n">bnmean_running</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="n">bnstd_running</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.0676608085632324
val 2.1091856956481934
</pre></div>
</div>
</div>
</div>
<p>And the resulting losses are basically identical. So, calculating running mean and std values eliminates the need for calculating them in a second step after training. Ok, so we are almost done with <strong>batchnorm</strong>. There are two more notes to make. First, is that we skipped the discussion of what the <span class="math notranslate nohighlight">\(\epsilon\)</span> term is that is added to the normalization step’s denominator square root. It is usually a small, fixed number (e.g. <code class="docutils literal notranslate"><span class="pre">1e-05</span></code>) by default. What this number does is that it prevents a division by <span class="math notranslate nohighlight">\(0\)</span> in the case that the variance over the batch is exactly <span class="math notranslate nohighlight">\(0\)</span>. We could add it in our example and feel free to, but we are just going to skip it since a <span class="math notranslate nohighlight">\(0\)</span> variance is very very unlikely in our very simple example. Second note is that we are being wasteful with <code class="docutils literal notranslate"><span class="pre">b1</span></code> in <code class="docutils literal notranslate"><span class="pre">forward()</span></code>. There, we are first adding <code class="docutils literal notranslate"><span class="pre">b1</span></code> to <code class="docutils literal notranslate"><span class="pre">embcat</span> <span class="pre">&#64;</span> <span class="pre">w1</span> </code> to calculate <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>, but then, within the <strong>batchnorm</strong> layer, we are normalizing by subtracting the pre-activation mean (that contains <code class="docutils literal notranslate"><span class="pre">b1</span></code>), which basically subtracts <code class="docutils literal notranslate"><span class="pre">b1</span></code> out, rendering it redundant:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="o">...</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># batchnorm</span>
    <span class="k">if</span> <span class="n">bnmean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bnstd</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>        
        <span class="n">bnstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">bnstd</span> <span class="o">+</span> <span class="n">bnbias</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Since it is being subtracted out, as a parameter it is neither contributing to the <strong>nn</strong> training or inference nor is it being optimized. If we inspect it’s gradient attribute, it is zero:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([     0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,
             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,
             0.0000,     -0.0000,     -0.0000,      0.0000,     -0.0000,
            -0.0000,     -0.0000,      0.0000,      0.0000,     -0.0000,
             0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,
            -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,
             0.0000,     -0.0000,      0.0000,      0.0000,     -0.0000,
             0.0000,     -0.0000,     -0.0000,     -0.0000,      0.0000,
            -0.0000,      0.0000,     -0.0000,      0.0000,      0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
             0.0000,     -0.0000,      0.0000,     -0.0000,     -0.0000,
             0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,
            -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,
             0.0000,     -0.0000,      0.0000,     -0.0000,     -0.0000,
            -0.0000,      0.0000,     -0.0000,      0.0000,     -0.0000,
            -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,
             0.0000,     -0.0000,     -0.0000,     -0.0000,     -0.0000,
             0.0000,     -0.0000,     -0.0000,      0.0000,     -0.0000,
             0.0000,      0.0000,     -0.0000,      0.0000,     -0.0000,
             0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,
             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,
            -0.0000,      0.0000,     -0.0000,      0.0000,      0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,
            -0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,
            -0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,
             0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,
             0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,
            -0.0000,     -0.0000,     -0.0000,     -0.0000,     -0.0000,
             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,
            -0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,
            -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
             0.0000,     -0.0000,      0.0000,     -0.0000,      0.0000,
            -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,
             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,
             0.0000,      0.0000,     -0.0000,      0.0000,     -0.0000])
</pre></div>
</div>
</div>
</div>
<p>Therefore, whenever using <strong>batchnorm</strong>, then if you have any layers with weights before it, like a linear layer or a convolutional layer or something like that, you are better off disabling the bias parameter for that layer, since we have the <strong>batchnorm</strong> bias (e.g. <code class="docutils literal notranslate"><span class="pre">bnbias</span></code> in our case) which compensates for it. To sum up this point: <strong>batchnorm</strong> has its own bias and thus there’s no need to have a bias in the layer before it, because that bias is going to be subtracted out anyway. So that’s the other small detail to be careful of sometimes. Of course, keeping a unnecessary bias in a layer is not going to do anything catastrophic but it is not going to be doing anything and is just wasteful, so it is better to remove it. Therefore, let’s deprecate <code class="docutils literal notranslate"><span class="pre">b1</span></code>, the first layer bias, from our network and add some nice comments:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">w1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b1_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">w2_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">b2_factor</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w1_factor</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w2_factor</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b2_factor</span>
    <span class="c1"># batchnorm layer parameters</span>
    <span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnmean_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">bnstd_running</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">bnmean_running</span><span class="p">,</span> <span class="n">bnstd_running</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bnmean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bnstd</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">bnmean_running</span><span class="p">,</span> <span class="n">bnstd_running</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># linear layer</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># batchnorm layer</span>
    <span class="k">if</span> <span class="n">bnmean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bnstd</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bnstd</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">bnstd</span> <span class="o">+</span> <span class="n">bnbias</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># disable gradient calculation</span>
        <span class="n">bnmean_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">bnmean_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">bnmean</span>
        <span class="n">bnstd_running</span> <span class="o">=</span> <span class="mf">0.999</span> <span class="o">*</span> <span class="n">bnstd_running</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">bnstd</span>
    <span class="c1"># non-linearity</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>  <span class="c1"># hidden layer</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># output layer</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># loss function</span>
    <span class="k">return</span> <span class="n">bnmean</span><span class="p">,</span> <span class="n">bnstd</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>For a final sum up: we use <strong>batchnorm</strong> to control the statistics of activations in a <strong>nn</strong>. It is common to sprinkle <strong>batchnorm</strong> layers across the <strong>nn</strong> and usually we will place it after layers that have multiplications (linear, convolutional, etc.). Internally, <strong>batchnorm</strong> has parameters for the gain (e.g. <code class="docutils literal notranslate"><span class="pre">bngain</span></code>) and the bias (e.g. <code class="docutils literal notranslate"><span class="pre">bnbias</span></code>). And these are trained using <strong>backprop</strong>. It also has two buffers. These are the running mean and the running mean of the std, which are <strong>not</strong> trained using <strong>backprop</strong> but which are updated during, and finally calculated after, training. So, what <strong>batchnorm</strong> does is it calculates the batch mean and std of the activations that are feeding into <strong>batchnorm</strong> layer, then it’s centering that batch to be unit Gaussian and then it’s offsetting and scaling it by the learned bias (e.g. <code class="docutils literal notranslate"><span class="pre">bnbias</span></code>) and gain (e.g. <code class="docutils literal notranslate"><span class="pre">bngain</span></code>). And then, on top of that, it’s keeping track of the mean and std of the inputs, which are then used during inference. In addition, this allows us to forward individual examples during test time. So, that’s the <strong>batchnorm</strong> layer, which is a fairly complicated layer, but this is a simple example of what it’s doing internally. Now, we are going to go through a real example.</p>
</section>
<section id="resnet">
<h2>ResNet<a class="headerlink" href="#resnet" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Residual_neural_network">residual <strong>nn</strong>s</a> ( <strong>resnet</strong>s) are common types of <strong>nn</strong>s used for image classification. Although we haven’t yet nor will we be covering or explaining all the pieces of these networks in detail, it is still worth noting that an image basically feeds into a <strong>resnet</strong>, and there are many many layers with repeating structure all the way to the output layer the gives us the predictions (e.g. what is inside the input image).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;resnet.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/62652b195be58b3ac3a41cfaf39ed8f3618ab5dc1be3db1235fa991099e9a818.png" src="../_images/62652b195be58b3ac3a41cfaf39ed8f3618ab5dc1be3db1235fa991099e9a818.png" />
</div>
</div>
<p><strong>resnet</strong>s (top network in the above image) are a repeating structure made up of blocks that are sequentially stacked-up. In PyTorch, each such block is defined as a <a class="reference external" href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108"><code class="docutils literal notranslate"><span class="pre">Bottleneck</span></code> object</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bottleneck</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)</span>
    <span class="c1"># while original implementation places the stride at the first 1x1 convolution(self.conv1)</span>
    <span class="c1"># according to &quot;Deep residual learning for image recognition&quot; https://arxiv.org/abs/1512.03385.</span>
    <span class="c1"># This variant is also known as ResNet V1.5 and improves accuracy according to</span>
    <span class="c1"># https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.</span>

    <span class="n">expansion</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inplanes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">downsample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">base_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">norm_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span>
        <span class="n">width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">planes</span> <span class="o">*</span> <span class="p">(</span><span class="n">base_width</span> <span class="o">/</span> <span class="mf">64.0</span><span class="p">))</span> <span class="o">*</span> <span class="n">groups</span>
        <span class="c1"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv1x1</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">width</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">width</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">conv1x1</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">planes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">expansion</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">planes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">expansion</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>Although we haven’t covered all the components of the above pytorch snippet (e.g. CNNs), let’s point out some small pieces of it. The constructor, <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, basically initializes the <strong>nn</strong>, similarly to our <code class="docutils literal notranslate"><span class="pre">define_nn</span></code> function. And, similarly to our <code class="docutils literal notranslate"><span class="pre">forward</span></code> function, the <code class="docutils literal notranslate"><span class="pre">Bottleneck.forward</span></code> method specifies how the <strong>nn</strong> acts for a given input <code class="docutils literal notranslate"><span class="pre">x</span></code>. Now, if you initialize a bunch of <code class="docutils literal notranslate"><span class="pre">Bottleneck</span></code> blocks and stack them up serially, you get a <strong>resnet</strong>. Notice what is happening here. We have convolutional layers (e.g. <code class="docutils literal notranslate"><span class="pre">conv1x1</span></code>, <code class="docutils literal notranslate"><span class="pre">conv3x3</span></code>). These are the same thing as a linear layer, except that convolutional layers are used for images and so they have spatial structure. What this means is that the linear multiplication and bias offset (e.g. <code class="docutils literal notranslate"><span class="pre">logits</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">w2</span> <span class="pre">+</span> <span class="pre">b2</span></code>) are done on overlapping patches, or parts, of the input, instead on the full input (since the images have spatial structure). Otherwise though, convolutional layers basically do an <code class="docutils literal notranslate"><span class="pre">wx</span> <span class="pre">+</span> <span class="pre">b</span></code> type of operation. Then, we have a <code class="docutils literal notranslate"><span class="pre">norm</span></code> layer (e.g. <code class="docutils literal notranslate"><span class="pre">bn1</span></code>), which is initialized to be a 2D <strong>batchnorm</strong> layer (<code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code>). And then, there is a <code class="docutils literal notranslate"><span class="pre">relu</span></code> non-linearity. We have used <span class="math notranslate nohighlight">\(tanh\)</span> so far, but these are both common non-linearities that can be used relatively interchangeably. But for very deep networks, <span class="math notranslate nohighlight">\(ReLU\)</span> typically and empirically works a bit better. And in the <code class="docutils literal notranslate"><span class="pre">Bottleneck.forward</span></code> method, you’ll notice the following pattern: conv layer -&gt; <strong>batchnorm</strong> layer -&gt; relu, repeated three times. This however is basically almost exactly the same pattern employed in our <code class="docutils literal notranslate"><span class="pre">forward</span></code> function: linear layer -&gt; <strong>batchnorm</strong> layer -&gt; tanh. And that’s the motif that you would be stacking up when you would be creating these deep <strong>nn</strong>s that are called <strong>resnet</strong>s. Now, if you dig deeper into the <a class="reference external" href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py">PyTorch <strong>resnet</strong> implementation</a>, you’ll notice that in the functions that return a convolutional layer, e.g. <code class="docutils literal notranslate"><span class="pre">conv1x1</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">conv1x1</span><span class="p">(</span><span class="n">in_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;1x1 convolution&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">out_planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="interim-summary">
<h2>Interim summary<a class="headerlink" href="#interim-summary" title="Link to this heading">#</a></h2>
<p>the bias is disabled (<code class="docutils literal notranslate"><span class="pre">bias=False</span></code>) for the exact same reason we deprecated the bias in our layer that precedes our <strong>batchnorm</strong> layer (like we said, keeping these parameters wouldn’t hurt performance, but they are practically useless). So, because of the motif, the convolutional layers don’t need a bias, as there is a bias in the following <strong>barchnorm</strong> layers to make up for them. Let’s now also briefly descend into the definitions of similar pytorch layers and the parameters that they take. Instead of a convolutional layer, we’re going to look at the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear layer as implemented by PyTorch</a>. As we discussed, convolutional layers are basically linear ones except on patches of the image. So a linear layer performs a <code class="docutils literal notranslate"><span class="pre">wx</span> <span class="pre">+</span> <span class="pre">b</span></code> or <span class="math notranslate nohighlight">\(xA^T + b\)</span> as described in the docs. And to initiliaze this <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer object, you need to know the fan-in (<code class="docutils literal notranslate"><span class="pre">in_features</span></code>) and the fan-out (<code class="docutils literal notranslate"><span class="pre">out_features</span></code>) in order to construct a weight matrix that has a shape <span class="math notranslate nohighlight">\([in\_features \times out\_features]\)</span>. In our case, the equivalent parameters for the first layer are: <code class="docutils literal notranslate"><span class="pre">n_embd</span> <span class="pre">*</span> <span class="pre">block_size,</span> <span class="pre">n_hidden</span></code>. Also there is an option to enable or disable the bias. Furthermore, if you see the <code class="docutils literal notranslate"><span class="pre">Variables</span></code> section of the docs, there’s a weight and a bias bulletpoint whose default initialization is described. So by default, PyTorch initializes the weights by taking the fan-in and then calculating the square root of <span class="math notranslate nohighlight">\(k = \frac{1}{in\_features}\)</span>. And then,  the weights are drawn from a <span class="math notranslate nohighlight">\(U(-\sqrt{k}, \sqrt{k})\)</span> uniform distribution. Despite the lack of the <code class="docutils literal notranslate"><span class="pre">tanh</span></code> gain <span class="math notranslate nohighlight">\(5/3\)</span> that we are using, this is the same kaiming initialization as we have described throughout this lesson. The reason is that if you have a roughly Gaussian input, a kaiming initialization will ensure that out of this layer (e.g. <code class="docutils literal notranslate"><span class="pre">Linear</span></code>) you will have a roughly Gaussian output. Let’s now look at the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">PyTorch <code class="docutils literal notranslate"><span class="pre">BatchNorm1D</span></code> <strong>batchnorm</strong> layer</a>. It takes a <code class="docutils literal notranslate"><span class="pre">num_features</span></code> argument (e.g. <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code> in our case) in order to initialize the gain, bias and running parameters, as well as an <span class="math notranslate nohighlight">\(\epsilon\)</span> parameter that is used for the square root of the normalization denominator. There is also a <code class="docutils literal notranslate"><span class="pre">momentum=0.1</span></code> parameter that is used in the calculation of the running mean and std values (our equivalent is <span class="math notranslate nohighlight">\(0.001\)</span>, e.g. <code class="docutils literal notranslate"><span class="pre">bnmean_running</span> <span class="pre">=</span> <span class="pre">0.999</span> <span class="pre">*</span> <span class="pre">bnmean_running</span> <span class="pre">+</span> <span class="pre">0.001</span> <span class="pre">*</span> <span class="pre">bnmean</span></code>) which you may want to change sometimes. Roughly speaking, if you have a very large batch size, typically what you’ll see is that when you estimate the mean and std, for every single batch size, if it’s large enough, you’re going to get roughly the same result. And therefore, slightly higher momentum like the default <span class="math notranslate nohighlight">\(0.1\)</span>. However, for a batch size as small as <span class="math notranslate nohighlight">\(32\)</span> (e.g. the one we use), the mean and std here might take on slightly different numbers, because there’s <em>only</em> <span class="math notranslate nohighlight">\(32\)</span> (instead of let’s say <span class="math notranslate nohighlight">\(128\)</span>) to estimate the mean and std. So in that case, the <span class="math notranslate nohighlight">\(0.001\)</span> in our example is more appropriate for convergence than the larger, potentially dangerous <span class="math notranslate nohighlight">\(0.1\)</span> that would cause more thrashing and higher inaccuracies in the calculations. There’s also the <code class="docutils literal notranslate"><span class="pre">affine</span></code> boolean parameter, that determines whether the <strong>batchnorm</strong> layer’s gain and bias parameters are learnable, and the <code class="docutils literal notranslate"><span class="pre">track_running_stats</span></code> boolean parameter. One reason you may want to skip running stats is because you may want to, for example, calculate them after training, as a second stage (e.g. through <code class="docutils literal notranslate"><span class="pre">mean_and_std_over_trainset()</span></code>). And so in that case, you wouldn’t want the <strong>batchnorm</strong> layer to do all this extra compute that you’re not gonna use. Finally, you can also specify the <code class="docutils literal notranslate"><span class="pre">device</span></code> that the <strong>batchnorm</strong> layer pass is going to happen on (either <code class="docutils literal notranslate"><span class="pre">cpu</span></code> or <code class="docutils literal notranslate"><span class="pre">gpu</span></code>) and what the datatype is going to be (half-precision, single-precision, double-precision, and so on). So that is more or less the <strong>batchnorm</strong> layer covered in the paper, as implemented by us and as quite-similarly provided in PyTorch. And that’s all we wanted to cover in this lecture: the importance of understanding the activations and the gradients and their statistics in <strong>nn</strong>s. And this becomes increasingly important especially as you make your <strong>nn</strong>s bigger, larger and deeper. We looked at the distributions at the output layer and we saw that if you have too confident mispredictions, because the activations are too messed up at the last layer, you can end up with these hockey stick losses. And if you fix this, you get a better loss at the end of training, because your training is not doing wasteful work. Then, we also saw that we need to control the activations as we don’t want them to squash to zero or explode to the flat regions of the non-linearity’s output range, because you can run into trouble (e.g. dead neurons). Basically, you want everything to be fairly homogeneous throughout the <strong>nn</strong>. You want roughly Gaussian activations throughout the <strong>nn</strong>. And then we pondered, if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the <strong>nn</strong> so that everything is as controlled as possible. By a bit of trial and error, we found some appropriate scaling factors that gave us the uniform activations that we seeked. Of course, we realize that scaling parameters like that is very very hard and practically unsustainable a method when the <strong>nn</strong> is much much deeper. So then we introduced the notion of the normalization layer that people use in practice: <strong>batchnorm</strong>, layer normalization, instance normalization, group normalization. And we introduced and covered the one that came out first. <strong>batchnorm</strong> is layer that you can sprinkle throughout your deep <strong>nn</strong> and the basic idea is that if you want roughly Gaussian activations, well then take your activations, find their mean and std and center your data. And you can do that because the centering operation is differentiable. On top of that, we had to add a lot of bells and whistles, giving us a sense of the complexity of <strong>batchnorm</strong>. Because, ok now we’re centering the data: that’s great. But suddenly we need the gain and bias parameters and now those are trainable. And because we are coupling all the training examples, the questioning is how do you do the inference? To do the inference, we then realized that we have to estimate the mean and std once on the entire training set and then use those at inference. But then, no one likes to do that as a second stage after training. So calculate those values as running averages during training and estimate these in a running manner so that everything is a bit simpler. And again! That was the <strong>batchnorm</strong> layer. Last time, I promise. Lol. Although helpful, no one likes this layer! It causes a huge amount of bugs and intuitively that’s because it’s coupling different examples (per batch) in the forward pass of a <strong>nn</strong>. And many have shot themselves in the foot with this layer, over and over again in their lifetimes. So, in order to avoid sufferring, try to avoid it as much as possible (e.g. by using other normalization alternatives). Nevertheless, <strong>batchnorm</strong> proven to be very influential when it came out in 2015 because that was the first time that you could train reliably much deeper <strong>nn</strong>s. The reason for that is that this layer is very effective at controlling the statistics of the activations in a <strong>nn</strong>. Now, that’s all for now. In the next notebooks, we can start going fully into recurrent <strong>nn</strong>s which are very very deep networks (due to unrolling during optimization). And that is where a lot of this analysis around the activation statistics and all these normalization layers will become very very important for good performance. So, we’ll see that next time. Bye!</p>
</section>
<section id="torchification">
<h2><strong>torch</strong>ification<a class="headerlink" href="#torchification" title="Link to this heading">#</a></h2>
<p><strong>Just kidding!</strong> - As a bonus, before the next lesson, we will cover one more summary of everything we have presented in this lesson so far. But also, it would be very useful to “torchify” our code a little bit so it looks much more like what you would encounter in PyTorch. We will structure our code into modules. Then we will construct our <strong>nn</strong> like we would in PyTorch and we will run our training loop to optimize it. Then, as one last thing we will visualize the activation statistics both in the forward pass and in the backward pass, before evaluating and sampling just like we have done before. Let’s start. Similar to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">torch.nn.Linear</a>, we will implement our own linear layer. By default, we initialize the weights by drawing numbers from a Gaussian distribution and doing a kaiming initialization and we initialize the bias to zero. When calling this module, we do a forward pass and calculate <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">&#64;</span> <span class="pre">w</span> <span class="pre">+</span> <span class="pre">b</span></code>, whereas calling a <code class="docutils literal notranslate"><span class="pre">parameters</span></code> method will return the weight and bias tensors of this layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span> <span class="o">/</span> <span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span> <span class="o">+</span> <span class="p">([]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Similar to the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">PyTorch <code class="docutils literal notranslate"><span class="pre">BatchNorm1D</span></code> <strong>batchnorm</strong> layer</a>, we will define our own. Apart from the parameters we discussed previously (<code class="docutils literal notranslate"><span class="pre">dim</span></code>, <code class="docutils literal notranslate"><span class="pre">eps</span></code>, <code class="docutils literal notranslate"><span class="pre">momentum</span></code>), we will also define the <code class="docutils literal notranslate"><span class="pre">training</span></code> attribute. When this boolean flag is enabled, the running mean values are calculated (training mode) and when it is disabled, they are not (testing mode). When calling this layer, we do a forward pass, wherein mean values are assigned and an output value is calculated (as described previously) and saved (in order to comfortably plot them later on) and finally we update the moving average buffers. Notice the <code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> context manager we use in order to make our code more efficient by bypassing unnecessary saving into a maintained computation graph (since we do not care about gradients for the buffer variables, there is no point in wasting memory for the allocation of gradient-related data). This context manager essentially signifies that we will not be calling <code class="docutils literal notranslate"><span class="pre">backward</span></code> on the variables inside it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm1d</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># parameters (trained with backprop)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># buffers (trained with a running &#39;momentum update&#39;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># calculate the forward pass</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># batch mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># batch variance</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>
        <span class="n">xhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">xvar</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>  <span class="c1"># normalize to unit variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">xhat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="c1"># update the buffers</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xmean</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xvar</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Lastly, in PyTorch fashion, we also calculate an equivalent <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html">torch.nn.Tanh</a> layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Tanh</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<p>So, by defining everything in layers it now becomes very easy to stack them up into a list and more intuitively “define” any <strong>nn</strong>. Let’s see how by updating our <code class="docutils literal notranslate"><span class="pre">define_nn()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span>
    <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="n">weight_gain</span><span class="o">=</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">add_batchnorm_last_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tanh_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">C</span><span class="p">,</span> <span class="n">g</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="c1"># define input layer</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">batchnorm_enabled</span><span class="p">:</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">if</span> <span class="n">tanh_enabled</span><span class="p">:</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tanh</span><span class="p">())</span>
    <span class="c1"># define hidden layers</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_hidden</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">batchnorm_enabled</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">tanh_enabled</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tanh</span><span class="p">())</span>
    <span class="c1"># define output layer</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">add_batchnorm_last_layer</span><span class="p">:</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">output_size</span><span class="p">))</span>
    <span class="c1"># scale parameters</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># last layer: make less confident</span>
        <span class="k">if</span> <span class="n">add_batchnorm_last_layer</span><span class="p">:</span>
            <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*=</span> <span class="mf">0.1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>
        <span class="c1"># all other layers: apply gain</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Linear</span><span class="p">):</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="n">weight_gain</span>
    <span class="c1"># collect parameters</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>  <span class="c1"># number of parameters in total</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s update the our <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">backward</span></code> and <code class="docutils literal notranslate"><span class="pre">train</span></code> functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xb</span><span class="p">]</span>  <span class="c1"># embed the characters into vectors</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate the vectors</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>  <span class="c1"># loss function</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
    <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">break_at_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
        <span class="c1"># minibatch construct</span>
        <span class="n">bix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,))</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bix</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">backward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="p">(</span><span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">150000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
        <span class="n">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">break_at_step</span><span class="p">:</span>
            <span class="k">break</span>  <span class="c1"># AFTER_DEBUG: would take out obviously to run full optimization</span>
    <span class="k">return</span> <span class="n">lossi</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we’ll define a new <strong>nn</strong> and train in debug mode, for only one step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2946
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualizing-activations-and-gradients">
<h2>Visualizing activations and gradients<a class="headerlink" href="#visualizing-activations-and-gradients" title="Link to this heading">#</a></h2>
<p>Now, since we defined an <code class="docutils literal notranslate"><span class="pre">out</span></code> attribute in our custom <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> layer, after training we have saved the activations which we can visualize! Specifically, we will plot the histogram of each layer’s <span class="math notranslate nohighlight">\(tanh\)</span> activations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Tanh</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># width and height of the plot</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>  <span class="c1"># note: exclude the output layer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">layer_cls</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">grad</span> <span class="k">if</span> <span class="n">grad</span> <span class="k">else</span> <span class="n">layer</span><span class="o">.</span><span class="n">out</span>
            <span class="k">if</span> <span class="n">grad</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;layer </span><span class="si">%d</span><span class="s2"> (</span><span class="si">%10s</span><span class="s2">): mean </span><span class="si">%+f</span><span class="s2">, std </span><span class="si">%e</span><span class="s2">&quot;</span>
                    <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;layer </span><span class="si">%d</span><span class="s2"> (</span><span class="si">%10s</span><span class="s2">): mean </span><span class="si">%+.2f</span><span class="s2">, std </span><span class="si">%.2f</span><span class="s2">, saturated: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span>
                    <span class="o">%</span> <span class="p">(</span>
                        <span class="n">i</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span>
                        <span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.97</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">hy</span><span class="p">,</span> <span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">hy</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grad</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;gradient distribution&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;activation distribution&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 (      Tanh): mean -0.03, std 0.77, saturated: 22.97%
layer 3 (      Tanh): mean -0.01, std 0.69, saturated: 8.75%
layer 5 (      Tanh): mean +0.02, std 0.67, saturated: 7.37%
layer 7 (      Tanh): mean -0.00, std 0.65, saturated: 5.34%
layer 9 (      Tanh): mean -0.02, std 0.66, saturated: 6.03%
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8f455c8952e74c55bb0ce824bb543e4e", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This histogram shows us how many values in these tensors take on any of the values on the x-axis. layer <span class="math notranslate nohighlight">\(1\)</span> is fairly saturated (~20%), with a significant amount of values being close to the saturation points at the tails (<span class="math notranslate nohighlight">\(-1\)</span>, <span class="math notranslate nohighlight">\(+1\)</span>) and the subsequent layers being more stable. And why the values are pretty stable is because the weight values of the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer are boosted by a gain of <span class="math notranslate nohighlight">\(5/3\)</span>. If we use a gain of <span class="math notranslate nohighlight">\(1\)</span> (aka no gain), let’s see what happens:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">weight_gain</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.3090
layer 1 (      Tanh): mean -0.03, std 0.63, saturated: 4.47%
layer 3 (      Tanh): mean -0.02, std 0.48, saturated: 0.06%
layer 5 (      Tanh): mean +0.01, std 0.41, saturated: 0.03%
layer 7 (      Tanh): mean +0.00, std 0.35, saturated: 0.00%
layer 9 (      Tanh): mean -0.01, std 0.33, saturated: 0.00%
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec87cf391e6d4a0883f734704f46336d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now, from the first to the last <span class="math notranslate nohighlight">\(tanh\)</span> layer, the std shrinks and the saturation goes to <span class="math notranslate nohighlight">\(0\)</span>. What this means is that the activations are being shrunk to <span class="math notranslate nohighlight">\(0\)</span>. The reason for that is that when you just have a sandwich of linear layer, <span class="math notranslate nohighlight">\(tanh\)</span> layer pairs, these <span class="math notranslate nohighlight">\(tanh\)</span> layers act as squashing functions that take a distribution and slightly squeeze it towards zero. Therefore, some gain is necessary in order to keep expanding the distributions and by doing so to fight the squashing phenomenon. So, if the gain is close to <span class="math notranslate nohighlight">\(1\)</span>, the activations will then come towards <span class="math notranslate nohighlight">\(0\)</span>, but if it is something too big (such as <span class="math notranslate nohighlight">\(3\)</span>), then, on the contrary, the saturations end up way too large:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">weight_gain</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2942
layer 1 (      Tanh): mean -0.07, std 0.86, saturated: 50.00%
layer 3 (      Tanh): mean +0.00, std 0.84, saturated: 40.91%
layer 5 (      Tanh): mean -0.00, std 0.84, saturated: 43.16%
layer 7 (      Tanh): mean -0.03, std 0.84, saturated: 41.12%
layer 9 (      Tanh): mean -0.00, std 0.84, saturated: 41.31%
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0bad81f17047447195d03d3a87713883", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>So, <span class="math notranslate nohighlight">\(5/3\)</span> (the default value) is a good setting for a sandwich of linear layers with <span class="math notranslate nohighlight">\(tanh\)</span> activations. And it roughly stabilizes the std at a reasonable value (~5%), which is a pretty good number and this is a good setting of the gain in this context:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.3047
layer 1 (      Tanh): mean -0.03, std 0.76, saturated: 19.34%
layer 3 (      Tanh): mean +0.00, std 0.70, saturated: 9.50%
layer 5 (      Tanh): mean +0.01, std 0.68, saturated: 7.69%
layer 7 (      Tanh): mean +0.01, std 0.66, saturated: 6.12%
layer 9 (      Tanh): mean -0.01, std 0.65, saturated: 6.25%
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6b2cbd427ba848df8d6b1c73c5dc70d4", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Similarly, we can do the exact same thing with the gradients. So, here we will run the exact same loop by using the exact same function, but instead of the layer outputs we will now visualize the gradients (<code class="docutils literal notranslate"><span class="pre">.grad</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2861
layer 1 (      Tanh): mean +0.000005, std 4.419007e-04
layer 3 (      Tanh): mean -0.000000, std 4.158158e-04
layer 5 (      Tanh): mean +0.000005, std 3.875846e-04
layer 7 (      Tanh): mean +0.000005, std 3.389598e-04
layer 9 (      Tanh): mean -0.000002, std 3.052316e-04
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2ca0079a03e043aa9e2ed482dddee185", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Here, you will see that the gradient distribution is fairly reasonable. And in particular, what we are looking for is that all of these layers (layer <span class="math notranslate nohighlight">\(1\)</span>, <span class="math notranslate nohighlight">\(2\)</span>, etc.) in this “sandwich” have roughly the same gradient. Things are not shrinking or exploding. So, let’s train and set the gain as way too small, <span class="math notranslate nohighlight">\(0.5\)</span> and see what happens to the activations and the gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">weight_gain</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2965
layer 1 (      Tanh): mean -0.01, std 0.42, saturated: 0.09%
layer 3 (      Tanh): mean -0.00, std 0.20, saturated: 0.00%
layer 5 (      Tanh): mean +0.00, std 0.10, saturated: 0.00%
layer 7 (      Tanh): mean +0.00, std 0.05, saturated: 0.00%
layer 9 (      Tanh): mean -0.00, std 0.02, saturated: 0.00%
layer 1 (      Tanh): mean -0.000000, std 1.770416e-05
layer 3 (      Tanh): mean -0.000001, std 3.721016e-05
layer 5 (      Tanh): mean +0.000002, std 7.565098e-05
layer 7 (      Tanh): mean +0.000004, std 1.499158e-04
layer 9 (      Tanh): mean -0.000008, std 3.020476e-04
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7f1cd4ddb21044c7a09d237efc2d2b40", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a92a9706a26f4a0d881ffed67adbcc6b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>First of all, now, the activations are shrinking to zero but also the gradients are doing something weird: they start off very narrow, around <span class="math notranslate nohighlight">\(0.0\)</span> (see layer <span class="math notranslate nohighlight">\(1\)</span>), but then in layers that follow, they are expanding out (layer 3, 5, etc.). If we now use a too-high of a gain, e.g. <span class="math notranslate nohighlight">\(3.0\)</span>, like we did before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">weight_gain</span><span class="o">=</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2981
layer 1 (      Tanh): mean +0.000022, std 1.098993e-03
layer 3 (      Tanh): mean -0.000023, std 7.887821e-04
layer 5 (      Tanh): mean +0.000010, std 6.063787e-04
layer 7 (      Tanh): mean +0.000011, std 4.196224e-04
layer 9 (      Tanh): mean -0.000010, std 2.980209e-04
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ef876bcd9f874b1f8532a53bb056d0bf", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>then we see that for the gradients there is some asymmetry going on where, as you go into deeper and deeper layers, the activations are also changing. Therefore, we have to very carefully set the grains to get nice activations in both the forward and backard passes. Now, before we move on to <strong>batchnorm</strong>, let’s see what happens with the activations when we remove the <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> units and thus only a giant linear sandwich remains as our <strong>nn</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">tanh_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Linear</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 4.4966
layer 0 (    Linear): mean -0.14, std 1.70, saturated: 57.47%
layer 1 (    Linear): mean +0.01, std 2.76, saturated: 71.41%
layer 2 (    Linear): mean +0.04, std 4.79, saturated: 82.16%
layer 3 (    Linear): mean +0.06, std 7.88, saturated: 90.72%
layer 4 (    Linear): mean -0.40, std 13.66, saturated: 94.12%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_1430637/3179552959.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(20, 4))  # width and height of the plot
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8e38fe847ef34a448499d4463eaaa596", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>What we are seeing is that the activations started out on the blue (layer <span class="math notranslate nohighlight">\(1\)</span>) and by layer 4 they have become very diffuse, so what is happening to the activations is that they are expanding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Linear</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 0 (    Linear): mean +0.000016, std 2.696820e-03
layer 1 (    Linear): mean -0.000022, std 1.605610e-03
layer 2 (    Linear): mean +0.000048, std 9.567880e-04
layer 3 (    Linear): mean -0.000002, std 5.531530e-04
layer 4 (    Linear): mean -0.000003, std 3.283872e-04
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "36c0c1b313ab4675bf2472913eed00ff", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Conversely, the gradients follow the opposite pattern, as you go down deeper in the layers. So basically you have an asymmetry in the <strong>nn</strong>. And you might imagine that if you have very deep <strong>nn</strong>s, say like 50 layers or something like that, the above pattern is not a good place to be! That’s why before the <strong>batchnorm</strong> technique, the grain was incredibly tricky to set. See what happens, for a very small gain:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">tanh_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_gain</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Linear</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Linear</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2954
layer 0 (    Linear): mean -0.04, std 0.49, saturated: 5.09%
layer 1 (    Linear): mean -0.01, std 0.24, saturated: 0.03%
layer 2 (    Linear): mean +0.00, std 0.12, saturated: 0.00%
layer 3 (    Linear): mean +0.00, std 0.06, saturated: 0.00%
layer 4 (    Linear): mean -0.00, std 0.03, saturated: 0.00%
layer 0 (    Linear): mean +0.000000, std 1.994393e-05
layer 1 (    Linear): mean -0.000000, std 4.033995e-05
layer 2 (    Linear): mean +0.000003, std 8.008869e-05
layer 3 (    Linear): mean +0.000005, std 1.561930e-04
layer 4 (    Linear): mean +0.000003, std 3.092947e-04
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "035d3ebce40a4b3290328add55192b18", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "49b789bb79d444eab794a1937765e259", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Basically, the reverse occurs: activations shrink and gradients diffuse, as we go deeper in the layers. Therefore, certainly these patterns are not what we would want and in this case the correct setting of the gain is exactly <span class="math notranslate nohighlight">\(1.0\)</span>, just as we are doing at initialization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">tanh_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Linear</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layer_cls</span><span class="o">=</span><span class="n">Linear</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.3175
layer 0 (    Linear): mean -0.04, std 0.97, saturated: 31.66%
layer 1 (    Linear): mean -0.01, std 0.98, saturated: 30.91%
layer 2 (    Linear): mean +0.01, std 1.01, saturated: 32.06%
layer 3 (    Linear): mean +0.01, std 0.99, saturated: 32.44%
layer 4 (    Linear): mean -0.06, std 1.00, saturated: 32.16%
layer 0 (    Linear): mean -0.000002, std 3.170026e-04
layer 1 (    Linear): mean -0.000003, std 3.147987e-04
layer 2 (    Linear): mean +0.000015, std 3.173883e-04
layer 3 (    Linear): mean +0.000002, std 3.088438e-04
layer 4 (    Linear): mean -0.000003, std 2.986307e-04
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "dfae44f1594a4d849eda23cc8f864b1f", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "187766cf5fe148ce846fdb0fb90fff09", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now we see that the statistics for the forward and backward passes are well behaved! And so the reason we are demonstrating these phenomena is to highlight how getting <strong>nn</strong>s to train before these normalization layers and before the use of advanced optimizers like <code class="docutils literal notranslate"><span class="pre">Adam</span></code> (which we still have to cover) and residual connections and so on, training <strong>nn</strong>s basically looked like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;pencil_balancing.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f1644c6be2925f96a56916eebb280a21eb66f0bf354b5e0bd97418a509900eaa.jpg" src="../_images/f1644c6be2925f96a56916eebb280a21eb66f0bf354b5e0bd97418a509900eaa.jpg" />
</div>
</div>
<p>Haha, like a total balancing act. You have to make sure that everything is precisely orchestrated and have to care about the activations and the gradients and the statistics and then maybe you can train something. But, it was basically impossible to train very deep networks, and this was fundamentally the reason for that. You would have to be very very careful with your initialization. The other point to make here is the question: why do we need <span class="math notranslate nohighlight">\(tanh\)</span> layers at all? Why do we include them and then have to worry about the gain? The reason for that of course is that if you just have a stack of linear layers, then certainly we are very easily getting nice activations and so on but this is just a massive linear sandwich. And it turns out that it collapses to a single linear layer in terms of its representation power. So, if you were to plot the output as a function of the input, in that case, you are just getting a linear function. No matter how many linear layers you stack up, you still end up with just a linear transformation: all the sets of <span class="math notranslate nohighlight">\(wx + b\)</span> just collapse into a large <span class="math notranslate nohighlight">\(WX + B\)</span> with a slightly different weight and bias matrix. Interestingly though, even though in that case, the forward pass collapses to just a linear layer, because of <strong>backprop</strong> and the dynamics of the backward pass, the optimization is really not identical. You actually end up with all kinds of interesting dynamics in the backward pass because of the way the chain rule is calculating it. And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layer, in both cases, those are just a linear transformation in the forward pass, but the training dynamics would be different. And there are actually in fact entire papers that analyze infinitely layered linear layers, etc. and so as you can imagine there’s a lot of things too that you can play with there. Basically, the <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> non-linearities allow us to turn this sandwich from just a linear transformation into a <strong>nn</strong> that can in priciple approximate any arbitrary function.</p>
</section>
<section id="further-visualization">
<h2>Further visualization<a class="headerlink" href="#further-visualization" title="Link to this heading">#</a></h2>
<p>Now, we’ll define a <strong>nn</strong> with <strong>batchnorm</strong> layers between the linear and <span class="math notranslate nohighlight">\(tanh\)</span> layers and we will look at another kind of visualization that is very important to consider when training <strong>nn</strong>s:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># width and height of the plot</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;weight </span><span class="si">%10s</span><span class="s2"> | mean </span><span class="si">%+f</span><span class="s2"> | std </span><span class="si">%e</span><span class="s2"> | grad:data ratio </span><span class="si">%e</span><span class="s2">&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">t</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
            <span class="p">)</span>
            <span class="n">hy</span><span class="p">,</span> <span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">hy</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;weights gradient distribution&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>46970
      0/ 200000: 3.2891
weight   (27, 10) | mean -0.000000 | std 1.300287e-03 | grad:data ratio 1.299346e-03
weight  (30, 100) | mean -0.000036 | std 1.190012e-03 | grad:data ratio 3.815812e-03
weight (100, 100) | mean -0.000003 | std 1.107649e-03 | grad:data ratio 6.667744e-03
weight (100, 100) | mean +0.000001 | std 9.681268e-04 | grad:data ratio 5.766639e-03
weight (100, 100) | mean -0.000003 | std 8.500073e-04 | grad:data ratio 5.084338e-03
weight (100, 100) | mean +0.000012 | std 7.212228e-04 | grad:data ratio 4.309830e-03
weight  (100, 27) | mean +0.000000 | std 2.098679e-02 | grad:data ratio 2.072655e+00
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "94c89df07fcb4143b6969f4923a872cd", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>So, ultimately what we are doing during training is that we are updating the parameters of the <strong>nn</strong>. So, we care about the parameters, their values and their gradients. Therefore, in <code class="docutils literal notranslate"><span class="pre">visualize_weight_gradients</span></code> what we are doing is we are iterating over all the available parameters and then we are only considering the 2-dimensional ones (by checking <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">p.ndim</span> <span class="pre">==</span> <span class="pre">2</span></code>), which are basically the weights of these linear layers. We are skipping the biases, the gammas and the betas in the <strong>batchnorm</strong> layer just for simplicity, because what is happening with the weights is instructive by itself. Here, we have printed the mean, std and gradient-to-data ratio, which is helpful for getting a sense of the scale of the gradient compared to the scale of the actual values. This is important because we are going to be taking a step update that is the learning rate times the gradient onto the data. And so if the gradient has too large of a magnitude (if the numbers in that tensor are too large) compared to the data (the numbers in the data tensor), then you are in trouble. But in our case, our grad-to-data ratios are low numbers (e.g. <code class="docutils literal notranslate"><span class="pre">1.209762e-03</span></code>) and the grad values are <span class="math notranslate nohighlight">\(100\)</span> to <span class="math notranslate nohighlight">\(1000\)</span> times smaller than the data values of these weight parameters. Notably, this is not true about the last layer (<span class="math notranslate nohighlight">\(16\)</span>, pink) which is a bit of a troublemaker in the way that it is currently arranged. Because you can see that this layer takes on values that are much larger than some of the other layer’s values inside the <strong>nn</strong>. And so the std values are roughly <span class="math notranslate nohighlight">\(10^{-3}\)</span> throughout the layers, except for the last linear layer that has an std of roughly <span class="math notranslate nohighlight">\(10^{-2}\)</span>. That is problematic, because in the simple stochastic gradient descent setup, you would be training the last layer about <span class="math notranslate nohighlight">\(10\)</span> times faster than you would be training the other layers at initialization. Now this actually fixes itself a little bit if you train for a bit longer. So, for example if we stop the training at step <span class="math notranslate nohighlight">\(1000\)</span> and plot the distributions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.3036
layer 1 (      Tanh): mean -0.07, std 0.77, saturated: 22.59%
layer 3 (      Tanh): mean -0.00, std 0.73, saturated: 13.75%
layer 5 (      Tanh): mean -0.01, std 0.74, saturated: 13.34%
layer 7 (      Tanh): mean -0.02, std 0.73, saturated: 11.97%
layer 9 (      Tanh): mean -0.04, std 0.71, saturated: 9.81%
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1a665f840df346d49514e40ef06e0d09", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Here we see how in the forward pass the neurons are saturating just a bit (~21% for layer 1, ~11% for layers 2+).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 (      Tanh): mean +0.000007, std 3.624916e-03
layer 3 (      Tanh): mean +0.000012, std 3.253933e-03
layer 5 (      Tanh): mean +0.000041, std 3.013918e-03
layer 7 (      Tanh): mean +0.000042, std 2.847814e-03
layer 9 (      Tanh): mean -0.000004, std 2.390941e-03
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "48a615d69ab042328643db409c5ad5d9", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>And if we also look at the backward pass, the stds are more or less equal and there is no shrinking to <span class="math notranslate nohighlight">\(0\)</span> or exploding to <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>weight   (27, 10) | mean +0.001014 | std 1.351586e-02 | grad:data ratio 1.350231e-02
weight  (30, 100) | mean +0.000045 | std 1.049890e-02 | grad:data ratio 3.353235e-02
weight (100, 100) | mean -0.000138 | std 8.640624e-03 | grad:data ratio 5.135019e-02
weight (100, 100) | mean -0.000059 | std 7.113333e-03 | grad:data ratio 4.192131e-02
weight (100, 100) | mean -0.000067 | std 6.203464e-03 | grad:data ratio 3.675764e-02
weight (100, 100) | mean +0.000043 | std 4.948972e-03 | grad:data ratio 2.940613e-02
weight  (100, 27) | mean -0.000000 | std 1.747323e-02 | grad:data ratio 2.483876e-01
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "4b567166e2834649b7f05d9c7cb47206", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>And last but not least, you can see here, in the weight gradients, things are also stabilizing a little bit. So the tails of the last layer (<span class="math notranslate nohighlight">\(6\)</span>, pink) are being drawn to <span class="math notranslate nohighlight">\(0\)</span> during the optimization. But this is certainly a little bit troubling. Especially if you are using a very simple update rule like stochastic gradient descent, instead of a modern optimizer like Adam. Now, let’s look at another plot that is very useful to look at when training <strong>nn</strong>s. First of all, let’s agree that the grad-to-data ratio is actually not that informative because what matters at the end instead is actually the update-to-data ratio. Because that is the amount by which we will actually change the data in these tensors. So, now let’s update the <code class="docutils literal notranslate"><span class="pre">train</span></code> function by introducing a new update-to-data ratio list (<code class="docutils literal notranslate"><span class="pre">ud</span></code>) that we are going to be building up for every single training iteration in order to keep track of this ratio:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
    <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">break_at_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ud</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
        <span class="c1"># minibatch construct</span>
        <span class="n">bix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,))</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bix</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">backward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="p">(</span><span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">150000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
        <span class="n">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">ud</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">((</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">())</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">break_at_step</span><span class="p">:</span>
            <span class="k">break</span>  <span class="c1"># AFTER_DEBUG: would take out obviously to run full optimization</span>
    <span class="k">return</span> <span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s initialize a new <strong>nn</strong> and train for <span class="math notranslate nohighlight">\(1000\)</span> iterations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2851
</pre></div>
</div>
</div>
</div>
<p>And look at the activations, the gradients and the weight gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 (      Tanh): mean -0.05, std 0.76, saturated: 21.22%
layer 3 (      Tanh): mean +0.04, std 0.71, saturated: 10.84%
layer 5 (      Tanh): mean -0.01, std 0.73, saturated: 11.84%
layer 7 (      Tanh): mean -0.03, std 0.74, saturated: 12.34%
layer 9 (      Tanh): mean -0.02, std 0.71, saturated: 11.22%
layer 1 (      Tanh): mean +0.000147, std 3.620432e-03
layer 3 (      Tanh): mean -0.000034, std 3.273357e-03
layer 5 (      Tanh): mean -0.000040, std 3.102373e-03
layer 7 (      Tanh): mean -0.000053, std 3.089136e-03
layer 9 (      Tanh): mean +0.000043, std 2.550589e-03
weight   (27, 10) | mean +0.001451 | std 1.459943e-02 | grad:data ratio 1.458455e-02
weight  (30, 100) | mean -0.000039 | std 1.180491e-02 | grad:data ratio 3.770098e-02
weight (100, 100) | mean -0.000031 | std 9.201036e-03 | grad:data ratio 5.468920e-02
weight (100, 100) | mean +0.000058 | std 7.712632e-03 | grad:data ratio 4.546781e-02
weight (100, 100) | mean -0.000025 | std 7.597501e-03 | grad:data ratio 4.500666e-02
weight (100, 100) | mean +0.000049 | std 7.046165e-03 | grad:data ratio 4.183645e-02
weight  (100, 27) | mean +0.000000 | std 2.525010e-02 | grad:data ratio 3.572145e-01
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "fa4aefbe2ba4477c9c11347940b8e2dc", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "5a18c3073fc54c8885b53e12d52581c4", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "c76a73e714ae41b6a4ad14e0600db112", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>but also one more plot we will now introduce:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">legends</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ud</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ud</span><span class="p">))])</span>
            <span class="n">legends</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;param </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ud</span><span class="p">)],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="s2">&quot;k&quot;</span>
    <span class="p">)</span>  <span class="c1"># these ratios should be ~1e-3, indicate on plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">legends</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c68bfd222a61409aa2c28cd9d58e8e8a", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>So, when we plot the <code class="docutils literal notranslate"><span class="pre">ud</span></code> ratios, you can see that they evolve over time. During initialization they take on certain values and these updates sort of like start stabilizing during training. But you’ll also notice we have plotted a straight black line. This is an approximate value that is a rough guide for what the ratios should roughly be, which in this case is roughly ~<code class="docutils literal notranslate"><span class="pre">1e-3</span></code>. That basically means that there are some certain values in the data tensor and the updates to those values at every single iteration are no more than roughly <span class="math notranslate nohighlight">\(1000th\)</span> of the actual magnitude in those tensors. If instead of roughly ~<code class="docutils literal notranslate"><span class="pre">1e-3</span></code>, the desired ratio value are much larger (e.g. ~<code class="docutils literal notranslate"><span class="pre">1e-1</span></code> or a log value of <code class="docutils literal notranslate"><span class="pre">-1</span></code> in this plot), then the data values are updating a lot, meaning that they are undergoing a lot of change. This is the case for the <code class="docutils literal notranslate"><span class="pre">ud</span></code> ratio values of the last layer, layer <span class="math notranslate nohighlight">\(6\)</span>. The reason why this layer is an outlier, is because this layer was artificially shrunk down to keep the softmax unconfident: see the <code class="docutils literal notranslate"><span class="pre">define_nn</span></code> function where we specifically do: <code class="docutils literal notranslate"><span class="pre">layers[-1].weight</span> <span class="pre">*=</span> <span class="pre">0.1</span></code>. This artificially made inside that last layer tensor way too low and that is why we are temporarily getting a very high <code class="docutils literal notranslate"><span class="pre">ud</span></code> ratio. But as you can see, that ratio does decrease and then stabilizes over time, once that weight starts to learn. In general, it’s helpful to look at the evolution of this <code class="docutils literal notranslate"><span class="pre">ud</span></code> ratio and as a rule of thumb to make sure that the values are not too much above roughly ~<code class="docutils literal notranslate"><span class="pre">1e-3</span></code> (<span class="math notranslate nohighlight">\(-3\)</span> on this log plot). If it’s below ~<code class="docutils literal notranslate"><span class="pre">1e-3</span></code>, usually this means that the parameters are not training fast enough. So, if our learning rate was very low, let’s say <span class="math notranslate nohighlight">\(0.001\)</span>, this plot will typically reveal it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span>
    <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.001</span>
<span class="p">)</span>
<span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2872
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "42b6940fd57c48b3b09c0914839efb5e", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>So you see how all of these updates are way too small. The size of the update is ~<code class="docutils literal notranslate"><span class="pre">1e-5</span></code> times smaller than the size of the data tensor values. And this is essentially a symptop of training way too slow. So this is another way to sometimes set the learning rate and to get a sense of what that learning rate should be after inspecting the <code class="docutils literal notranslate"><span class="pre">ud</span></code> ratio evolution. If anything, the default learning rate of <span class="math notranslate nohighlight">\(0.1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
      0/ 200000: 3.2913
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9d25f01ab38846749f956cb9ffd566b7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>is a little bit on the higher side. Because you see that we’re above the black line of <code class="docutils literal notranslate"><span class="pre">1e-3</span></code> a little bit, but everything is somewhat stabilizing. So this looks like a pretty decent setting of learning rates. But this is something to look at in general. And when something is miscalibrated you will quickly realize it. So for example, everything looks pretty well behaved, right? But, just as a comparison, when things are not properly calibrated, what does that look like? For example, let’s simulate the scenario were we initialize the weights of the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers from a Gaussian distribution <strong>without</strong> the <code class="docutils literal notranslate"><span class="pre">fan_in</span></code> normalization (<code class="docutils literal notranslate"><span class="pre">torch.randn((fan_in,</span> <span class="pre">fan_out),</span> <span class="pre">generator=generator)</span></code> and not <code class="docutils literal notranslate"><span class="pre">torch.randn((fan_in,</span> <span class="pre">fan_out),</span> <span class="pre">generator=generator)</span> <span class="pre">/</span> <span class="pre">fan_in**0.5</span></code>). An easy way to do this without having to re-define the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> class and re-write stuff is to simply call a function after defining our <strong>nn</strong> that multiplies each layer’s weight tensor with a <code class="docutils literal notranslate"><span class="pre">fan_in**0.5</span></code> in order to revert the effect of division by the same number (that happened during initialization):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">revert_fan_in_normalization</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Linear</span><span class="p">):</span>
            <span class="n">fan_in</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="c1"># revert division by fan_in**0.5 and simulate initialization</span>
            <span class="c1"># of weight by sampling from plain Gaussian distribution:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">revert_fan_in_normalization</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>45970
</pre></div>
</div>
</div>
</div>
<p>Now, how do we notice in this case that something is off? Well, after training, the activations plot should tell you “woaw! your neurons are way too saturated”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/ 200000: 3.6046
layer 1 (      Tanh): mean +0.04, std 0.99, saturated: 97.16%
layer 3 (      Tanh): mean +0.08, std 0.98, saturated: 91.34%
layer 5 (      Tanh): mean +0.00, std 0.98, saturated: 90.28%
layer 7 (      Tanh): mean +0.01, std 0.98, saturated: 89.53%
layer 9 (      Tanh): mean +0.03, std 0.98, saturated: 90.56%
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "52aa839cf20344cb94b5732e65f12b88", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Also, the gradients and weight gradients are going to be all messed up:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>layer 1 (      Tanh): mean +0.002450, std 1.395893e-01
layer 3 (      Tanh): mean +0.000484, std 5.279354e-02
layer 5 (      Tanh): mean +0.000254, std 1.787987e-02
layer 7 (      Tanh): mean -0.000173, std 6.437032e-03
layer 9 (      Tanh): mean -0.000014, std 2.148476e-03
weight   (27, 10) | mean +0.004719 | std 2.368491e-01 | grad:data ratio 8.764252e-02
weight  (30, 100) | mean +0.001443 | std 1.174272e-01 | grad:data ratio 5.857516e-02
weight (100, 100) | mean -0.000085 | std 3.704451e-02 | grad:data ratio 2.215002e-02
weight (100, 100) | mean -0.000166 | std 1.271184e-02 | grad:data ratio 7.566377e-03
weight (100, 100) | mean -0.000024 | std 4.991787e-03 | grad:data ratio 2.985652e-03
weight (100, 100) | mean +0.000010 | std 1.871466e-03 | grad:data ratio 1.118368e-03
weight  (100, 27) | mean +0.000000 | std 3.144594e-02 | grad:data ratio 4.646735e-01
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e2cc699e5d414f6d9e8099ba9a9c94a4", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "3a4ea20d8368450d8678707b9dcd289d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>And if we look at the update-to-data ratios, they are also quite messed up and all over the places:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0def0cb5eaec45c59b38a9465d7e8d15", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Yikes! As you can see, there is a lot of discrepancy in how fast these layers are learning and some of them are learning way too fast. And so <code class="docutils literal notranslate"><span class="pre">1e-1</span></code>, <code class="docutils literal notranslate"><span class="pre">1e-1.5</span></code>, etc. are very large numbers in terms of this ratio. Again, we should be somewhere around ~<code class="docutils literal notranslate"><span class="pre">1e-3</span></code> and not much more above that. So, this is how miscalibrations of your <strong>nn</strong>s are going to manifest. And therefore such plots are a good way of bringing those miscalibrations to your attention, so you can address them. Okay so so far we have seen that when we have such a linear <span class="math notranslate nohighlight">\(tanh\)</span> sandwich as the one we have constructed, we can actually precisely calibrate the gains and make the activations, the gradients and the parameters and the updates all look pretty decent. But it definitely does feel like trying to balance a pencil on your finger and that’s because the gain has to be very precisely calibrated. So now let’s introduce <strong>batchnorm</strong> layers into the magical sandwich and let’s see how that helps fix the problem, by placing them in-between our linear and tanh layers (note: placing them after the tanh layers would also yield similar results). Luckily, we have already implemented an option for that and all we have to do is to enable the <strong>batchnorm</strong> option. But now we will also add a <strong>batchnorm</strong> after the last layer too using the corresponding option:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_batchnorm_last_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>47024
</pre></div>
</div>
</div>
</div>
<p>Now, let’s train and look at the distributions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/ 200000: 3.3037
layer 2 (      Tanh): mean -0.01, std 0.63, saturated: 2.84%
layer 5 (      Tanh): mean +0.00, std 0.64, saturated: 2.78%
layer 8 (      Tanh): mean +0.01, std 0.64, saturated: 2.22%
layer 11 (      Tanh): mean -0.00, std 0.65, saturated: 1.69%
layer 14 (      Tanh): mean -0.01, std 0.65, saturated: 1.62%
layer 2 (      Tanh): mean +0.000000, std 3.910133e-03
layer 5 (      Tanh): mean +0.000000, std 3.199076e-03
layer 8 (      Tanh): mean +0.000000, std 2.847068e-03
layer 11 (      Tanh): mean +0.000000, std 2.580181e-03
layer 14 (      Tanh): mean -0.000000, std 2.521838e-03
weight   (27, 10) | mean -0.000000 | std 1.063054e-02 | grad:data ratio 1.061966e-02
weight  (30, 100) | mean +0.000073 | std 9.105187e-03 | grad:data ratio 2.913094e-02
weight (100, 100) | mean +0.000007 | std 7.453867e-03 | grad:data ratio 4.459023e-02
weight (100, 100) | mean -0.000043 | std 6.202964e-03 | grad:data ratio 3.678501e-02
weight (100, 100) | mean +0.000010 | std 5.674492e-03 | grad:data ratio 3.381422e-02
weight (100, 100) | mean +0.000030 | std 5.529360e-03 | grad:data ratio 3.293567e-02
weight  (100, 27) | mean +0.000064 | std 1.152379e-02 | grad:data ratio 6.914081e-02
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "de751e0bc3ec40a7bf50e86a07e1a0cc", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "9b8387007985401fa7f0c015038c1fa7", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "6309aa034ad24661864761344fb4b71b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>which of course look very good. And they are necessarily going to look good because now before every single <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> layer there’s a <strong>batchnorm</strong> happening. This yields a saturation of ~<span class="math notranslate nohighlight">\(2\%\)</span> and roughly equal std across all layers and everything looks very homogeneous in the activations distribution, with the gradient and weight gradient distributions also looking great. Also, the updates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "116940beed29420babd2ee4bf7c5cdc0", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>also look pretty reasonable, with all parameters training at around roughly the same rate. Now, what we have gained is that we can now be slightly less brittle with respect to the gain values of the weights. Meaning, that if for example we make the gain be <span class="math notranslate nohighlight">\(0.2\)</span> (much lower than the default <span class="math notranslate nohighlight">\(tanh\)</span> gain of <span class="math notranslate nohighlight">\(5/3\)</span>) and then train and print the same distributions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_batchnorm_last_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_gain</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>47024
      0/ 200000: 3.3041
layer 2 (      Tanh): mean -0.01, std 0.64, saturated: 3.00%
layer 5 (      Tanh): mean +0.00, std 0.65, saturated: 2.09%
layer 8 (      Tanh): mean -0.00, std 0.65, saturated: 1.38%
layer 11 (      Tanh): mean -0.00, std 0.66, saturated: 0.81%
layer 14 (      Tanh): mean -0.00, std 0.67, saturated: 0.72%
layer 2 (      Tanh): mean +0.000000, std 1.356849e-03
layer 5 (      Tanh): mean +0.000000, std 1.092489e-03
layer 8 (      Tanh): mean -0.000000, std 1.002747e-03
layer 11 (      Tanh): mean +0.000000, std 1.016688e-03
layer 14 (      Tanh): mean -0.000000, std 1.143892e-03
weight   (27, 10) | mean -0.000000 | std 8.272509e-03 | grad:data ratio 8.264745e-03
weight  (30, 100) | mean +0.000205 | std 1.605308e-02 | grad:data ratio 2.640624e-01
weight (100, 100) | mean -0.000010 | std 7.176930e-03 | grad:data ratio 2.413751e-01
weight (100, 100) | mean -0.000048 | std 6.421504e-03 | grad:data ratio 2.342190e-01
weight (100, 100) | mean -0.000007 | std 6.354468e-03 | grad:data ratio 2.399155e-01
weight (100, 100) | mean +0.000019 | std 6.318578e-03 | grad:data ratio 2.404839e-01
weight  (100, 27) | mean -0.000009 | std 1.378118e-02 | grad:data ratio 3.142304e-01
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d6de78cf046a492396da4a64544620da", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "e23c3ad52eb14df387903ce6b4748031", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "253447916936472ab2942f0117fc00c9", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>They will all look pretty ok and unaffected! However, if we plot the updates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "10a28ea1308441f380026200cff8f029", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>we see that these do in fact change. And so even though the forward and backward pass to a very large extent look okay because of the backward pass of the <strong>batchnorm</strong> and how the specifically the scale of the incoming activations interacts in the <strong>batchnorm</strong> and its backward pass, the decrease of the gain is actually changing the scale of the updates on these parameters. So, the gradients on these weights are affected. So, we still don’t get a completely free pass to pass any arbitrary weight gain, but everything else is significantly more robust in terms of the forward and backward passes and the weight gradients. It’s just that you may in such a case need to retune the learning rate if you are changing sufficiently the scale of the activations that are coming into the <strong>batchnorm</strong> layers. To verify this, we can see how making the gain to a greater value like <span class="math notranslate nohighlight">\(5.0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_batchnorm_last_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_gain</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>47024
      0/ 200000: 3.2738
layer 2 (      Tanh): mean -0.01, std 0.63, saturated: 2.84%
layer 5 (      Tanh): mean -0.00, std 0.63, saturated: 2.44%
layer 8 (      Tanh): mean +0.00, std 0.64, saturated: 2.31%
layer 11 (      Tanh): mean -0.00, std 0.64, saturated: 2.09%
layer 14 (      Tanh): mean +0.00, std 0.64, saturated: 2.72%
layer 2 (      Tanh): mean -0.000000, std 2.696700e-03
layer 5 (      Tanh): mean -0.000000, std 2.428798e-03
layer 8 (      Tanh): mean +0.000000, std 2.221820e-03
layer 11 (      Tanh): mean +0.000000, std 2.076553e-03
layer 14 (      Tanh): mean +0.000000, std 1.946961e-03
weight   (27, 10) | mean -0.000000 | std 6.479451e-03 | grad:data ratio 6.473200e-03
weight  (30, 100) | mean -0.000037 | std 2.023350e-03 | grad:data ratio 2.162614e-03
weight (100, 100) | mean +0.000024 | std 2.014056e-03 | grad:data ratio 4.040985e-03
weight (100, 100) | mean -0.000014 | std 1.705210e-03 | grad:data ratio 3.385467e-03
weight (100, 100) | mean +0.000003 | std 1.588601e-03 | grad:data ratio 3.167232e-03
weight (100, 100) | mean +0.000005 | std 1.499648e-03 | grad:data ratio 2.987035e-03
weight  (100, 27) | mean +0.000037 | std 2.765074e-03 | grad:data ratio 5.592591e-03
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e163bac57c0a4d4bbd186748f456149c", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "cb4fa5f26b6047ec81dc7d0bc25682a5", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a925269c8ca64634b613424645b2e64a", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "0540cd5f274f48c08f5bfe57c45330f3", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>now causes the updates to come out lower, as a result. Finally let’s now remove the weight gain by setting it to <span class="math notranslate nohighlight">\(1.0\)</span> notice that with <strong>batchnorm</strong> enabled we can now also skip the <code class="docutils literal notranslate"><span class="pre">fan_in</span></code> normalization at initialization. So, like we did before, if we define our <strong>nn</strong> by sampling the initial weights from a plain Gaussian:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span>
    <span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_batchnorm_last_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_gain</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
<span class="n">revert_fan_in_normalization</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>47024
      0/ 200000: 3.2920
layer 2 (      Tanh): mean -0.00, std 0.64, saturated: 2.78%
layer 5 (      Tanh): mean -0.01, std 0.64, saturated: 1.97%
layer 8 (      Tanh): mean -0.00, std 0.64, saturated: 2.53%
layer 11 (      Tanh): mean -0.00, std 0.64, saturated: 2.25%
layer 14 (      Tanh): mean +0.00, std 0.63, saturated: 2.59%
layer 2 (      Tanh): mean -0.000000, std 3.312148e-03
layer 5 (      Tanh): mean -0.000000, std 3.053182e-03
layer 8 (      Tanh): mean +0.000000, std 2.762014e-03
layer 11 (      Tanh): mean -0.000000, std 2.388142e-03
layer 14 (      Tanh): mean -0.000000, std 2.036850e-03
weight   (27, 10) | mean +0.000000 | std 5.950560e-03 | grad:data ratio 5.945425e-03
weight  (30, 100) | mean +0.000005 | std 2.227116e-03 | grad:data ratio 2.173013e-03
weight (100, 100) | mean -0.000008 | std 1.128165e-03 | grad:data ratio 1.131871e-03
weight (100, 100) | mean +0.000001 | std 9.868351e-04 | grad:data ratio 9.796767e-04
weight (100, 100) | mean +0.000009 | std 8.436788e-04 | grad:data ratio 8.410787e-04
weight (100, 100) | mean +0.000007 | std 7.515551e-04 | grad:data ratio 7.485138e-04
weight  (100, 27) | mean -0.000007 | std 1.182972e-03 | grad:data ratio 1.196482e-03
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8f2c91bd3a664b709e0a51cbb04d9f20", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "4f680bfb726242d7a873af0ea83c8d5b", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "9673baae9300458bbc3d7f9de36bb8b5", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a65a0948e2fc46d6991041333e6e5b89", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>everything looks pretty much ok. But from the update plot you can see that everything looks below <code class="docutils literal notranslate"><span class="pre">1e-3</span></code>, so we would have to bump up the learning rate in order to make sure that we are training more properly. Intuitively, we would probably need to 10x the learning rate from <span class="math notranslate nohighlight">\(0.1\)</span> (default) to <span class="math notranslate nohighlight">\(1.0\)</span>. Let’s try it out:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span>
    <span class="n">batchnorm_enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_batchnorm_last_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_gain</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
<span class="n">revert_fan_in_normalization</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">lossi</span><span class="p">,</span> <span class="n">ud</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span>
    <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">initial_lr</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">visualize_layer_values</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualize_weight_gradients</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">visualize_update_ratios</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">ud</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>47024
      0/ 200000: 3.2936
layer 2 (      Tanh): mean -0.00, std 0.63, saturated: 4.19%
layer 5 (      Tanh): mean -0.02, std 0.63, saturated: 3.72%
layer 8 (      Tanh): mean +0.00, std 0.64, saturated: 3.56%
layer 11 (      Tanh): mean -0.02, std 0.64, saturated: 3.41%
layer 14 (      Tanh): mean +0.00, std 0.64, saturated: 2.78%
layer 2 (      Tanh): mean +0.000000, std 3.989876e-03
layer 5 (      Tanh): mean +0.000000, std 3.745467e-03
layer 8 (      Tanh): mean +0.000000, std 3.609461e-03
layer 11 (      Tanh): mean +0.000000, std 3.665544e-03
layer 14 (      Tanh): mean +0.000000, std 3.469911e-03
weight   (27, 10) | mean -0.000000 | std 8.855599e-03 | grad:data ratio 8.541719e-03
weight  (30, 100) | mean +0.000016 | std 2.512573e-03 | grad:data ratio 2.443840e-03
weight (100, 100) | mean +0.000017 | std 1.302984e-03 | grad:data ratio 1.306176e-03
weight (100, 100) | mean +0.000009 | std 1.208597e-03 | grad:data ratio 1.199037e-03
weight (100, 100) | mean +0.000001 | std 1.189904e-03 | grad:data ratio 1.185508e-03
weight (100, 100) | mean +0.000007 | std 1.228369e-03 | grad:data ratio 1.222727e-03
weight  (100, 27) | mean +0.000006 | std 2.232471e-03 | grad:data ratio 2.253838e-03
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a112f6ced1d049f68592e9946fdf2128", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "d78263a2b02d491586587ab7a5103077", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "900dc521dff840a4bf951c331c4d9332", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "2a6531da7da4487dbd0c28415341395f", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>everything again looks good and voilà! Now our updates are more reasonable. So, long story short, with <strong>barchnorm</strong>, we are now significantly more robust to the gain of these linear layers, whether or not we have to apply the <code class="docutils literal notranslate"><span class="pre">fan_in</span></code> normalization, with the caveat (in terms of the former) that we do have to worry about the update scales and making sure that the learning rate is properly calibrated here. So, the forward and backward pass statistics are all looking significantly more behaved, except for the scales of the updates that should be taken into consideration.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Ok, so now let’s summarize (again, lol). There are three things this section was intended to teach:</p>
<ol class="arabic simple">
<li><p>introducing you to <strong>batchnorm</strong>, which is one of the first modern innovations that helped stabilize very deep <strong>nn</strong>s and their training</p></li>
<li><p>PyTorch-ifying some of our code by wrapping it up into layer modules (<code class="docutils literal notranslate"><span class="pre">Linear</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchNorm1D</span></code>, <code class="docutils literal notranslate"><span class="pre">Tanh</span></code>, etc.) that can be stacked up into <strong>nn</strong> like lego building blocks. Since these synonymous layers exist as objects in the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> API, the way we have constructed it, we could easily replace each one of our custom modules (<code class="docutils literal notranslate"><span class="pre">Linear</span></code> with <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and so on) and everything would probably work just fine.</p></li>
<li><p>present you with the diagnostic tools that you would use to understand whether your <strong>nn</strong> is in a good state dynamically. This means looking at histograms of the forward pass activations and backward pass gradients. And then also the weights that are going to be activated as part of stochastic gradient descent by looking at their means, stds and also the gradient-to-data ratios or even better, the update-to-data ratios. And we saw that what people usually do is look at the evolution of these update-to-data ratios, instead of single step snapshots frozen in time, and make sure everything looks fine. In particular, we highlighted that around <code class="docutils literal notranslate"><span class="pre">1e-3</span></code> (<span class="math notranslate nohighlight">\(-3\)</span> on the log scale) is a good rough heuristic of what you want this ratio to be and if it’s way too high, then probably the learning rate is a little too big. Whereas, if it’s too small, then the learning rate is probably too small. So, these are the things that you might want to play with when you want to get your <strong>nn</strong> to work very well.</p></li>
</ol>
<p>Now, there are certain things we did not try to achieve in this lesson. As an example, we did not try to beat the performace from our previous lessons. If we do actually try to, by using <strong>batchnorm</strong> layers (by using the learning rate finding mechanism described in the previous lesson), we would end up with results that are very very similar to the ones that we obtained before. And in that case, that would be because our performance now is not bottlenecked by the optimization, which is what <strong>batchnorm</strong> is helping with. But, the performance in actually most likely bottlenecked by the context length we are choosing as our context. Currently we are taking in <span class="math notranslate nohighlight">\(3\)</span> characters in order to predict the <span class="math notranslate nohighlight">\(4th\)</span> one. To go beyond that, we would need to look at more powerful architectures, like RNNs and Transformers, in order to further push the log probabilities that we’re achieving on this dataset. Also, we did not give a full explanation of all of these activations and the gradients (e.g. from the backward pass or the weights). Maybe you found those parts slightly unintuitive and maybe you’re slightly confused about: okay, if I change the gain, how come that we need a different learning rate? And the reason we didn’t go into full detail to make such questions clearer is because we’d have to actually look at the backward pass of <em>all</em> these different layers and get an intuitive understanding of how that works, and so we did not go into that in this lesson. The purpose really was just to introduce you to the diagnostic tools and what they look like. But of course there’s still a lot of work remaining on the intuitive level to understand the initialization, the backward pass and how all of these interact.</p>
</section>
<section id="outro">
<h2>Outro<a class="headerlink" href="#outro" title="Link to this heading">#</a></h2>
<p>We certainly haven’t <em>solved</em> initialization, nor have we <em>solved</em> <strong>backprop</strong>, or anything of that sorts. These are still very much an active area of research with lots of people trying to figure out what the best way is to initialize these networks, what is the best update rule to use and so on. So none of all this is <em>solved</em> and we don’t really have all the answers to all these cases but at least we are making progress and at least we have some tools to tell us whether or not things are on the right track, for now. So, all in all, we have made progress in this lesson and I hope you enjoyed it. See you in the next lesson!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./micrograduate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="makemore2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">3. <strong>makemore</strong> (part 2): mlp</p>
      </div>
    </a>
    <a class="right-next"
       href="makemore4.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rebuilding-mlp">Rebuilding <strong>mlp</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-bad-weights">Dealing with bad weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dealing-with-dead-neurons">Dealing with dead neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-to-set-the-factors">Learning to set the factors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batchnorm">Batchnorm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">ResNet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-summary">Interim summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchification"><strong>torch</strong>ification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-activations-and-gradients">Visualizing activations and gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-visualization">Further visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christos Karaneen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>