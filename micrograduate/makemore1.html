
<!DOCTYPE html>


<html lang="en" data-content_root="../" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2. makemore (part 1): implementing a bigram character-level language model &#8212; microgra‚àáuate</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/hide_theme_switch_button.css?v=c72df8c4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'micrograduate/makemore1';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. makemore (part 2): mlp" href="makemore2.html" />
    <link rel="prev" title="1. micrograd: implementing an autograd engine" href="micrograd.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
    
    <img src="../_static/book_logo.png" class="logo__image only-light" alt="microgra‚àáuate - Home"/>
    <script>document.write(`<img src="../_static/book_logo.png" class="logo__image only-dark" alt="microgra‚àáuate - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    ü§î Description ‚ÑπÔ∏è
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="micrograd.html">1. <strong>micrograd</strong>: implementing an autograd engine</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore2.html">3. <strong>makemore</strong> (part 2): mlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore3.html">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore4.html">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore5.html">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="picogpt.html">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate/issues/new?title=Issue%20on%20page%20%2Fmicrograduate/makemore1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/micrograduate/makemore1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>2. makemore (part 1): implementing a bigram character-level language model</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-bigram-language-model">Building a bigram language model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#casting-the-model-as-a-nn">Casting the model as a <strong>nn</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="makemore-part-1-implementing-a-bigram-character-level-language-model">
<h1>2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model<a class="headerlink" href="#makemore-part-1-implementing-a-bigram-character-level-language-model" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cloning repo...&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>--quiet<span class="w"> </span>https://github.com/ckarageorgkaneen/micrograduate.git<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
    <span class="o">%</span><span class="k">cd</span> micrograduate
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Installing requirements...&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>uv
    <span class="o">!</span>uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--system<span class="w"> </span>--quiet<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</div>
</div>
<section id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Link to this heading">#</a></h2>
<p>Just like <a class="reference internal" href="#1.-micrograd"><span class="xref myst">micrograd</span></a> before it, here, <em>step-by-step</em> with everything <em>spelled-out</em>, we will build <strong>makemore</strong>: a bigram character-level language model. We‚Äôre going to build it out slowly and together! But what is <strong>makemore</strong>? As the name suggests, <strong>makemore</strong> <em>makes more</em> of things that you give it. <a class="reference download internal" download="" href="../_downloads/da0c0f8f1685ed3dfb3d2dcb79ebe440/names.txt"><span class="xref download myst">names.txt</span></a> is an example dataset. Specifically, it is a very large list of different names. If you train <strong>makemore</strong> on this dataset, it will learn to <em>make more</em> of name-like things, basically more unique names! So, maybe if you have a baby and you‚Äôre looking for a new, cool-sounding unique name, <strong>makemore</strong> might help you. Here are some examples of such names that the <strong>makemore</strong> will be able to generate:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>dontell
khylum
camatena
aeriline
najlah
sherrith
ryel
irmi
taislee
mortaz
akarli
maxfelynn
biolett
zendy
laisa
halliliana
goralynn
brodynn
romima
chiyomin
loghlyn
melichae
mahmed
irot
helicha
besdy
ebokun
lucianno
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">dontell</span></code>, <code class="docutils literal notranslate"><span class="pre">irot</span></code>, <code class="docutils literal notranslate"><span class="pre">zendy</span></code>, and so on, you name it! So under the hood, <strong>makemore</strong> is a character-level language model. That means that it‚Äôs treating every single line (i.e. name) of <a class="reference download internal" download="" href="../_downloads/da0c0f8f1685ed3dfb3d2dcb79ebe440/names.txt"><span class="xref download myst">its training dataset</span></a> as an example. And each example is treated as a sequence of individual characters. For instance, it treats the name <code class="docutils literal notranslate"><span class="pre">reese</span></code> as the sequence of characters: <code class="docutils literal notranslate"><span class="pre">r</span></code>, <code class="docutils literal notranslate"><span class="pre">e</span></code>, <code class="docutils literal notranslate"><span class="pre">e</span></code>, <code class="docutils literal notranslate"><span class="pre">s</span></code>, <code class="docutils literal notranslate"><span class="pre">e</span></code>. That is the level on which we are building out <strong>makemore</strong>. Basically, its purpose is this: given a character, it can predict the next character in the sequence based upon the names that it has seen so far. Now, we‚Äôre actually going to implement a large number of character-level language models, following a few key innovations:</p>
<ul class="simple">
<li><p>Bigram (one character predicts the next one with a lookup table of counts)</p></li>
<li><p>MLP, following <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et al. 2003</a></p></li>
<li><p>CNN, following <a class="reference external" href="https://arxiv.org/abs/1609.03499">DeepMind WaveNet 2016</a> (in progress‚Ä¶)</p></li>
<li><p>RNN, following <a class="reference external" href="https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Mikolov et al. 2010</a></p></li>
<li><p>LSTM, following <a class="reference external" href="https://arxiv.org/abs/1308.0850">Graves et al. 2014</a></p></li>
<li><p>GRU, following <a class="reference external" href="https://arxiv.org/abs/1409.1259">Kyunghyun Cho et al. 2014</a></p></li>
<li><p>Transformer, following <a class="reference external" href="https://arxiv.org/abs/1706.03762">Vaswani et al. 2017</a></p></li>
</ul>
<p>In fact, the transformer we are going to build will be the equivalent of <a class="reference external" href="https://en.wikipedia.org/wiki/GPT-2">GPT-2</a>. Kind of a big deal, since it‚Äôs a modern network and by the end of this guide you‚Äôll actually understand how it works at the level of characters. Later on, we will probably spend some time on the word level, so we can generate documents of words, not just segments of characters. And then we‚Äôre probably going to go into image and image-text networks such as <a class="reference external" href="https://en.wikipedia.org/wiki/DALL-E">DALL-E</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a>, and so on. But first, let‚Äôs jump into character-level modeling.</p>
</section>
<section id="building-a-bigram-language-model">
<h2>Building a bigram language model<a class="headerlink" href="#building-a-bigram-language-model" title="Link to this heading">#</a></h2>
<p>Let‚Äôs start by reading all the names into a list:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;names.txt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;emma&#39;,
 &#39;olivia&#39;,
 &#39;ava&#39;,
 &#39;isabella&#39;,
 &#39;sophia&#39;,
 &#39;charlotte&#39;,
 &#39;mia&#39;,
 &#39;amelia&#39;,
 &#39;harper&#39;,
 &#39;evelyn&#39;]
</pre></div>
</div>
</div>
</div>
<p>Now, we want to learn a bit more about this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32033
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">))</span>  <span class="c1"># shortest</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">))</span>  <span class="c1"># longest</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs think through our very first language model. A character-level language model is predicting the next character in the sequence given already some concrete sequence of characters before it. What we have to realize here is that every single word like <code class="docutils literal notranslate"><span class="pre">isabella</span></code> is actually quite a few examples packed in that single word. Because, let‚Äôs think: what is a word telling us really? It‚Äôs saying that the character <code class="docutils literal notranslate"><span class="pre">i</span></code> is a very likely character to come first in the sequence that constitutes a name. The character <code class="docutils literal notranslate"><span class="pre">s</span></code> is likely to come after <code class="docutils literal notranslate"><span class="pre">i</span></code>, the character <code class="docutils literal notranslate"><span class="pre">a</span></code> is likely to come after <code class="docutils literal notranslate"><span class="pre">is</span></code>, the character <code class="docutils literal notranslate"><span class="pre">b</span></code> is likely to come after <code class="docutils literal notranslate"><span class="pre">isa</span></code>, and so on all the way to <code class="docutils literal notranslate"><span class="pre">a</span></code> following <code class="docutils literal notranslate"><span class="pre">isabell</span></code>. And then there‚Äôs one more important piece of information in here. And that is that after <code class="docutils literal notranslate"><span class="pre">isabella</span></code>, the word is very likely to end. So, time to build our first network: a bigram language model. In these, we are working with two characters at a time. So, we are only looking for one character we are given and we are trying to predict the next character in a sequence. For example, in the name <code class="docutils literal notranslate"><span class="pre">charlotte</span></code>, we ask: what characters are likely to follow <code class="docutils literal notranslate"><span class="pre">r</span></code>?  In the name <code class="docutils literal notranslate"><span class="pre">sophia</span></code>: we ask what characters are likely to follow <code class="docutils literal notranslate"><span class="pre">p</span></code>? And so on. This mean we are just modeling that local structure. Meaning, we only look at the previous character, even though there might be a lot of useful information before it. This is a very simple model, which is why it‚Äôs a great place to start! We can learn about the statistics of which characters are likely to follow which other characters by counting. So by iterating over all names, we can count how often each consecutive pair (bigram) of characters appears.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">bigram</span> <span class="o">=</span> <span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
        <span class="n">b</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that we have also added the character <code class="docutils literal notranslate"><span class="pre">.</span></code> to signify the start and end of each word. And obviously, the variable <code class="docutils literal notranslate"><span class="pre">b</span></code> now holds the statistics of the entire dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">sorted</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">tup</span><span class="p">:</span> <span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[((&#39;n&#39;, &#39;.&#39;), 6763),
 ((&#39;a&#39;, &#39;.&#39;), 6640),
 ((&#39;a&#39;, &#39;n&#39;), 5438),
 ((&#39;.&#39;, &#39;a&#39;), 4410),
 ((&#39;e&#39;, &#39;.&#39;), 3983),
 ((&#39;a&#39;, &#39;r&#39;), 3264),
 ((&#39;e&#39;, &#39;l&#39;), 3248),
 ((&#39;r&#39;, &#39;i&#39;), 3033),
 ((&#39;n&#39;, &#39;a&#39;), 2977),
 ((&#39;.&#39;, &#39;k&#39;), 2963),
 ((&#39;l&#39;, &#39;e&#39;), 2921),
 ((&#39;e&#39;, &#39;n&#39;), 2675),
 ((&#39;l&#39;, &#39;a&#39;), 2623),
 ((&#39;m&#39;, &#39;a&#39;), 2590),
 ((&#39;.&#39;, &#39;m&#39;), 2538),
 ((&#39;a&#39;, &#39;l&#39;), 2528),
 ((&#39;i&#39;, &#39;.&#39;), 2489),
 ((&#39;l&#39;, &#39;i&#39;), 2480),
 ((&#39;i&#39;, &#39;a&#39;), 2445),
 ((&#39;.&#39;, &#39;j&#39;), 2422),
 ((&#39;o&#39;, &#39;n&#39;), 2411),
 ((&#39;h&#39;, &#39;.&#39;), 2409),
 ((&#39;r&#39;, &#39;a&#39;), 2356),
 ((&#39;a&#39;, &#39;h&#39;), 2332),
 ((&#39;h&#39;, &#39;a&#39;), 2244),
 ((&#39;y&#39;, &#39;a&#39;), 2143),
 ((&#39;i&#39;, &#39;n&#39;), 2126),
 ((&#39;.&#39;, &#39;s&#39;), 2055),
 ((&#39;a&#39;, &#39;y&#39;), 2050),
 ((&#39;y&#39;, &#39;.&#39;), 2007),
 ((&#39;e&#39;, &#39;r&#39;), 1958),
 ((&#39;n&#39;, &#39;n&#39;), 1906),
 ((&#39;y&#39;, &#39;n&#39;), 1826),
 ((&#39;k&#39;, &#39;a&#39;), 1731),
 ((&#39;n&#39;, &#39;i&#39;), 1725),
 ((&#39;r&#39;, &#39;e&#39;), 1697),
 ((&#39;.&#39;, &#39;d&#39;), 1690),
 ((&#39;i&#39;, &#39;e&#39;), 1653),
 ((&#39;a&#39;, &#39;i&#39;), 1650),
 ((&#39;.&#39;, &#39;r&#39;), 1639),
 ((&#39;a&#39;, &#39;m&#39;), 1634),
 ((&#39;l&#39;, &#39;y&#39;), 1588),
 ((&#39;.&#39;, &#39;l&#39;), 1572),
 ((&#39;.&#39;, &#39;c&#39;), 1542),
 ((&#39;.&#39;, &#39;e&#39;), 1531),
 ((&#39;j&#39;, &#39;a&#39;), 1473),
 ((&#39;r&#39;, &#39;.&#39;), 1377),
 ((&#39;n&#39;, &#39;e&#39;), 1359),
 ((&#39;l&#39;, &#39;l&#39;), 1345),
 ((&#39;i&#39;, &#39;l&#39;), 1345),
 ((&#39;i&#39;, &#39;s&#39;), 1316),
 ((&#39;l&#39;, &#39;.&#39;), 1314),
 ((&#39;.&#39;, &#39;t&#39;), 1308),
 ((&#39;.&#39;, &#39;b&#39;), 1306),
 ((&#39;d&#39;, &#39;a&#39;), 1303),
 ((&#39;s&#39;, &#39;h&#39;), 1285),
 ((&#39;d&#39;, &#39;e&#39;), 1283),
 ((&#39;e&#39;, &#39;e&#39;), 1271),
 ((&#39;m&#39;, &#39;i&#39;), 1256),
 ((&#39;s&#39;, &#39;a&#39;), 1201),
 ((&#39;s&#39;, &#39;.&#39;), 1169),
 ((&#39;.&#39;, &#39;n&#39;), 1146),
 ((&#39;a&#39;, &#39;s&#39;), 1118),
 ((&#39;y&#39;, &#39;l&#39;), 1104),
 ((&#39;e&#39;, &#39;y&#39;), 1070),
 ((&#39;o&#39;, &#39;r&#39;), 1059),
 ((&#39;a&#39;, &#39;d&#39;), 1042),
 ((&#39;t&#39;, &#39;a&#39;), 1027),
 ((&#39;.&#39;, &#39;z&#39;), 929),
 ((&#39;v&#39;, &#39;i&#39;), 911),
 ((&#39;k&#39;, &#39;e&#39;), 895),
 ((&#39;s&#39;, &#39;e&#39;), 884),
 ((&#39;.&#39;, &#39;h&#39;), 874),
 ((&#39;r&#39;, &#39;o&#39;), 869),
 ((&#39;e&#39;, &#39;s&#39;), 861),
 ((&#39;z&#39;, &#39;a&#39;), 860),
 ((&#39;o&#39;, &#39;.&#39;), 855),
 ((&#39;i&#39;, &#39;r&#39;), 849),
 ((&#39;b&#39;, &#39;r&#39;), 842),
 ((&#39;a&#39;, &#39;v&#39;), 834),
 ((&#39;m&#39;, &#39;e&#39;), 818),
 ((&#39;e&#39;, &#39;i&#39;), 818),
 ((&#39;c&#39;, &#39;a&#39;), 815),
 ((&#39;i&#39;, &#39;y&#39;), 779),
 ((&#39;r&#39;, &#39;y&#39;), 773),
 ((&#39;e&#39;, &#39;m&#39;), 769),
 ((&#39;s&#39;, &#39;t&#39;), 765),
 ((&#39;h&#39;, &#39;i&#39;), 729),
 ((&#39;t&#39;, &#39;e&#39;), 716),
 ((&#39;n&#39;, &#39;d&#39;), 704),
 ((&#39;l&#39;, &#39;o&#39;), 692),
 ((&#39;a&#39;, &#39;e&#39;), 692),
 ((&#39;a&#39;, &#39;t&#39;), 687),
 ((&#39;s&#39;, &#39;i&#39;), 684),
 ((&#39;e&#39;, &#39;a&#39;), 679),
 ((&#39;d&#39;, &#39;i&#39;), 674),
 ((&#39;h&#39;, &#39;e&#39;), 674),
 ((&#39;.&#39;, &#39;g&#39;), 669),
 ((&#39;t&#39;, &#39;o&#39;), 667),
 ((&#39;c&#39;, &#39;h&#39;), 664),
 ((&#39;b&#39;, &#39;e&#39;), 655),
 ((&#39;t&#39;, &#39;h&#39;), 647),
 ((&#39;v&#39;, &#39;a&#39;), 642),
 ((&#39;o&#39;, &#39;l&#39;), 619),
 ((&#39;.&#39;, &#39;i&#39;), 591),
 ((&#39;i&#39;, &#39;o&#39;), 588),
 ((&#39;e&#39;, &#39;t&#39;), 580),
 ((&#39;v&#39;, &#39;e&#39;), 568),
 ((&#39;a&#39;, &#39;k&#39;), 568),
 ((&#39;a&#39;, &#39;a&#39;), 556),
 ((&#39;c&#39;, &#39;e&#39;), 551),
 ((&#39;a&#39;, &#39;b&#39;), 541),
 ((&#39;i&#39;, &#39;t&#39;), 541),
 ((&#39;.&#39;, &#39;y&#39;), 535),
 ((&#39;t&#39;, &#39;i&#39;), 532),
 ((&#39;s&#39;, &#39;o&#39;), 531),
 ((&#39;m&#39;, &#39;.&#39;), 516),
 ((&#39;d&#39;, &#39;.&#39;), 516),
 ((&#39;.&#39;, &#39;p&#39;), 515),
 ((&#39;i&#39;, &#39;c&#39;), 509),
 ((&#39;k&#39;, &#39;i&#39;), 509),
 ((&#39;o&#39;, &#39;s&#39;), 504),
 ((&#39;n&#39;, &#39;o&#39;), 496),
 ((&#39;t&#39;, &#39;.&#39;), 483),
 ((&#39;j&#39;, &#39;o&#39;), 479),
 ((&#39;u&#39;, &#39;s&#39;), 474),
 ((&#39;a&#39;, &#39;c&#39;), 470),
 ((&#39;n&#39;, &#39;y&#39;), 465),
 ((&#39;e&#39;, &#39;v&#39;), 463),
 ((&#39;s&#39;, &#39;s&#39;), 461),
 ((&#39;m&#39;, &#39;o&#39;), 452),
 ((&#39;i&#39;, &#39;k&#39;), 445),
 ((&#39;n&#39;, &#39;t&#39;), 443),
 ((&#39;i&#39;, &#39;d&#39;), 440),
 ((&#39;j&#39;, &#39;e&#39;), 440),
 ((&#39;a&#39;, &#39;z&#39;), 435),
 ((&#39;i&#39;, &#39;g&#39;), 428),
 ((&#39;i&#39;, &#39;m&#39;), 427),
 ((&#39;r&#39;, &#39;r&#39;), 425),
 ((&#39;d&#39;, &#39;r&#39;), 424),
 ((&#39;.&#39;, &#39;f&#39;), 417),
 ((&#39;u&#39;, &#39;r&#39;), 414),
 ((&#39;r&#39;, &#39;l&#39;), 413),
 ((&#39;y&#39;, &#39;s&#39;), 401),
 ((&#39;.&#39;, &#39;o&#39;), 394),
 ((&#39;e&#39;, &#39;d&#39;), 384),
 ((&#39;a&#39;, &#39;u&#39;), 381),
 ((&#39;c&#39;, &#39;o&#39;), 380),
 ((&#39;k&#39;, &#39;y&#39;), 379),
 ((&#39;d&#39;, &#39;o&#39;), 378),
 ((&#39;.&#39;, &#39;v&#39;), 376),
 ((&#39;t&#39;, &#39;t&#39;), 374),
 ((&#39;z&#39;, &#39;e&#39;), 373),
 ((&#39;z&#39;, &#39;i&#39;), 364),
 ((&#39;k&#39;, &#39;.&#39;), 363),
 ((&#39;g&#39;, &#39;h&#39;), 360),
 ((&#39;t&#39;, &#39;r&#39;), 352),
 ((&#39;k&#39;, &#39;o&#39;), 344),
 ((&#39;t&#39;, &#39;y&#39;), 341),
 ((&#39;g&#39;, &#39;e&#39;), 334),
 ((&#39;g&#39;, &#39;a&#39;), 330),
 ((&#39;l&#39;, &#39;u&#39;), 324),
 ((&#39;b&#39;, &#39;a&#39;), 321),
 ((&#39;d&#39;, &#39;y&#39;), 317),
 ((&#39;c&#39;, &#39;k&#39;), 316),
 ((&#39;.&#39;, &#39;w&#39;), 307),
 ((&#39;k&#39;, &#39;h&#39;), 307),
 ((&#39;u&#39;, &#39;l&#39;), 301),
 ((&#39;y&#39;, &#39;e&#39;), 301),
 ((&#39;y&#39;, &#39;r&#39;), 291),
 ((&#39;m&#39;, &#39;y&#39;), 287),
 ((&#39;h&#39;, &#39;o&#39;), 287),
 ((&#39;w&#39;, &#39;a&#39;), 280),
 ((&#39;s&#39;, &#39;l&#39;), 279),
 ((&#39;n&#39;, &#39;s&#39;), 278),
 ((&#39;i&#39;, &#39;z&#39;), 277),
 ((&#39;u&#39;, &#39;n&#39;), 275),
 ((&#39;o&#39;, &#39;u&#39;), 275),
 ((&#39;n&#39;, &#39;g&#39;), 273),
 ((&#39;y&#39;, &#39;d&#39;), 272),
 ((&#39;c&#39;, &#39;i&#39;), 271),
 ((&#39;y&#39;, &#39;o&#39;), 271),
 ((&#39;i&#39;, &#39;v&#39;), 269),
 ((&#39;e&#39;, &#39;o&#39;), 269),
 ((&#39;o&#39;, &#39;m&#39;), 261),
 ((&#39;r&#39;, &#39;u&#39;), 252),
 ((&#39;f&#39;, &#39;a&#39;), 242),
 ((&#39;b&#39;, &#39;i&#39;), 217),
 ((&#39;s&#39;, &#39;y&#39;), 215),
 ((&#39;n&#39;, &#39;c&#39;), 213),
 ((&#39;h&#39;, &#39;y&#39;), 213),
 ((&#39;p&#39;, &#39;a&#39;), 209),
 ((&#39;r&#39;, &#39;t&#39;), 208),
 ((&#39;q&#39;, &#39;u&#39;), 206),
 ((&#39;p&#39;, &#39;h&#39;), 204),
 ((&#39;h&#39;, &#39;r&#39;), 204),
 ((&#39;j&#39;, &#39;u&#39;), 202),
 ((&#39;g&#39;, &#39;r&#39;), 201),
 ((&#39;p&#39;, &#39;e&#39;), 197),
 ((&#39;n&#39;, &#39;l&#39;), 195),
 ((&#39;y&#39;, &#39;i&#39;), 192),
 ((&#39;g&#39;, &#39;i&#39;), 190),
 ((&#39;o&#39;, &#39;d&#39;), 190),
 ((&#39;r&#39;, &#39;s&#39;), 190),
 ((&#39;r&#39;, &#39;d&#39;), 187),
 ((&#39;h&#39;, &#39;l&#39;), 185),
 ((&#39;s&#39;, &#39;u&#39;), 185),
 ((&#39;a&#39;, &#39;x&#39;), 182),
 ((&#39;e&#39;, &#39;z&#39;), 181),
 ((&#39;e&#39;, &#39;k&#39;), 178),
 ((&#39;o&#39;, &#39;v&#39;), 176),
 ((&#39;a&#39;, &#39;j&#39;), 175),
 ((&#39;o&#39;, &#39;h&#39;), 171),
 ((&#39;u&#39;, &#39;e&#39;), 169),
 ((&#39;m&#39;, &#39;m&#39;), 168),
 ((&#39;a&#39;, &#39;g&#39;), 168),
 ((&#39;h&#39;, &#39;u&#39;), 166),
 ((&#39;x&#39;, &#39;.&#39;), 164),
 ((&#39;u&#39;, &#39;a&#39;), 163),
 ((&#39;r&#39;, &#39;m&#39;), 162),
 ((&#39;a&#39;, &#39;w&#39;), 161),
 ((&#39;f&#39;, &#39;i&#39;), 160),
 ((&#39;z&#39;, &#39;.&#39;), 160),
 ((&#39;u&#39;, &#39;.&#39;), 155),
 ((&#39;u&#39;, &#39;m&#39;), 154),
 ((&#39;e&#39;, &#39;c&#39;), 153),
 ((&#39;v&#39;, &#39;o&#39;), 153),
 ((&#39;e&#39;, &#39;h&#39;), 152),
 ((&#39;p&#39;, &#39;r&#39;), 151),
 ((&#39;d&#39;, &#39;d&#39;), 149),
 ((&#39;o&#39;, &#39;a&#39;), 149),
 ((&#39;w&#39;, &#39;e&#39;), 149),
 ((&#39;w&#39;, &#39;i&#39;), 148),
 ((&#39;y&#39;, &#39;m&#39;), 148),
 ((&#39;z&#39;, &#39;y&#39;), 147),
 ((&#39;n&#39;, &#39;z&#39;), 145),
 ((&#39;y&#39;, &#39;u&#39;), 141),
 ((&#39;r&#39;, &#39;n&#39;), 140),
 ((&#39;o&#39;, &#39;b&#39;), 140),
 ((&#39;k&#39;, &#39;l&#39;), 139),
 ((&#39;m&#39;, &#39;u&#39;), 139),
 ((&#39;l&#39;, &#39;d&#39;), 138),
 ((&#39;h&#39;, &#39;n&#39;), 138),
 ((&#39;u&#39;, &#39;d&#39;), 136),
 ((&#39;.&#39;, &#39;x&#39;), 134),
 ((&#39;t&#39;, &#39;l&#39;), 134),
 ((&#39;a&#39;, &#39;f&#39;), 134),
 ((&#39;o&#39;, &#39;e&#39;), 132),
 ((&#39;e&#39;, &#39;x&#39;), 132),
 ((&#39;e&#39;, &#39;g&#39;), 125),
 ((&#39;f&#39;, &#39;e&#39;), 123),
 ((&#39;z&#39;, &#39;l&#39;), 123),
 ((&#39;u&#39;, &#39;i&#39;), 121),
 ((&#39;v&#39;, &#39;y&#39;), 121),
 ((&#39;e&#39;, &#39;b&#39;), 121),
 ((&#39;r&#39;, &#39;h&#39;), 121),
 ((&#39;j&#39;, &#39;i&#39;), 119),
 ((&#39;o&#39;, &#39;t&#39;), 118),
 ((&#39;d&#39;, &#39;h&#39;), 118),
 ((&#39;h&#39;, &#39;m&#39;), 117),
 ((&#39;c&#39;, &#39;l&#39;), 116),
 ((&#39;o&#39;, &#39;o&#39;), 115),
 ((&#39;y&#39;, &#39;c&#39;), 115),
 ((&#39;o&#39;, &#39;w&#39;), 114),
 ((&#39;o&#39;, &#39;c&#39;), 114),
 ((&#39;f&#39;, &#39;r&#39;), 114),
 ((&#39;b&#39;, &#39;.&#39;), 114),
 ((&#39;m&#39;, &#39;b&#39;), 112),
 ((&#39;z&#39;, &#39;o&#39;), 110),
 ((&#39;i&#39;, &#39;b&#39;), 110),
 ((&#39;i&#39;, &#39;u&#39;), 109),
 ((&#39;k&#39;, &#39;r&#39;), 109),
 ((&#39;g&#39;, &#39;.&#39;), 108),
 ((&#39;y&#39;, &#39;v&#39;), 106),
 ((&#39;t&#39;, &#39;z&#39;), 105),
 ((&#39;b&#39;, &#39;o&#39;), 105),
 ((&#39;c&#39;, &#39;y&#39;), 104),
 ((&#39;y&#39;, &#39;t&#39;), 104),
 ((&#39;u&#39;, &#39;b&#39;), 103),
 ((&#39;u&#39;, &#39;c&#39;), 103),
 ((&#39;x&#39;, &#39;a&#39;), 103),
 ((&#39;b&#39;, &#39;l&#39;), 103),
 ((&#39;o&#39;, &#39;y&#39;), 103),
 ((&#39;x&#39;, &#39;i&#39;), 102),
 ((&#39;i&#39;, &#39;f&#39;), 101),
 ((&#39;r&#39;, &#39;c&#39;), 99),
 ((&#39;c&#39;, &#39;.&#39;), 97),
 ((&#39;m&#39;, &#39;r&#39;), 97),
 ((&#39;n&#39;, &#39;u&#39;), 96),
 ((&#39;o&#39;, &#39;p&#39;), 95),
 ((&#39;i&#39;, &#39;h&#39;), 95),
 ((&#39;k&#39;, &#39;s&#39;), 95),
 ((&#39;l&#39;, &#39;s&#39;), 94),
 ((&#39;u&#39;, &#39;k&#39;), 93),
 ((&#39;.&#39;, &#39;q&#39;), 92),
 ((&#39;d&#39;, &#39;u&#39;), 92),
 ((&#39;s&#39;, &#39;m&#39;), 90),
 ((&#39;r&#39;, &#39;k&#39;), 90),
 ((&#39;i&#39;, &#39;x&#39;), 89),
 ((&#39;v&#39;, &#39;.&#39;), 88),
 ((&#39;y&#39;, &#39;k&#39;), 86),
 ((&#39;u&#39;, &#39;w&#39;), 86),
 ((&#39;g&#39;, &#39;u&#39;), 85),
 ((&#39;b&#39;, &#39;y&#39;), 83),
 ((&#39;e&#39;, &#39;p&#39;), 83),
 ((&#39;g&#39;, &#39;o&#39;), 83),
 ((&#39;s&#39;, &#39;k&#39;), 82),
 ((&#39;u&#39;, &#39;t&#39;), 82),
 ((&#39;a&#39;, &#39;p&#39;), 82),
 ((&#39;e&#39;, &#39;f&#39;), 82),
 ((&#39;i&#39;, &#39;i&#39;), 82),
 ((&#39;r&#39;, &#39;v&#39;), 80),
 ((&#39;f&#39;, &#39;.&#39;), 80),
 ((&#39;t&#39;, &#39;u&#39;), 78),
 ((&#39;y&#39;, &#39;z&#39;), 78),
 ((&#39;.&#39;, &#39;u&#39;), 78),
 ((&#39;l&#39;, &#39;t&#39;), 77),
 ((&#39;r&#39;, &#39;g&#39;), 76),
 ((&#39;c&#39;, &#39;r&#39;), 76),
 ((&#39;i&#39;, &#39;j&#39;), 76),
 ((&#39;w&#39;, &#39;y&#39;), 73),
 ((&#39;z&#39;, &#39;u&#39;), 73),
 ((&#39;l&#39;, &#39;v&#39;), 72),
 ((&#39;h&#39;, &#39;t&#39;), 71),
 ((&#39;j&#39;, &#39;.&#39;), 71),
 ((&#39;x&#39;, &#39;t&#39;), 70),
 ((&#39;o&#39;, &#39;i&#39;), 69),
 ((&#39;e&#39;, &#39;u&#39;), 69),
 ((&#39;o&#39;, &#39;k&#39;), 68),
 ((&#39;b&#39;, &#39;d&#39;), 65),
 ((&#39;a&#39;, &#39;o&#39;), 63),
 ((&#39;p&#39;, &#39;i&#39;), 61),
 ((&#39;s&#39;, &#39;c&#39;), 60),
 ((&#39;d&#39;, &#39;l&#39;), 60),
 ((&#39;l&#39;, &#39;m&#39;), 60),
 ((&#39;a&#39;, &#39;q&#39;), 60),
 ((&#39;f&#39;, &#39;o&#39;), 60),
 ((&#39;p&#39;, &#39;o&#39;), 59),
 ((&#39;n&#39;, &#39;k&#39;), 58),
 ((&#39;w&#39;, &#39;n&#39;), 58),
 ((&#39;u&#39;, &#39;h&#39;), 58),
 ((&#39;e&#39;, &#39;j&#39;), 55),
 ((&#39;n&#39;, &#39;v&#39;), 55),
 ((&#39;s&#39;, &#39;r&#39;), 55),
 ((&#39;o&#39;, &#39;z&#39;), 54),
 ((&#39;i&#39;, &#39;p&#39;), 53),
 ((&#39;l&#39;, &#39;b&#39;), 52),
 ((&#39;i&#39;, &#39;q&#39;), 52),
 ((&#39;w&#39;, &#39;.&#39;), 51),
 ((&#39;m&#39;, &#39;c&#39;), 51),
 ((&#39;s&#39;, &#39;p&#39;), 51),
 ((&#39;e&#39;, &#39;w&#39;), 50),
 ((&#39;k&#39;, &#39;u&#39;), 50),
 ((&#39;v&#39;, &#39;r&#39;), 48),
 ((&#39;u&#39;, &#39;g&#39;), 47),
 ((&#39;o&#39;, &#39;x&#39;), 45),
 ((&#39;u&#39;, &#39;z&#39;), 45),
 ((&#39;z&#39;, &#39;z&#39;), 45),
 ((&#39;j&#39;, &#39;h&#39;), 45),
 ((&#39;b&#39;, &#39;u&#39;), 45),
 ((&#39;o&#39;, &#39;g&#39;), 44),
 ((&#39;n&#39;, &#39;r&#39;), 44),
 ((&#39;f&#39;, &#39;f&#39;), 44),
 ((&#39;n&#39;, &#39;j&#39;), 44),
 ((&#39;z&#39;, &#39;h&#39;), 43),
 ((&#39;c&#39;, &#39;c&#39;), 42),
 ((&#39;r&#39;, &#39;b&#39;), 41),
 ((&#39;x&#39;, &#39;o&#39;), 41),
 ((&#39;b&#39;, &#39;h&#39;), 41),
 ((&#39;p&#39;, &#39;p&#39;), 39),
 ((&#39;x&#39;, &#39;l&#39;), 39),
 ((&#39;h&#39;, &#39;v&#39;), 39),
 ((&#39;b&#39;, &#39;b&#39;), 38),
 ((&#39;m&#39;, &#39;p&#39;), 38),
 ((&#39;x&#39;, &#39;x&#39;), 38),
 ((&#39;u&#39;, &#39;v&#39;), 37),
 ((&#39;x&#39;, &#39;e&#39;), 36),
 ((&#39;w&#39;, &#39;o&#39;), 36),
 ((&#39;c&#39;, &#39;t&#39;), 35),
 ((&#39;z&#39;, &#39;m&#39;), 35),
 ((&#39;t&#39;, &#39;s&#39;), 35),
 ((&#39;m&#39;, &#39;s&#39;), 35),
 ((&#39;c&#39;, &#39;u&#39;), 35),
 ((&#39;o&#39;, &#39;f&#39;), 34),
 ((&#39;u&#39;, &#39;x&#39;), 34),
 ((&#39;k&#39;, &#39;w&#39;), 34),
 ((&#39;p&#39;, &#39;.&#39;), 33),
 ((&#39;g&#39;, &#39;l&#39;), 32),
 ((&#39;z&#39;, &#39;r&#39;), 32),
 ((&#39;d&#39;, &#39;n&#39;), 31),
 ((&#39;g&#39;, &#39;t&#39;), 31),
 ((&#39;g&#39;, &#39;y&#39;), 31),
 ((&#39;h&#39;, &#39;s&#39;), 31),
 ((&#39;x&#39;, &#39;s&#39;), 31),
 ((&#39;g&#39;, &#39;s&#39;), 30),
 ((&#39;x&#39;, &#39;y&#39;), 30),
 ((&#39;y&#39;, &#39;g&#39;), 30),
 ((&#39;d&#39;, &#39;m&#39;), 30),
 ((&#39;d&#39;, &#39;s&#39;), 29),
 ((&#39;h&#39;, &#39;k&#39;), 29),
 ((&#39;y&#39;, &#39;x&#39;), 28),
 ((&#39;q&#39;, &#39;.&#39;), 28),
 ((&#39;g&#39;, &#39;n&#39;), 27),
 ((&#39;y&#39;, &#39;b&#39;), 27),
 ((&#39;g&#39;, &#39;w&#39;), 26),
 ((&#39;n&#39;, &#39;h&#39;), 26),
 ((&#39;k&#39;, &#39;n&#39;), 26),
 ((&#39;g&#39;, &#39;g&#39;), 25),
 ((&#39;d&#39;, &#39;g&#39;), 25),
 ((&#39;l&#39;, &#39;c&#39;), 25),
 ((&#39;r&#39;, &#39;j&#39;), 25),
 ((&#39;w&#39;, &#39;u&#39;), 25),
 ((&#39;l&#39;, &#39;k&#39;), 24),
 ((&#39;m&#39;, &#39;d&#39;), 24),
 ((&#39;s&#39;, &#39;w&#39;), 24),
 ((&#39;s&#39;, &#39;n&#39;), 24),
 ((&#39;h&#39;, &#39;d&#39;), 24),
 ((&#39;w&#39;, &#39;h&#39;), 23),
 ((&#39;y&#39;, &#39;j&#39;), 23),
 ((&#39;y&#39;, &#39;y&#39;), 23),
 ((&#39;r&#39;, &#39;z&#39;), 23),
 ((&#39;d&#39;, &#39;w&#39;), 23),
 ((&#39;w&#39;, &#39;r&#39;), 22),
 ((&#39;t&#39;, &#39;n&#39;), 22),
 ((&#39;l&#39;, &#39;f&#39;), 22),
 ((&#39;y&#39;, &#39;h&#39;), 22),
 ((&#39;r&#39;, &#39;w&#39;), 21),
 ((&#39;s&#39;, &#39;b&#39;), 21),
 ((&#39;m&#39;, &#39;n&#39;), 20),
 ((&#39;f&#39;, &#39;l&#39;), 20),
 ((&#39;w&#39;, &#39;s&#39;), 20),
 ((&#39;k&#39;, &#39;k&#39;), 20),
 ((&#39;h&#39;, &#39;z&#39;), 20),
 ((&#39;g&#39;, &#39;d&#39;), 19),
 ((&#39;l&#39;, &#39;h&#39;), 19),
 ((&#39;n&#39;, &#39;m&#39;), 19),
 ((&#39;x&#39;, &#39;z&#39;), 19),
 ((&#39;u&#39;, &#39;f&#39;), 19),
 ((&#39;f&#39;, &#39;t&#39;), 18),
 ((&#39;l&#39;, &#39;r&#39;), 18),
 ((&#39;p&#39;, &#39;t&#39;), 17),
 ((&#39;t&#39;, &#39;c&#39;), 17),
 ((&#39;k&#39;, &#39;t&#39;), 17),
 ((&#39;d&#39;, &#39;v&#39;), 17),
 ((&#39;u&#39;, &#39;p&#39;), 16),
 ((&#39;p&#39;, &#39;l&#39;), 16),
 ((&#39;l&#39;, &#39;w&#39;), 16),
 ((&#39;p&#39;, &#39;s&#39;), 16),
 ((&#39;o&#39;, &#39;j&#39;), 16),
 ((&#39;r&#39;, &#39;q&#39;), 16),
 ((&#39;y&#39;, &#39;p&#39;), 15),
 ((&#39;l&#39;, &#39;p&#39;), 15),
 ((&#39;t&#39;, &#39;v&#39;), 15),
 ((&#39;r&#39;, &#39;p&#39;), 14),
 ((&#39;l&#39;, &#39;n&#39;), 14),
 ((&#39;e&#39;, &#39;q&#39;), 14),
 ((&#39;f&#39;, &#39;y&#39;), 14),
 ((&#39;s&#39;, &#39;v&#39;), 14),
 ((&#39;u&#39;, &#39;j&#39;), 14),
 ((&#39;v&#39;, &#39;l&#39;), 14),
 ((&#39;q&#39;, &#39;a&#39;), 13),
 ((&#39;u&#39;, &#39;y&#39;), 13),
 ((&#39;q&#39;, &#39;i&#39;), 13),
 ((&#39;w&#39;, &#39;l&#39;), 13),
 ((&#39;p&#39;, &#39;y&#39;), 12),
 ((&#39;y&#39;, &#39;f&#39;), 12),
 ((&#39;c&#39;, &#39;q&#39;), 11),
 ((&#39;j&#39;, &#39;r&#39;), 11),
 ((&#39;n&#39;, &#39;w&#39;), 11),
 ((&#39;n&#39;, &#39;f&#39;), 11),
 ((&#39;t&#39;, &#39;w&#39;), 11),
 ((&#39;m&#39;, &#39;z&#39;), 11),
 ((&#39;u&#39;, &#39;o&#39;), 10),
 ((&#39;f&#39;, &#39;u&#39;), 10),
 ((&#39;l&#39;, &#39;z&#39;), 10),
 ((&#39;h&#39;, &#39;w&#39;), 10),
 ((&#39;u&#39;, &#39;q&#39;), 10),
 ((&#39;j&#39;, &#39;y&#39;), 10),
 ((&#39;s&#39;, &#39;z&#39;), 10),
 ((&#39;s&#39;, &#39;d&#39;), 9),
 ((&#39;j&#39;, &#39;l&#39;), 9),
 ((&#39;d&#39;, &#39;j&#39;), 9),
 ((&#39;k&#39;, &#39;m&#39;), 9),
 ((&#39;r&#39;, &#39;f&#39;), 9),
 ((&#39;h&#39;, &#39;j&#39;), 9),
 ((&#39;v&#39;, &#39;n&#39;), 8),
 ((&#39;n&#39;, &#39;b&#39;), 8),
 ((&#39;i&#39;, &#39;w&#39;), 8),
 ((&#39;h&#39;, &#39;b&#39;), 8),
 ((&#39;b&#39;, &#39;s&#39;), 8),
 ((&#39;w&#39;, &#39;t&#39;), 8),
 ((&#39;w&#39;, &#39;d&#39;), 8),
 ((&#39;v&#39;, &#39;v&#39;), 7),
 ((&#39;v&#39;, &#39;u&#39;), 7),
 ((&#39;j&#39;, &#39;s&#39;), 7),
 ((&#39;m&#39;, &#39;j&#39;), 7),
 ((&#39;f&#39;, &#39;s&#39;), 6),
 ((&#39;l&#39;, &#39;g&#39;), 6),
 ((&#39;l&#39;, &#39;j&#39;), 6),
 ((&#39;j&#39;, &#39;w&#39;), 6),
 ((&#39;n&#39;, &#39;x&#39;), 6),
 ((&#39;y&#39;, &#39;q&#39;), 6),
 ((&#39;w&#39;, &#39;k&#39;), 6),
 ((&#39;g&#39;, &#39;m&#39;), 6),
 ((&#39;x&#39;, &#39;u&#39;), 5),
 ((&#39;m&#39;, &#39;h&#39;), 5),
 ((&#39;m&#39;, &#39;l&#39;), 5),
 ((&#39;j&#39;, &#39;m&#39;), 5),
 ((&#39;c&#39;, &#39;s&#39;), 5),
 ((&#39;j&#39;, &#39;v&#39;), 5),
 ((&#39;n&#39;, &#39;p&#39;), 5),
 ((&#39;d&#39;, &#39;f&#39;), 5),
 ((&#39;x&#39;, &#39;d&#39;), 5),
 ((&#39;z&#39;, &#39;b&#39;), 4),
 ((&#39;f&#39;, &#39;n&#39;), 4),
 ((&#39;x&#39;, &#39;c&#39;), 4),
 ((&#39;m&#39;, &#39;t&#39;), 4),
 ((&#39;t&#39;, &#39;m&#39;), 4),
 ((&#39;z&#39;, &#39;n&#39;), 4),
 ((&#39;z&#39;, &#39;t&#39;), 4),
 ((&#39;p&#39;, &#39;u&#39;), 4),
 ((&#39;c&#39;, &#39;z&#39;), 4),
 ((&#39;b&#39;, &#39;n&#39;), 4),
 ((&#39;z&#39;, &#39;s&#39;), 4),
 ((&#39;f&#39;, &#39;w&#39;), 4),
 ((&#39;d&#39;, &#39;t&#39;), 4),
 ((&#39;j&#39;, &#39;d&#39;), 4),
 ((&#39;j&#39;, &#39;c&#39;), 4),
 ((&#39;y&#39;, &#39;w&#39;), 4),
 ((&#39;v&#39;, &#39;k&#39;), 3),
 ((&#39;x&#39;, &#39;w&#39;), 3),
 ((&#39;t&#39;, &#39;j&#39;), 3),
 ((&#39;c&#39;, &#39;j&#39;), 3),
 ((&#39;q&#39;, &#39;w&#39;), 3),
 ((&#39;g&#39;, &#39;b&#39;), 3),
 ((&#39;o&#39;, &#39;q&#39;), 3),
 ((&#39;r&#39;, &#39;x&#39;), 3),
 ((&#39;d&#39;, &#39;c&#39;), 3),
 ((&#39;g&#39;, &#39;j&#39;), 3),
 ((&#39;x&#39;, &#39;f&#39;), 3),
 ((&#39;z&#39;, &#39;w&#39;), 3),
 ((&#39;d&#39;, &#39;k&#39;), 3),
 ((&#39;u&#39;, &#39;u&#39;), 3),
 ((&#39;m&#39;, &#39;v&#39;), 3),
 ((&#39;c&#39;, &#39;x&#39;), 3),
 ((&#39;l&#39;, &#39;q&#39;), 3),
 ((&#39;p&#39;, &#39;b&#39;), 2),
 ((&#39;t&#39;, &#39;g&#39;), 2),
 ((&#39;q&#39;, &#39;s&#39;), 2),
 ((&#39;t&#39;, &#39;x&#39;), 2),
 ((&#39;f&#39;, &#39;k&#39;), 2),
 ((&#39;b&#39;, &#39;t&#39;), 2),
 ((&#39;j&#39;, &#39;n&#39;), 2),
 ((&#39;k&#39;, &#39;c&#39;), 2),
 ((&#39;z&#39;, &#39;k&#39;), 2),
 ((&#39;s&#39;, &#39;j&#39;), 2),
 ((&#39;s&#39;, &#39;f&#39;), 2),
 ((&#39;z&#39;, &#39;j&#39;), 2),
 ((&#39;n&#39;, &#39;q&#39;), 2),
 ((&#39;f&#39;, &#39;z&#39;), 2),
 ((&#39;h&#39;, &#39;g&#39;), 2),
 ((&#39;w&#39;, &#39;w&#39;), 2),
 ((&#39;k&#39;, &#39;j&#39;), 2),
 ((&#39;j&#39;, &#39;k&#39;), 2),
 ((&#39;w&#39;, &#39;m&#39;), 2),
 ((&#39;z&#39;, &#39;c&#39;), 2),
 ((&#39;z&#39;, &#39;v&#39;), 2),
 ((&#39;w&#39;, &#39;f&#39;), 2),
 ((&#39;q&#39;, &#39;m&#39;), 2),
 ((&#39;k&#39;, &#39;z&#39;), 2),
 ((&#39;j&#39;, &#39;j&#39;), 2),
 ((&#39;z&#39;, &#39;p&#39;), 2),
 ((&#39;j&#39;, &#39;t&#39;), 2),
 ((&#39;k&#39;, &#39;b&#39;), 2),
 ((&#39;m&#39;, &#39;w&#39;), 2),
 ((&#39;h&#39;, &#39;f&#39;), 2),
 ((&#39;c&#39;, &#39;g&#39;), 2),
 ((&#39;t&#39;, &#39;f&#39;), 2),
 ((&#39;h&#39;, &#39;c&#39;), 2),
 ((&#39;q&#39;, &#39;o&#39;), 2),
 ((&#39;k&#39;, &#39;d&#39;), 2),
 ((&#39;k&#39;, &#39;v&#39;), 2),
 ((&#39;s&#39;, &#39;g&#39;), 2),
 ((&#39;z&#39;, &#39;d&#39;), 2),
 ((&#39;q&#39;, &#39;r&#39;), 1),
 ((&#39;d&#39;, &#39;z&#39;), 1),
 ((&#39;p&#39;, &#39;j&#39;), 1),
 ((&#39;q&#39;, &#39;l&#39;), 1),
 ((&#39;p&#39;, &#39;f&#39;), 1),
 ((&#39;q&#39;, &#39;e&#39;), 1),
 ((&#39;b&#39;, &#39;c&#39;), 1),
 ((&#39;c&#39;, &#39;d&#39;), 1),
 ((&#39;m&#39;, &#39;f&#39;), 1),
 ((&#39;p&#39;, &#39;n&#39;), 1),
 ((&#39;w&#39;, &#39;b&#39;), 1),
 ((&#39;p&#39;, &#39;c&#39;), 1),
 ((&#39;h&#39;, &#39;p&#39;), 1),
 ((&#39;f&#39;, &#39;h&#39;), 1),
 ((&#39;b&#39;, &#39;j&#39;), 1),
 ((&#39;f&#39;, &#39;g&#39;), 1),
 ((&#39;z&#39;, &#39;g&#39;), 1),
 ((&#39;c&#39;, &#39;p&#39;), 1),
 ((&#39;p&#39;, &#39;k&#39;), 1),
 ((&#39;p&#39;, &#39;m&#39;), 1),
 ((&#39;x&#39;, &#39;n&#39;), 1),
 ((&#39;s&#39;, &#39;q&#39;), 1),
 ((&#39;k&#39;, &#39;f&#39;), 1),
 ((&#39;m&#39;, &#39;k&#39;), 1),
 ((&#39;x&#39;, &#39;h&#39;), 1),
 ((&#39;g&#39;, &#39;f&#39;), 1),
 ((&#39;v&#39;, &#39;b&#39;), 1),
 ((&#39;j&#39;, &#39;p&#39;), 1),
 ((&#39;g&#39;, &#39;z&#39;), 1),
 ((&#39;v&#39;, &#39;d&#39;), 1),
 ((&#39;d&#39;, &#39;b&#39;), 1),
 ((&#39;v&#39;, &#39;h&#39;), 1),
 ((&#39;h&#39;, &#39;h&#39;), 1),
 ((&#39;g&#39;, &#39;v&#39;), 1),
 ((&#39;d&#39;, &#39;q&#39;), 1),
 ((&#39;x&#39;, &#39;b&#39;), 1),
 ((&#39;w&#39;, &#39;z&#39;), 1),
 ((&#39;h&#39;, &#39;q&#39;), 1),
 ((&#39;j&#39;, &#39;b&#39;), 1),
 ((&#39;x&#39;, &#39;m&#39;), 1),
 ((&#39;w&#39;, &#39;g&#39;), 1),
 ((&#39;t&#39;, &#39;b&#39;), 1),
 ((&#39;z&#39;, &#39;x&#39;), 1)]
</pre></div>
</div>
</div>
</div>
<p>And this is the sorted list of counts of the individual bigrams across all the words in the dataset! Now let‚Äôs convert our current bigram-to-occurence-frequency map into a bigram counts array, where every row index represents the first character and every column index represents the second character of each bigram. Before doing so, we must first find a way to convert each character into a unique integer index:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chars</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
<span class="n">ctoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ctoi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;.&#39;: 0, &#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 5, &#39;f&#39;: 6, &#39;g&#39;: 7, &#39;h&#39;: 8, &#39;i&#39;: 9, &#39;j&#39;: 10, &#39;k&#39;: 11, &#39;l&#39;: 12, &#39;m&#39;: 13, &#39;n&#39;: 14, &#39;o&#39;: 15, &#39;p&#39;: 16, &#39;q&#39;: 17, &#39;r&#39;: 18, &#39;s&#39;: 19, &#39;t&#39;: 20, &#39;u&#39;: 21, &#39;v&#39;: 22, &#39;w&#39;: 23, &#39;x&#39;: 24, &#39;y&#39;: 25, &#39;z&#39;: 26}
</pre></div>
</div>
</div>
</div>
<p>Now that we have a character-to-index map, we may construct our bigram counts array <code class="docutils literal notranslate"><span class="pre">N</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">nchars</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nchars</span><span class="p">,</span> <span class="n">nchars</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">N</span><span class="p">[</span><span class="n">ctoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">],</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
          134,  535,  929],
        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,
         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,
          182, 2050,  435],
        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,
          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,
            0,   83,    0],
        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,
          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,
            3,  104,    4],
        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,
           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,
            0,  317,    1],
        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,
         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,
          132, 1070,  181],
        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,
           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,
            0,   14,    2],
        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,
           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,
            0,   31,    1],
        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,
          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,
            0,  213,   20],
        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,
         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,
           89,  779,  277],
        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,
            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,
            0,   10,    0],
        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,
          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,
            0,  379,    2],
        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,
         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,
            0, 1588,   10],
        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,
            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,
            0,  287,   11],
        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,
          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,
            6,  465,  145],
        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,
          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,
           45,  103,   54],
        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,
           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,
            0,   12,    0],
        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,
            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,
            0,    0,    0],
        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,
          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,
            3,  773,   23],
        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,
          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,
            0,  215,   10],
        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,
          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,
            2,  341,  105],
        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,
          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,
           34,   13,   45],
        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,
           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,
            0,  121,    0],
        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,
           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,
            0,   73,    1],
        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,
           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,
           38,   30,   19],
        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,
         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,
           28,   23,   78],
        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,
          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,
            1,  147,   45]], dtype=torch.int32)
</pre></div>
</div>
</div>
</div>
<p>Done! Of course, this looks like a mess. So let‚Äôs visualize it better.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> inline
<span class="k">else</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> ipympl

<span class="n">itoc</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">c</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ctoi</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">27</span><span class="p">):</span>
        <span class="n">chstr</span> <span class="o">=</span> <span class="n">itoc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">itoc</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chstr</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">N</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(np.float64(-0.5), np.float64(26.5), np.float64(26.5), np.float64(-0.5))
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2e90789e8ce84917a2a5d630a82c41ef", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>The color-graded bigram counts array! Looks good. This array actually has all the necessary information for us to start sampling from this bigram character language model. Let‚Äôs just start by sampling the start character (of course) of each name: the <code class="docutils literal notranslate"><span class="pre">.</span></code> character. The first row tells us how often each other character follows it. In other words, the first row tells us how often each character is the first character of a word:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
         134,  535,  929], dtype=torch.int32)
</pre></div>
</div>
</div>
</div>
<p>To get the probability of each of character being the first:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">p</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,
        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,
        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])
</pre></div>
</div>
</div>
</div>
<p>Each value of this probability distribution corresponds simply to the probability of the corresponding character being the first character of a word. And of course it sums to <code class="docutils literal notranslate"><span class="pre">1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we‚Äôll sample numbers according to this probability distribution using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial"><code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code></a>. And to do so deterministically we are going to use a generator. So, let‚Äôs take a brief detour and test out how to sample. First we create a probability distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">2147483647</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">ptest</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">ptest</span> <span class="o">=</span> <span class="n">ptest</span> <span class="o">/</span> <span class="n">ptest</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">ptest</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.6064, 0.3033, 0.0903])
</pre></div>
</div>
</div>
</div>
<p>Then, we sample from this distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">ptest</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0,
        0, 1, 1, 1])
</pre></div>
</div>
</div>
</div>
<p>Simple. Now, notice that it outputs the same tensor however many times you run the cells. That‚Äôs because we have set a fixed seed and passed the generator object to the functions. Now, notice the output of <code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code>. What we expect is that around <span class="math notranslate nohighlight">\(60.64\%\)</span> of the numbers to be <code class="docutils literal notranslate"><span class="pre">0</span></code>, <span class="math notranslate nohighlight">\(30.33\%\)</span> to be <code class="docutils literal notranslate"><span class="pre">1</span></code> and <span class="math notranslate nohighlight">\(9.03\%\)</span> to be <code class="docutils literal notranslate"><span class="pre">2</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sbc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio of </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sbc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">sbc</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ratio of 0: 0.6100000143051147
Ratio of 1: 0.33000001311302185
Ratio of 2: 0.05999999865889549
</pre></div>
</div>
</div>
</div>
<p>Not too far away from what we expected! But, if we increase the number of samples, we will get much closer to the probabilities of our distribution. Try it out! The more samples we take, the more the actual occurence ratios match the probabilities of the distribution the numbers were sampled from. Now, it‚Äôs time to sample from our initial character probability distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">itoc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;j&#39;
</pre></div>
</div>
</div>
</div>
<p>We are now ready to write out our name generator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">N</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">/</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># sum over the column dimension and keep column dimension</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>junide.
janasah.
p.
cony.
a.
nn.
kohin.
tolian.
juee.
ksahnaauranilevias.
dedainrwieta.
ssonielylarte.
faveumerifontume.
phynslenaruani.
core.
yaenon.
ka.
jabdinerimikimaynin.
anaasn.
ssorionsush.
</pre></div>
</div>
</div>
</div>
<p>It works! It yields names. Well, kinda. Some look name-like enough but most are just terrible. Lol. This is a bigrams model for you! To recap, we trained a bigrams language model essentially just by counting how frequently any pairing of characters occurs and then normalizing so that we get a nice probability distribution. Really, the elements of array <code class="docutils literal notranslate"><span class="pre">P</span></code> are the parameters of our model that summarize the statistics of these bigrams. We train the model and iteratively sample the next character and feed it in each time and get the next character. But how do we evaluate our model? We can do so, by looking at the probability of each bigram.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ch1</span><span class="si">}{</span><span class="n">ch2</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.e: 0.0478
em: 0.0377
mm: 0.0253
ma: 0.3899
a.: 0.1960
.o: 0.0123
ol: 0.0780
li: 0.1777
iv: 0.0152
vi: 0.3541
ia: 0.1381
a.: 0.1960
.a: 0.1377
av: 0.0246
va: 0.2495
a.: 0.1960
</pre></div>
</div>
</div>
</div>
<p>Here we are looking at the probabilities that the model assigns to every bigram in the dataset. Just keep in mind that we have <span class="math notranslate nohighlight">\(27\)</span> characters, so if everything was equally likely we would expect all probabilities to be:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">/</span><span class="mi">27</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.037037037037037035
</pre></div>
</div>
</div>
</div>
<p>Since they are not and we have mostly higher probabilities, it means that our model has learned something useful. In an ideal case, we would expect the bigram probabilities to be near <span class="math notranslate nohighlight">\(1.0\)</span> (perfect prediction probability). Now, when you look at the literature of <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a>, statistical modelling and so on, you‚Äôll see that what‚Äôs typically used here is something called the likelihood: the product of all the above probabilities. This gives us the probability of the entire dataset assigned by the model that you made. But, because the product of these probabilities is an unwieldly, very tiny number to work with (think <span class="math notranslate nohighlight">\(0.0478 \times 0.0377 \times 0.0253 \times ...\)</span>), for convenience, what people usually work with is not the likelihood, but the log-likelihood. The log, as you can see:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f6a401776d0&gt;]
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8cff68e9d3544a6aa5001375df6992e9", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>is a monotonic transformation of the probability, where if you pass in probability <span class="math notranslate nohighlight">\(1.0\)</span> you get log-probability of <span class="math notranslate nohighlight">\(0\)</span>, and as the probabilities you pass in decrease, the log-probability decreases all the way to <span class="math notranslate nohighlight">\(-\infty\)</span> as the probability approaches <span class="math notranslate nohighlight">\(0\)</span>. Therefore, let‚Äôs also add the log probability in our loop to see what that looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_model</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">print_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">calc_ll</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">print_nll</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">print_nll</span><span class="p">:</span>
        <span class="n">calc_ll</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">iterable</span><span class="p">:</span>
        <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ctoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">],</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]]</span>
            <span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">calc_ll</span><span class="p">:</span>
                <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">logprob</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">print_probs</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ch1</span><span class="si">}{</span><span class="n">ch2</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">logprob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">calc_ll</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">log_likelihood</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">print_nll</span><span class="p">:</span>
        <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">nll</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loss=</span><span class="si">{</span><span class="n">nll</span><span class="o">/</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">log_likelihood</span>


<span class="n">_</span> <span class="o">=</span> <span class="n">test_model</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
</pre></div>
</div>
</div>
</div>
<p>As you can see, for higher probabilities we get closer and closer to <span class="math notranslate nohighlight">\(0\)</span>, but lower probabilities gives us a more negative number. And so to calculate the log-likelihood, we just sum up all the log probabilities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">test_model</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">calc_ll</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.e: 0.0478 -3.0408
em: 0.0377 -3.2793
mm: 0.0253 -3.6772
ma: 0.3899 -0.9418
a.: 0.1960 -1.6299
.o: 0.0123 -4.3982
ol: 0.0780 -2.5508
li: 0.1777 -1.7278
iv: 0.0152 -4.1867
vi: 0.3541 -1.0383
ia: 0.1381 -1.9796
a.: 0.1960 -1.6299
.a: 0.1377 -1.9829
av: 0.0246 -3.7045
va: 0.2495 -1.3882
a.: 0.1960 -1.6299
log_likelihood=-38.785636603832245
</pre></div>
</div>
</div>
</div>
<p>Now, how high can log-likelihood get? As high as <span class="math notranslate nohighlight">\(0\)</span>! So, when all the probabilities are <span class="math notranslate nohighlight">\(1.0\)</span>, it will be <span class="math notranslate nohighlight">\(0\)</span>. But the further away from <span class="math notranslate nohighlight">\(1.0\)</span> they are, the more negative the log-likehood will get. Now, we don‚Äôt actually like this because we are looking to define here is a <strong>loss</strong> function, that has the semantics where high is bad and low is good, since we are trying to minimize it. Any ideas? Well, we actually just need to invert the log-likelihood, aka take the negative log-likelihood (<strong>nll</strong>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">nll</span><span class="si">=}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nll=38.785636603832245
</pre></div>
</div>
</div>
</div>
<p><strong>nll</strong> is a very nice loss function because the lowest it can get is zero and the higher it is the worse off the predictions are that we are making. People also usually like to see the average of the <strong>nll</strong> instead of just the sum:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_model</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">print_probs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">calc_ll</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">print_nll</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>log_likelihood=-38.785636603832245
nll=38.785636603832245
loss=2.4241022877395153
</pre></div>
</div>
</div>
</div>
<p>Our <strong>loss</strong> function for the training set assigned by the model yields a <strong>loss</strong> of <span class="math notranslate nohighlight">\(2.424\)</span>. The lower it is, the better off we are. The higher it is, the worse off we are. So, the job of training is produce a high-quality model, by finding the parameters that minimize the <strong>loss</strong>. In this case, ones that minimize the <strong>nll</strong> <strong>loss</strong>. To summarize, our <strong>goal</strong> is to maximize likelihood of the data <strong>w.r.t.</strong> model parameters (in our statistical modeling case these are the bigram probabilities), which is:</p>
<ul class="simple">
<li><p>equivalent to maximizing the log-likelihood (because the <span class="math notranslate nohighlight">\(\log\)</span> function is monotonic)</p></li>
<li><p>equivalent to minimizing the <strong>nll</strong></p></li>
<li><p>equivalent to minimizing the average <strong>nll</strong></p></li>
</ul>
<p>The lower the <strong>nll</strong> <strong>loss</strong> the better, since that would mean assigning high probabilities. Remember: <span class="math notranslate nohighlight">\(\log(a \cdot b \cdot c) = \log(a) + \log(b) + \log(c)\)</span>. Also, keep in mind that here we store the probabilities in a table format. But in what‚Äôs coming up, these numbers will not be kept explicitly but they will be calculated by a <strong>nn</strong> and we will change its parameters to maximize the likelihood of these probabilities. Let‚Äôs now test out our model with a random name:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_model</span><span class="p">(</span><span class="n">iterable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;christosqj&#39;</span><span class="p">],</span> <span class="n">calc_ll</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">print_nll</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.c: 0.0481 -3.0337
ch: 0.1880 -1.6713
hr: 0.0268 -3.6199
ri: 0.2388 -1.4320
is: 0.0743 -2.5990
st: 0.0944 -2.3605
to: 0.1197 -2.1224
os: 0.0635 -2.7563
sq: 0.0001 -9.0004
qj: 0.0000 -inf
j.: 0.0245 -3.7098
log_likelihood=-inf
nll=inf
loss=inf
</pre></div>
</div>
</div>
</div>
<p>As you can see, the probability of the bigram <code class="docutils literal notranslate"><span class="pre">sq</span></code> is super low. Whereas the probability for <code class="docutils literal notranslate"><span class="pre">qj</span></code>, since it is never encountered in our training data (see our bigram count table!), is <span class="math notranslate nohighlight">\(0\)</span>, which predictably yields a log-probability of <span class="math notranslate nohighlight">\(-\infty\)</span>, which in turn causes the <strong>loss</strong> to be <span class="math notranslate nohighlight">\(-\infty\)</span>. What this means is that this model is exactly <span class="math notranslate nohighlight">\(0 \%\)</span> likely to predict this name (infinite <strong>loss</strong>). If you look up the table you see that <code class="docutils literal notranslate"><span class="pre">q</span></code> is followed by <code class="docutils literal notranslate"><span class="pre">j</span></code> zero times. This kind of behavior people don‚Äôt usually like too much, so there is a simple trick to alleviate it: model smoothing. It involves adding some fake counts to the bigram counts array so that never is there a bigram with <code class="docutils literal notranslate"><span class="pre">0</span></code> counts (and therefore <code class="docutils literal notranslate"><span class="pre">0</span></code> probability). This ensures that there are no zeros in our bigram counts matrix. E.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">/</span> <span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># sum over the column dimension and keep column dimension</span>
<span class="n">test_model</span><span class="p">(</span><span class="n">iterable</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;christosqj&quot;</span><span class="p">],</span> <span class="n">calc_ll</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">print_nll</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.c: 0.0481 -3.0339
ch: 0.1869 -1.6774
hr: 0.0268 -3.6185
ri: 0.2384 -1.4338
is: 0.0743 -2.5998
st: 0.0942 -2.3625
to: 0.1193 -2.1257
os: 0.0634 -2.7578
sq: 0.0002 -8.3105
qj: 0.0033 -5.7004
j.: 0.0246 -3.7051
log_likelihood=-37.32549834251404
nll=37.32549834251404
loss=3.393227122046731
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-37.32549834251404
</pre></div>
</div>
</div>
</div>
<p>Now, we avoid getting a loss of <span class="math notranslate nohighlight">\(-\infty\)</span>. Cool! So we‚Äôve now trained a respectable bigram character-level language model. We trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions. We saw that we can also then use those parameters of this model to perform sampling of new words (sample new names according to these distributions) and evaluate the quality of this model which is summarized by a single number: the <strong>nll</strong>. And the lower this number is, the better the model is because it is giving high probabilities to the actual mixed characters of all the bigrams in our training set. Great! We basically, counted and then normalized those counts, which is sensible enough.</p>
</section>
<section id="casting-the-model-as-a-nn">
<h2>Casting the model as a <strong>nn</strong><a class="headerlink" href="#casting-the-model-as-a-nn" title="Link to this heading">#</a></h2>
<p>Let‚Äôs now try a different approach by casting such a bigram language model into a <strong>nn</strong> framework to achieve the same goal. Our <strong>nn</strong> is still going to be a bigram character-level language model. It will receive a single character as an input that will pass through a bunch of weighted neurons and then output the probability distribution over the next character in the sequence. It‚Äôs going to make guesses about what character is going to follow the input character. In addition, we‚Äôll be able to evaluate any setting of the parameters of the <strong>nn</strong>, since we have a <strong>loss</strong> function. Basically, we‚Äôre going to take a look at the probabilities distributions our model assigns for our next character and find the <strong>loss</strong> between those and the labels (which are the character that we expect to come next in the bigram). By doing so, we can use gradient-based optimization to tune the weights of our <strong>nn</strong> that give us the output probabilities. Let‚Äôs begin this alternative approach by first constructing our dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create training dataset of bigrams (x, y)</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span><span class="p">)</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">])</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">])</span>
<span class="c1"># Convert to pytorch tensor (https://pytorch.org/docs/stable/generated/torch.tensor.html)</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>. e
e m
m m
m a
a .
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0,  5, 13, 13,  1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ys</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 5, 13, 13,  1,  0])
</pre></div>
</div>
</div>
</div>
<p>Now, how do we pass each character into the <strong>nn</strong>? <a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">One-hot encoding</a>! With this encoding, each integer is encoded with bits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">xenc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 27])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">xenc</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "958200875f3d4e87b042cf5e9da17908", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Let‚Äôs create our neuron:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.3270],
        [-1.4539],
        [-0.8739],
        [-0.8739],
        [ 1.4181]])
</pre></div>
</div>
</div>
</div>
<p>Our neuron receives one character of size <span class="math notranslate nohighlight">\(27\)</span> and spits out <span class="math notranslate nohighlight">\(1\)</span> output value. However, as you can see, since PyTorch supports matrix multiplication, our neuron can receive <span class="math notranslate nohighlight">\(5\)</span> characters of size <span class="math notranslate nohighlight">\(27\)</span> in parallel and output each character‚Äôs output in a <span class="math notranslate nohighlight">\(5 \times 1\)</span> matrix (<span class="math notranslate nohighlight">\([5 \times 27] \cdot [27 \times 1] \rightarrow [5 \times 1]\)</span>). Now, let‚Äôs pass our <span class="math notranslate nohighlight">\(5\)</span> characters as inputs through <span class="math notranslate nohighlight">\(27\)</span> neurons instead of just <span class="math notranslate nohighlight">\(1\)</span> neuron:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">))</span>
<span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.9979,  0.2825,  1.1355,  0.3798, -0.2801,  0.0672, -1.1496,  2.1393,
         -0.2687, -1.4350,  1.1158,  0.4346, -0.4915, -0.1916,  1.4139, -0.4590,
         -0.5869,  1.6688,  0.8819,  0.8542, -0.0366, -0.6968,  0.1041,  0.8881,
          0.7592, -0.5573,  0.9596],
        [-0.1725, -1.5476,  1.5005,  1.4560,  0.9079, -1.2025,  0.1265,  0.1533,
         -0.2189, -1.3150,  1.6275,  0.3342,  1.4620, -0.3458, -0.2391,  0.5896,
          1.7679,  1.1726, -0.6278, -0.1539, -0.6117, -0.0106,  0.7131,  2.0526,
          1.2183,  1.6270, -1.3764],
        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,
          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,
          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,
          0.1524,  1.5829,  0.3142],
        [-1.0529, -0.4609,  0.6597,  0.5148, -1.1303,  0.5723,  0.1907, -0.1367,
          0.3072,  1.2870, -2.0319, -0.2964,  0.3874, -1.2633, -1.3800,  1.4614,
          0.2344,  0.1867,  0.0559, -2.1201, -0.7034,  0.7074, -0.5500, -1.3492,
          0.1524,  1.5829,  0.3142],
        [ 0.2948,  0.0746, -0.4187,  0.4092, -0.6537,  1.1562,  0.6917, -1.2596,
         -0.1424, -0.5520, -1.1731, -0.4088, -0.6465, -0.2629, -0.3580,  0.8126,
         -1.7589,  1.7377, -0.5665,  1.9188, -0.6135, -1.2176,  0.0166,  0.1594,
         -0.8806,  0.6167, -0.9173]])
</pre></div>
</div>
</div>
</div>
<p>Predictably, we get <span class="math notranslate nohighlight">\(5\)</span> arrays (one per input/character) of <span class="math notranslate nohighlight">\(27\)</span> outputs (<span class="math notranslate nohighlight">\([5 \times 27] \cdot [27 \times 27] \rightarrow [5 \times 27]\)</span>). Each output number represents each neuron‚Äôs firing rate of a specific input. For example, the following is the firing rate of the 13th neuron of the 3rd input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span><span class="p">)[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">13</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-1.2633)
</pre></div>
</div>
</div>
</div>
<p>What PyTorch allows is matrix multiplication that enables parallel dot products of many inputs in a batch with the weights of neurons of a <strong>nn</strong>. For example, this is how to multiply the inputs that represent the 3rd character with the weights of the 13th neuron:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="p">[:,</span> <span class="mi">13</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.1916, -0.2629, -1.1183,  0.9108,  0.7797, -0.3458, -1.2783, -0.7899,
        -0.3221, -0.4800,  0.3307,  0.2826, -0.5372, -1.2633,  0.3663,  0.1210,
         0.0446, -0.1690, -0.3741, -0.0798, -0.5883, -0.9373, -0.1367, -0.2475,
        -0.4424, -2.0253, -0.1943])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">xenc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[:,</span> <span class="mi">13</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-1.2633)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span><span class="p">)[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">13</span><span class="p">]</span>  <span class="c1"># same as above</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-1.2633)
</pre></div>
</div>
</div>
</div>
<p>Ok, so what did is we fed our <span class="math notranslate nohighlight">\(27\)</span>-dimensional inputs into the first layer of a <strong>nn</strong> that has 27 neurons. These neurons perform <code class="docutils literal notranslate"><span class="pre">W</span> <span class="pre">*</span> <span class="pre">x</span></code>. They don‚Äôt have a bias and they don‚Äôt have a non-linearity like <code class="docutils literal notranslate"><span class="pre">tanh</span></code>. We are going to leave our network as is: a 1-layer <em>linear</em> <strong>nn</strong>. That‚Äôs it. Basically, the dumbest, smallest, simplest <strong>nn</strong>. Remember, what we trying to produce is a probability distribution for a next character in a sequence. And there‚Äôs <span class="math notranslate nohighlight">\(27\)</span> of them. But we have to come up with exact semantics as to how we are going to interpret these <span class="math notranslate nohighlight">\(27\)</span> numbers that these neurons take on. Intuitively, as we can see in the <code class="docutils literal notranslate"><span class="pre">xenc</span> <span class="pre">&#64;</span> <span class="pre">W</span></code> output, some of these outputs numbers are positive and some negative. That‚Äôs because they come out of a <strong>nn</strong> layer with weights are initialized from the normal <span class="math notranslate nohighlight">\([-1, 1]\)</span> distribution. But, what we want however is something like a bigram count table that we previously produced, where each row told us the counts which we then normalized to get the probabilities. So, we want something similar to come out of our <strong>nn</strong>. But, what we have right now, are some negative and positive numbers. Now, we therefore want these numbers to represent the probabilities for the next character with their unique characteristics. For example, probabilities are positive numbers and they sum to 1. Also, they obviously have to be probabilities. They can‚Äôt be counts because counts are positive integers; not a great output from a <strong>nn</strong>. Instead, what the <strong>nn</strong> is going to output and how we are going to interpret these <span class="math notranslate nohighlight">\(27\)</span> output numbers is as log counts. One way to accomplish this is by exponentiating each output number so that the result is always positive. Specifically, exponentiating a  negative number yields a result that is a positive value <em>less</em> than <span class="math notranslate nohighlight">\(1\)</span>. Whereas, exponentiating a positive number yields a result whose value is between greater than <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[2.7125, 1.3265, 3.1128, 1.4619, 0.7557, 1.0695, 0.3168, 8.4932, 0.7644,
         0.2381, 3.0519, 1.5444, 0.6117, 0.8256, 4.1119, 0.6319, 0.5561, 5.3059,
         2.4156, 2.3495, 0.9641, 0.4982, 1.1098, 2.4304, 2.1365, 0.5727, 2.6108],
        [0.8415, 0.2128, 4.4841, 4.2888, 2.4792, 0.3004, 1.1348, 1.1657, 0.8034,
         0.2685, 5.0910, 1.3968, 4.3146, 0.7077, 0.7873, 1.8032, 5.8586, 3.2305,
         0.5338, 0.8574, 0.5424, 0.9895, 2.0402, 7.7883, 3.3814, 5.0888, 0.2525],
        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,
         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,
         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],
        [0.3489, 0.6307, 1.9343, 1.6733, 0.3229, 1.7724, 1.2100, 0.8722, 1.3596,
         3.6221, 0.1311, 0.7435, 1.4731, 0.2827, 0.2516, 4.3118, 1.2641, 1.2053,
         1.0575, 0.1200, 0.4949, 2.0286, 0.5769, 0.2595, 1.1647, 4.8691, 1.3691],
        [1.3429, 1.0774, 0.6579, 1.5056, 0.5201, 3.1778, 1.9972, 0.2838, 0.8673,
         0.5758, 0.3094, 0.6644, 0.5239, 0.7688, 0.6990, 2.2538, 0.1722, 5.6841,
         0.5675, 6.8131, 0.5415, 0.2959, 1.0168, 1.1728, 0.4145, 1.8528, 0.3996]])
</pre></div>
</div>
</div>
</div>
<p>Such exponentiation is a great way to make the <strong>nn</strong> predict counts. Which are positive numbers that can take on various values depending on the setting of <code class="docutils literal notranslate"><span class="pre">W</span></code>. Let‚Äôs break it down more:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># log-counts</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  <span class="c1"># equivalent to the N bigram counts array</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">probs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,
         0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,
         0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502],
        [0.0139, 0.0035, 0.0739, 0.0707, 0.0409, 0.0050, 0.0187, 0.0192, 0.0132,
         0.0044, 0.0840, 0.0230, 0.0711, 0.0117, 0.0130, 0.0297, 0.0966, 0.0533,
         0.0088, 0.0141, 0.0089, 0.0163, 0.0336, 0.1284, 0.0558, 0.0839, 0.0042],
        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,
         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,
         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],
        [0.0099, 0.0178, 0.0547, 0.0473, 0.0091, 0.0501, 0.0342, 0.0247, 0.0385,
         0.1025, 0.0037, 0.0210, 0.0417, 0.0080, 0.0071, 0.1220, 0.0358, 0.0341,
         0.0299, 0.0034, 0.0140, 0.0574, 0.0163, 0.0073, 0.0329, 0.1377, 0.0387],
        [0.0371, 0.0298, 0.0182, 0.0416, 0.0144, 0.0879, 0.0552, 0.0078, 0.0240,
         0.0159, 0.0086, 0.0184, 0.0145, 0.0213, 0.0193, 0.0623, 0.0048, 0.1572,
         0.0157, 0.1884, 0.0150, 0.0082, 0.0281, 0.0324, 0.0115, 0.0512, 0.0111]])
</pre></div>
</div>
</div>
</div>
<p>Therefore, we have a way to get the probabilities, where each row sums to <span class="math notranslate nohighlight">\(1\)</span> (since they are normalized), e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 27])
</pre></div>
</div>
</div>
</div>
<p>What we have achieved is that for every one of our <span class="math notranslate nohighlight">\(5\)</span> examples, we now have a row that came out of our <strong>nn</strong>. And because of the transformations here, we made sure that this output of the <strong>nn</strong> can be interpreted as probabilities. In other words, what we have done is that we took inputs, applied differentiable operations on them (e.g. <code class="docutils literal notranslate"><span class="pre">&#64;</span></code>, <code class="docutils literal notranslate"><span class="pre">exp()</span></code>) that we can <strong>backprop</strong> through and we are getting out probability distributions. Take the first input character that was fed in as an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xenc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>that corresponds to the <code class="docutils literal notranslate"><span class="pre">.</span></code> symbol from the name:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;emma&#39;
</pre></div>
</div>
</div>
</div>
<p>The way we fed this character into the neural network is that we first got its index, then we one-hot encoded it, then it went into the <strong>nn</strong> and out came this distribution of probabilities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0522, 0.0255, 0.0599, 0.0281, 0.0145, 0.0206, 0.0061, 0.1634, 0.0147,
        0.0046, 0.0587, 0.0297, 0.0118, 0.0159, 0.0791, 0.0122, 0.0107, 0.1021,
        0.0465, 0.0452, 0.0185, 0.0096, 0.0214, 0.0468, 0.0411, 0.0110, 0.0502])
</pre></div>
</div>
</div>
</div>
<p>with a shape of:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([27])
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(27\)</span> numbers. We interpret these numbers of <code class="docutils literal notranslate"><span class="pre">probs[0]</span></code> as the probability or <em>how likely it is</em> for each of the corresponding characters to come next. As we train the <strong>nn</strong> by tuning the weights <code class="docutils literal notranslate"><span class="pre">W</span></code>, we are of course going to be getting different probabilities out for every character that you input. So, the question is: can we tune <code class="docutils literal notranslate"><span class="pre">W</span></code> such that the probabilities coming out are <em>pretty good</em>? The way we measure <em>pretty good</em> is by the <strong>loss</strong> function. Below you can see what have done in a simple summary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># SUMMARY ------------------------------&gt;&gt;&gt;&gt;</span>
<span class="n">xs</span>  <span class="c1"># inputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0,  5, 13, 13,  1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ys</span>  <span class="c1"># targets</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 5, 13, 13,  1,  0])
</pre></div>
</div>
</div>
</div>
<p>Both <code class="docutils literal notranslate"><span class="pre">xs</span></code> and <code class="docutils literal notranslate"><span class="pre">ys</span></code> constitute the dataset. They are integers representing characters of a sequence/word.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use a generator for reproducability and randomly initialize 27 neurons&#39; weights. Each neuron receives 27 inputs.</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>  <span class="c1"># 27 incoming weights for 27 neurons</span>
<span class="c1"># Encode the inputs into one-hot representations</span>
<span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># input to the network: one-hot encoding</span>
<span class="c1"># Pass encoded inputs through first layer to get logits</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># predict log-counts</span>
<span class="c1"># Exponentiate the logits to get fake counts</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  <span class="c1"># counts, equivalent to N</span>
<span class="c1"># Normalize these counts to get probabilities</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># probabilities for next character</span>
<span class="c1"># NOTE: the 2 lines above constitute what is called a &#39;softmax&#39;</span>
<span class="n">probs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 27])
</pre></div>
</div>
</div>
</div>
<p>Softmax is a very-often-used <strong>loss</strong> function in <strong>nn</strong>s. It takes in logits, exponentiates them, then divides and normalizes. It‚Äôs a way of taking outputs of a linear layer that might be positive or negative and it outputs numbers that are only positive and always sum to <span class="math notranslate nohighlight">\(1\)</span>, adhering to the properties of probability distributions. It can be viewed as a normalization function if you want to think of it that way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;softmax.jpeg&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f9f0739cf6309c37bdde2838f1b7267dd30a7de7477461bc0003092552e6c1f3.jpg" src="../_images/f9f0739cf6309c37bdde2838f1b7267dd30a7de7477461bc0003092552e6c1f3.jpg" />
</div>
</div>
<p>Now, since every operation in the forward pass is differentiable, we can <strong>backprop</strong> through. Below, we iterate over every input character and describe what is going on:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nlls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># i-th bigram:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># input character index</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># label character index</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;bigram example </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">itoc</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="si">}{</span><span class="n">itoc</span><span class="p">[</span><span class="n">y</span><span class="p">]</span><span class="si">}</span><span class="s2"> (indexes </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input to the nn:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output probabilities from the nn:&quot;</span><span class="p">,</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label (actual next character):&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;probability assigned by the nn to the correct next character:&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;log likelihood:&quot;</span><span class="p">,</span> <span class="n">logp</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">logp</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;negative log likelihood:&quot;</span><span class="p">,</span> <span class="n">nll</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">nlls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">nll</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nlls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=========&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;average negative log likelihood, i.e. loss =&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------
bigram example 1: .e (indexes 0,5)
input to the nn: 0
output probabilities from the nn: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,
        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,
        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])
label (actual next character): 5
probability assigned by the nn to the correct next character: 0.01228625513613224
log likelihood: -4.399273872375488
negative log likelihood: 4.399273872375488
--------
bigram example 2: em (indexes 5,13)
input to the nn: 5
output probabilities from the nn: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,
        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,
        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])
label (actual next character): 13
probability assigned by the nn to the correct next character: 0.018050700426101685
log likelihood: -4.014570713043213
negative log likelihood: 4.014570713043213
--------
bigram example 3: mm (indexes 13,13)
input to the nn: 13
output probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,
        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,
        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])
label (actual next character): 13
probability assigned by the nn to the correct next character: 0.026691533625125885
log likelihood: -3.623408794403076
negative log likelihood: 3.623408794403076
--------
bigram example 4: ma (indexes 13,1)
input to the nn: 13
output probabilities from the nn: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,
        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,
        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])
label (actual next character): 1
probability assigned by the nn to the correct next character: 0.07367686182260513
log likelihood: -2.6080665588378906
negative log likelihood: 2.6080665588378906
--------
bigram example 5: a. (indexes 1,0)
input to the nn: 1
output probabilities from the nn: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,
        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,
        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])
label (actual next character): 0
probability assigned by the nn to the correct next character: 0.014977526850998402
log likelihood: -4.201204299926758
negative log likelihood: 4.201204299926758
=========
average negative log likelihood, i.e. loss = 3.7693049907684326
</pre></div>
</div>
</div>
</div>
<p>As you can see, the probabilities assigned by the <strong>nn</strong> to the correct next character are bad (pretty low). See for example the probability predicted by the network of <code class="docutils literal notranslate"><span class="pre">m</span></code> following <code class="docutils literal notranslate"><span class="pre">e</span></code> (<code class="docutils literal notranslate"><span class="pre">em</span></code> example): the <strong>nll</strong> value is very high (e.g. <span class="math notranslate nohighlight">\(4.0145\)</span>). And in general, for the whole word, the <strong>loss</strong> (the average <strong>nll</strong>) is high! This means that this is not a favorable setting of weights and we can do better. One easy way to do better is to reinitialize <code class="docutils literal notranslate"><span class="pre">W</span></code> using a different seed for example and pray to god that the loss is smaller or repeat until it is. But that is what amateurs do. We are professionals or, at least, we want to be! And what professionals do is they start with random weights, like we did, and then they optimize those weights in order to minimize the loss. We do so by some gradient-based optimization (e.g. gradient descent) which entails first doing backprop in order to compute the gradients of that weight <strong>w.r.t.</strong> to those weights and then changing the weights by some such gradient amount in order to optimize them and minimize the loss. As we did with <strong>micrograd</strong>, we will write an optimization loop for doing the backward pass. But instead of mean-squared error, we are using the <strong>nll</strong> as a loss function, since we are dealing with a classification task and not a regression one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># 27 incoming weights for 27 neurons</span>


<span class="k">def</span><span class="w"> </span><span class="nf">forward_pass</span><span class="p">(</span><span class="n">regularize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
    <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
        <span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span>
    <span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># input to the network: one-hot encoding</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># predict log-counts</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  <span class="c1"># counts, equivalent to N</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># probabilities for next character</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">loss</span>


<span class="n">W</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">()</span>
<span class="c1"># backward pass</span>
<span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># set to zero</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now, something magical happened when <code class="docutils literal notranslate"><span class="pre">backward</span></code> ran. Like <strong>micrograd</strong>, PyTorch, during the forward pass, keeps track of all the operations under the hood and builds a full computational graph. So, it knows all the dependencies and all the mathematical operations of everything. Therefore, calling <code class="docutils literal notranslate"><span class="pre">backward</span></code> on the <strong>loss</strong> fills in the gradients of all the intermediate nodes, all the way back to the <code class="docutils literal notranslate"><span class="pre">W</span></code> value nodes. Take a look:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,
          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,
          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,
          0.0024,  0.0307,  0.0292],
        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,
          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,
          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,
          0.0131,  0.0101,  0.0018],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,
          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,
          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,
          0.0024,  0.0004,  0.0094],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,
          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,
          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,
          0.0482,  0.0187,  0.0051],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000]])
</pre></div>
</div>
</div>
</div>
<p>And obviously:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<p>What a gradient value is telling us, e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.012119228951632977
</pre></div>
</div>
</div>
</div>
<p>is that nudging the specific corresponding weight by a small <code class="docutils literal notranslate"><span class="pre">h</span></code> value, would nudge the <strong>loss</strong> by that gradient amount. Since we want to decrease the <strong>loss</strong>, we simply need to change the weights by a small negative fraction of the gradients in order to move them in the direction that locally most steeply decreases the <strong>loss</strong> value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<p>We just did a single gradient descent optimization step, which means that if we re-calculate the loss, it will be lower:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.7492127418518066
</pre></div>
</div>
</div>
</div>
<p>Tada! All we have to do now is put everything together and stick the single step into a loop so that we can do multi-step gradient descent optimization. This time, for all the words in our dataset, not just <code class="docutils literal notranslate"><span class="pre">emma</span></code>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the dataset</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">])</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">])</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of examples (bigrams): &quot;</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
<span class="c1"># initialize the &#39;network&#39;</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>number of examples (bigrams):  228146
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># gradient descent</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="c1"># backward pass</span>
    <span class="n">W</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># set to zero the gradient</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># update</span>
    <span class="n">W</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">50</span> <span class="o">*</span> <span class="n">W</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.758953809738159
3.371098756790161
3.1540417671203613
3.020373821258545
2.9277119636535645
2.860402822494507
2.8097293376922607
2.7701027393341064
2.7380733489990234
2.711496591567993
2.6890034675598145
2.6696884632110596
2.6529300212860107
2.638277292251587
2.6253881454467773
2.6139907836914062
2.603863477706909
2.5948219299316406
2.586712121963501
2.57940411567688
2.572789192199707
2.5667762756347656
2.5612881183624268
2.5562589168548584
2.551633596420288
2.547365665435791
2.5434155464172363
2.539748430252075
2.5363364219665527
2.5331544876098633
2.5301806926727295
2.5273969173431396
2.5247862339019775
2.522334575653076
2.520029067993164
2.517857789993286
2.515810966491699
2.513878345489502
2.512052059173584
2.510324001312256
2.5086867809295654
2.5071346759796143
2.5056610107421875
2.5042612552642822
2.502929210662842
2.5016613006591797
2.5004522800445557
2.4992990493774414
2.498197317123413
2.497144937515259
2.496137857437134
2.495173692703247
2.4942495822906494
2.493363380432129
2.4925124645233154
2.4916954040527344
2.4909095764160156
2.4901540279388428
2.4894261360168457
2.488725185394287
2.4880495071411133
2.4873974323272705
2.4867680072784424
2.4861605167388916
2.4855728149414062
2.4850049018859863
2.484455108642578
2.4839231967926025
2.483408212661743
2.4829084873199463
2.482424020767212
2.481955051422119
2.481499195098877
2.4810571670532227
2.4806275367736816
2.480210304260254
2.479804754257202
2.479410171508789
2.4790265560150146
2.4786536693573
2.478290557861328
2.4779367446899414
2.477592706680298
2.477257251739502
2.4769301414489746
2.476611852645874
2.4763011932373047
2.4759981632232666
2.4757025241851807
2.475414276123047
2.475132703781128
2.474858045578003
2.4745893478393555
2.474327802658081
2.474071741104126
2.4738216400146484
2.4735770225524902
2.4733383655548096
2.47310471534729
2.47287654876709
</pre></div>
</div>
</div>
</div>
<p>Awesome! What we least expect is that our <strong>loss</strong>, by using such gradient-based optimization, becomes as small as the <strong>loss</strong> we got by our more primitive bigram-count-matrix way that we previously employed for optimizing. So, basically, before, we achieved roughly the same <strong>loss</strong> just by counting, whereas now we used gradient descent. It just happens that the explicit, counting approach nicely optimizes the model without the need for any gradient-based optimization because the setup for bigram language models is so straightforward and simple that we can afford to just directly estimate the probabilities and keep them in a table. However, the <strong>nn</strong> approach is much more flexible and scalable! And we have actually gained a lot. What we can do from hereon is expand and complexify our approach. Meaning, that instead of just taking a single character and predicting the next one in an extremely simple <strong>nn</strong>, as we have done so far, we will be taking multiple previous characters and we will be feeding them into increasingly more complex <strong>nn</strong>s. But, fundamentally, we will still be just calculating logits that will be going through exactly the same transformation by passing them through a softmax and doing the same gradient-based optimization process we just did. But before we do that, remember the smoothing we did by adding fake counts to our bigram count matrix? Turns out, we can do equivalent smoothing in our <strong>nn</strong> too! In particular, just incentivizing the weights to be zero for example leads to the probabilities being uniform, which is a form of smoothing. Such incentivization can be accomplished through regularization. It involves just adding a term like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.6880, grad_fn=&lt;MeanBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>to the <strong>loss</strong> as such:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span> <span class="n">ys</span><span class="p">]</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">W</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">0.01</span></code> represents the strength of the regulatization term. Optimizing with this term included in the loss would smoothen the model. Yay! Lastly, let‚Äôs sample from our <strong>nn</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">ix</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># predict log-counts</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  <span class="c1"># counts, equivalent to N</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># probabilities for next character</span>
        <span class="c1"># sample from probabilities distribution</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">ix</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>junide.
janasah.
p.
cfay.
a.
nn.
kohin.
tolian.
juwe.
kilanaauranilevias.
dedainrwieta.
ssonielylarte.
faveumerifontume.
phynslenaruani.
core.
yaenon.
ka.
jabi.
werimikimaynin.
anaasn.
</pre></div>
</div>
</div>
</div>
<p>We are getting kind of the same results as we previously did with our counting method! Not unpredictable at all, since our <strong>loss</strong> values are close enough. If we trained our <strong>nn</strong> more and the <strong>loss</strong> values became the same, it would means that the two models are identical. Meaning that given the same inputs, they would spit out the same outputs.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>All in all, we have actually covered lots of ground. To sum up, we introduced the bigram character language model, we saw how we can train the model, how we can sample from the model and how we can evaluate the quality of the model using the <strong>nll</strong> <strong>loss</strong>. We actually trained the model in two completely different ways that actually give or can give (with adequate training) the same result. In the first way, we just counted up the frequency of all the bigrams and normalized. Whereas, in the second way, we used the <strong>nll</strong> <strong>loss</strong> as a guide to optimizing the counts matrix or the counts array, so that the <strong>loss</strong> is minimized in a gradient-based framework. Despite our <strong>nn</strong> being super simple (single linear layer), it is the more flexible approach.</p>
</section>
<section id="outro">
<h2>Outro<a class="headerlink" href="#outro" title="Link to this heading">#</a></h2>
<p>In the follow-up lessons, we are going to complexify by taking more and more of these characters and we are going to be feeding them into a new <strong>nn</strong> that does more exciting stuff. Buckle up!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./micrograduate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="micrograd.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">1. <strong>micrograd</strong>: implementing an autograd engine</p>
      </div>
    </a>
    <a class="right-next"
       href="makemore2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3. <strong>makemore</strong> (part 2): mlp</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-bigram-language-model">Building a bigram language model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#casting-the-model-as-a-nn">Casting the model as a <strong>nn</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christos Karaneen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>