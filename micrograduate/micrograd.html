
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. micrograd: implementing an autograd engine &#8212; micrograâˆ‡uate</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'micrograduate/micrograd';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. makemore (part 1): implementing a bigram character-level language model" href="makemore1.html" />
    <link rel="prev" title="ðŸ“– Read ðŸŒ»" href="README.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/book_logo.png" class="logo__image only-light" alt="micrograâˆ‡uate - Home"/>
    <script>document.write(`<img src="../_static/book_logo.png" class="logo__image only-dark" alt="micrograâˆ‡uate - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    ðŸ“– Read ðŸŒ»
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. <strong>micrograd</strong>: implementing an autograd engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore1.html">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore2.html">3. <strong>makemore</strong> (part 2): mlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore3.html">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore4.html">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore5.html">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="picogpt.html">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate/issues/new?title=Issue%20on%20page%20%2Fmicrograduate/micrograd.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/micrograduate/micrograd.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>1. micrograd: implementing an autograd engine</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sneak-peak">Sneak peak</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multilayer-perceptron-example">Multilayer perceptron example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-demo">Training demo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="micrograd-implementing-an-autograd-engine">
<h1>1. <strong>micrograd</strong>: implementing an autograd engine<a class="headerlink" href="#micrograd-implementing-an-autograd-engine" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cloning repo...&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>--quiet<span class="w"> </span>https://github.com/ckarageorgkaneen/micrograduate.git<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
    <span class="o">%</span><span class="k">cd</span> micrograduate
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Installing requirements...&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>uv
    <span class="o">!</span>uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--system<span class="w"> </span>--quiet<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</div>
</div>
<section id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Link to this heading">#</a></h2>
<p>Welcome! Letâ€™s begin by implementing <strong>micrograd</strong>: an autograd engine. <strong>autograd</strong> stands for automatic gradient computation/differentiation and it is a feature that facilitates the backpropagation algorithm (<strong>backprop</strong>), enabling efficient calculation the gradient of an error or <strong>loss</strong> function <strong>w.r.t.</strong> (<strong>w.r.t.</strong>) the weights and biases of a neural network (<strong>nn</strong>). This allows us to iteratively tune these parameters, minimize the <strong>loss</strong> and therefore improve the accuracy of the network. <strong>backprop</strong> is at the mathematical core of any modern deep neural network library, like PyTorch or jax. <strong>micrograd</strong>â€™s functionality is best illustrated by an example.</p>
</section>
<section id="sneak-peak">
<h2>Sneak peak<a class="headerlink" href="#sneak-peak" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">micrograd.engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">Value</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">b</span><span class="o">**</span><span class="mi">3</span>
<span class="n">c</span> <span class="o">+=</span> <span class="n">c</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">c</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">)</span>
<span class="n">d</span> <span class="o">+=</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
<span class="n">d</span> <span class="o">+=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">e</span><span class="o">**</span><span class="mi">2</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">f</span> <span class="o">/</span> <span class="mf">2.0</span>
<span class="n">g</span> <span class="o">+=</span> <span class="mf">10.0</span> <span class="o">/</span> <span class="n">f</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">g</span><span class="o">.</span><span class="n">data</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, the outcome of this forward pass&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, i.e. the numerical value of dg/da&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, i.e. the numerical value of dg/db&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>24.7041, the outcome of this forward pass
138.8338, i.e. the numerical value of dg/da
645.5773, i.e. the numerical value of dg/db
</pre></div>
</div>
</div>
</div>
<p>Youâ€™ll see that <strong>micrograd</strong> allows us to build out mathematical expressions. Here, we have an expression that weâ€™re building out where you have two inputs <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> whose values have been wrapped into a <code class="docutils literal notranslate"><span class="pre">Value</span></code> object. In this example, we build a mathematical expression graph. <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> are transformed into <code class="docutils literal notranslate"><span class="pre">c</span></code> and <code class="docutils literal notranslate"><span class="pre">d</span></code>, and eventually into <code class="docutils literal notranslate"><span class="pre">e</span></code>, <code class="docutils literal notranslate"><span class="pre">f</span></code> and <code class="docutils literal notranslate"><span class="pre">g</span></code> through some intermediary operations such as <code class="docutils literal notranslate"><span class="pre">+</span></code> (addition), <code class="docutils literal notranslate"><span class="pre">*</span></code> (multiplication) and exponentiation to a constant power (e.g. <code class="docutils literal notranslate"><span class="pre">b**3</span></code>). Other operations include offsetting by one, negation, squashing at zero, squaring, division, etc. And so from these values <strong>micrograd</strong> will, in the background, build out a mathematical expression graph. It will know, for example, that <code class="docutils literal notranslate"><span class="pre">c</span></code> is also a value but also a result of the addition operation of two child value nodes <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, to which it will maintain pointers. So, weâ€™ll basically know exactly how all this operations graph is laid out. Then, we can do a forward pass (<code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code> -&gt; â€¦ -&gt; <code class="docutils literal notranslate"><span class="pre">g</span></code>), but also initiate the backward pass that constitutes <strong>backprop</strong> by calling <code class="docutils literal notranslate"><span class="pre">backward</span></code> at node <code class="docutils literal notranslate"><span class="pre">g</span></code>: <code class="docutils literal notranslate"><span class="pre">g.backward()</span></code>. What <strong>backprop</strong> is going to do is start at <code class="docutils literal notranslate"><span class="pre">g</span></code> and itâ€™s going to go backwards through that expression graph and itâ€™s going to recursively apply the chain rule from calculus. This allows it to evaluate the derivative of <code class="docutils literal notranslate"><span class="pre">g</span></code> <strong>w.r.t.</strong> all the internal nodes, like <code class="docutils literal notranslate"><span class="pre">e</span></code>, <code class="docutils literal notranslate"><span class="pre">d</span></code> and <code class="docutils literal notranslate"><span class="pre">c</span></code> (i.e. <span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial e}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial d}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial c}\)</span>), but also <strong>w.r.t.</strong> the inputs <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> (i.e. <span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial a}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial b}\)</span>). Then we can actually query this derivative of <code class="docutils literal notranslate"><span class="pre">g</span></code> <strong>w.r.t.</strong>, for example, <code class="docutils literal notranslate"><span class="pre">a</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial a}\)</span>) as such: <code class="docutils literal notranslate"><span class="pre">a.grad</span></code>, or the derivative of <code class="docutils literal notranslate"><span class="pre">g</span></code> <strong>w.r.t.</strong> <code class="docutils literal notranslate"><span class="pre">b</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial g}{\partial b}\)</span>) as such: <code class="docutils literal notranslate"><span class="pre">b.grad</span></code>. The values of these <code class="docutils literal notranslate"><span class="pre">grad</span></code> attributes tell us how a tiny change of the values of <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> is affecting <code class="docutils literal notranslate"><span class="pre">g</span></code>. If we slightly nudge <code class="docutils literal notranslate"><span class="pre">a</span></code> and make it slightly larger, <code class="docutils literal notranslate"><span class="pre">a.grad</span> <span class="pre">==</span> <span class="pre">138.8338</span></code> is telling us that <code class="docutils literal notranslate"><span class="pre">g</span></code> will grow and that the slope of that growth is going to be equal to that number. Similarly, the slope of the growth of <code class="docutils literal notranslate"><span class="pre">g</span></code> <strong>w.r.t.</strong> a slight positive nudge of <code class="docutils literal notranslate"><span class="pre">b</span></code> is going to be <span class="math notranslate nohighlight">\(645.5773\)</span>. A <strong>nn</strong> is a function that takes in inputs and weights and returns an output: <span class="math notranslate nohighlight">\(F(in, w) = out\)</span>. <strong>micrograd</strong> will help you understand <strong>nn</strong>s at such a fundamental level by disregarding non-essential elements (e.g. tensors) that you would otherwise encounter in practice, production, etc. <strong>micrograd</strong> is all you need in order to understand the basics. Everything else is just efficiency. Now, letâ€™s dive right in and start implementing!</p>
<p>First, we must make sure that we develop a very good, intuitive understanding of what a derivative is and exactly what information it gives us.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> inline
<span class="k">else</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> ipympl
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>20.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8944ae2541004187a74481785b5dc7a2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Above, we plotted a parabola. Now, to think through what the derivative of this function at any point x is letâ€™s look up the definition of <a class="reference external" href="https://en.wikipedia.org/wiki/Derivative#Definition">what it means for a function to be differentiable</a>. Understanding the derivative boils down to finding how a such a function <code class="docutils literal notranslate"><span class="pre">f</span></code> responds to a slight nudge of its input <code class="docutils literal notranslate"><span class="pre">x</span></code> by a small number <code class="docutils literal notranslate"><span class="pre">h</span></code>. Meaning, with what sensitivity does it respond? Does the value go up or down? Or, to be more precise, what is the slope at that point <code class="docutils literal notranslate"><span class="pre">x</span></code>? Is it positive or negative? This exact amount we are looking for is given to us by the limit equation that we can very simply approximate numerically with a small enough <code class="docutils literal notranslate"><span class="pre">h</span></code> value, as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mf">0.00000001</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">3.0</span>  <span class="c1"># try -3.0, 2/3, etc.</span>
<span class="p">(</span><span class="n">y</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">h</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14.00000009255109
</pre></div>
</div>
</div>
</div>
<p>Now, with a more complex example, to start building an intuition about the derivative, letâ€™s find out the derivatives of <code class="docutils literal notranslate"><span class="pre">d</span></code> <strong>w.r.t.</strong> <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code> and <code class="docutils literal notranslate"><span class="pre">c</span></code> (i.e. <span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial a}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial b}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial c}\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mf">0.00000000000001</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">3.0</span>
<span class="n">c</span> <span class="o">=</span> <span class="mf">10.0</span>
<span class="n">d1</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span>
<span class="n">a</span> <span class="o">+=</span> <span class="n">h</span>  <span class="c1"># try nudging `b` or `c` instead to see what happens</span>
<span class="n">d2</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d1: </span><span class="si">{</span><span class="n">d1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d2: </span><span class="si">{</span><span class="n">d2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;slope: </span><span class="si">{</span><span class="p">(</span><span class="n">d2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">d1</span><span class="p">)</span><span class="o">/</span><span class="n">h</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>d1: 4.0
d2: 3.99999999999997
slope: -3.019806626980426
</pre></div>
</div>
</div>
</div>
<p>We now have some intuitive sense of what a derivative is telling us about a function. So now letâ€™s move unto <strong>nn</strong>s, which are much more massive mathematical expressions. To easily do so, we must first build some infrastructure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">,</span>
        <span class="n">prev</span><span class="o">=</span><span class="p">(),</span>
        <span class="n">op</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prev</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">prev</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(data=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&#39;&quot;</span> <span class="o">+</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;, label=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="si">}</span><span class="s2">&#39;)&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="k">else</span> <span class="s2">&quot;)&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
            <span class="n">prev</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span>
            <span class="n">op</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># other + self</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="n">other</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
            <span class="n">prev</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span>
            <span class="n">op</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># other * self</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># -self</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># self - other</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
            <span class="n">prev</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
            <span class="n">op</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
        <span class="p">)</span>


<span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;c&quot;</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">forward_pass</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">+</span> <span class="n">c</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">e</span> <span class="o">*</span> <span class="n">f</span>
    <span class="k">if</span> <span class="n">label</span><span class="p">:</span>
        <span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;d&quot;</span>
        <span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;e&quot;</span>
        <span class="n">L</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;L&quot;</span>
    <span class="k">return</span> <span class="n">L</span>


<span class="n">L</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">L</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=&#39;-8.0&#39;, label=&#39;L&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">prev</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{Value(data=&#39;-2.0&#39;, label=&#39;f&#39;), Value(data=&#39;4.0&#39;, label=&#39;e&#39;)}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">op</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;*&#39;
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Value</span></code> object we defined acts as a value holder. It can also perform operations (<code class="docutils literal notranslate"><span class="pre">+</span></code>, etc.) with other <code class="docutils literal notranslate"><span class="pre">Value</span></code> objects to produce new <code class="docutils literal notranslate"><span class="pre">Value</span></code> instances. Each such instance holds:</p>
<ul class="simple">
<li><p>some value <code class="docutils literal notranslate"><span class="pre">data</span></code></p></li>
<li><p>the operation <code class="docutils literal notranslate"><span class="pre">op</span></code> that produced it (e.g. <code class="docutils literal notranslate"><span class="pre">+</span></code>)</p></li>
<li><p>its children <code class="docutils literal notranslate"><span class="pre">prev</span></code>: a tuple of the <code class="docutils literal notranslate"><span class="pre">Value</span></code> instances that produced it</p></li>
<li><p>its gradient <code class="docutils literal notranslate"><span class="pre">grad</span></code> which represents the derivative of some value (e.g. <code class="docutils literal notranslate"><span class="pre">L</span></code>)  (<strong>w.r.t.</strong> the value/instance itself)</p></li>
</ul>
<p>To recap what we have done so far: we have built scalar-valued mathematical expression graphs using operations such as addition and multiplication. To see such a graph that each <code class="docutils literal notranslate"><span class="pre">Value</span></code> object represents, letâ€™s implement visualization function <code class="docutils literal notranslate"><span class="pre">draw()</span></code> and call it on a <code class="docutils literal notranslate"><span class="pre">Value</span></code> object, e.g. <code class="docutils literal notranslate"><span class="pre">L</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">graphviz</span><span class="w"> </span><span class="kn">import</span> <span class="n">Digraph</span>


<span class="k">def</span><span class="w"> </span><span class="nf">trace</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
    <span class="c1"># builds a set of all nodes and edges in a graph</span>
    <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="n">nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">prev</span><span class="p">:</span>
                <span class="n">edges</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">child</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
                <span class="n">build</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="n">build</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span>


<span class="k">def</span><span class="w"> </span><span class="nf">draw</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
    <span class="n">dot</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">&quot;svg&quot;</span><span class="p">,</span> <span class="n">graph_attr</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;rankdir&quot;</span><span class="p">:</span> <span class="s2">&quot;LR&quot;</span><span class="p">})</span>  <span class="c1"># LR = left to right</span>
    <span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span> <span class="o">=</span> <span class="n">trace</span><span class="p">(</span><span class="n">root</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
        <span class="n">uid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
        <span class="c1"># for any value in the graph, create a rectangular (&#39;record&#39;) node for it</span>
        <span class="n">dot</span><span class="o">.</span><span class="n">node</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">uid</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;{ </span><span class="si">%s</span><span class="s2"> | data </span><span class="si">%.4f</span><span class="s2"> | grad </span><span class="si">%.4f</span><span class="s2">}&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span>
            <span class="n">shape</span><span class="o">=</span><span class="s2">&quot;record&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span><span class="p">:</span>
            <span class="c1"># if this value is a result of some operation, create an op node for it</span>
            <span class="n">dot</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">uid</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">n</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
            <span class="c1"># and connect this node to it</span>
            <span class="n">dot</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">uid</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">op</span><span class="p">,</span> <span class="n">uid</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">n1</span><span class="p">,</span> <span class="n">n2</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>
        <span class="c1"># connect n1 to the op node of n2</span>
        <span class="n">dot</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n1</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">n2</span><span class="p">))</span> <span class="o">+</span> <span class="n">n2</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dot</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/51eda2b05c795bcfaa7631a2a24654e77e55ae0a3ccc4a99d7f2169534364452.svg" src="../_images/51eda2b05c795bcfaa7631a2a24654e77e55ae0a3ccc4a99d7f2169534364452.svg" />
</div>
</div>
<p>The above graph depicts a forward pass. As a reminder, each <code class="docutils literal notranslate"><span class="pre">grad</span></code> attribute refers to how much each nudge of the value of a node (e.g. <code class="docutils literal notranslate"><span class="pre">c</span></code>) contributes to the output value <code class="docutils literal notranslate"><span class="pre">L</span></code> (i.e. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c}\)</span>). Letâ€™s now, fill in those gradients by doing <strong>backprop</strong> manually. Starting at <code class="docutils literal notranslate"><span class="pre">L</span></code> (the end of the graph), we ask: what is the derivative of <code class="docutils literal notranslate"><span class="pre">L</span></code> <strong>w.r.t.</strong> itself? Since <span class="math notranslate nohighlight">\(f(L) = L\)</span>, it is: <span class="math notranslate nohighlight">\(\frac{f(L + h) - f(L)}{h} = \frac{L + h - L}{h}\)</span>, â€¦</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">L</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="p">((</span><span class="n">L</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">L</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
<span class="n">L</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.000000000000334
</pre></div>
</div>
</div>
</div>
<p>â€¦which of course equals <span class="math notranslate nohighlight">\(1\)</span>! Now, moving backwards in the graph, what about the derivative of <code class="docutils literal notranslate"><span class="pre">L</span></code> <strong>w.r.t.</strong> <code class="docutils literal notranslate"><span class="pre">e</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial e}\)</span>), for example? Letâ€™s do the same and find out. Since <span class="math notranslate nohighlight">\(L(e) = e \cdot f \Longrightarrow \dfrac{\partial L}{\partial e} = \frac{L(e + h) - L(e)}{h} = \frac{(e + h) \cdot f - e \cdot f}{h} = \frac{e \cdot f + h \cdot f - e \cdot f}{h} = f\)</span>. Similarly, <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial f} = e\)</span>. Therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">data</span>
<span class="n">f</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-2.0
-7.0
</pre></div>
</div>
</div>
</div>
<p>Now, we will find <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c}\)</span>, which basically constitutes the crux of <strong>backprop</strong> for this example and the most important node to understand. So pay attention!
How <em>do</em> we find the derivative of <code class="docutils literal notranslate"><span class="pre">L</span></code> <strong>w.r.t.</strong> <code class="docutils literal notranslate"><span class="pre">c</span></code>? Purely intuitively, just by knowing the sensitivity of <code class="docutils literal notranslate"><span class="pre">L</span></code> to <code class="docutils literal notranslate"><span class="pre">e</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial e}\)</span>) and how <code class="docutils literal notranslate"><span class="pre">e</span></code> is sensitive to <code class="docutils literal notranslate"><span class="pre">c</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial e}{\partial c}\)</span>), we should be able to <em>somehow</em> put this information together and get our result. Since we know the former, we just need to find the latter: <span class="math notranslate nohighlight">\(e(c) = c + d\)</span> and so <span class="math notranslate nohighlight">\(\dfrac{\partial e}{\partial c} = \frac{e(c + h) - e(c)}{h} = \frac{(c + h + d) - (c + d)}{h} = 1\)</span>. And similarly: <span class="math notranslate nohighlight">\(\dfrac{\partial e}{\partial d} = 1\)</span>. That â€œ<em>somehow</em>â€ is the <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>, according to which: <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c} = \dfrac{\partial L}{\partial e} \cdot \dfrac{\partial e}{\partial c}\)</span>. For us, this means:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="mf">1.0</span>
<span class="n">c</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-2.0
</pre></div>
</div>
</div>
</div>
<p>To sum up:</p>
<ul class="simple">
<li><p>we want <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c}\)</span></p></li>
<li><p>we know <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial e}\)</span></p></li>
<li><p>we know <span class="math notranslate nohighlight">\(\dfrac{\partial e}{\partial c}\)</span></p></li>
<li><p>we apply the chain rule: <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c} = \dfrac{\partial L}{\partial e} \cdot \dfrac{\partial e}{\partial c} = \dfrac{\partial L}{\partial e} \cdot 1 = \dfrac{\partial L}{\partial e}\)</span></p></li>
<li><p>similarly: <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial d} = \dfrac{\partial L}{\partial e} \cdot \dfrac{\partial e}{\partial d} = \dfrac{\partial L}{\partial e} \cdot 1 = \dfrac{\partial L}{\partial e}\)</span></p></li>
</ul>
<p>Easy! Notice how <span class="math notranslate nohighlight">\(\dfrac{\partial e}{\partial c} = \dfrac{\partial e}{\partial d} = 1\)</span>. This equality verifies the fact that the addition operator (<code class="docutils literal notranslate"><span class="pre">+</span></code>) just acts as a router, merely distributing the derivative of the result (e.g. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial e}\)</span>) backwards, through all the children nodes (e.g. <code class="docutils literal notranslate"><span class="pre">c</span></code> and <code class="docutils literal notranslate"><span class="pre">d</span></code>), unperturbed.</p>
<p>To complete this session of manual <strong>backprop</strong>, we are going to recurse this process and re-apply the chain rule all the way back to the input nodes, <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>. Thus, we are looking for <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial a}\)</span> and <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial b}\)</span>, meaning:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial a} = \dfrac{\partial L}{\partial d} \cdot \dfrac{\partial d}{\partial a}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial b} = \dfrac{\partial L}{\partial d} \cdot \dfrac{\partial d}{\partial b}\)</span></p></li>
</ul>
<p>We already know that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sa">f</span><span class="s1">&#39;dL/dd = dL/de = </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;dL/dd = dL/de = -2.0&#39;
</pre></div>
</div>
</div>
</div>
<p>But, what about <span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial a}\)</span> and <span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial b}\)</span>? Letâ€™s derive them, the same way we did with <span class="math notranslate nohighlight">\(\dfrac{\partial e}{\partial c}\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial a} = \frac{d(a + h) - d(a)}{h} = \frac{(a + h) \cdot b - a \cdot b}{h} = b\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial d}{\partial b} = a\)</span></p></li>
</ul>
<p>Therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
<span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.0
-4.0
</pre></div>
</div>
</div>
</div>
<p>If we re-draw our computational graph, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4aa049e3d82fbe09701b957a7831afca5adcfa87e6a758f7bfbe087e79c81a6b.svg" src="../_images/4aa049e3d82fbe09701b957a7831afca5adcfa87e6a758f7bfbe087e79c81a6b.svg" />
</div>
</div>
<p>And thatâ€™s what <strong>backprop</strong> is! Just the recursive application of the chain rule backwards through the computational graph for calculating the gradient of a <strong>loss</strong> value (<code class="docutils literal notranslate"><span class="pre">L</span></code>) <strong>w.r.t.</strong> all other values that produced it (<code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, etc.). The purpose being, to be able to use the gradient values to nudge the <strong>loss</strong> value: optimization. This process goes as follows:</p>
<ul class="simple">
<li><p>to <em>increase</em> <code class="docutils literal notranslate"><span class="pre">L</span></code>, let each value follow the gradient in the <em>positive</em> direction</p></li>
<li><p>to <em>decrease</em> <code class="docutils literal notranslate"><span class="pre">L</span></code>, let each value follow the gradient in the <em>negative</em> direction</p></li>
</ul>
<p>Running the following example should increase the value of <code class="docutils literal notranslate"><span class="pre">L</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Lold</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">data</span>
<span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="n">b</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="n">c</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span>
<span class="n">d</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span>
<span class="n">e</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">e</span><span class="o">.</span><span class="n">grad</span>
<span class="n">f</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">f</span><span class="o">.</span><span class="n">grad</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;old L: </span><span class="si">{</span><span class="n">Lold</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;new L: </span><span class="si">{</span><span class="n">L</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>old L: -8.0
new L: -7.695431999999999
</pre></div>
</div>
</div>
</div>
<p>You just learned how to apply one optimization step! Optimization is fundamental when it comes to training <strong>nn</strong>s, as you will seeâ€¦ Eventually we want to build up a level of understanding for training multilayer perceptrons (<strong>mlp</strong>s), as they are called.</p>
</section>
<section id="multilayer-perceptron-example">
<h2>Multilayer perceptron example<a class="headerlink" href="#multilayer-perceptron-example" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;mlp.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/649d51c291077a114e32cdbd282b49841124383ff63a91d612a4d0d2310641c8.jpg" src="../_images/649d51c291077a114e32cdbd282b49841124383ff63a91d612a4d0d2310641c8.jpg" />
</div>
</div>
<p>To gain some intuition necessary for training <strong>mlp</strong>s, letâ€™s first explore an example of doing <strong>backprop</strong> through a simplified neuron.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;neuron.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7f26605feed34a10370b6b0f15a50d3c85808ac73646121e42520def7833f856.jpg" src="../_images/7f26605feed34a10370b6b0f15a50d3c85808ac73646121e42520def7833f856.jpg" />
</div>
</div>
<p>Such a neuron:</p>
<ul class="simple">
<li><p>receives input <span class="math notranslate nohighlight">\(w_i x_i\)</span> (output <span class="math notranslate nohighlight">\(x_i\)</span> of incoming neuron multiplied by a synaptic weights <span class="math notranslate nohighlight">\(w_i\)</span>)</p></li>
<li><p>summates those inputs <span class="math notranslate nohighlight">\(\displaystyle \sum_i w_i x_i\)</span> at the cell body</p></li>
<li><p>adds a bias <span class="math notranslate nohighlight">\(b\)</span></p></li>
<li><p>passes the result through an activation function <span class="math notranslate nohighlight">\(f\)</span></p></li>
<li><p>outputs the value <span class="math notranslate nohighlight">\(f\bigg(\displaystyle \sum_i w_i x_i + b\bigg)\)</span></p></li>
</ul>
<p>An activation function <span class="math notranslate nohighlight">\(f\)</span> is usually represented by a squashing function such as <span class="math notranslate nohighlight">\(\tanh\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "bf89e66d55fd4f949aee777ade6b7e24", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Letâ€™s extend our <code class="docutils literal notranslate"><span class="pre">Value</span></code> class to incorporate this <code class="docutils literal notranslate"><span class="pre">tanh</span></code> activation function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
            <span class="n">prev</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
            <span class="n">op</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To simulate the functionality of this neuron:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">activate_neuron</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x1w1</span><span class="p">,</span> <span class="n">x2w2</span><span class="p">,</span> <span class="n">sum_</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">o</span>
    <span class="c1"># inputs x1,x2</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x2&quot;</span><span class="p">)</span>
    <span class="c1"># weights w1,w2</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;w1&quot;</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;w2&quot;</span><span class="p">)</span>
    <span class="c1"># bias of the neuron</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">6.8813735870195432</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
    <span class="c1"># x1*w1 + x2*w2 + b</span>
    <span class="n">x1w1</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">w1</span>
    <span class="n">x1w1</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;x1*w1&quot;</span>
    <span class="n">x2w2</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">w2</span>
    <span class="n">x2w2</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;x2*w2&quot;</span>
    <span class="n">sum_</span> <span class="o">=</span> <span class="n">x1w1</span> <span class="o">+</span> <span class="n">x2w2</span>
    <span class="n">sum_</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;x1*w1 + x2*w2&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">sum_</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">n</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;n&quot;</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
    <span class="n">o</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;o&quot;</span>  <span class="c1"># see the newly implemented Value.tanh method</span>


<span class="n">activate_neuron</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4082cd13648f5f7d9d0fdbf4cfe80a389702962d8f988645afe93c557178f713.svg" src="../_images/4082cd13648f5f7d9d0fdbf4cfe80a389702962d8f988645afe93c557178f713.svg" />
</div>
</div>
<p>As you can see, this graph represents a neuron that takes inputs and weights and produces a value <code class="docutils literal notranslate"><span class="pre">n</span></code> squashed by an activation function <code class="docutils literal notranslate"><span class="pre">tanh</span></code> that yields an output <code class="docutils literal notranslate"><span class="pre">o</span></code>. But, the <code class="docutils literal notranslate"><span class="pre">grad</span></code> attributes are empty. Letâ€™s fill them up by doing another manual <strong>backprop</strong>! Doing so requires finding out the derivatives of <code class="docutils literal notranslate"><span class="pre">o</span></code> <strong>w.r.t.</strong> the inputs. But of course in a typical <strong>nn</strong> setting what we really care about the most are the derivatives <strong>w.r.t.</strong> the weights <span class="math notranslate nohighlight">\(w_i\)</span> (e.g. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial w_i}\)</span>) because those are the free parameters that are usually changed when improving performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">o</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\dfrac{\partial o}{\partial n} = \dfrac{\partial \tanh(n)}{\partial n} = 1 - \tanh(n)^2\)</span>, according to the <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Derivatives">derivatives of hyperbolic functions</a>. And therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">o</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span>
<span class="n">n</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.4999999999999998
</pre></div>
</div>
</div>
</div>
<p>And since we learnt from before that the <code class="docutils literal notranslate"><span class="pre">+</span></code> operation acts as a gradient router and we can see from our graph that <code class="docutils literal notranslate"><span class="pre">n</span></code> is recursively preceeded by two <code class="docutils literal notranslate"><span class="pre">+</span></code> operations, the children nodesâ€™ <code class="docutils literal notranslate"><span class="pre">grad</span></code> attribute will be the same value as <code class="docutils literal notranslate"><span class="pre">n.grad</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x2w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x1w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">sum_</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">grad</span>
<span class="n">x2w2</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.4999999999999998
</pre></div>
</div>
</div>
</div>
<p>And according to the chain rule, as previously described, the derivatives of the outputs <strong>w.r.t.</strong> to the inputs will be:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial o}{\partial x_1} = \dfrac{\partial o}{\partial {(x_1 \cdot w_1)}} \cdot \dfrac{\partial {(x_1 \cdot w_1)}}{\partial x_1} = \dfrac{\partial o}{\partial {(x_1 \cdot w_1)}} \cdot w_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial o}{\partial w_1} = \dfrac{\partial o}{\partial {(x_1 \cdot w_1)}} \cdot \dfrac{\partial {(x_1 \cdot w_1)}}{\partial w_1} = \dfrac{\partial o}{\partial {(x_1 \cdot w_1)}} \cdot x_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial o}{\partial x_2} = \dfrac{\partial o}{\partial {(x_2 \cdot w_2)}} \cdot \dfrac{\partial {(x_2 \cdot w_2)}}{\partial x_2} = \dfrac{\partial o}{\partial {(x_2 \cdot w_2)}} \cdot w_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial o}{\partial w_2} = \dfrac{\partial o}{\partial {(x_2 \cdot w_2)}} \cdot \dfrac{\partial {(x_2 \cdot w_2)}}{\partial w_2} = \dfrac{\partial o}{\partial {(x_2 \cdot w_2)}} \cdot x_2\)</span></p></li>
</ul>
<p>Thus:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x1w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">w1</span><span class="o">.</span><span class="n">data</span>
<span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x1w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">x1</span><span class="o">.</span><span class="n">data</span>
<span class="n">x2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x2w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">w2</span><span class="o">.</span><span class="n">data</span>
<span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">x2w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">x2</span><span class="o">.</span><span class="n">data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-1.4999999999999993
0.9999999999999996
0.4999999999999998
0.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1bd82d2bfad413539f9a5dc13e3952d9cfad91d5e0632316e2c7db6c709af439.svg" src="../_images/1bd82d2bfad413539f9a5dc13e3952d9cfad91d5e0632316e2c7db6c709af439.svg" />
</div>
</div>
<p>Done. Ok now, doing manual <strong>backprop</strong> is obviously ridiculous! So, we are gonna put an end to our suffering and see how we can do the backward pass automatically by codifying what we have learnt so far. Specifically, inside each operation of a <code class="docutils literal notranslate"><span class="pre">Value</span></code> instance, we will define a <code class="docutils literal notranslate"><span class="pre">backward</span></code> function that calculates the gradient:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<p>Notice how we accumulate each gradient (<code class="docutils literal notranslate"><span class="pre">+=</span></code>) instead of just assigning it (<code class="docutils literal notranslate"><span class="pre">=</span></code>). This guarantees that multiple backward calls on the same node are not overwritten, ensuring that the gradient is properly accumulated. This reflects the contributions from all paths leading to that node which respects the <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule#Multivariable_case">multivariable case of the chain rule</a>. Now, with our enhanced class, letâ€™s reactivate the neuron and calculate its new output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activate_neuron</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Then, letâ€™s do automatic <strong>backprop</strong> with the help of the <code class="docutils literal notranslate"><span class="pre">backward</span></code> functions we just defined. Calling <code class="docutils literal notranslate"><span class="pre">backward</span></code> on a value node will populate the <code class="docutils literal notranslate"><span class="pre">grad</span></code> attribute of the value nodes that produced it: essentially <strong>backprop</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># base case</span>
<span class="n">o</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="n">n</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="n">b</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>  <span class="c1"># leaf node: nothing happens!</span>
<span class="n">sum_</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="n">x2w2</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="n">x1w1</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="n">x2</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>  <span class="c1"># leaf node: nothing happens!</span>
<span class="n">w2</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>  <span class="c1"># leaf node: nothing happens!</span>
<span class="n">x1</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>  <span class="c1"># leaf node: nothing happens!</span>
<span class="n">w1</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>  <span class="c1"># leaf node: nothing happens!</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0a8b673082d6dd30e2ce0005ba3d038ac98f3ba60e438dd5987f5366f4889b81.svg" src="../_images/0a8b673082d6dd30e2ce0005ba3d038ac98f3ba60e438dd5987f5366f4889b81.svg" />
</div>
</div>
<p>Hurray! Awesome. We managed to ease the pain of manually calculating the gradients of each node. However, thereâ€™s a problem: calling <code class="docutils literal notranslate"><span class="pre">backward</span></code> on a node (e.g. <code class="docutils literal notranslate"><span class="pre">x1w1</span></code>) will only work if <code class="docutils literal notranslate"><span class="pre">backward</span></code> has already been called on its descendant node(s) (e.g. <code class="docutils literal notranslate"><span class="pre">n</span></code>). In our graph topology this means that <code class="docutils literal notranslate"><span class="pre">backward</span></code> must be called from the rightmost to the leftmost nodes. This can be done easily by laying out our graph using <a class="reference external" href="https://en.wikipedia.org/wiki/Topological_sorting">topological sorting</a>. Below, we re-do the forward pass and define a function that does so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activate_neuron</span><span class="p">()</span>
<span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
        <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">prev</span><span class="p">:</span>
            <span class="n">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
        <span class="n">topo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>


<span class="n">build_topo</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
<span class="n">topo</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=&#39;1.0&#39;, label=&#39;w2&#39;),
 Value(data=&#39;0.0&#39;, label=&#39;x2&#39;),
 Value(data=&#39;0.0&#39;, label=&#39;x2*w2&#39;),
 Value(data=&#39;-3.0&#39;, label=&#39;w1&#39;),
 Value(data=&#39;2.0&#39;, label=&#39;x1&#39;),
 Value(data=&#39;-6.0&#39;, label=&#39;x1*w1&#39;),
 Value(data=&#39;-6.0&#39;, label=&#39;x1*w1 + x2*w2&#39;),
 Value(data=&#39;6.881373587019543&#39;, label=&#39;b&#39;),
 Value(data=&#39;0.8813735870195432&#39;, label=&#39;n&#39;),
 Value(data=&#39;0.7071067811865477&#39;, label=&#39;o&#39;)]
</pre></div>
</div>
</div>
</div>
<p>Basically, it adds children nodes first before adding non-children nodes. For examples, the first node in the resulting <code class="docutils literal notranslate"><span class="pre">topo</span></code> list is a child node. Whereas, the non-child node <code class="docutils literal notranslate"><span class="pre">o</span></code> is last. In essence, what this sorting allows us to do is to traverse the graph and recursively call <code class="docutils literal notranslate"><span class="pre">backward</span></code> in a safe way, solving the aforementioned problem and thus facilitating automatic <strong>backprop</strong> with a single call. Something like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
    <span class="n">node</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/96ab6330c7b6c45f972c81d2b0c4a48087f307047d05dbc058f0f7846b0c62b9.svg" src="../_images/96ab6330c7b6c45f972c81d2b0c4a48087f307047d05dbc058f0f7846b0c62b9.svg" />
</div>
</div>
<p>Letâ€™s incorporate this feature into our new <code class="docutils literal notranslate"><span class="pre">Value</span></code> class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">prev</span><span class="p">:</span>
                    <span class="n">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                <span class="n">topo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

        <span class="n">build_topo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">node</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>To verify it works, we reactivate our neuron and produce a new output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activate_neuron</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>and verify our one-call <strong>backprop</strong> works:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">draw</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/991a1c2f76a668866b27c0842cc1be0666a6973261aca92f7307b069d945d271.svg" src="../_images/991a1c2f76a668866b27c0842cc1be0666a6973261aca92f7307b069d945d271.svg" />
</div>
</div>
<p>VoilÃ ! <strong>backprop</strong> with a single <code class="docutils literal notranslate"><span class="pre">backward</span></code> call. Letâ€™s test it out one more time:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
<span class="n">c</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;c&quot;</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">d</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;d&quot;</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">d</span> <span class="o">*</span> <span class="n">c</span>
<span class="n">e</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;e&quot;</span>
<span class="n">e</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">draw</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3e25c57924f16fb9c99c650cc0eee8964f903367c7a3c6c6f51f2bd7cf51d450.svg" src="../_images/3e25c57924f16fb9c99c650cc0eee8964f903367c7a3c6c6f51f2bd7cf51d450.svg" />
</div>
</div>
<p>Nice. That was easyâ€¦ right? Next up, just to prove that this process works more generally, letâ€™s add a few more complex operations to our <code class="docutils literal notranslate"><span class="pre">Value</span></code> class. First, a power operation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;only supporting int/float powers for now&quot;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="n">other</span><span class="p">,</span>
            <span class="n">prev</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
            <span class="n">op</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;**</span><span class="si">{</span><span class="n">other</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">**</span> <span class="p">(</span><span class="n">other</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">3.0</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">**</span><span class="n">b</span>
<span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">c</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">b</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">**</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Also, a division operation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># self / other</span>
        <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="n">other</span><span class="o">**-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
<span class="k">assert</span> <span class="n">c</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="o">**-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
<span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span> <span class="n">c</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<p>And last, but not least, an exponentiation operation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">),</span>
            <span class="n">prev</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
            <span class="n">op</span><span class="o">=</span><span class="s2">&quot;exp&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">out</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">out</span><span class="o">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">a_exp</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">a_exp</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">;</span> <span class="n">a_exp</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">a_exp</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<p>Using these newly defined operations, we can now explicitly define our own <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Exponential_definitions"><code class="docutils literal notranslate"><span class="pre">tanh</span></code> operation</a>, expressed as a <em>composite</em> operation, consisting of a combination of operations, as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Value</span><span class="p">(</span><span class="n">Value</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">e</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">e</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">a_tanh</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
<span class="n">a_tanh</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a_tanh</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Furthermore, reactivating the neuron, now  with the new <em>composite</em> <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, and doing <strong>backprop</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activate_neuron</span><span class="p">()</span>
<span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>yieldsâ€¦</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7c2c1852a0566914028f702d1165ce5c35c5657ec4c8a44480823a7a85ecb785.svg" src="../_images/7c2c1852a0566914028f702d1165ce5c35c5657ec4c8a44480823a7a85ecb785.svg" />
</div>
</div>
<p>â€¦the same gradients as before! In the end, what matters is being able to do a functional forward pass and backward pass (<strong>backprop</strong>) on the output of any of such operation we have defined, no matter how â€œcompositeâ€ it is. If you apply the chain rule properly, as demonstrated, the design of the function and itâ€™s complexity are totally up to you. As long as the backward processes are correct, properly, all is good.</p>
<p>So now letâ€™s do the exact same thing using a modern deep neural network library, like <a class="reference external" href="https://PyTorch.org/">PyTorch</a>, on which <strong>micrograd</strong> is roughly modeled.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<p>In PyTorch, the equivalent of our <code class="docutils literal notranslate"><span class="pre">Value</span></code> object is the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> object. Tensors are just n-dimensional arrays of scalars. Examples:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">])</span>  <span class="c1"># 1x1 tensor</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]])</span>  <span class="c1"># 2x3 tensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 2., 3.],
        [4., 5., 6.]])
</pre></div>
</div>
</div>
</div>
<p>Like <strong>micrograd</strong>, PyTorch letâ€™s us construct the mathematical expression of a neuron function on which we can perform <strong>backprop</strong>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">6.8813735870195432</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">,</span> <span class="n">x2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w2&quot;</span><span class="p">,</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7071066904050358
---
x2 0.5000001283844369
w2 0.0
x1 -1.5000003851533106
w1 1.0000002567688737
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tensor()</span></code> makes a tensor, <code class="docutils literal notranslate"><span class="pre">double()</span></code> casts it to a double-precision data type (as Python also does, by default) and <code class="docutils literal notranslate"><span class="pre">requires_grad_()</span></code> enables gradient calculation for that tensor. In PyTorch, one has to explicitly enable it, as gradient calculation is disabled by default for efficiency reasons. Above, we just use single-element scalar tensors. But the whole point of PyTorch is to work with multi-dimensional tensors that can be acted upon in parallel by many different operations. Nevertheless, what we have built very much agrees with the API of PyTorch! Letâ€™s now construct the mathematical expression that constitutes a <strong>nn</strong>, piece by piece, specifically a 2-layer <strong>mlp</strong>. Weâ€™ll start out by writing up a neuron as weâ€™ve implemented it so far, but making it subscribe to the PyTorch API model in how it designs its own <strong>nn</strong> modules.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Neuron</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nin</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">Value</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nin</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">w</span></code> is an array of values that represents the weights of the <code class="docutils literal notranslate"><span class="pre">nin</span></code> input neurons and the bias <code class="docutils literal notranslate"><span class="pre">b</span></code> of this neuron. Now, we are gonna implement the forward pass call: the sum of the products of each <code class="docutils literal notranslate"><span class="pre">w</span></code> and input <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> (<span class="math notranslate nohighlight">\(\sum_i w_i x_i + b\)</span>) passed through a non-linear activation function (e.g. <code class="docutils literal notranslate"><span class="pre">tanh</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Neuron</span><span class="p">(</span><span class="n">Neuron</span><span class="p">):</span>
    <span class="c1"># w * x + b</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">preact</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">preact</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<p>Hereâ€™s an example of passing two inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> through the neuron:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">nin</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">Neuron</span><span class="p">(</span><span class="n">nin</span><span class="p">)</span>
<span class="n">n</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=&#39;-0.43235771996736344&#39;)
</pre></div>
</div>
</div>
</div>
<p>Having done so, letâ€™s create a <code class="docutils literal notranslate"><span class="pre">Layer</span></code> object that represents a layer composed of many neurons as in typical <strong>nn</strong> fashion. It takes the number of input and output neurons (<code class="docutils literal notranslate"><span class="pre">nin</span></code>, <code class="docutils literal notranslate"><span class="pre">nout</span></code>) as arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Layer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nin</span><span class="p">,</span> <span class="n">nout</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="p">[</span><span class="n">Neuron</span><span class="p">(</span><span class="n">nin</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nout</span><span class="p">)]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">outs</span>
</pre></div>
</div>
</div>
</div>
<p>Hereâ€™s an example of passing two inputs <code class="docutils literal notranslate"><span class="pre">x</span></code> through a layer of neurons:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]</span>
<span class="n">nin</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">nout</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">nin</span><span class="p">,</span> <span class="n">nout</span><span class="p">)</span>
<span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=&#39;0.43948525359013224&#39;),
 Value(data=&#39;-0.9903943088037261&#39;),
 Value(data=&#39;0.9985533287198426&#39;)]
</pre></div>
</div>
</div>
</div>
<p>As expected, after passing in <code class="docutils literal notranslate"><span class="pre">nin</span></code> input values, we get <code class="docutils literal notranslate"><span class="pre">nout</span></code> output values. Finally, letâ€™s implement an <strong>mlp</strong>, in which layers basically feed into each other sequentially. First, weâ€™ll define a constructor that creates a list of layers, each one initialized with the proper number of inputs and outputs, given an initial <code class="docutils literal notranslate"><span class="pre">nin</span></code> and <code class="docutils literal notranslate"><span class="pre">nouts</span></code> list. Then, a forward pass method that passes an initial input <code class="docutils literal notranslate"><span class="pre">x</span></code> through each of the layers, from first to last.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nin</span><span class="p">,</span> <span class="n">nouts</span><span class="p">):</span>
        <span class="n">sz</span> <span class="o">=</span> <span class="p">[</span><span class="n">nin</span><span class="p">]</span> <span class="o">+</span> <span class="n">nouts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Layer</span><span class="p">(</span><span class="n">sz</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sz</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nouts</span><span class="p">))]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Hereâ€™s an example reconstructing the <a class="reference internal" href="#MLP-example"><span class="xref myst">mlp example</span></a> we saw previously. It has 3 input neurons, two hidden 4-neuron layers and a 1-neuron output layer, therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">mlp_out</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=&#39;0.8021316553902108&#39;)
</pre></div>
</div>
</div>
</div>
<p>And if we draw our single <strong>mlp</strong> outputâ€¦</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">mlp_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f9a75f39bd0e3400a10c228861e3048bd2f82200b432e10cdda03efa3285f431.svg" src="../_images/f9a75f39bd0e3400a10c228861e3048bd2f82200b432e10cdda03efa3285f431.svg" />
</div>
</div>
<p>Wow, thatâ€™s a big graph! Now obviously, no one sane enough would dare differentiate these expressions with pen and paper. But with <strong>micrograd</strong>, youâ€™ll be able <strong>backprop</strong> all the way through this graph back into the leaf nodes, meaning the weights of our neurons (<code class="docutils literal notranslate"><span class="pre">Neuron.w</span></code>). To test it out, letâ€™s define our own inputs dataset (<code class="docutils literal notranslate"><span class="pre">xs</span></code>) to be fed into our <strong>mlp</strong>, as well as their corresponding target values <code class="docutils literal notranslate"><span class="pre">ys</span></code>. Basically, a simple binary classifier <strong>nn</strong>, since input corresponds to either <code class="docutils literal notranslate"><span class="pre">1.0</span></code> or <code class="docutils literal notranslate"><span class="pre">-1.0</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># desired targets</span>
</pre></div>
</div>
</div>
</div>
<p>Each one of the input vectors corresponds to each target value of <code class="docutils literal notranslate"><span class="pre">y</span></code>. E.g. <code class="docutils literal notranslate"><span class="pre">[2.0,</span> <span class="pre">3.0,</span> <span class="pre">-1.0]</span></code> corresponds to target <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, <code class="docutils literal notranslate"><span class="pre">[3.0,</span> <span class="pre">-1.0,</span> <span class="pre">0.5]</span></code> to <code class="docutils literal notranslate"><span class="pre">-1.0</span></code>, and so on. Now, <code class="docutils literal notranslate"><span class="pre">ys</span></code> are the <em>desired</em> values. Now, letâ€™s pass <code class="docutils literal notranslate"><span class="pre">xs</span></code> through our <strong>mlp</strong> and get the predicted values <code class="docutils literal notranslate"><span class="pre">ypreds</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>


<span class="n">ypreds</span> <span class="o">=</span> <span class="n">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
<span class="n">ypreds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=&#39;0.8021316553902108&#39;),
 Value(data=&#39;0.694914372587992&#39;),
 Value(data=&#39;0.8630928931795546&#39;),
 Value(data=&#39;0.9566959159578626&#39;)]
</pre></div>
</div>
</div>
</div>
<p>As you can see, currently, our <strong>mlp</strong> outputs values (<code class="docutils literal notranslate"><span class="pre">ypreds</span></code>) that are different from the desired target values we want (<code class="docutils literal notranslate"><span class="pre">ys</span></code>). So, how do we make <code class="docutils literal notranslate"><span class="pre">ypreds</span></code> equal, or at least as close as possible, to <code class="docutils literal notranslate"><span class="pre">ys</span></code>? Specifically, how do we tune the weights (aka, train our <strong>nn</strong> <strong>w.r.t.</strong> them) in order to better predict the desired targets? The trick used in deep learning that helps us achieve this, is to calculate a single number that somehow measures how well the <strong>nn</strong> is performing. This number is called the <strong>loss</strong>. Right now, <code class="docutils literal notranslate"><span class="pre">ypreds</span></code> is not close to <code class="docutils literal notranslate"><span class="pre">ys</span></code>, so we are not perfoming very well, meaning that the <strong>loss</strong> value is high. Our goal is to minimize the <strong>loss</strong> value and, by doing so, bring <code class="docutils literal notranslate"><span class="pre">ypreds</span></code> closer to <code class="docutils literal notranslate"><span class="pre">ys</span></code>. One way to calculate the <strong>loss</strong> value is through the mean squared error (<strong>MSE</strong>) function. This function will iterate over pairs of <code class="docutils literal notranslate"><span class="pre">ys</span></code> and <code class="docutils literal notranslate"><span class="pre">ypreds</span></code> and sum the squares of their differences. The squared difference of two values (e.g. a prediction and target value) discards any negative signs and gives us a positive quantification that represents how those values differ: their <strong>loss</strong>. If the <strong>loss</strong> is zero, the two values do not differ and are equal. Whereas if the <strong>loss</strong> is another, non-zero number, the two values are different and unequal. In general, the more two values differ, the greater their <strong>loss</strong> will be. The less they differ, the lower their <strong>loss</strong> will be. The final <strong>MSE</strong> <strong>loss</strong> of all such pairs will just be the sum of all the squared differences:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calc_loss</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">([(</span><span class="n">ypreds</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">ypreds</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)])</span>


<span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we want to minimize the <strong>loss</strong>, because if the <strong>loss</strong> is low, then every one of the predictions is equal to its target. The lowest the <strong>loss</strong> value can be is <code class="docutils literal notranslate"><span class="pre">0</span></code> (the ideal value). Whereas, the greater it is, the worse off the <strong>nn</strong> is predicting. So, how do we minimize the <strong>loss</strong>? First, <strong>backprop</strong>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>After a backward call, something magical happens! All gradients of the <strong>loss</strong> <strong>w.r.t.</strong> the weights of our <strong>mlp</strong> get  calculated. To see this with our eyes, letâ€™s print out the <code class="docutils literal notranslate"><span class="pre">grad</span></code> attribute of any of the weights, e.g.:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.15533830793163983
</pre></div>
</div>
</div>
</div>
<p>We see that because the gradient of this particular weight of this particular neuron of this particular layer of our <strong>nn</strong> is negative, we infer that its influence on the <strong>loss</strong> is also negative. Which means, that slightly increasing this particular weight, would make the <strong>loss</strong> value go down. Now if we draw the <strong>loss</strong>â€¦</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fcb3d095e3beb1ffbd82d53e8a883a9312d7160c7ee154fdd6feb7c9a9b76ded.svg" src="../_images/fcb3d095e3beb1ffbd82d53e8a883a9312d7160c7ee154fdd6feb7c9a9b76ded.svg" />
</div>
</div>
<p>we see that itâ€™s massive! The reason is that the loss depends on all the values produced by forward passing all the <code class="docutils literal notranslate"><span class="pre">x</span></code> values contained in <code class="docutils literal notranslate"><span class="pre">xs</span></code> into the <strong>mlp</strong>. Now, although the gradients of all the values in the graph have been calculated, we only care about the gradients of the parameters we want to change, namely the weights and biases. So, our next steps are to gather those parameters and tune them by nudging them based on their gradient information. To do so, we first extend the <strong>mlp</strong> component classes by implementing parameter getter methods:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Neuron</span><span class="p">(</span><span class="n">Neuron</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Layer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">neuron</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">(</span><span class="n">MLP</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<p>After getting our output and loss again (due to redefining the classes)â€¦</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ypreds</span> <span class="o">=</span> <span class="n">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>we may inspect all the parametersâ€¦</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp_params</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=&#39;-0.5891124237861678&#39;), Value(data=&#39;0.28090981749146215&#39;), Value(data=&#39;0.7547091264633694&#39;), Value(data=&#39;-0.5146778088364283&#39;), Value(data=&#39;0.8672995598598101&#39;), Value(data=&#39;0.7406510232644734&#39;), Value(data=&#39;0.5288406626421873&#39;), Value(data=&#39;0.08447495239111946&#39;), Value(data=&#39;-0.6563246126894655&#39;), Value(data=&#39;0.3981255695120931&#39;), Value(data=&#39;-0.012968303988030394&#39;), Value(data=&#39;0.7542094002676307&#39;), Value(data=&#39;0.0033286930198876963&#39;), Value(data=&#39;0.2556304142635857&#39;), Value(data=&#39;-0.9314080289080375&#39;), Value(data=&#39;0.4054173907532759&#39;), Value(data=&#39;0.774679311925067&#39;), Value(data=&#39;-0.18811978158086262&#39;), Value(data=&#39;0.7575932811823123&#39;), Value(data=&#39;0.4477844375844444&#39;), Value(data=&#39;0.39651768767863205&#39;), Value(data=&#39;0.05901494877107494&#39;), Value(data=&#39;0.8809980312429269&#39;), Value(data=&#39;-0.7064254928263687&#39;), Value(data=&#39;0.17787965036070652&#39;), Value(data=&#39;0.9795480701510118&#39;), Value(data=&#39;-0.345249225309854&#39;), Value(data=&#39;0.5363299378699073&#39;), Value(data=&#39;-0.34760846138944257&#39;), Value(data=&#39;0.7486671457569554&#39;), Value(data=&#39;0.7160267321114855&#39;), Value(data=&#39;0.5698199508762045&#39;), Value(data=&#39;-0.5563952037955955&#39;), Value(data=&#39;-0.4432545915770989&#39;), Value(data=&#39;-0.7637467843038042&#39;), Value(data=&#39;0.46496054602256986&#39;), Value(data=&#39;-0.8182517028250273&#39;), Value(data=&#39;-0.6649446947755555&#39;), Value(data=&#39;0.39870787529352847&#39;), Value(data=&#39;-0.37824313155178624&#39;), Value(data=&#39;-0.884314155961383&#39;)]
41
</pre></div>
</div>
</div>
</div>
<p>and change them in a manner that decreases the loss. Before we optimize all of them though, letâ€™s first optimize only one to see exactly how the process is done:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.34342225762161804
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.5891124237861678
</pre></div>
</div>
</div>
</div>
<p>To understand how to optimize the parameters, we need to understand the relationship between <code class="docutils literal notranslate"><span class="pre">grad</span></code> and <strong>loss</strong> through two distinct cases:</p>
<p><strong>A</strong>. A <em>parameter</em>â€™s positive <code class="docutils literal notranslate"><span class="pre">grad</span></code> (<code class="docutils literal notranslate"><span class="pre">&gt;</span> <span class="pre">0</span></code>) value tells us that:</p>
<ul class="simple">
<li><p>(<strong>1</strong>) <em>increasing</em> that <em>parameter</em> would <em>increase</em> the <strong>loss</strong></p></li>
<li><p>(<strong>2</strong>) <em>decreasing</em> that <em>parameter</em> would <em>decrease</em> the <strong>loss</strong></p></li>
</ul>
<p><strong>B</strong>. A <em>parameter</em>â€™s negative <code class="docutils literal notranslate"><span class="pre">grad</span></code> (<code class="docutils literal notranslate"><span class="pre">&lt;</span> <span class="pre">0</span></code>) value tells us that:</p>
<ul class="simple">
<li><p>(<strong>1</strong>) <em>increasing</em> that parameter would <em>decrease</em> the <strong>loss</strong></p></li>
<li><p>(<strong>2</strong>) <em>decreasing</em> that parameter would <em>increase</em> the <strong>loss</strong></p></li>
</ul>
<p>Since, we only care about <em>decreasing</em> the <strong>loss</strong> (since we want to minimize it), we only care about cases <strong>A2</strong> and <strong>B1</strong>. Therefore, we now know that <em>decreasing</em> the <strong>loss</strong> depends on changing the weight by an amount whose sign is the opposite of the <code class="docutils literal notranslate"><span class="pre">grad</span></code> value. Since the opposite of a value is just its negation, we describe the general optimization step required to change each parameter as follows:</p>
<blockquote>
<div><p>To decrease the <strong>loss</strong>, change each parameter <code class="docutils literal notranslate"><span class="pre">p</span></code> in the direction that is opposite the direction of the the gradient <code class="docutils literal notranslate"><span class="pre">grad</span></code> of the <strong>loss</strong> <strong>w.r.t.</strong> <code class="docutils literal notranslate"><span class="pre">p</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial p}\)</span>).</p>
</div></blockquote>
<p>This can be achieved by subtracting the product of the gradient and a small step size <span class="math notranslate nohighlight">\(\alpha\)</span> from the current parameter value:</p>
<p><span class="math notranslate nohighlight">\(p = p - \alpha\dfrac{\partial loss}{\partial p}\)</span>, where <span class="math notranslate nohighlight">\(Î±\)</span> is a usually small positive number (e.g. <span class="math notranslate nohighlight">\(0.001\)</span>) called the learning rate, which determines how big of a step size is to be taken during the optimization step. Since we descend the gradient, we call this process gradient descent. Letâ€™s now optimize one weight parameter with one step of gradient descent:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ypreds</span> <span class="o">=</span> <span class="n">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="c1"># zero out gradients (so they don&#39;t accummulate)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">mlp</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.001</span> <span class="o">*</span> <span class="n">mlp</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span>
<span class="n">ypreds_after</span> <span class="o">=</span> <span class="n">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
<span class="n">loss_after</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">ypreds_after</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss before optimization step: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss after optimization step: </span><span class="si">{</span><span class="n">loss_after</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss before optimization step: Value(data=&#39;6.606895868054532&#39;)
Loss after optimization step: Value(data=&#39;6.6067779753029345&#39;)
</pre></div>
</div>
</div>
</div>
<p>See? Loss after the step is lower than before it. So, optimizing even one parameter with this procedure, decreases the <strong>loss</strong> of our <strong>mlp</strong>! But in order to decrease it even more, let alone minimize it, we must also optimize all of the parameters. How? Like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ypreds</span> <span class="o">=</span> <span class="n">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="c1"># zero gradients (so they don&#39;t accummulate)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.001</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
<span class="n">ypreds_after</span> <span class="o">=</span> <span class="p">[</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
<span class="n">loss_after</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([(</span><span class="n">ypreds</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">ypreds</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ypreds_after</span><span class="p">,</span> <span class="n">ys</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss before optimization step: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss after optimization step: </span><span class="si">{</span><span class="n">loss_after</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">y</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ypreds</span><span class="p">])</span>
<span class="nb">print</span><span class="p">([</span><span class="n">y</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ypreds_after</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loss before optimization step: Value(data=&#39;6.6067779753029345&#39;)
Loss after optimization step: Value(data=&#39;6.56156718545472&#39;)
[-0.7786370603743357, -0.41961729625360783, -0.9397577797813043, -0.7614638727095784]
[-0.7716021275092538, -0.41675489429569573, -0.9388225451270673, -0.7547294814149134]
[1.0, -1.0, -1.0, 1.0]
</pre></div>
</div>
</div>
</div>
<p>Same thing happens: <strong>loss</strong> decreases! Most importantly, each time we re-run the optimization, the prediction values get even closer to the target values, which is our primary goal! In general, we must be careful with our step size. Too small of a step size will take many many steps to decrease the <strong>loss</strong>, whereas too big of a step size might be an <em>overstep</em> that causes an increase instead of a decrease of the <strong>loss</strong>. Sometimes, if the increase is too big, the <strong>loss</strong> value explodes. Finding a step size that is just right can be a subtle art sometimes. You can play around with the step size, re-running the cells each time, to see its effect. All in all, we have been able to derive a set of parameters (weights and biases):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Value(data=&#39;-0.5897989998671106&#39;),
 Value(data=&#39;0.2803740639577263&#39;),
 Value(data=&#39;0.7550020342196525&#39;),
 Value(data=&#39;-0.5149260060116513&#39;),
 Value(data=&#39;0.8673092472790822&#39;),
 Value(data=&#39;0.7407064996722453&#39;),
 Value(data=&#39;0.5287909012313932&#39;),
 Value(data=&#39;0.08450678345753262&#39;),
 Value(data=&#39;-0.6577188091598949&#39;),
 Value(data=&#39;0.3963168801724268&#39;),
 Value(data=&#39;-0.012013893833775947&#39;),
 Value(data=&#39;0.7532520827391431&#39;),
 Value(data=&#39;0.002588577562609616&#39;),
 Value(data=&#39;0.2557082084784828&#39;),
 Value(data=&#39;-0.931431643158572&#39;),
 Value(data=&#39;0.40511915330400977&#39;),
 Value(data=&#39;0.7764577716116262&#39;),
 Value(data=&#39;-0.189860095784784&#39;),
 Value(data=&#39;0.7563382171213389&#39;),
 Value(data=&#39;0.44572918234887604&#39;),
 Value(data=&#39;0.3945828727895028&#39;),
 Value(data=&#39;0.05929905243207291&#39;),
 Value(data=&#39;0.8807232529636616&#39;),
 Value(data=&#39;-0.7066137541919688&#39;),
 Value(data=&#39;0.17756069037644506&#39;),
 Value(data=&#39;0.9792410059041483&#39;),
 Value(data=&#39;-0.3452696799447055&#39;),
 Value(data=&#39;0.5363447266323415&#39;),
 Value(data=&#39;-0.34752650190640577&#39;),
 Value(data=&#39;0.7487529046819845&#39;),
 Value(data=&#39;0.7160476085724314&#39;),
 Value(data=&#39;0.5696464123075088&#39;),
 Value(data=&#39;-0.5562098813916081&#39;),
 Value(data=&#39;-0.4436842004682603&#39;),
 Value(data=&#39;-0.7640376420712507&#39;),
 Value(data=&#39;0.4651341160686098&#39;),
 Value(data=&#39;-0.8165050609850243&#39;),
 Value(data=&#39;-0.6632768321623527&#39;),
 Value(data=&#39;0.40059372617437855&#39;),
 Value(data=&#39;-0.3808573235417481&#39;),
 Value(data=&#39;-0.8824037909061284&#39;)]
</pre></div>
</div>
</div>
</div>
<p>that makes our network predict the target values. Basically, we have learned how to train a <strong>nn</strong>! But, letâ€™s make it a bit more respectable by implementing a training loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="c1"># forward pass</span>
    <span class="n">ypreds</span> <span class="o">=</span> <span class="n">forward_pass_mlp</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss</span><span class="p">(</span><span class="n">ypreds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
    <span class="c1"># zero gradients (so they don&#39;t accummulate)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="c1"># backward pass</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># update</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">mlp</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6.56156718545472
1 3.1886742323366386
2 1.3032015611952863
3 0.769939759300913
4 0.5477680622174333
5 0.4342573296227225
6 0.3561670649634663
7 0.2988718838759568
8 0.255522186577933
9 0.2218579442103602
10 0.19512822582659206
11 0.17349684405903604
12 0.15570168219740943
13 0.14085232054584612
14 0.12830585626266888
15 0.11758842447875281
16 0.10834425517148419
17 0.1003017900448846
18 0.09325064258909059
19 0.08702561304645334
</pre></div>
</div>
</div>
</div>
<p>After training, the prediction values should be significantly closer to target values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">([</span><span class="n">y</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ypreds</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.8563656958279531, -0.7991263758236825, -0.9425547459839091, 0.849186772421105]
[1.0, -1.0, -1.0, 1.0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-demo">
<h2>Training demo<a class="headerlink" href="#training-demo" title="Link to this heading">#</a></h2>
<p>Last but not least, letâ€™s implement a small yet full demo of training an 2-layer <strong>nn</strong> (MLP) binary classifier. This is achieved by initializing a <strong>nn</strong> from micrograd.nn module, implementing a simple <a class="reference external" href="https://en.wikipedia.org/wiki/Support_vector_machine">svm</a> â€œmax-marginâ€ binary classification <strong>loss</strong> and using SGD for optimization. Try and figure out what is happening here by reading the code!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_moons</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1337</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1337</span><span class="p">)</span>

<span class="c1"># make up a dataset</span>
<span class="n">X_moons</span><span class="p">,</span> <span class="n">y_moons</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">y_moons</span> <span class="o">=</span> <span class="n">y_moons</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># make y be -1 or 1</span>
<span class="c1"># visualize in 2D</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_moons</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;jet&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a498b736b65c40a3a14cf85a64a9e6c1", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">micrograd.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLP</span>

<span class="c1"># initialize a model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># 2-layer neural network</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of parameters&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]
number of parameters 337
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">micrograd.engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">Value</span>


<span class="c1"># loss function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">loss</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># inline DataLoader :)</span>
    <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">X_moons</span><span class="p">,</span> <span class="n">y_moons</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ri</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X_moons</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">X_moons</span><span class="p">[</span><span class="n">ri</span><span class="p">],</span> <span class="n">y_moons</span><span class="p">[</span><span class="n">ri</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">Value</span><span class="p">,</span> <span class="n">xrow</span><span class="p">))</span> <span class="k">for</span> <span class="n">xrow</span> <span class="ow">in</span> <span class="n">Xb</span><span class="p">]</span>

    <span class="c1"># forward the model to get scores</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span>

    <span class="c1"># svm &quot;max-margin&quot; loss</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span> <span class="o">+</span> <span class="o">-</span><span class="n">yi</span> <span class="o">*</span> <span class="n">scorei</span><span class="p">)</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span> <span class="k">for</span> <span class="n">yi</span><span class="p">,</span> <span class="n">scorei</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">yb</span><span class="p">,</span> <span class="n">scores</span><span class="p">)]</span>
    <span class="n">data_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span>
    <span class="c1"># L2 regularization</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">reg_loss</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">((</span><span class="n">p</span> <span class="o">*</span> <span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">data_loss</span> <span class="o">+</span> <span class="n">reg_loss</span>

    <span class="c1"># also get accuracy</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">[(</span><span class="n">yi</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">scorei</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">yi</span><span class="p">,</span> <span class="n">scorei</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">yb</span><span class="p">,</span> <span class="n">scores</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">total_loss</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>


<span class="n">total_loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">loss</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">total_loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Value(data=0.8958441028683222, grad=0) 0.5
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># optimization</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>

    <span class="c1"># forward</span>
    <span class="n">total_loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">loss</span><span class="p">()</span>

    <span class="c1"># backward</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># update (sgd)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">k</span> <span class="o">/</span> <span class="mi">100</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;step </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> loss </span><span class="si">{</span><span class="n">total_loss</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">, accuracy </span><span class="si">{</span><span class="n">acc</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>step 0 loss 0.8958441028683222, accuracy 50.0%
step 1 loss 1.7235905336972022, accuracy 81.0%
step 2 loss 0.742900631385113, accuracy 77.0%
step 3 loss 0.7705641260584201, accuracy 82.0%
step 4 loss 0.36927933859765383, accuracy 84.0%
step 5 loss 0.313545481918522, accuracy 86.0%
step 6 loss 0.2814234349772434, accuracy 89.0%
step 7 loss 0.2688873331398391, accuracy 91.0%
step 8 loss 0.2567147286057417, accuracy 91.0%
step 9 loss 0.2704862551637922, accuracy 91.0%
step 10 loss 0.2450702385365804, accuracy 91.0%
step 11 loss 0.25099055297915035, accuracy 92.0%
step 12 loss 0.2156095185192295, accuracy 91.0%
step 13 loss 0.23090378446402726, accuracy 93.0%
step 14 loss 0.20152151227899445, accuracy 92.0%
step 15 loss 0.2257450627928222, accuracy 93.0%
step 16 loss 0.19447987596204108, accuracy 92.0%
step 17 loss 0.21089496199246363, accuracy 93.0%
step 18 loss 0.15983077356303602, accuracy 94.0%
step 19 loss 0.18453748746883916, accuracy 93.0%
step 20 loss 0.1897752285608764, accuracy 91.0%
step 21 loss 0.19072704042579644, accuracy 93.0%
step 22 loss 0.11733695088756486, accuracy 97.0%
step 23 loss 0.12173524408232453, accuracy 95.0%
step 24 loss 0.12615712612770452, accuracy 95.0%
step 25 loss 0.16049097780801674, accuracy 95.0%
step 26 loss 0.1874719770524581, accuracy 92.0%
step 27 loss 0.16741837891059402, accuracy 95.0%
step 28 loss 0.09586583491455397, accuracy 97.0%
step 29 loss 0.08778783707420913, accuracy 96.0%
step 30 loss 0.1173129756901185, accuracy 95.0%
step 31 loss 0.09340146460619832, accuracy 97.0%
step 32 loss 0.12454454903103453, accuracy 95.0%
step 33 loss 0.07984002652777268, accuracy 97.0%
step 34 loss 0.07727519232921673, accuracy 97.0%
step 35 loss 0.07661250143094474, accuracy 98.0%
step 36 loss 0.10610492379198366, accuracy 96.0%
step 37 loss 0.0906280842926597, accuracy 99.0%
step 38 loss 0.10671887043036926, accuracy 95.0%
step 39 loss 0.052256599219758455, accuracy 98.0%
step 40 loss 0.06016009895234463, accuracy 100.0%
step 41 loss 0.08596724533333938, accuracy 96.0%
step 42 loss 0.05112107943179595, accuracy 99.0%
step 43 loss 0.05240142401642826, accuracy 97.0%
step 44 loss 0.04530684179001569, accuracy 100.0%
step 45 loss 0.07211073370655094, accuracy 97.0%
step 46 loss 0.0333423865131023, accuracy 99.0%
step 47 loss 0.03143222795751125, accuracy 100.0%
step 48 loss 0.03658536747111507, accuracy 99.0%
step 49 loss 0.048291393823903, accuracy 99.0%
step 50 loss 0.09875114765619629, accuracy 96.0%
step 51 loss 0.05449063965875445, accuracy 99.0%
step 52 loss 0.03392679435708304, accuracy 100.0%
step 53 loss 0.05261517263568441, accuracy 97.0%
step 54 loss 0.03250295251424922, accuracy 99.0%
step 55 loss 0.028883273872078178, accuracy 100.0%
step 56 loss 0.0413915110402724, accuracy 98.0%
step 57 loss 0.018987407426128484, accuracy 100.0%
step 58 loss 0.02523833523883741, accuracy 100.0%
step 59 loss 0.020796565213418904, accuracy 100.0%
step 60 loss 0.032597111578102314, accuracy 99.0%
step 61 loss 0.017863351693480252, accuracy 100.0%
step 62 loss 0.023008717832211714, accuracy 100.0%
step 63 loss 0.02207932546358144, accuracy 100.0%
step 64 loss 0.029432917853529687, accuracy 99.0%
step 65 loss 0.016251514644091886, accuracy 100.0%
step 66 loss 0.028468534483264467, accuracy 99.0%
step 67 loss 0.013994365546208726, accuracy 100.0%
step 68 loss 0.015552344843651377, accuracy 100.0%
step 69 loss 0.03389119946160171, accuracy 99.0%
step 70 loss 0.014229870065926912, accuracy 100.0%
step 71 loss 0.013255281583285499, accuracy 100.0%
step 72 loss 0.012300277590022056, accuracy 100.0%
step 73 loss 0.012676052498355978, accuracy 100.0%
step 74 loss 0.02059381195595472, accuracy 100.0%
step 75 loss 0.01184539820536442, accuracy 100.0%
step 76 loss 0.01601269747288313, accuracy 100.0%
step 77 loss 0.025458360239222075, accuracy 100.0%
step 78 loss 0.014382930289661883, accuracy 100.0%
step 79 loss 0.011698962425817966, accuracy 100.0%
step 80 loss 0.012318500800515811, accuracy 100.0%
step 81 loss 0.014121117031464226, accuracy 100.0%
step 82 loss 0.01166459196244621, accuracy 100.0%
step 83 loss 0.011589314549188727, accuracy 100.0%
step 84 loss 0.010990299347735226, accuracy 100.0%
step 85 loss 0.01098922672069161, accuracy 100.0%
step 86 loss 0.010988193757655071, accuracy 100.0%
step 87 loss 0.010987200447388707, accuracy 100.0%
step 88 loss 0.010986246779084925, accuracy 100.0%
step 89 loss 0.010985332742365276, accuracy 100.0%
step 90 loss 0.010984458327280174, accuracy 100.0%
step 91 loss 0.010983623524308865, accuracy 100.0%
step 92 loss 0.010982828324359074, accuracy 100.0%
step 93 loss 0.010982072718767003, accuracy 100.0%
step 94 loss 0.010981356699297043, accuracy 100.0%
step 95 loss 0.010980680258141725, accuracy 100.0%
step 96 loss 0.010980043387921508, accuracy 100.0%
step 97 loss 0.010979446081684675, accuracy 100.0%
step 98 loss 0.010978888332907227, accuracy 100.0%
step 99 loss 0.010978370135492717, accuracy 100.0%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize decision boundary</span>

<span class="n">h</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
<span class="n">Xmesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">Value</span><span class="p">,</span> <span class="n">xrow</span><span class="p">))</span> <span class="k">for</span> <span class="n">xrow</span> <span class="ow">in</span> <span class="n">Xmesh</span><span class="p">]</span>
<span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_moons</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_moons</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">());</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "4e3b3c67b2ff4b8c8ea10862d06fac9e", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Letâ€™s now bring together and summarize what we have learned. <strong>nn</strong>s are functions that take the data as well as the weights and biases (parameters) as inputs to produce outputs. The outputs, or <em>predictions</em>, together with target values are then passed through a <strong>loss</strong> function that yields an error called the <strong>loss</strong> value that represents their distance, in terms of how close or far apart they are. We train <strong>nn</strong>s by using <strong>backprop</strong> to calculate the gradient of the <strong>loss</strong> <strong>w.r.t.</strong> the parameters. These gradients are used to then optimize those parameters using gradient descent in order to minimize the <strong>loss</strong>. The end goal is for the <strong>nn</strong> to perform a given task. Although relatively simple expressions, it turns out that <strong>nn</strong>s can solve very complicated problems. Sometimes, if the problem is complex enough, and the <strong>nn</strong> big enough (&gt; 100 billion parameters), fascinating proprties arise, as in the case of <a class="reference external" href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer">GPT</a>, for example. This model, trained with large amounts of text from the internet can predict, given some words, the next words in a sequence.</p>
</section>
<section id="outro">
<h2>Outro<a class="headerlink" href="#outro" title="Link to this heading">#</a></h2>
<p>This brings us to the end of the <a class="reference internal" href="#1.-micrograd"><span class="xref myst"><strong>micrograd</strong> tutorial</span></a>. I hope you enjoyed this tutorial and found it helpful! For more (tests, demo, etc.), see the <a class="reference external" href="https://github.com/karpathy/micrograd">original micrograd repository</a>. Next up, itâ€™s time to <em>make more</em>!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./micrograduate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="README.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">ðŸ“– Read ðŸŒ»</p>
      </div>
    </a>
    <a class="right-next"
       href="makemore1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sneak-peak">Sneak peak</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multilayer-perceptron-example">Multilayer perceptron example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-demo">Training demo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christos Karaneen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <style>
  button#theme-switch-button {
    display: none !important;
  }
</style>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>