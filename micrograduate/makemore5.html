
<!DOCTYPE html>


<html lang="en" data-content_root="../" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. makemore (part 5): building a WaveNet &#8212; microgra‚àáuate</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=5dd6a0ac" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'micrograduate/makemore5';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. picoGPT: implementing a tiny GPT from scratch" href="picogpt.html" />
    <link rel="prev" title="5. makemore (part 4): becoming a backprop ninja" href="makemore4.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
    
    <img src="../_static/book_logo.png" class="logo__image only-light" alt="microgra‚àáuate - Home"/>
    <script>document.write(`<img src="../_static/book_logo.png" class="logo__image only-dark" alt="microgra‚àáuate - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    ü§î Description ‚ÑπÔ∏è
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="micrograd.html">1. <strong>micrograd</strong>: implementing an autograd engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore1.html">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore2.html">3. <strong>makemore</strong> (part 2): mlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore3.html">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore4.html">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="picogpt.html">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate/issues/new?title=Issue%20on%20page%20%2Fmicrograduate/makemore5.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/micrograduate/makemore5.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>6. makemore (part 5): building a WaveNet</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#starter-code-walkthrough">Starter code walkthrough</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-loss-plot">Fixing the <strong>loss</strong> plot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchifying-the-code-layers-containers-torch-nn">torchifying the code: layers, containers, torch.nn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wavenet-overview"><strong>WaveNet</strong> overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bumping-the-context-size-to-8">Bumping the context size to <span class="math notranslate nohighlight">\(8\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-wavenet">Implementing <strong>WaveNet</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-wavenet-first-pass">Training the <strong>WaveNet</strong>: first pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-batchnorm1d-bug">Fixing the BatchNorm1d bug</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#re-training-the-wavenet-after-bug-fix">Re-training the <strong>WaveNet</strong> after bug fix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-up-our-wavenet">Scaling up our <strong>WaveNet</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wavenet-but-with-dilated-causal-convolutions">WaveNet but with dilated causal convolutions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="makemore-part-5-building-a-wavenet">
<h1>6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong><a class="headerlink" href="#makemore-part-5-building-a-wavenet" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cloning repo...&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>--quiet<span class="w"> </span>https://github.com/ckarageorgkaneen/micrograduate.git<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
    <span class="o">%</span><span class="k">cd</span> micrograduate
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Installing requirements...&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>uv
    <span class="o">!</span>uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--system<span class="w"> </span>--quiet<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</div>
</div>
<section id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Link to this heading">#</a></h2>
<p>Hi, everyone! Today we are continuing our implementation of <strong>makemore</strong>, our favorite character-level language model. Now, over the last few lectures, we‚Äôve built up an architecture that is a <strong>mlp</strong> character-level language model. So we see that it receives <span class="math notranslate nohighlight">\(3\)</span> previous characters and tries to predict the <span class="math notranslate nohighlight">\(4th\)</span> character in a sequence using one hidden layer of neurons with <code class="docutils literal notranslate"><span class="pre">tanh</span></code> nonlinearities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;bengio2003nn.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b4c83268a9c4ea410c7992495e79920b1fed6d29199d3c6fa361c4a52adc46b9.jpg" src="../_images/b4c83268a9c4ea410c7992495e79920b1fed6d29199d3c6fa361c4a52adc46b9.jpg" />
</div>
</div>
<p>So what we‚Äôd like to do now in this lecture is to complexify this architecture. In particular, we would like to take more characters in a sequence as an input, not just <span class="math notranslate nohighlight">\(3\)</span>. In addition to that, we don‚Äôt just want to feed them all into a single hidden layer, because that squashes too much information too quickly. Instead, we would like to make a deeper model that progressively fuses this information to make its guess about the next character in a sequence. We‚Äôre actually going to arrive at something that looks very much like <a class="reference external" href="https://arxiv.org/abs/1609.03499"><strong>WaveNet</strong>, a paper published by DeepMind in 2016</a>. Which is a language model basically, but it tries to predict audio sequences instead of character-level sequences or word-level sequences:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig1.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0131453c5797c5501bbe064d62f21ba93ea2aeb9d87f5ed517da0ae8d2ba687a.png" src="../_images/0131453c5797c5501bbe064d62f21ba93ea2aeb9d87f5ed517da0ae8d2ba687a.png" />
</div>
</div>
<p>But fundamentally, the modeling setup is identical. It is an autoregressive model and it tries to predict the next character in a sequence:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_eq1.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/291439b0a5a8a1e9d7ab7f6a61dc32a9070f588ceb76f45a7b3ba23b27d7f0f9.png" src="../_images/291439b0a5a8a1e9d7ab7f6a61dc32a9070f588ceb76f45a7b3ba23b27d7f0f9.png" />
</div>
</div>
<p>And the architecture actually takes this interesting hierarchical sort of approach to predicting the next character in a sequence with this tree-like structure:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig3.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" src="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" />
</div>
</div>
<p>And this is the architecture:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig4.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0abd535b358d10352195a86e330cde56781960bd5c47d6b440db9cbc74a6eeb2.png" src="../_images/0abd535b358d10352195a86e330cde56781960bd5c47d6b440db9cbc74a6eeb2.png" />
</div>
</div>
<p>And we‚Äôre going to implement it in this lesson. So let‚Äôs get started!</p>
</section>
<section id="starter-code-walkthrough">
<h2>Starter code walkthrough<a class="headerlink" href="#starter-code-walkthrough" title="Link to this heading">#</a></h2>
<p>The starter code for this part is very similar to where we ended up in <strong>makemore</strong> (part 3). So very briefly, we are doing imports:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span> <span class="c1"># for making figures</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> inline
<span class="k">else</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> ipympl
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2147483647</span>
</pre></div>
</div>
</div>
</div>
<p>We are reading our data set of words:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># read in all the words</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;names.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32033
15
[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;, &#39;charlotte&#39;, &#39;mia&#39;, &#39;amelia&#39;]
</pre></div>
</div>
</div>
</div>
<p>And we are processing the dataset of words into lots and lots of individual examples:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build the vocabulary of characters and mappings to/from integers</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
<span class="n">ctoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ctoi</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">itoc</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ctoi</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">itoc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">itoc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}
27
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">:</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
            <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">build_all_datasets</span><span class="p">(</span><span class="n">block_size</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
    <span class="n">xtrain_dataset</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="n">n1</span><span class="p">],</span> <span class="n">block_size</span><span class="p">)</span>  <span class="c1"># 80%</span>
    <span class="n">xval_dataset</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">],</span> <span class="n">block_size</span><span class="p">)</span>  <span class="c1"># 10%</span>
    <span class="n">xtest_dataset</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n2</span><span class="p">:],</span> <span class="n">block_size</span><span class="p">)</span>  <span class="c1"># 10%</span>
    <span class="k">return</span> <span class="n">xtrain_dataset</span><span class="p">,</span> <span class="n">xval_dataset</span><span class="p">,</span> <span class="n">xtest_dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">print_next_character</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">ix</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="n">x</span><span class="p">),</span> <span class="s2">&quot;--&gt;&quot;</span><span class="p">,</span> <span class="n">itoc</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</pre></div>
</div>
</div>
</div>
<p>Specifically many examples of‚Ä¶</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mi">3</span>  <span class="c1"># context length: how many characters do we take to predict the next one?</span>
<span class="p">)</span>
<span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">),</span> <span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">),</span> <span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">ytest</span><span class="p">)</span> <span class="o">=</span> <span class="n">build_all_datasets</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])
</pre></div>
</div>
</div>
</div>
<p>‚Ä¶ <code class="docutils literal notranslate"><span class="pre">block_size=3</span></code> characters and we are trying to predict the fourth one:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_next_character</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span> <span class="n">ytrain</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>... --&gt; y
..y --&gt; u
.yu --&gt; h
yuh --&gt; e
uhe --&gt; n
hen --&gt; g
eng --&gt; .
... --&gt; d
..d --&gt; i
.di --&gt; o
dio --&gt; n
ion --&gt; d
ond --&gt; r
ndr --&gt; e
dre --&gt; .
... --&gt; x
..x --&gt; a
.xa --&gt; v
xav --&gt; i
avi --&gt; e
</pre></div>
</div>
</div>
</div>
<p>Basically, we are breaking down each of these word into little problems of ‚Äúgiven <span class="math notranslate nohighlight">\(3\)</span> characters, predict the <span class="math notranslate nohighlight">\(4th\)</span> one‚Äù. So this is our data set and this is what we‚Äôre trying to get the <strong>nn</strong> to do. Now in <strong>makemore</strong> (part 3), we started to develop our code around these following layer modules. We‚Äôre doing this because we want to think of these modules as lego building blocks that we can sort of stack up into <strong>nn</strong>s and we can feed data between these layers and stack them up into sort of graphs. Now we also developed these layers to have APIs and signatures very similar to <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">those that are found in PyTorch</a>. And so we have the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer, the <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code> layer and the <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> layer that we developed previously:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">))</span> <span class="o">/</span> <span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span> <span class="o">+</span> <span class="p">([]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">])</span>


<span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm1d</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># parameters (trained with backprop)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># buffers (trained with a running &#39;momentum update&#39;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># calculate the forward pass</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># batch mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># batch variance</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>
        <span class="n">xhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">xvar</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>  <span class="c1"># normalize to unit variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">xhat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="c1"># update the buffers</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xmean</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xvar</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Tanh</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<p>Œënd <code class="docutils literal notranslate"><span class="pre">Linear</span></code> just does a matrix multiply in the forward pass of this module, <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code> of course is this crazy layer that we developed in the previous lecture. What‚Äôs crazy about it is‚Ä¶ well there‚Äôs many things. Number one, it has these running mean and variances that are trained outside of <strong>backprop</strong>. They are trained using exponential moving average inside this layer when we call the forward pass. In addition to that, there‚Äôs this <code class="docutils literal notranslate"><span class="pre">self.training</span></code> flag because the behavior of <strong>batchnorm</strong> is different during train time and evaluation time. And so suddenly we have to be very careful that <strong>batchnorm</strong> is in its correct state. That it‚Äôs in the evaluation state or training state. So that‚Äôs something to now keep track of something that sometimes introduces bugs because you forget to put it into the right mode. And finally, we saw that <strong>batchnorm</strong> couples the statistics or the activations across the examples in the batch. So normally we thought of the batch as just an efficiency thing, but now we are coupling the computation across batch elements and it‚Äôs done for the purposes of controlling the activation statistics as we saw in the previous video. So <strong>batchnorm</strong> is a very weird layer because you have to modulate the training and eval phase. What‚Äôs more, you have to wait for the mean and the variance to settle and to actually reach a steady state and a state can become the source of many bugs, usually. And now let‚Äôs define the appropriate functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># seed rng for reproducability</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7ffa740b3d90&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_embd</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># the dimensionality of the character embedding vectors</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># the number of neurons in the hidden layer of the MLP</span>


<span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">C</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">))</span>
    <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
        <span class="n">Tanh</span><span class="p">(),</span>
        <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="c1"># parameter init</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>  <span class="c1"># last layer make less confident</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>  <span class="c1"># number of parameters in total</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xb</span><span class="p">]</span>  <span class="c1"># embed the characters into vectors</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate the vectors</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>  <span class="c1"># loss function</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
    <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">break_at_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
        <span class="c1"># minibatch construct</span>
        <span class="n">bix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,))</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bix</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">150000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
        <span class="n">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">break_at_step</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">lossi</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trigger_eval_mode</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">l</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">infer_loss</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sample_from_model</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>  <span class="c1"># initialize with all ...</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># forward pass the neural net</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">])]</span>  <span class="c1"># (1, block_size, n_embd)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate the vectors</span>
            <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sample from the distribution</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># shift the context window and track the samples</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="c1"># if we sample the special &#39;.&#39; token, break</span>
            <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>  <span class="c1"># decode and print the generated word</span>
</pre></div>
</div>
</div>
</div>
<p>These should look somewhat familiar to you by now. Let‚Äôs train!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12097
      0/ 200000: 3.2966
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  10000/ 200000: 2.2322
  20000/ 200000: 2.4111
  30000/ 200000: 2.1004
  40000/ 200000: 2.3157
  50000/ 200000: 2.2104
  60000/ 200000: 1.9653
  70000/ 200000: 1.9767
  80000/ 200000: 2.6738
  90000/ 200000: 2.0837
 100000/ 200000: 2.2730
 110000/ 200000: 1.7491
 120000/ 200000: 2.2891
 130000/ 200000: 2.3443
 140000/ 200000: 2.1731
 150000/ 200000: 1.8246
 160000/ 200000: 1.7614
 170000/ 200000: 2.2419
 180000/ 200000: 2.0803
 190000/ 200000: 2.1326
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lossi</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c0ff1cdd47ba47c79578bbd5f0c5f983", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This <strong>loss</strong> function looks very crazy. We should probably fix this. And that‚Äôs because <span class="math notranslate nohighlight">\(32\)</span> batch elements are too few. And so you can get very lucky or unlucky in any one of these batches, and it creates a very thicc <strong>loss</strong> function. So we‚Äôre gonna fix that soon. Now, before we evaluate the trained <strong>nn</strong> by inferring the training and validation <strong>loss</strong>, we need to remember because of the <strong>batchnorm</strong> layers to set all the layers‚Äô <code class="docutils literal notranslate"><span class="pre">training</span></code> flag to <code class="docutils literal notranslate"><span class="pre">False</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trigger_eval_mode</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.0583250522613525
val 2.1065292358398438
</pre></div>
</div>
</div>
</div>
<p>We still have a ways to go, as far as the validation <strong>loss</strong> is concerned. But if we sample from our model, we see that we get relatively name-like results that do no exist in the training set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_from_model</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>damiara.
alyzah.
fard.
azalee.
sayah.
ayvi.
reino.
sophemuellani.
ciaub.
alith.
sira.
liza.
jah.
grancealynna.
jamaur.
ben.
quan.
torie.
coria.
cer.
</pre></div>
</div>
</div>
</div>
<p>But we can improve our <strong>loss</strong> and improve our results even further. We‚Äôll start by fixing that thicc loss plot!</p>
</section>
<section id="fixing-the-loss-plot">
<h2>Fixing the <strong>loss</strong> plot<a class="headerlink" href="#fixing-the-loss-plot" title="Link to this heading">#</a></h2>
<p>One way to turn this thicc loss plot into a normal one is to only plot the mean. Remember, <code class="docutils literal notranslate"><span class="pre">lossi</span></code> is a very long list of floats that contains a <strong>loss</strong> for each training episode:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">lossi</span><span class="p">),</span> <span class="n">lossi</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(200000,
 [0.5180676579475403,
  0.5164594054222107,
  0.507362961769104,
  0.507546603679657,
  0.4992470443248749])
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs segment this very long list into a <span class="math notranslate nohighlight">\(2D\)</span> tensor of rows, with each row containing <span class="math notranslate nohighlight">\(1000\)</span> loss values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">t_loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.5181, 0.5165, 0.5074,  ..., 0.4204, 0.3860, 0.4014],
        [0.3937, 0.3930, 0.4177,  ..., 0.3788, 0.3896, 0.4054],
        [0.3426, 0.4191, 0.3918,  ..., 0.4447, 0.4419, 0.2821],
        ...,
        [0.3625, 0.3517, 0.3376,  ..., 0.3266, 0.3191, 0.3271],
        [0.2550, 0.3659, 0.2968,  ..., 0.2744, 0.3853, 0.3300],
        [0.3041, 0.2740, 0.3213,  ..., 0.3081, 0.4082, 0.3207]])
</pre></div>
</div>
</div>
</div>
<p>Now, if we take the mean of each row, we end up with a list of <strong>loss</strong> averages:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_t_loss</span> <span class="o">=</span> <span class="n">t_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mean_t_loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.4059, 0.3791, 0.3698, 0.3681, 0.3657, 0.3639, 0.3624, 0.3593, 0.3557,
        0.3561, 0.3516, 0.3515, 0.3504, 0.3501, 0.3491, 0.3477, 0.3498, 0.3474,
        0.3494, 0.3449, 0.3456, 0.3440, 0.3452, 0.3461, 0.3429, 0.3456, 0.3458,
        0.3438, 0.3408, 0.3437, 0.3435, 0.3407, 0.3424, 0.3412, 0.3415, 0.3404,
        0.3419, 0.3391, 0.3414, 0.3396, 0.3392, 0.3408, 0.3394, 0.3416, 0.3389,
        0.3390, 0.3376, 0.3407, 0.3364, 0.3376, 0.3393, 0.3362, 0.3371, 0.3349,
        0.3393, 0.3369, 0.3363, 0.3349, 0.3338, 0.3386, 0.3366, 0.3388, 0.3370,
        0.3379, 0.3349, 0.3378, 0.3325, 0.3358, 0.3353, 0.3390, 0.3369, 0.3366,
        0.3354, 0.3350, 0.3375, 0.3347, 0.3352, 0.3352, 0.3318, 0.3359, 0.3348,
        0.3338, 0.3350, 0.3367, 0.3331, 0.3333, 0.3346, 0.3356, 0.3339, 0.3339,
        0.3332, 0.3331, 0.3352, 0.3356, 0.3350, 0.3335, 0.3330, 0.3299, 0.3344,
        0.3350, 0.3318, 0.3295, 0.3328, 0.3336, 0.3345, 0.3341, 0.3319, 0.3342,
        0.3329, 0.3299, 0.3346, 0.3312, 0.3312, 0.3344, 0.3340, 0.3305, 0.3319,
        0.3344, 0.3302, 0.3315, 0.3335, 0.3319, 0.3345, 0.3326, 0.3331, 0.3319,
        0.3317, 0.3331, 0.3316, 0.3313, 0.3319, 0.3340, 0.3306, 0.3329, 0.3306,
        0.3322, 0.3332, 0.3313, 0.3309, 0.3348, 0.3297, 0.3324, 0.3305, 0.3311,
        0.3316, 0.3308, 0.3301, 0.3323, 0.3289, 0.3313, 0.3199, 0.3201, 0.3196,
        0.3233, 0.3184, 0.3179, 0.3180, 0.3172, 0.3175, 0.3176, 0.3200, 0.3194,
        0.3196, 0.3195, 0.3186, 0.3166, 0.3192, 0.3179, 0.3168, 0.3171, 0.3173,
        0.3188, 0.3175, 0.3176, 0.3174, 0.3197, 0.3182, 0.3167, 0.3187, 0.3217,
        0.3165, 0.3187, 0.3144, 0.3165, 0.3183, 0.3187, 0.3179, 0.3161, 0.3182,
        0.3177, 0.3171, 0.3187, 0.3194, 0.3183, 0.3157, 0.3156, 0.3167, 0.3168,
        0.3187, 0.3179])
</pre></div>
</div>
</div>
</div>
<p>If we plot this tensor list of mean losses, we should get a nicer <strong>loss</strong> plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_t_loss</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "42dc20cba22c45458c6f9877637adf70", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now, the progress we make during training is much more clearly visible! Also, notice the learning rate decay, where the <strong>loss</strong> drops to a even lower minimum. This is the <strong>loss</strong> plot we are going to be using going forward.</p>
</section>
<section id="torchifying-the-code-layers-containers-torch-nn">
<h2>torchifying the code: layers, containers, torch.nn<a class="headerlink" href="#torchifying-the-code-layers-containers-torch-nn" title="Link to this heading">#</a></h2>
<p>Now it‚Äôs time to simplify our forward function a little bit. Notice how the embeddings and flattening operations are calculated outside of the layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xb</span><span class="p">]</span> <span class="c1"># embed the characters into vectors</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># concatenate the vectors</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>To start tidying things up, let‚Äôs mirror <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding"><code class="docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten</span></code></a> with our own incredibly simplified equivalent modules:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Embedding</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">embd_dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">embd_dim</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ix</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Flatten</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<p>These will simply be responsible for indexing and flattening. We can now simplify our forward pass by including the embedding and flattening operations as modules in the definition of the layers:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
        <span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
        <span class="n">Tanh</span><span class="p">(),</span>
        <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="c1"># parameter init</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>  <span class="c1"># last layer make less confident</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>  <span class="c1"># number of parameters in total</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">layers</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xb</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>  <span class="c1"># loss function</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>Awesome. Now we can even further simplify our forward pass by replacing the list that contains our layers with our simplified implementation of the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"><code class="docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code></a> container: this object contains layers and the functionality to iteratively pass data through them. Meaning that we now define a bunch of layers as a <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> object (i.e. a model) through which we can pass input data (e.g. <code class="docutils literal notranslate"><span class="pre">x</span></code>), without the need to explicitly loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Sequential</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># get parameters of all layers and stretch them out into one list</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs now further simplify our functions by replacing the <code class="docutils literal notranslate"><span class="pre">layers</span></code> list with <code class="docutils literal notranslate"><span class="pre">model</span></code>, a <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> object:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
            <span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># parameter init</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>  <span class="c1"># last layer make less confident</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>  <span class="c1"># number of parameters in total</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>  <span class="c1"># loss function</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
    <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">break_at_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
        <span class="c1"># minibatch construct</span>
        <span class="n">bix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batchsize</span><span class="p">,))</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">bix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bix</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="n">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">150000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
        <span class="n">update</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="c1"># track stats</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">break_at_step</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">lossi</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trigger_eval_mode</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">l</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sample_from_model</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>  <span class="c1"># initialize with all ...</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># forward pass the neural net</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">]))</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sample from the distribution</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># shift the context window and track the samples</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="c1"># if we sample the special &#39;.&#39; token, break</span>
            <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>  <span class="c1"># decode and print the generated word</span>
</pre></div>
</div>
</div>
</div>
<p>And let‚Äôs verify that our new definitions work by re-training our <strong>nn</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12097
      0/ 200000: 3.3055
  10000/ 200000: 2.1954
  20000/ 200000: 2.2630
  30000/ 200000: 2.0618
  40000/ 200000: 2.0468
  50000/ 200000: 2.1775
  60000/ 200000: 2.1750
  70000/ 200000: 1.9390
  80000/ 200000: 2.1816
  90000/ 200000: 2.0516
 100000/ 200000: 2.0578
 110000/ 200000: 2.2706
 120000/ 200000: 2.3313
 130000/ 200000: 2.1557
 140000/ 200000: 2.0983
 150000/ 200000: 1.9418
 160000/ 200000: 1.9421
 170000/ 200000: 2.1256
 180000/ 200000: 2.2467
 190000/ 200000: 1.6821
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3bb3fea1b9d14a5aa8d6287c63f7ddac", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trigger_eval_mode</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.0581207275390625
val 2.105104684829712
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_from_model</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>masea.
iman.
ryy.
ayee.
havajine.
miliakendalikain.
amagntanton.
aviona.
jah.
wiseegh.
avon.
man.
tovi.
sullessa.
marcuz.
jazia.
abellabell.
athin.
ahkiara.
krister.
</pre></div>
</div>
</div>
</div>
<p>Cool. Now it‚Äôs time to decrease the loss even further by scaling up our model to make it bigger and deeper!</p>
</section>
<section id="wavenet-overview">
<h2><strong>WaveNet</strong> overview<a class="headerlink" href="#wavenet-overview" title="Link to this heading">#</a></h2>
<p>Currently, we are using this architecture here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;bengio2003nn.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b4c83268a9c4ea410c7992495e79920b1fed6d29199d3c6fa361c4a52adc46b9.jpg" src="../_images/b4c83268a9c4ea410c7992495e79920b1fed6d29199d3c6fa361c4a52adc46b9.jpg" />
</div>
</div>
<p>where we are taking in some number of characters, going into a single hidden layer, and then going to the prediction of the next character. The problem here is we don‚Äôt have a naive way of making this bigger in a productive way. We could, of course, use our <strong>nn</strong>. We could use our layers, sort of like building block materials to introduce additional layers here and make the network deeper. But it is still the case that we are crushing all of the characters into a single layer all the way at the beginning. And even if we make this a layer bigger by adding neurons, it‚Äôs still kind of like silly to squash all that information so fast in a single step. What we‚Äôd like to do instead is we‚Äôd like our network to look a lot more like this <strong>WaveNet</strong> case:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig3.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" src="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" />
</div>
</div>
<p>So you see in <strong>WaveNet</strong>, when we are trying to make the prediction for the next character in a sequence a function of the previous characters that feed in. But it is not the case that all of these different characters are just crushed to a single layer and then you have a sandwich. They are crushed slowly. So in particular, we take two characters and we fuse them into sort of like a bigram representation. And we do that for all these characters consecutively. And then we take the bigrams and we fuse those into four character level chunks. And then we fuse again. And so we do that in this tree-like hierarchical manner. So we fuse the information from the previous context <em>gradually</em>, as the network deepens. This is the kind of architecture that we want to implement. Now in the <strong>WaveNet</strong> case, this is a visualization of a stack of <em>dilated</em> causal convolution layers. And this makes it sound very scary, but actually the idea is quite simple. And the fact that it‚Äôs a <em>dilated</em> causal convolution layer is really just an implementation detail to make everything fast. We‚Äôre going to see that later. But for now, let‚Äôs just keep going. We‚Äôre going to keep the basic idea of it, which is this progressive fusion. So we want to make the network deeper, and at each level, we want to fuse only two consecutive elements. Two characters, then two bigrams, then two fourgrams, and so on. So let‚Äôs implement this.</p>
</section>
<section id="bumping-the-context-size-to-8">
<h2>Bumping the context size to <span class="math notranslate nohighlight">\(8\)</span><a class="headerlink" href="#bumping-the-context-size-to-8" title="Link to this heading">#</a></h2>
<p>Okay, so first up, let me scroll to where we built the dataset, and let‚Äôs change the block size from <span class="math notranslate nohighlight">\(3\)</span> to <span class="math notranslate nohighlight">\(8\)</span>. So we‚Äôre going to be taking <span class="math notranslate nohighlight">\(8\)</span> characters of context to predict the <span class="math notranslate nohighlight">\(9th\)</span> character:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">),</span> <span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">),</span> <span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">ytest</span><span class="p">)</span> <span class="o">=</span> <span class="n">build_all_datasets</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([182473, 8]) torch.Size([182473])
torch.Size([22827, 8]) torch.Size([22827])
torch.Size([22846, 8]) torch.Size([22846])
</pre></div>
</div>
</div>
</div>
<p>So the dataset now looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_next_character</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span> <span class="n">ytrain</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>........ --&gt; c
.......c --&gt; a
......ca --&gt; t
.....cat --&gt; h
....cath --&gt; y
...cathy --&gt; .
........ --&gt; k
.......k --&gt; e
......ke --&gt; n
.....ken --&gt; a
....kena --&gt; d
...kenad --&gt; i
..kenadi --&gt; .
........ --&gt; a
.......a --&gt; m
......am --&gt; i
.....ami --&gt; .
........ --&gt; l
.......l --&gt; a
......la --&gt; r
</pre></div>
</div>
</div>
</div>
<p>These <span class="math notranslate nohighlight">\(8\)</span> characters are going to be processed in the above tree-like structure. Let‚Äôs find out how to implement this hierarchical scheme! But before doing that, let‚Äôs train our simple fully-connected <strong>nn</strong> with this new dataset and see how well it performs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22097
      0/ 200000: 3.3024
  10000/ 200000: 2.1462
  20000/ 200000: 2.2304
  30000/ 200000: 2.1978
  40000/ 200000: 2.3442
  50000/ 200000: 2.1926
  60000/ 200000: 2.4338
  70000/ 200000: 2.0021
  80000/ 200000: 2.0781
  90000/ 200000: 1.7328
 100000/ 200000: 2.2064
 110000/ 200000: 1.9591
 120000/ 200000: 1.9200
 130000/ 200000: 1.7876
 140000/ 200000: 2.0151
 150000/ 200000: 1.9124
 160000/ 200000: 1.9154
 170000/ 200000: 2.4858
 180000/ 200000: 2.0312
 190000/ 200000: 1.7150
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ff615d43a98747bd9c51619102571855", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trigger_eval_mode</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.9159809350967407
val 2.0343399047851562
</pre></div>
</div>
</div>
</div>
<p>Interesting! The <strong>loss</strong> has improved compared to the <code class="docutils literal notranslate"><span class="pre">block_size</span> <span class="pre">=</span> <span class="pre">3</span></code> case. Let‚Äôs log our losses so far:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105</span>
<span class="c1"># context: 3 -&gt; 8 (22K params): train 1.915, val 2.034</span>
</pre></div>
</div>
<p>Also, if we sample from the model, we can see the names improving qualitatively as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_from_model</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>kobi.
pran.
marlecm.
lunghan.
camillo.
shatar.
elizee.
lumarius.
deris.
brook.
madaniy.
yarel.
milaal.
aylen.
nikora.
niani.
sahanlaa.
elaya.
malixa.
dalioluw.
</pre></div>
</div>
</div>
</div>
<p>So we could, of course, spend a lot of time here tuning things and scaling up our network further. But let‚Äôs continue and let‚Äôs implement the hierarchical model and treat this as just a rough baseline performance. There‚Äôs a lot of optimization left on the table in terms of some of the hyperparameters that you‚Äôre hopefully getting a sense of now.</p>
</section>
<section id="implementing-wavenet">
<h2>Implementing <strong>WaveNet</strong><a class="headerlink" href="#implementing-wavenet" title="Link to this heading">#</a></h2>
<p>Let‚Äôs now create a bit of a scratch space for us to just look at the forward pass of the  <strong>nn</strong> and inspect the shape of the tensors along the way of the forward pass:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s look at a batch of just 4 examples</span>
<span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">xtrain</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">ytrain</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">xb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 8])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0,  0,  0,  0,  0,  0,  0, 12],
        [ 0,  0,  0,  0,  0,  0, 18,  5],
        [ 0,  0,  0, 11,  1, 12,  9, 14],
        [ 0,  0,  0,  0,  0, 11,  9, 18]])
</pre></div>
</div>
</div>
</div>
<p>Here we are just temporarily, for debugging purposes, creating a batch of just, say, <span class="math notranslate nohighlight">\(4\)</span> examples. So <span class="math notranslate nohighlight">\(4\)</span> random integers. Then, we are plucking out those rows from our training set. And then we are passing into the model the input <code class="docutils literal notranslate"><span class="pre">xb</span></code>. Now the shape of <code class="docutils literal notranslate"><span class="pre">xb</span></code> here, because we only have <span class="math notranslate nohighlight">\(4\)</span> examples. And <span class="math notranslate nohighlight">\(8\)</span> is the current block size. So <code class="docutils literal notranslate"><span class="pre">xb</span></code> contains <span class="math notranslate nohighlight">\(4\)</span> rows/examples of <span class="math notranslate nohighlight">\(8\)</span>  characters each. And each integer tensor row of <code class="docutils literal notranslate"><span class="pre">xb</span></code> just contains the identities of those characters. Therefore, the first layer of our <strong>nn</strong> is the embedding layer. So passing <code class="docutils literal notranslate"><span class="pre">xb</span></code>, this integer tensor, through the <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer creates an output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># output of Embedding layer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 8, 10])
</pre></div>
</div>
</div>
</div>
<p>So our embedding table <code class="docutils literal notranslate"><span class="pre">C</span></code> has, for each character, a <span class="math notranslate nohighlight">\(10\)</span>-dimensional vector (<code class="docutils literal notranslate"><span class="pre">n_embd=10</span></code>) that we are trying to learn. What the layer does here is it plucks out the embedding vector for each one of these integers (of <code class="docutils literal notranslate"><span class="pre">xb</span></code> and organizes it all in a <span class="math notranslate nohighlight">\(4\times8\times10\)</span> tensor. So all of these integers are translated into <span class="math notranslate nohighlight">\(10\)</span>-dimensional vectors inside this <span class="math notranslate nohighlight">\(3\)</span>-dimensional tensor now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># output of Flatten layer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 80])
</pre></div>
</div>
</div>
</div>
<p>Now passing that through the <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer, as you recall, what this does is it views this tensor as just a <span class="math notranslate nohighlight">\(4\times80\)</span> tensor. And what that effectively does is that all these <span class="math notranslate nohighlight">\(10\)</span>-dimensional embeddings for all these <span class="math notranslate nohighlight">\(8\)</span> characters just end up being stretched out into a long row. And that looks kind of like a concatenation operation, basically. So by viewing the tensor differently, we now have a <span class="math notranslate nohighlight">\(4\times80\)</span>. And inside this <span class="math notranslate nohighlight">\(80\)</span>, it‚Äôs all the <span class="math notranslate nohighlight">\(10\)</span>-dimensional vectors just concatenated next to each other.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># output of Linear layer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 200])
</pre></div>
</div>
</div>
</div>
<p>And the linear layer, of course, takes <span class="math notranslate nohighlight">\(80\)</span> and creates <span class="math notranslate nohighlight">\(200\)</span> channels just via matrix multiplication. So far, so good. Now let‚Äôs see something surprising. Let‚Äôs look at the insides of the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer and remind ourselves how it works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">fan_out</span><span class="p">))</span> <span class="o">/</span> <span class="n">fan_in</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">fan_out</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
<span class="o">...</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer here in a forward pass takes the input <code class="docutils literal notranslate"><span class="pre">x</span></code>, multiplies it with a weight and then optionally adds a bias. And the weight is <span class="math notranslate nohighlight">\(2D\)</span>, as defined here, and the bias is <span class="math notranslate nohighlight">\(1D\)</span>. So effectively, in terms of the shapes involved, what‚Äôs happening inside this <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer looks like this right now. And we‚Äôre using random numbers here, but just to illustrate the shapes and what happens:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 200])
</pre></div>
</div>
</div>
</div>
<p>Basically, a <span class="math notranslate nohighlight">\(4\times80\)</span> comes into the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer, gets multiplied by a <span class="math notranslate nohighlight">\(80\times200\)</span> weight matrix inside, and then there‚Äôs a plus <span class="math notranslate nohighlight">\(200\)</span> bias. And the shape of the whole thing that comes out of the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer is <span class="math notranslate nohighlight">\(4\times200\)</span>, as we see here. Notice, by the way, that the matrix multiplication here will create a <span class="math notranslate nohighlight">\(4x200\)</span> tensor, and then when adding <span class="math notranslate nohighlight">\(200\)</span> there‚Äôs a broadcasting happening here, but since <span class="math notranslate nohighlight">\(4x200\)</span> broadcasts with <span class="math notranslate nohighlight">\(200\)</span>, everything works here. So now the surprising thing is how this works. Specifically, something you may not expect is that this input here, that is being matrix-multiplied, doesn‚Äôt actually have to be <span class="math notranslate nohighlight">\(2D\)</span>. This matrix multiply operator in <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> is quite powerful, and in fact, you can actually pass in higher dimensional arrays or tensors, and everything works fine. So for example, <code class="docutils literal notranslate"><span class="pre">torch.randn(4,</span> <span class="pre">80)</span></code> could instead be <code class="docutils literal notranslate"><span class="pre">torch.randn(4,</span> <span class="pre">5,</span> <span class="pre">80)</span></code> (<span class="math notranslate nohighlight">\(4\times5\times80\)</span>) and the result in that case would become <span class="math notranslate nohighlight">\(4\times5\times200\)</span>. You can add as many dimensions as you like to the left of the last dimension of the input tensor (here, dimension <span class="math notranslate nohighlight">\(80\)</span>). And so effectively, what‚Äôs happening is that the matrix multiplication only works on a matrix multiplication on the last dimension, and the dimensions before it in the input tensor are left unchanged. So basically, these dimensions to the left of the last dimension are all treated as just a batch dimension. So we can have multiple batch dimensions (e.g. <code class="docutils literal notranslate"><span class="pre">torch.randn(4,</span> <span class="pre">5,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">80)</span></code>), and then in parallel over all those dimensions, we are doing the matrix multiplication only on the last dimension. So this is quite convenient, because we can use that in our <strong>nn</strong> now. Remember that we have these <span class="math notranslate nohighlight">\(8\)</span> characters coming in, e.g.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1 2 3 4 5 6 7 8</span>
</pre></div>
</div>
<p>And we don‚Äôt want to now flatten all of it out into a large <span class="math notranslate nohighlight">\(8\)</span>-dimensional vector, because we don‚Äôt want to matrix multiply <span class="math notranslate nohighlight">\(80\)</span> into a weight matrix multiply immediately. Instead, we want to group these like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># (1 2) (3 4) (5 6) (7 8)</span>
</pre></div>
</div>
<p>So every consecutive two elements should now basically be flattened and multiplied by a weight matrix. But the idea is that all of these four groups here, we‚Äôd like to process in parallel. So it‚Äôs kind of like a extra batch dimension that we can introduce. And then we can, in parallel, basically process all of these bigram groups in the four extra batch dimension of an individual example, and also over the actual batch dimension of the four examples. So let‚Äôs see what this is all about and how that works. Right now, we take a <span class="math notranslate nohighlight">\(4\times80\)</span> and multiply it by <span class="math notranslate nohighlight">\(80\times200\)</span> in the linear layer. Effectively, what we want is instead of <span class="math notranslate nohighlight">\(8\)</span> characters (<span class="math notranslate nohighlight">\(80\)</span> embedding numbers) coming in, we only want <span class="math notranslate nohighlight">\(2\)</span> characters (<span class="math notranslate nohighlight">\(20\)</span> embedding numbers) to come in. Therefore, if we want that, we can‚Äôt have a <span class="math notranslate nohighlight">\(4x80\)</span> feeding into the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer, but instead <span class="math notranslate nohighlight">\(4\)</span> groups of <span class="math notranslate nohighlight">\(2\)</span> characters to be feeding in, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 4, 200])
</pre></div>
</div>
</div>
</div>
<p>Therefore, what we would want to do now is change the <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer so that it doesn‚Äôt output a <span class="math notranslate nohighlight">\(4x80\)</span> but a <span class="math notranslate nohighlight">\(4x4x20\)</span> where basically in each row tensor of <code class="docutils literal notranslate"><span class="pre">xb</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0,  0,  0,  0,  0,  0,  0, 12],
        [ 0,  0,  0,  0,  0,  0, 18,  5],
        [ 0,  0,  0, 11,  1, 12,  9, 14],
        [ 0,  0,  0,  0,  0, 11,  9, 18]])
</pre></div>
</div>
</div>
</div>
<p>every two consecutive characters (e.g. <span class="math notranslate nohighlight">\((0, 0), (10, 21), (12,  9), (5, 1)\)</span>) are packed in on the very last dimension (i.e. <span class="math notranslate nohighlight">\(20\)</span>). So that the first dimension (i.e. <span class="math notranslate nohighlight">\(4\)</span>) is the first batch dimension and the second dimension (i.e. <span class="math notranslate nohighlight">\(4\)</span>) is the second batch dimension. And this is where we want to get to:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 4, 200])
</pre></div>
</div>
</div>
</div>
<p>Now we have to change our <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer (so that it doesn‚Äôt fully flatten out the examples, but creates a <span class="math notranslate nohighlight">\(4\times4\times20\)</span> instead of a <span class="math notranslate nohighlight">\(4\times80\)</span>) and our <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer (to expect <span class="math notranslate nohighlight">\(20\)</span> instead of <span class="math notranslate nohighlight">\(80\)</span>). So let‚Äôs see how this could be implemented. Basically, right now we have an input that is a <span class="math notranslate nohighlight">\(4\times8\times10\)</span> that feeds into the <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer, and currently the <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer just stretches it out:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Flatten</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
<span class="o">...</span>
</pre></div>
</div>
<p>through the <code class="docutils literal notranslate"><span class="pre">view</span></code> operation. Effectively what it does now is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
    <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span>
<span class="p">)</span>  <span class="c1"># goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated</span>
<span class="n">e</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># yields 4x80</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 80])
</pre></div>
</div>
</div>
</div>
<p>But we want to just view the same tensor as a <span class="math notranslate nohighlight">\(4x4x20\)</span> instead, so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># yields 4x4x20: this is what we want!</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([4, 4, 20])
</pre></div>
</div>
</div>
</div>
<p>Easy, right? Let‚Äôs now rewrite <code class="docutils literal notranslate"><span class="pre">Flatten</span></code>, but since ours will now start to depart from <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten</span></code></a>, we‚Äôll rename it to <code class="docutils literal notranslate"><span class="pre">FlattenConsecutive</span></code> just to make sure that our APIs are somewhat similar but not the same:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">FlattenConsecutive</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
<p>So <code class="docutils literal notranslate"><span class="pre">FlattenConsecutive</span></code> takes in and flattens only some <code class="docutils literal notranslate"><span class="pre">n</span></code> consecutive elements and puts them into the last dimension. In <code class="docutils literal notranslate"><span class="pre">__call__</span></code> we parse the <span class="math notranslate nohighlight">\(3\)</span> dimensions of the input <code class="docutils literal notranslate"><span class="pre">x</span></code> as <code class="docutils literal notranslate"><span class="pre">b</span></code>, <code class="docutils literal notranslate"><span class="pre">c</span></code>, <code class="docutils literal notranslate"><span class="pre">t</span></code> (e.g. <span class="math notranslate nohighlight">\(4\)</span>, <span class="math notranslate nohighlight">\(8\)</span>, <span class="math notranslate nohighlight">\(10\)</span>) and then we view <code class="docutils literal notranslate"><span class="pre">x</span></code> as a <code class="docutils literal notranslate"><span class="pre">b</span></code>, <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">//</span> <span class="pre">n</span></code>, <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">*</span> <span class="pre">n</span></code> tensor (e.g. <span class="math notranslate nohighlight">\(4\)</span>, <span class="math notranslate nohighlight">\(8/2\)</span>, <span class="math notranslate nohighlight">\(10 \cdot 2\)</span>, <em>a.k.a.</em>: <span class="math notranslate nohighlight">\(4\)</span>, <span class="math notranslate nohighlight">\(4\)</span>, <span class="math notranslate nohighlight">\(20\)</span>). Last but not least, we check whether the middle dimension of <code class="docutils literal notranslate"><span class="pre">x</span></code> (<code class="docutils literal notranslate"><span class="pre">x.shape[1]</span></code>) is <span class="math notranslate nohighlight">\(1\)</span> and if so, then we simply squeeze out that dimension (e.g. <span class="math notranslate nohighlight">\(4\times1\times10\)</span> would become <span class="math notranslate nohighlight">\(4\times10\)</span>). Let‚Äôs now replace <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> with our new <code class="docutils literal notranslate"><span class="pre">FlattenConsecutive</span></code>, while maintaining the same functionality:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
            <span class="n">FlattenConsecutive</span><span class="p">(</span><span class="n">block_size</span><span class="p">),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># parameter init</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>  <span class="c1"># last layer make less confident</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>  <span class="c1"># number of parameters in total</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let‚Äôs define the model and verify that the shapes of the layer outputs are the same after feeding one batch of data into it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22097
torch.Size([4, 8])
Embedding : (4, 8, 10)
FlattenConsecutive : (4, 80)
Linear : (4, 200)
BatchNorm1d : (4, 200)
Tanh : (4, 200)
Linear : (4, 27)
</pre></div>
</div>
</div>
</div>
<p>So, we see the shapes as we expect them after every single layer in its output. Now, let‚Äôs try to restructure it and do it hierarchically:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="n">n_consec</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">n_consec</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span>
            <span class="n">FlattenConsecutive</span><span class="p">(</span><span class="n">n_consec</span><span class="p">),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">FlattenConsecutive</span><span class="p">(</span><span class="n">n_consec</span><span class="p">),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span> <span class="o">*</span> <span class="n">n_consec</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">FlattenConsecutive</span><span class="p">(</span><span class="n">n_consec</span><span class="p">),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span> <span class="o">*</span> <span class="n">n_consec</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">),</span>
            <span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># parameter init</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">*=</span> <span class="mf">0.1</span>  <span class="c1"># last layer make less confident</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>  <span class="c1"># number of parameters in total</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let‚Äôs inspect the numbers in between after a forward pass on a new <strong>nn</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>170897
torch.Size([4, 8])
Embedding : (4, 8, 10)
FlattenConsecutive : (4, 4, 20)
Linear : (4, 4, 200)
BatchNorm1d : (4, 4, 200)
Tanh : (4, 4, 200)
FlattenConsecutive : (4, 2, 400)
Linear : (4, 2, 200)
BatchNorm1d : (4, 2, 200)
Tanh : (4, 2, 200)
FlattenConsecutive : (4, 400)
Linear : (4, 200)
BatchNorm1d : (4, 200)
Tanh : (4, 200)
Linear : (4, 27)
</pre></div>
</div>
</div>
</div>
<p>So <span class="math notranslate nohighlight">\(4\times8\times20\)</span> was flattened consecutively into <span class="math notranslate nohighlight">\(4\times4\times20\)</span>. Through the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer, this was projected into <span class="math notranslate nohighlight">\(4\times4\times200\)</span>. And then <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code> just works out of the box and so does <code class="docutils literal notranslate"><span class="pre">Tanh</span></code>, which is element-wise. Then we crushed it again. So we flattened consecutively once more and ended up with a <span class="math notranslate nohighlight">\(4\times2\times400\)</span> now. Then <code class="docutils literal notranslate"><span class="pre">Linear</span></code> brought it back down to <span class="math notranslate nohighlight">\(4\times2\times200\)</span>, <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code> and <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> didn‚Äôt change the shape and for the last flattening,
it squeezed out that dimension of <span class="math notranslate nohighlight">\(1\)</span>, we end up with <span class="math notranslate nohighlight">\(4\times400\)</span>. And then <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code>, <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> and the last <code class="docutils literal notranslate"><span class="pre">Linear</span></code> yield our logits that end up in the same shape as they were before. Now, we actually have a nice three-layer <strong>nn</strong> that basically corresponds to this <strong>WaveNet</strong> network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig3.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" src="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" />
</div>
</div>
<p>with the only difference that we are using a blocksize of <span class="math notranslate nohighlight">\(8\)</span> instead of <span class="math notranslate nohighlight">\(16\)</span>, as depicted above. Now with a new architecture, we just have to kind of figure out some good channel numbers (numbers of hidden units) to use here. If we decrease the number to:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">68</span>
</pre></div>
</div>
</div>
</div>
<p>then the total number of parameters comes out to <span class="math notranslate nohighlight">\(22000\)</span>: exactly the same that we had before (when <code class="docutils literal notranslate"><span class="pre">n_hidden=200</span></code>). So we have the same amount of capacity with this <strong>nn</strong> in terms of the number of parameters. But the question is whether we are utilizing those parameters in a more efficient architecture.</p>
</section>
<section id="training-the-wavenet-first-pass">
<h2>Training the <strong>WaveNet</strong>: first pass<a class="headerlink" href="#training-the-wavenet-first-pass" title="Link to this heading">#</a></h2>
<p>Let‚Äôs train this <strong>WaveNet</strong> and see the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22397
      0/ 200000: 3.2978
  10000/ 200000: 2.1271
  20000/ 200000: 2.0807
  30000/ 200000: 1.6842
  40000/ 200000: 2.0252
  50000/ 200000: 2.3853
  60000/ 200000: 2.4678
  70000/ 200000: 1.7907
  80000/ 200000: 2.2092
  90000/ 200000: 2.3790
 100000/ 200000: 1.7643
 110000/ 200000: 1.6553
 120000/ 200000: 1.9414
 130000/ 200000: 1.9827
 140000/ 200000: 1.7703
 150000/ 200000: 1.8300
 160000/ 200000: 1.6640
 170000/ 200000: 1.9619
 180000/ 200000: 1.7971
 190000/ 200000: 1.9981
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ebbb1fbb33064be7876271d293fca166", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trigger_eval_mode</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.9376758337020874
val 2.026397943496704
</pre></div>
</div>
</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105</span>
<span class="c1"># context: 3 -&gt; 8 (22K params): train 1.915, val 2.034</span>
<span class="c1"># flat -&gt; hierachical (22K params): train 1.937, val 2.026</span>
</pre></div>
</div>
<p>As you can see, changing from the flat to hierachical model (while keeping the same number of parameters) is not giving us any noticeable significant benefit in terms of the <strong>loss</strong>.  That said, there are two things to point out. Number one, we didn‚Äôt really ‚Äútorture‚Äù the architecture here very much. And there‚Äôs a bunch of hyperparameter search that we could do in terms of how we allocate our budget of parameters to what layers. Number two, we still may have a bug inside the <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code> layer. So let‚Äôs take a look at that because it runs, but doesn‚Äôt do the right thing.</p>
</section>
<section id="fixing-the-batchnorm1d-bug">
<h2>Fixing the BatchNorm1d bug<a class="headerlink" href="#fixing-the-batchnorm1d-bug" title="Link to this heading">#</a></h2>
<p>If we train for just one step and we print the layer output shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/ 200000: 2.0969
Embedding : (4, 8, 10)
FlattenConsecutive : (4, 4, 20)
Linear : (4, 4, 68)
BatchNorm1d : (4, 4, 68)
Tanh : (4, 4, 68)
FlattenConsecutive : (4, 2, 136)
Linear : (4, 2, 68)
BatchNorm1d : (4, 2, 68)
Tanh : (4, 2, 68)
FlattenConsecutive : (4, 136)
Linear : (4, 68)
BatchNorm1d : (4, 68)
Tanh : (4, 68)
Linear : (4, 27)
</pre></div>
</div>
</div>
</div>
<p>currently, it looks like the BatchNorm is receiving an input that is <span class="math notranslate nohighlight">\(32\times4\times68\)</span>, right? Let‚Äôs take a look at the implementation of <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm1d</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># parameters (trained with backprop)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># buffers (trained with a running &#39;momentum update&#39;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
  
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># calculate the forward pass</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># batch mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># batch variance</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>
        <span class="n">xhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">xvar</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="c1"># normalize to unit variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">xhat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="c1"># update the buffers</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xmean</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xvar</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
  
    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>
</pre></div>
</div>
<p>It assumed, in the way we wrote it and at the time, that the input <code class="docutils literal notranslate"><span class="pre">x</span></code> is <span class="math notranslate nohighlight">\(2D\)</span>. So it was <span class="math notranslate nohighlight">\(N \times D\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> was the batch size. So that‚Äôs why we only reduced the mean and the variance over the <span class="math notranslate nohighlight">\(0th\)</span> dimension. But now <code class="docutils literal notranslate"><span class="pre">x</span></code> will basically become <span class="math notranslate nohighlight">\(3D\)</span>. So what‚Äôs happening inside the <strong>batchnorm</strong> layer right now? And how come it‚Äôs working at all and not giving any errors? The reason for that is basically because everything broadcasts properly, but the <strong>batchnorm</strong> is not doing what we want it to do. So in particular, let‚Äôs basically think through what‚Äôs happening inside the <strong>batchnorm</strong>. Let‚Äôs look at what‚Äôs happening here in a simplified example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">68</span><span class="p">)</span>
<span class="n">emean</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1, 4, 68</span>
<span class="n">evar</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1, 4, 68</span>
<span class="n">ehat</span> <span class="o">=</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="n">emean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">evar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># 32, 4, 68</span>
<span class="n">ehat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 4, 68])
</pre></div>
</div>
</div>
</div>
<p>So we‚Äôre receiving an input of <span class="math notranslate nohighlight">\(32\times4\times68\)</span>. And then we are doing here <code class="docutils literal notranslate"><span class="pre">x.mean()</span></code>, but we have <code class="docutils literal notranslate"><span class="pre">e</span></code> instead of <code class="docutils literal notranslate"><span class="pre">x</span></code>. But we‚Äôre doing the mean over <span class="math notranslate nohighlight">\(0\)</span> and that‚Äôs actually giving us <span class="math notranslate nohighlight">\(1\times4\times68\)</span>. So we‚Äôre doing the mean only over the very first dimension. And it‚Äôs giving us a mean and a variance that still maintains the middle dimension in between (i.e. <span class="math notranslate nohighlight">\(4\)</span>). So these means are only taken over <span class="math notranslate nohighlight">\(32\)</span> numbers in the first dimension. And then, when we perform the <code class="docutils literal notranslate"><span class="pre">ehat</span></code> assignment, everything broadcasts correctly still. But basically what ends up happening is when we also look at the running mean:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">running_mean</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 4, 68])
</pre></div>
</div>
</div>
</div>
<p>the shape of this running mean now is <span class="math notranslate nohighlight">\(1\times4\times68\)</span>. Instead of it being just a size of dimension, because we have <span class="math notranslate nohighlight">\(68\)</span> channels, we expect to have <span class="math notranslate nohighlight">\(68\)</span> means and variances that we‚Äôre maintaining. But actually, we have an array of <span class="math notranslate nohighlight">\(4\times68\)</span>. And so basically what this is telling us is this <strong>batchnorm</strong> is currently working in parallel over <span class="math notranslate nohighlight">\(4\times68\)</span> instead of just <span class="math notranslate nohighlight">\(68\)</span> channels. So basically we are maintaining this. We are maintaining statistics for every one of these four positions individually and independently. And instead, what we want to do is we want to treat this middle <span class="math notranslate nohighlight">\(4\)</span> dimension as a batch dimension, just like the <span class="math notranslate nohighlight">\(0th\)</span> dimension. So as far as the <strong>batchnorm</strong> is concerned, it doesn‚Äôt want to average‚Ä¶ We don‚Äôt want to average over <span class="math notranslate nohighlight">\(32\)</span> numbers. But instead, we want to now average over <span class="math notranslate nohighlight">\(32\times4\)</span> numbers for every single one of these <span class="math notranslate nohighlight">\(68\)</span> channels. Since <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.mean.html"><code class="docutils literal notranslate"><span class="pre">torch.mean</span></code></a> allows us to reduce over multiple (and not just one) dimensions at the same time, we‚Äôll do just that and reduce over both the <span class="math notranslate nohighlight">\(0th\)</span> and <span class="math notranslate nohighlight">\(1st\)</span> dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">68</span><span class="p">)</span>
<span class="n">emean</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1, 1, 68</span>
<span class="n">evar</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">var</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 1, 1, 68</span>
<span class="n">ehat</span> <span class="o">=</span> <span class="p">(</span><span class="n">e</span> <span class="o">-</span> <span class="n">emean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">evar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># 32, 4, 68</span>
<span class="n">ehat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 4, 68])
</pre></div>
</div>
</div>
</div>
<p>Although the final shape of <code class="docutils literal notranslate"><span class="pre">ehat</span></code> remains the same, we see now that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emean</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">evar</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([1, 1, 68]), torch.Size([1, 1, 68]))
</pre></div>
</div>
</div>
</div>
<p>instead of <code class="docutils literal notranslate"><span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">68</span></code>, since we reduced over both of the batch dimensions, it yields only <span class="math notranslate nohighlight">\(68\)</span> numbers total for each tensor, with a bunch of spurious leftover <span class="math notranslate nohighlight">\(1\)</span> dimensions remaining. Therefore, this is what should be happening with our <code class="docutils literal notranslate"><span class="pre">running_mean</span></code> and <code class="docutils literal notranslate"><span class="pre">running_var</span></code> tensors inside our <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code> implementation. So the change is pretty straightforward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm1d</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># parameters (trained with backprop)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># buffers (trained with a running &#39;momentum update&#39;)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># calculate the forward pass</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">elif</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># batch mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># batch variance</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xmean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">xvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>
        <span class="n">xhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">xmean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">xvar</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>  <span class="c1"># normalize to unit variance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">xhat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="c1"># update the buffers</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xmean</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span>
                <span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">xvar</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Basically, now in <code class="docutils literal notranslate"><span class="pre">__call__</span></code> we are checking the dimensionality of <code class="docutils literal notranslate"><span class="pre">x</span></code> and based on it we are determining the <code class="docutils literal notranslate"><span class="pre">dim</span></code> parameters to be passed to the <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">var</span></code> functions. Now, to point out one more thing. We‚Äôre actually departing from the API of PyTorch here a little bit, because when you go read the documentation of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm1d</span></code></a>, it says:</p>
<blockquote>
<div><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N,C)\)</span> or <span class="math notranslate nohighlight">\((N,C,L)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the batch size, <span class="math notranslate nohighlight">\(C\)</span> is the number of features or channels, and <span class="math notranslate nohighlight">\(L\)</span> is the sequence length</p></li>
</ul>
</div></blockquote>
<p>Notice, the input to this layer can either be <span class="math notranslate nohighlight">\(N\)</span> (batch size) <span class="math notranslate nohighlight">\(\times\)</span> <span class="math notranslate nohighlight">\(C\)</span> (number of features or channels) or it actually does accept three-dimensional inputs, but it expects it to be <span class="math notranslate nohighlight">\(N\times C \times L\)</span> (sequence legth). So this is a problem because you see how <span class="math notranslate nohighlight">\(C\)</span> is nested here in the middle. And so when it gets <span class="math notranslate nohighlight">\(3D\)</span> inputs, this <strong>batchnorm</strong> layer will reduce over <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">2</span></code> instead of <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>. So basically, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm1d</span></code></a> layer assumes that <span class="math notranslate nohighlight">\(C\)</span> will always be the first dimension, whereas we assume here that <span class="math notranslate nohighlight">\(C\)</span> is the last dimension, and there are some number of batch dimensions beforehand. And so, it expects <span class="math notranslate nohighlight">\(N\times C\)</span> or <span class="math notranslate nohighlight">\(N\times C\times L\)</span>, whereas we expect <span class="math notranslate nohighlight">\(N\times C\)</span> or <span class="math notranslate nohighlight">\(N\times L\times C\)</span>. So just a small deviation from the <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> API. Now, after updating our <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code>, if we redefine our <strong>nn</strong> and run for one step:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22397
      0/ 200000: 3.2907
Embedding : (4, 8, 10)
FlattenConsecutive : (4, 4, 20)
Linear : (4, 4, 68)
BatchNorm1d : (4, 4, 68)
Tanh : (4, 4, 68)
FlattenConsecutive : (4, 2, 136)
Linear : (4, 2, 68)
BatchNorm1d : (4, 2, 68)
Tanh : (4, 2, 68)
FlattenConsecutive : (4, 136)
Linear : (4, 68)
BatchNorm1d : (4, 68)
Tanh : (4, 68)
Linear : (4, 27)
</pre></div>
</div>
</div>
</div>
<p>the shapes are of course the same as before, but now if we inspect the shape of the <code class="docutils literal notranslate"><span class="pre">running_mean</span></code> inside the <strong>batchnorm</strong> layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">running_mean</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 1, 68])
</pre></div>
</div>
</div>
</div>
<p>So correctly now we are only maintaining <span class="math notranslate nohighlight">\(68\)</span> means, for every one of our channels, and we are treating the <span class="math notranslate nohighlight">\(0th\)</span> and the <span class="math notranslate nohighlight">\(1st\)</span> dimension as batch dimensions, which is exactly what we want!</p>
</section>
<section id="re-training-the-wavenet-after-bug-fix">
<h2>Re-training the <strong>WaveNet</strong> after bug fix<a class="headerlink" href="#re-training-the-wavenet-after-bug-fix" title="Link to this heading">#</a></h2>
<p>So let‚Äôs retrain the <strong>nn</strong> now, after the bug fix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22397
      0/ 200000: 3.3054
  10000/ 200000: 2.2512
  20000/ 200000: 2.3514
  30000/ 200000: 2.5115
  40000/ 200000: 1.6012
  50000/ 200000: 2.1728
  60000/ 200000: 1.8859
  70000/ 200000: 2.1417
  80000/ 200000: 2.0348
  90000/ 200000: 1.7458
 100000/ 200000: 1.7257
 110000/ 200000: 1.9065
 120000/ 200000: 2.0347
 130000/ 200000: 2.1898
 140000/ 200000: 2.2163
 150000/ 200000: 1.7984
 160000/ 200000: 1.4846
 170000/ 200000: 1.7952
 180000/ 200000: 2.0809
 190000/ 200000: 2.2824
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "089f12aea1ff4c02959e39af0a8a4ef2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trigger_eval_mode</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.911558985710144
val 2.019017219543457
</pre></div>
</div>
</div>
</div>
<p>And we can see that we are getting a tiny improvement in our training and validation losses:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105</span>
<span class="c1"># context: 3 -&gt; 8 (22K params): train 1.915, val 2.034</span>
<span class="c1"># flat -&gt; hierachical (22K params): train 1.937, val 2.026</span>
<span class="c1"># fix bug in batchnorm: train 1.911, val 2.019</span>
</pre></div>
</div>
<p>The reason this improvement is to be expected is that now we have less numbers going into the estimates of the mean and variance which allows everything to be more stable and less wiggly.</p>
</section>
<section id="scaling-up-our-wavenet">
<h2>Scaling up our <strong>WaveNet</strong><a class="headerlink" href="#scaling-up-our-wavenet" title="Link to this heading">#</a></h2>
<p>And with this more general architecture in place, we are now set up to push the performance further by increasing the size of the network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_embd</span> <span class="o">=</span> <span class="mi">24</span>  <span class="c1"># the dimensionality of the character embedding vectors</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># the number of neurons in the hidden layer of the MLP</span>
</pre></div>
</div>
</div>
</div>
<p>And using the exact same architecture, we now have</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>76579
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(76579\)</span> parameters and the training takes a lot longer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      0/ 200000: 3.3060
  10000/ 200000: 2.1627
  20000/ 200000: 1.8125
  30000/ 200000: 2.1831
  40000/ 200000: 1.9874
  50000/ 200000: 2.3684
  60000/ 200000: 2.1482
  70000/ 200000: 1.7558
  80000/ 200000: 1.8260
  90000/ 200000: 1.8867
 100000/ 200000: 1.9521
 110000/ 200000: 1.9662
 120000/ 200000: 1.9416
 130000/ 200000: 2.0037
 140000/ 200000: 1.9890
 150000/ 200000: 1.7099
 160000/ 200000: 1.8770
 170000/ 200000: 1.5360
 180000/ 200000: 1.4641
 190000/ 200000: 1.8875
</pre></div>
</div>
</div>
</div>
<p>but we do get a nice curve:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">lossi</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "338ac53bc3b84c37828d28bf9e4e8ce7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>and we are now getting even better performance:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trigger_eval_mode</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">infer_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 1.7666022777557373
val 1.9965752363204956
</pre></div>
</div>
</div>
</div>
<p>So, to compare to previous performances:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105</span>
<span class="c1"># context: 3 -&gt; 8 (22K params): train 1.915, val 2.034</span>
<span class="c1"># flat -&gt; hierachical (22K params): train 1.937, val 2.026</span>
<span class="c1"># fix bug in batchnorm: train 1.911, val 2.019</span>
<span class="c1"># scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.768, val 1.990</span>
</pre></div>
</div>
<p>However because the experiments are starting to take longer to train, we are a little bit in the dark with respect to the correct setting of the hyperparameters here and the learning rates and so on. And so we are missing sort of like an experimental harness on which we could run a number of experiments and really tune this architecture very well.</p>
</section>
<section id="wavenet-but-with-dilated-causal-convolutions">
<h2>WaveNet but with dilated causal convolutions<a class="headerlink" href="#wavenet-but-with-dilated-causal-convolutions" title="Link to this heading">#</a></h2>
<p>So let‚Äôs conclude now with a few notes. We basically improved our performance noticeably from a val <strong>loss</strong> of <span class="math notranslate nohighlight">\(2.10\)</span> to <span class="math notranslate nohighlight">\(1.99\)</span>. But this shouldn‚Äôt be the focus as we are kind of in the dark. We have no experimental harness, we are just guessing and checking. And this whole thing is pretty terrible to be honest. We are just looking at the training <strong>loss</strong>, whereas we should be looking at the training and validation <strong>loss</strong> together. That said, we did implement the <strong>WaveNet</strong> architecture from the <a class="reference external" href="https://arxiv.org/abs/1609.03499">paper</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig3.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" src="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" />
</div>
</div>
<p>But we did not implement this specific forward pass of it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig4.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0abd535b358d10352195a86e330cde56781960bd5c47d6b440db9cbc74a6eeb2.png" src="../_images/0abd535b358d10352195a86e330cde56781960bd5c47d6b440db9cbc74a6eeb2.png" />
</div>
</div>
<p>where you have a more complicated kind of gated linear layer with residual connections, skip connections and so on‚Ä¶ So we did not implement this, but only the tree-like model. All things considered, let‚Äôs briefly go over how what we‚Äôve done here relates to convolutional neural networks as used in the <a class="reference external" href="https://arxiv.org/abs/1609.03499">WaveNet paper</a>. Basically the use of convolutions is strictly for efficiency. It doesn‚Äôt actually change the model we‚Äôve implemented. So, here for example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_next_character</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[</span><span class="mi">46</span><span class="p">:</span><span class="mi">54</span><span class="p">],</span> <span class="n">ytrain</span><span class="p">[</span><span class="mi">46</span><span class="p">:</span><span class="mi">54</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>........ --&gt; a
.......a --&gt; n
......an --&gt; a
.....ana --&gt; l
....anal --&gt; i
...anali --&gt; s
..analis --&gt; a
.analisa --&gt; .
</pre></div>
</div>
</div>
</div>
<p>we see the name <code class="docutils literal notranslate"><span class="pre">analisa</span></code> from our training set and it has <span class="math notranslate nohighlight">\(7\)</span> letters, so that is <span class="math notranslate nohighlight">\(8\)</span> rows which correspond to independent examples of that name. Now, we can forward any one of these rows independently:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward a single example</span>
<span class="n">single_example</span> <span class="o">=</span> <span class="n">xtrain</span><span class="p">[[</span><span class="mi">7</span><span class="p">]]</span>  <span class="c1"># index by [[7]] to get an extra batch dimension</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">single_example</span><span class="p">)</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 27])
</pre></div>
</div>
</div>
</div>
<p>Now imagine that instead of just a single example, you would like to forward all of the <span class="math notranslate nohighlight">\(8\)</span> examples that make up the name into the network at the same time:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward all of them</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">27</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[[</span><span class="mi">7</span> <span class="o">+</span> <span class="n">i</span><span class="p">]])</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([8, 27])
</pre></div>
</div>
</div>
</div>
<p>Of course, as we‚Äôve implemented this right now, this is <span class="math notranslate nohighlight">\(8\)</span> independent calls to our model. But what convolutions allow you to do is they allow you to ‚Äúslide‚Äù this model efficiently over the input sequence. And so this for loop we just wrote out can be done not ‚Äúoutside‚Äù, through iteration, in Python, but ‚Äúinside‚Äù of kernels in <a class="reference external" href="https://en.wikipedia.org/wiki/CUDA"><code class="docutils literal notranslate"><span class="pre">CUDA</span></code></a>. And so this for loop gets hidden into the convolution. So basically you can think of the convolution as a for loop applying a little linear filter over space of some input sequence. And in our case the space we‚Äôre interested in is one dimensional. And we are interested in sliding these filter over the input data. This diagram is quite helpful for understanding actually:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;wavenet_fig3.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" src="../_images/8e402bdae629c44c12a7978fbdfc1791e4bde2a90d70f0e9367678a28530b45c.png" />
</div>
</div>
<p>Here, you can see highlighted with black arrows a single tree of the calculation we just described. So depicted here, calculating a single orange node at the <strong>Output</strong> layer corresponds to us in our example forwarding a single example and getting out a single output. But what convolutions allow you to do is it allows you to take this black tree-like structure and kind of like slide it over the <strong>Input</strong> sequence (blue nodes) and calculate all of the orange outputs at the same time. In the above figure, this sliding action is represented by the dashed connections between the nodes. In our example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_next_character</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[</span><span class="mi">46</span><span class="p">:</span><span class="mi">54</span><span class="p">],</span> <span class="n">ytrain</span><span class="p">[</span><span class="mi">46</span><span class="p">:</span><span class="mi">54</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>........ --&gt; a
.......a --&gt; n
......an --&gt; a
.....ana --&gt; l
....anal --&gt; i
...anali --&gt; s
..analis --&gt; a
.analisa --&gt; .
</pre></div>
</div>
</div>
</div>
<p>this sliding operation would correspond to calculating all the above <span class="math notranslate nohighlight">\(8\)</span> outputs at all the positions of the name (like we did in an explicit loop) at the same time. And the reason this is much more efficient is because the for loop is inside the <code class="docutils literal notranslate"><span class="pre">CUDA</span></code> kernels. That makes it efficient. Also, notice the node re-use in the diagram were for example in the first <strong>Hidden Layer</strong> each white node is the right child of the white node above it (in the second <strong>Hidden Layer</strong>), but also the left child of another white node (also in the second <strong>Hidden Layer</strong>). In the first <strong>Hidden Layer</strong>, each node and its value is used twice. Therefore, in our above example snippet, with our naive way we would have to recalculate the value that corresponds to such a node, whereas with such a convolutional <strong>nn</strong> we are allowed to reuse it. So, in the convolutional <strong>nn</strong> you can think of the linear layer as filters. And we take these filters and their linear filters and we slide them over the input sequence and we calculate the first layer, the second layer, the third layer and then the output layer of the sandwich and it is all done very efficiently using these convolutions.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Another thing to take away from this lecture is having modeled our <strong>nn</strong> lego blocks: the module classes (<code class="docutils literal notranslate"><span class="pre">Linear</span></code>, etc.) after modules from <a class="reference external" href="https://pytorch.org/docs/stable/nn.html"><code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></a>. So it is now very easy to start using modules directly from PyTorch, from hereon. The next thing I hope you got a bit of a sense of is what the development process of building deep neural networks looks like. Which I think was relatively representative to some extent. So number one, we are spending a lot of time in the <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">documentation page of PyTorch</a>. And we‚Äôre reading through all the layers, looking at documentations, what are the shapes of the inputs, what they can be, what does the layer do, and so on. Unfortunately, however, the PyTorch documentation is not very good, at least not at the time these lectures were implemented. The PyTorch developers spend a ton of time on hardcore engineering of all kinds of distributed primitives, etc. But no one is rigorously maintaining documentation. It will lie to you, it will be wrong, it will be incomplete, it will be unclear. So unfortunately, it is what it is and you just kind of have to do your best with what they give us. Also, there‚Äôs a ton of trying to make the shapes work. And there‚Äôs a lot of gymnastics around these multi-dimensional arrays. Are they <span class="math notranslate nohighlight">\(2D\)</span>, <span class="math notranslate nohighlight">\(3D\)</span>, <span class="math notranslate nohighlight">\(4D\)</span>? What shapes do the layers take? Is it <span class="math notranslate nohighlight">\(N\times C\times L\)</span> or <span class="math notranslate nohighlight">\(N\times L\times C\)</span>? And you‚Äôre permuting and viewing, and it just gets pretty messy. And so that brings me to number three. It‚Äôs often helpful to first prototype these layers and implementations in jupyter notebooks and make sure that all the shapes work out, initially making sure everything is correct. And then, once you‚Äôre satisfied with the functionality, you can copy-paste the code into your actual code base or repository (e.g. in VSCode). So these are roughly only some notes on the development process of working with <strong>nn</strong>s.</p>
</section>
<section id="outro">
<h2>Outro<a class="headerlink" href="#outro" title="Link to this heading">#</a></h2>
<p>Lastly, this lecture unlocks a lot of potential further lectures because, number one, we have to convert our <strong>nn</strong> to actually use these dilated causal convolutional layers, so implementing the convnet. Number two, we potentially start to get into what this means, where are residual connections and skip connections and why are they useful. Number three, as we already mentioned, we don‚Äôt have any experimental harness. So right now, we are just guessing and checking everything. This is not representative of typical deep learning workflows. You usually have to set up your evaluation harness. You have lots of arguments that your script can take. You‚Äôre more comfortably kicking off a lot of experiments. You‚Äôre looking at a lot of plots of training and validation losses, and you‚Äôre looking at what is working and what is not working. And you‚Äôre working on this like population level, and you‚Äôre doing all these hyperparameter searches. So we‚Äôve done none of that so far. So how to set that up and how to make it good, I think is a whole other topic. And number four, we should probably cover RNNs, LSTMs, GRUs, and of course Transformers. So many places to go, and we‚Äôll cover that in the future. That‚Äôs all for now. Bye! :)</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./micrograduate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="makemore4.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</p>
      </div>
    </a>
    <a class="right-next"
       href="picogpt.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#starter-code-walkthrough">Starter code walkthrough</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-loss-plot">Fixing the <strong>loss</strong> plot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchifying-the-code-layers-containers-torch-nn">torchifying the code: layers, containers, torch.nn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wavenet-overview"><strong>WaveNet</strong> overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bumping-the-context-size-to-8">Bumping the context size to <span class="math notranslate nohighlight">\(8\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-wavenet">Implementing <strong>WaveNet</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-wavenet-first-pass">Training the <strong>WaveNet</strong>: first pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-the-batchnorm1d-bug">Fixing the BatchNorm1d bug</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#re-training-the-wavenet-after-bug-fix">Re-training the <strong>WaveNet</strong> after bug fix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-up-our-wavenet">Scaling up our <strong>WaveNet</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wavenet-but-with-dilated-causal-convolutions">WaveNet but with dilated causal convolutions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christos Karaneen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>