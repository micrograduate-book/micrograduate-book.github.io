
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5. makemore (part 4): becoming a backprop ninja &#8212; micrograâˆ‡uate</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'micrograduate/makemore4';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. makemore (part 5): building a WaveNet" href="makemore5.html" />
    <link rel="prev" title="4. makemore (part 3): activations &amp; gradients, batchnorm" href="makemore3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/book_logo.png" class="logo__image only-light" alt="micrograâˆ‡uate - Home"/>
    <script>document.write(`<img src="../_static/book_logo.png" class="logo__image only-dark" alt="micrograâˆ‡uate - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    ðŸ“– Read ðŸŒ»
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="micrograd.html">1. <strong>micrograd</strong>: implementing an autograd engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore1.html">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore2.html">3. <strong>makemore</strong> (part 2): mlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore3.html">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore5.html">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="picogpt.html">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate/issues/new?title=Issue%20on%20page%20%2Fmicrograduate/makemore4.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/micrograduate/makemore4.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>5. makemore (part 4): becoming a backprop ninja</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backprop-back-in-the-day"><strong>Backprop</strong> back in the day</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manually-implementing-a-backward-pass">Manually implementing a backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implementing-the-backward-pass">Exercise 1: implementing the backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-backprop-through-cross-entropy-but-all-in-one-go">Exercise 2: <strong>backprop</strong> through cross_entropy but all in one go</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-backprop-through-batchnorm-but-all-in-one-go">Exercise 3: <strong>backprop</strong> through <strong>batchnorm</strong> but all in one go</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-training-the-mlp-with-our-own-manual-backward-pass">Exercise 4: training the <strong>mlp</strong> with our own manual backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="makemore-part-4-becoming-a-backprop-ninja">
<h1>5. <strong>makemore</strong> (part 4): becoming a backprop ninja<a class="headerlink" href="#makemore-part-4-becoming-a-backprop-ninja" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cloning repo...&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>--quiet<span class="w"> </span>https://github.com/ckarageorgkaneen/micrograduate.git<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
    <span class="o">%</span><span class="k">cd</span> micrograduate
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Installing requirements...&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>uv
    <span class="o">!</span>uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--system<span class="w"> </span>--quiet<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</div>
</div>
<section id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Link to this heading">#</a></h2>
<p>Hi everyone. So today we are once again continuing our implementation of <strong>makemore</strong>!</p>
<ul class="simple">
<li><p>Bigram (one character predicts the next one with a lookup table of counts)</p></li>
<li><p>MLP, following <a class="reference external" href="https://dl.acm.org/doi/10.5555/944919.944966">Bengio et al. 2003</a></p></li>
<li><p>RNN, following <a class="reference external" href="https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Mikolov et al. 2010</a></p></li>
<li><p>LSTM, following <a class="reference external" href="https://arxiv.org/abs/1308.0850">Graves et al. 2014</a></p></li>
<li><p>GRU, following <a class="reference external" href="https://arxiv.org/abs/1409.1259">Kyunghyun Cho et al. 2014</a></p></li>
<li><p>CNN, following <a class="reference external" href="https://arxiv.org/abs/1609.03499">Oord et al., 2016</a></p></li>
<li><p>Transformer, following <a class="reference external" href="https://arxiv.org/abs/1706.03762">Vaswani et al. 2017</a></p></li>
</ul>
<p>Now so far, from the above list, weâ€™ve come up to <strong>mlp</strong>s and our <strong>nn</strong> has looked like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;bengio2003nn.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b4c83268a9c4ea410c7992495e79920b1fed6d29199d3c6fa361c4a52adc46b9.jpg" src="../_images/b4c83268a9c4ea410c7992495e79920b1fed6d29199d3c6fa361c4a52adc46b9.jpg" />
</div>
</div>
<p>and we have been implementing this over the last few lessons. Now, Iâ€™m sure everyone is very excited to go into recurrent neural networks (<strong>rnn</strong>s) and all of their variants and how they work and their diagrams look cool:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;rnns.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e78bc46dbf4ca95029ed73eaaaa4c18731d0cbf337e7f71135eff4db7d329726.png" src="../_images/e78bc46dbf4ca95029ed73eaaaa4c18731d0cbf337e7f71135eff4db7d329726.png" />
</div>
</div>
<p>and such models are very exciting and interesting, and weâ€™re going to get a better result. But unfortunately, I think we have to remain here for one more lecture. And the reason for that is weâ€™ve already trained this <strong>mlp</strong> and we are getting pretty good loss and we have developed a pretty decent understanding of the architecture and how it works. But in <code class="docutils literal notranslate"><span class="pre">train()</span></code>, we are using <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, a line of code that we should take an issue with if we want to have a holistic and intuitive understanding of how gradients are actually being calculated and what exactly is going on here. That is, we have been using PyTorchâ€™s autograd and using it to calculate all of our gradients along the way. Here, what we will do is remove the use of <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, and we will write our backward pass <em>manually</em>, at the level of tensors.</p>
</section>
<section id="backprop-back-in-the-day">
<h2><strong>Backprop</strong> back in the day<a class="headerlink" href="#backprop-back-in-the-day" title="Link to this heading">#</a></h2>
<p>This will prove to be a very useful exercise for the following reasons. Andrej has actually an entire blogpost on this topic: <a class="reference external" href="https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand <strong>backprop</strong></a>, where he basically characterizes <strong>backprop</strong> as a leaky abstraction. And what I mean by that is that <strong>backprop</strong> doesnâ€™t just make your <strong>nn</strong>s just work <em>magically</em>. Itâ€™s not the case that you can just stack up arbitrary lego blocks of differentiable functions and just cross your fingers, do <em>backprop</em> and everything is great. Things donâ€™t just work automatically. It is a leaky abstraction in the sense that you can shoot yourself in the foot if you do not understand its internals. It may magically not work or not work optimally. And you will need to understand how it works under the hood if youâ€™re hoping to debug an issue with it and if you are hoping to address it in your <strong>nn</strong>. So <a class="reference external" href="https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b">the blog post</a> goes into some of those examples. So for example, weâ€™ve already covered some of them already. For example, the flat tails of functions such as the <span class="math notranslate nohighlight">\(sigmoid\)</span> or <span class="math notranslate nohighlight">\(tanh\)</span> and how you do not want to saturate them too much because your gradients will die:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;sigmoid_derivative.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bd93652974ba70991c1558b202455b3091660871c963eab9c9408f375ee8e9d8.jpg" src="../_images/bd93652974ba70991c1558b202455b3091660871c963eab9c9408f375ee8e9d8.jpg" />
</div>
</div>
<p>Thereâ€™s the case of dead neurons, which weâ€™ve already covered as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;relu_derivative.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2ccfd894dbb5a9d0706910765159f9cc40785ce1043fd21e696dd238cdbb539c.jpg" src="../_images/2ccfd894dbb5a9d0706910765159f9cc40785ce1043fd21e696dd238cdbb539c.jpg" />
</div>
</div>
<p>Also, the case of exploding or vanishing gradients in the case of <strong>rnn</strong>s, which we are about to cover:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;rnngradients.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0bf7d0ce4b4b1a75556a2308aa9457822b82bab082cfb3919436f131c5998bd0.jpg" src="../_images/0bf7d0ce4b4b1a75556a2308aa9457822b82bab082cfb3919436f131c5998bd0.jpg" />
</div>
</div>
<p>And then also you will often come across some examples in the wild:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;dqnbadclipping.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0449c98a8df16eaebb62607dbcf8cc0682ea47042151918a7cd59c300caa2ca9.jpg" src="../_images/0449c98a8df16eaebb62607dbcf8cc0682ea47042151918a7cd59c300caa2ca9.jpg" />
</div>
</div>
<p>This is a snippet that Andrej found in a random code base on the internet where they actually have like a very subtle but pretty-major bug in their implementation. And the bug points at the fact that the author of this code does not actually understand <strong>backprop</strong>. So what theyâ€™re trying to do here is theyâ€™re trying to clip the <strong>loss</strong> at a certain maximum value. But actually what theyâ€™re trying to do is theyâ€™re trying to clip the gradients to have a maximum value instead of trying to clip the <strong>loss</strong> at a maximum value. And indirectly, theyâ€™re basically causing some of the outliers to be actually ignored. Because, when you clip the <strong>loss</strong> of an outlier, you are setting its gradient to <span class="math notranslate nohighlight">\(0\)</span>. And so have a look through this and read through it. But thereâ€™s basically a bunch of subtle issues that youâ€™re going to avoid if you actually know what youâ€™re doing. And thatâ€™s why I donâ€™t think itâ€™s the case that because PyTorch or other frameworks offer autograd, it is okay for us to ignore how it works. Now, weâ€™ve actually already covered autograd and we wrote <a class="reference internal" href="#1.-micrograd"><span class="xref myst"><strong>micrograd</strong></span></a>. But <a class="reference internal" href="#1.-micrograd"><span class="xref myst"><strong>micrograd</strong></span></a> was an autograd engine only at the level of individual scalars. So the atoms were single individual numbers which I donâ€™t think is enough. And Iâ€™d like us to basically think about <strong>backprop</strong> at the level of tensors as well. And so in a summary, I think itâ€™s a good exercise. I think it is very, very valuable. Youâ€™re going to become better at debugging <strong>nn</strong>s and making sure that you understand what youâ€™re doing. It is going to make everything fully explicit. So youâ€™re not going to be nervous about what is hidden away from you. Basically, weâ€™re going to emerge stronger! So letâ€™s get into it. A bit of a fun historical note here is that today, manually writing your backward pass by hand is not recommended and no one does it, except for the purposes of exercise and education. But about around <span class="math notranslate nohighlight">\(10\)</span>+ years ago in deep learning, this was fairly standard and in fact pervasive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;backwardmemelol.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0c3966abe84e8c54ed777ddab4e97015cdf431108b640fa74f406effb540467e.png" src="../_images/0c3966abe84e8c54ed777ddab4e97015cdf431108b640fa74f406effb540467e.png" />
</div>
</div>
<p>So at the time, everyone (including Andrej himself) used to write their backward pass manually, by hand. Now, itâ€™s our turn too! Now of course everyone just calls <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>. But, weâ€™ve lost something. I want to give you a few examples of this. So,  hereâ€™s a <span class="math notranslate nohighlight">\(2006\)</span> paper from Geoffrey Hinton and Ruslan Salakhutdinov in Science that was influential at the time: <a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/science.pdf">Reducing the Dimensionality of Data with Neural Networks</a>. And this was training some architectures called restricted Boltzmann machines (<strong>rbm</strong>s). And basically, itâ€™s an autoencoder trained here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;rbms.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/789009e8c15612877a56e4146ed0f0956ec125734b0473a868f7cd68b37a52bf.jpg" src="../_images/789009e8c15612877a56e4146ed0f0956ec125734b0473a868f7cd68b37a52bf.jpg" />
</div>
</div>
<p>And this is from roughly <span class="math notranslate nohighlight">\(2010\)</span>, when Andrej had written a MATLAB library for training (<strong>rbm</strong>s) called <a class="reference external" href="https://code.google.com/archive/p/matrbm/">matrbm</a>. And this was at the time when Python was not used for deep learning as pervasively as it is now. It was all MATLAB, which was this scientific computing package that everyone would use:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;matlab.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d538e0354736c59842548ddb39a5d80bb4a81b6fc0c9af3c8806d20209b44594.png" src="../_images/d538e0354736c59842548ddb39a5d80bb4a81b6fc0c9af3c8806d20209b44594.png" />
</div>
</div>
<p>So we would write MATLAB, which is barely a programming language as well. But it had a very convenient tensor class. And it was this computing environment and you would run here. It would all run on the CPU, of course. But you would have very nice plots to go with it and a built-in debugger. And it was pretty decent. Now, the code in the <span class="math notranslate nohighlight">\(2010\)</span> <a class="reference external" href="https://code.google.com/archive/p/matrbm/">matrbm</a> package that written for fitting <strong>rbm</strong>s to a large extent is recognizable. Letâ€™s open up the <a class="reference external" href="https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/matrbm/RBMLIB.zip"><code class="docutils literal notranslate"><span class="pre">rbmFit.m</span></code> source code file</a> and see if we can get a rough idea of what is going on. Well first Andrej creates the data and the x, y batches:</p>
<div class="highlight-MATLAB notranslate"><div class="highlight"><pre><span></span><span class="k">...</span>
<span class="c">%Create targets: 1-of-k encodings for each discrete label</span>
<span class="n">u</span><span class="p">=</span><span class="w"> </span><span class="nb">unique</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
<span class="n">targets</span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">nclasses</span><span class="p">);</span>
<span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="nb">length</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
<span class="w">    </span><span class="n">targets</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="n">u</span><span class="p">(</span><span class="nb">i</span><span class="p">),</span><span class="nb">i</span><span class="p">)=</span><span class="mi">1</span><span class="p">;</span>
<span class="k">end</span>

<span class="c">%Create batches</span>
<span class="n">numbatches</span><span class="p">=</span><span class="w"> </span><span class="nb">ceil</span><span class="p">(</span><span class="n">N</span><span class="o">/</span><span class="n">batchsize</span><span class="p">);</span>
<span class="n">groups</span><span class="p">=</span><span class="w"> </span><span class="nb">repmat</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">numbatches</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">batchsize</span><span class="p">);</span>
<span class="n">groups</span><span class="p">=</span><span class="w"> </span><span class="n">groups</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span><span class="p">);</span>
<span class="n">groups</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">groups</span><span class="p">(</span><span class="nb">randperm</span><span class="p">(</span><span class="n">N</span><span class="p">));</span>
<span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">numbatches</span>
<span class="w">    </span><span class="n">batchdata</span><span class="p">{</span><span class="nb">i</span><span class="p">}=</span><span class="w"> </span><span class="n">X</span><span class="p">(</span><span class="n">groups</span><span class="o">==</span><span class="nb">i</span><span class="p">,:);</span>
<span class="w">    </span><span class="n">batchtargets</span><span class="p">{</span><span class="nb">i</span><span class="p">}=</span><span class="w"> </span><span class="n">targets</span><span class="p">(</span><span class="n">groups</span><span class="o">==</span><span class="nb">i</span><span class="p">,:);</span>
<span class="k">end</span>
<span class="k">...</span>
</pre></div>
</div>
<p>Heâ€™s initializing the <strong>nn</strong> so that itâ€™s got weights as and biases just like weâ€™re used to:</p>
<div class="highlight-MATLAB notranslate"><div class="highlight"><pre><span></span><span class="k">...</span>
<span class="c">%fit RBM</span>
<span class="n">numcases</span><span class="p">=</span><span class="n">N</span><span class="p">;</span>
<span class="n">numdims</span><span class="p">=</span><span class="n">d</span><span class="p">;</span>
<span class="n">numclasses</span><span class="p">=</span><span class="w"> </span><span class="nb">length</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
<span class="n">W</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mf">0.1</span><span class="o">*</span><span class="nb">randn</span><span class="p">(</span><span class="n">numdims</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">c</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numdims</span><span class="p">);</span>
<span class="n">b</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">Wc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mf">0.1</span><span class="o">*</span><span class="nb">randn</span><span class="p">(</span><span class="n">numclasses</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">cc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numclasses</span><span class="p">);</span>
<span class="n">ph</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">nh</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">phstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">nhstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">negdata</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numdims</span><span class="p">);</span>
<span class="n">negdatastates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numdims</span><span class="p">);</span>
<span class="n">Winc</span><span class="w">  </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numdims</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">binc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">cinc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numdims</span><span class="p">);</span>
<span class="n">Wcinc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">numclasses</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="n">ccinc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numclasses</span><span class="p">);</span>
<span class="n">Wavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">W</span><span class="p">;</span>
<span class="n">bavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
<span class="n">cavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="n">Wcavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Wc</span><span class="p">;</span>
<span class="n">ccavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cc</span><span class="p">;</span>
<span class="n">t</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="n">errors</span><span class="p">=</span><span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxepoch</span><span class="p">);</span>
<span class="k">...</span>
</pre></div>
</div>
<p>And then this is the training loop where we actually do the forward pass:</p>
<div class="highlight-MATLAB notranslate"><div class="highlight"><pre><span></span><span class="k">...</span>
<span class="k">for</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="n">maxepoch</span>
<span class="w">    </span>
<span class="w">	</span><span class="n">errsum</span><span class="p">=</span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">anneal</span><span class="p">)</span>
<span class="w">        </span><span class="n">penalty</span><span class="p">=</span><span class="w"> </span><span class="n">oldpenalty</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">0.9</span><span class="o">*</span><span class="n">epoch</span><span class="o">/</span><span class="n">maxepoch</span><span class="o">*</span><span class="n">oldpenalty</span><span class="p">;</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="n">numbatches</span>
<span class="w">		</span><span class="p">[</span><span class="n">numcases</span><span class="w"> </span><span class="n">numdims</span><span class="p">]=</span><span class="nb">size</span><span class="p">(</span><span class="n">batchdata</span><span class="p">{</span><span class="n">batch</span><span class="p">});</span>
<span class="w">		</span><span class="n">data</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">batchdata</span><span class="p">{</span><span class="n">batch</span><span class="p">};</span>
<span class="w">		</span><span class="n">classes</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">batchtargets</span><span class="p">{</span><span class="n">batch</span><span class="p">};</span>
<span class="w">        </span>
<span class="w">        </span><span class="c">%go up</span>
<span class="w">        </span><span class="n">ph</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">logistic</span><span class="p">(</span><span class="n">data</span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">classes</span><span class="o">*</span><span class="n">Wc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">repmat</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">numcases</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
<span class="w">		</span><span class="n">phstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">ph</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="nb">rand</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">isequal</span><span class="p">(</span><span class="n">method</span><span class="p">,</span><span class="s">&#39;SML&#39;</span><span class="p">))</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">                </span><span class="n">nhstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">phstates</span><span class="p">;</span>
<span class="w">            </span><span class="k">end</span>
<span class="w">        </span><span class="k">elseif</span><span class="w"> </span><span class="p">(</span><span class="nb">isequal</span><span class="p">(</span><span class="n">method</span><span class="p">,</span><span class="s">&#39;CD&#39;</span><span class="p">))</span>
<span class="w">            </span><span class="n">nhstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">phstates</span><span class="p">;</span>
<span class="w">        </span><span class="k">end</span>
<span class="k">...</span>
</pre></div>
</div>
<p>And then here, at this time, they didnâ€™t even necessarily use back propagation to train <strong>nn</strong>s. So this, in particular, implements a lot of the training that weâ€™re doing. It implements contrastive divergence, which estimates a gradient:</p>
<div class="highlight-MATLAB notranslate"><div class="highlight"><pre><span></span><span class="k">...</span>
<span class="w">        </span><span class="c">%go down</span>
<span class="w">		</span><span class="n">negdata</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">logistic</span><span class="p">(</span><span class="n">nhstates</span><span class="o">*</span><span class="n">W</span><span class="o">&#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">repmat</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">numcases</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
<span class="w">		</span><span class="n">negdatastates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">negdata</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="nb">rand</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numdims</span><span class="p">);</span>
<span class="w">		</span><span class="n">negclasses</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">softmaxPmtk</span><span class="p">(</span><span class="n">nhstates</span><span class="o">*</span><span class="n">Wc</span><span class="o">&#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">repmat</span><span class="p">(</span><span class="n">cc</span><span class="p">,</span><span class="n">numcases</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
<span class="w">		</span><span class="n">negclassesstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">softmax_sample</span><span class="p">(</span><span class="n">negclasses</span><span class="p">);</span>
<span class="w">		</span>
<span class="w">        </span><span class="c">%go up one more time</span>
<span class="w">		</span><span class="n">nh</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">logistic</span><span class="p">(</span><span class="n">negdatastates</span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">negclassesstates</span><span class="o">*</span><span class="n">Wc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">...</span><span class="c"> </span>
<span class="w">            </span><span class="nb">repmat</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">numcases</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
<span class="w">		</span><span class="n">nhstates</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">nh</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="nb">rand</span><span class="p">(</span><span class="n">numcases</span><span class="p">,</span><span class="n">numhid</span><span class="p">);</span>
<span class="k">...</span>
</pre></div>
</div>
<p>And then here, we take that gradient and use it for a parameter update along the lines that weâ€™re used to:</p>
<div class="highlight-MATLAB notranslate"><div class="highlight"><pre><span></span><span class="k">...</span>
<span class="w">        </span><span class="c">%update weights and biases</span>
<span class="w">        </span><span class="n">dW</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="o">&#39;*</span><span class="n">ph</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">negdatastates</span><span class="o">&#39;*</span><span class="n">nh</span><span class="p">);</span>
<span class="w">        </span><span class="n">dc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">negdatastates</span><span class="p">);</span>
<span class="w">        </span><span class="n">db</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">ph</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">nh</span><span class="p">);</span>
<span class="w">        </span><span class="n">dWc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="n">classes</span><span class="o">&#39;*</span><span class="n">ph</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">negclassesstates</span><span class="o">&#39;*</span><span class="n">nh</span><span class="p">);</span>
<span class="w">        </span><span class="n">dcc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">negclassesstates</span><span class="p">);</span>
<span class="w">		</span><span class="n">Winc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">momentum</span><span class="o">*</span><span class="n">Winc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">dW</span><span class="o">/</span><span class="n">numcases</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">penalty</span><span class="o">*</span><span class="n">W</span><span class="p">);</span>
<span class="w">		</span><span class="n">binc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">momentum</span><span class="o">*</span><span class="n">binc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">db</span><span class="o">/</span><span class="n">numcases</span><span class="p">);</span>
<span class="w">		</span><span class="n">cinc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">momentum</span><span class="o">*</span><span class="n">cinc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">dc</span><span class="o">/</span><span class="n">numcases</span><span class="p">);</span>
<span class="w">		</span><span class="n">Wcinc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">momentum</span><span class="o">*</span><span class="n">Wcinc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">dWc</span><span class="o">/</span><span class="n">numcases</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">penalty</span><span class="o">*</span><span class="n">Wc</span><span class="p">);</span>
<span class="w">		</span><span class="n">ccinc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">momentum</span><span class="o">*</span><span class="n">ccinc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">dcc</span><span class="o">/</span><span class="n">numcases</span><span class="p">);</span>
<span class="w">		</span><span class="n">W</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Winc</span><span class="p">;</span>
<span class="w">		</span><span class="n">b</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">binc</span><span class="p">;</span>
<span class="w">		</span><span class="n">c</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">cinc</span><span class="p">;</span>
<span class="w">		</span><span class="n">Wc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Wc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Wcinc</span><span class="p">;</span>
<span class="w">		</span><span class="n">cc</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ccinc</span><span class="p">;</span>
<span class="w">		</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">avgstart</span><span class="p">)</span>
<span class="w">            </span><span class="c">%apply averaging</span>
<span class="w">			</span><span class="n">Wavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Wavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">Wavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">W</span><span class="p">);</span>
<span class="w">			</span><span class="n">cavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">cavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">c</span><span class="p">);</span>
<span class="w">			</span><span class="n">bavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">bavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">bavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
<span class="w">			</span><span class="n">Wcavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Wcavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">Wcavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Wc</span><span class="p">);</span>
<span class="w">			</span><span class="n">ccavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">ccavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">ccavg</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cc</span><span class="p">);</span>
<span class="w">			</span><span class="n">t</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">;</span>
<span class="w">		</span><span class="k">else</span>
<span class="w">			</span><span class="n">Wavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">W</span><span class="p">;</span>
<span class="w">			</span><span class="n">bavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
<span class="w">			</span><span class="n">cavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="w">			</span><span class="n">Wcavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Wc</span><span class="p">;</span>
<span class="w">			</span><span class="n">ccavg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cc</span><span class="p">;</span>
<span class="w">        </span><span class="k">end</span>
<span class="w">        </span>
<span class="w">        </span><span class="c">%accumulate reconstruction error</span>
<span class="w">        </span><span class="n">err</span><span class="p">=</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="o">-</span><span class="n">negdata</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="w"> </span><span class="p">));</span>
<span class="w">        </span><span class="n">errsum</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">err</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">errsum</span><span class="p">;</span>
<span class="w">    </span><span class="k">end</span>
<span class="k">...</span>
</pre></div>
</div>
<p>But you can see that basically people are meddling with these gradients directly and inline all by themselves. It wasnâ€™t that common to use an autograd engine. Hereâ€™s one more example from Andrejâ€™s paper from <span class="math notranslate nohighlight">\(2014\)</span> called <a class="reference external" href="https://arxiv.org/abs/1406.5679">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</a>. Here, what he was doing is he was aligning images and text:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;karpathy2014fig2.jpeg&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/842fc839e8682dbbcc940541c3749aee20017d7ec6f28c9883b51e07e2bfd70b.jpg" src="../_images/842fc839e8682dbbcc940541c3749aee20017d7ec6f28c9883b51e07e2bfd70b.jpg" />
</div>
</div>
<p>And so itâ€™s kind of like a clip if youâ€™re familiar with it, but instead of working at the level of entire images and entire sentences, he was working on the level of individual objects and little pieces of sentences and he was embedding them calculating a clip-like <strong>loss</strong>: the cost. Back in 2014, it was standard to implement not just the cost, but also the backward pass manually. Take a look at the <code class="docutils literal notranslate"><span class="pre">DeFragCost</span></code> function from <a class="reference external" href="https://cs.stanford.edu/people/karpathy/defrag/code.zip">the paperâ€™s source code</a>:</p>
<div class="highlight-MATLAB notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span><span class="nf">[cost_struct, grad, df_CNN] = DeFragCost</span><span class="p">(</span>theta,decodeInfo,params, oWe,imgFeats,depTrees, rcnn_model<span class="p">)</span>
<span class="c">% returns cost, gradient, and gradient wrt image vectors which</span>
<span class="c">% can be forwarded to the CNN for finetuning outside of this</span>

<span class="n">domil</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">getparam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;domil&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="n">useglobal</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">getparam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;useglobal&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="n">uselocal</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">getparam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;uselocal&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="n">gmargin</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">gmargin</span><span class="p">;</span>
<span class="n">lmargin</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">lmargin</span><span class="p">;</span>
<span class="n">gscale</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">gscale</span><span class="p">;</span>
<span class="n">lscale</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">lscale</span><span class="p">;</span>
<span class="n">thrglobalscore</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">thrglobalscore</span><span class="p">;</span>
<span class="n">smoothnum</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">smoothnum</span><span class="p">;</span>
<span class="n">maxaccum</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">maxaccum</span><span class="p">;</span>

<span class="n">finetuneCNN</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="n">df_CNN</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c">% unpack parameters</span>
<span class="p">[</span><span class="n">Wi2s</span><span class="p">,</span><span class="w"> </span><span class="n">Wsem</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">stack2param</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="w"> </span><span class="n">decodeInfo</span><span class="p">);</span>
<span class="n">cost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="n">N</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">length</span><span class="p">(</span><span class="n">depTrees</span><span class="p">);</span><span class="w"> </span><span class="c">% number of samples</span>

<span class="c">% forward prop all image fragments and arrange them into a single large matrix</span>
<span class="n">imgVecsCell</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cell</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="n">imgVecICell</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cell</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="n">finetuneCNN</span>
<span class="w">    </span><span class="c">% forward RCNN</span>
<span class="w">    </span><span class="n">imgVecs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">andrej_forwardRCNN</span><span class="p">(</span><span class="n">imgFeats</span><span class="p">,</span><span class="w"> </span><span class="n">rcnn_model</span><span class="p">,</span><span class="w"> </span><span class="n">params</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">        </span><span class="n">imgVecICell</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">imgFeats</span><span class="p">{</span><span class="nb">i</span><span class="p">}.</span><span class="n">codes</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="nb">i</span><span class="p">;</span>
<span class="w">    </span><span class="k">end</span>
<span class="k">else</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span><span class="w">        </span>
<span class="w">        </span><span class="n">imgVecsCell</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">imgFeats</span><span class="p">{</span><span class="nb">i</span><span class="p">}.</span><span class="n">codes</span><span class="p">;</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="n">imgVecs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">imgVecsCell</span><span class="p">{:});</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">        </span><span class="n">imgVecICell</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">imgVecsCell</span><span class="p">{</span><span class="nb">i</span><span class="p">},</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="nb">i</span><span class="p">;</span>
<span class="w">    </span><span class="k">end</span>
<span class="k">end</span>
<span class="n">imgVecI</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">imgVecICell</span><span class="p">{:});</span>
<span class="n">allImgVecs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Wi2s</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">imgVecs</span><span class="o">&#39;</span><span class="p">;</span><span class="w"> </span><span class="c">% the mapped vectors are now columns</span>
<span class="n">Ni</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">size</span><span class="p">(</span><span class="n">allImgVecs</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>

<span class="c">% forward prop all sentences and arrange them</span>
<span class="n">sentVecsCell</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cell</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="n">sentTriplesCell</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cell</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="n">sentVecICell</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cell</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">    </span><span class="p">[</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">ts</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">ForwardSent</span><span class="p">(</span><span class="n">depTrees</span><span class="p">{</span><span class="nb">i</span><span class="p">},</span><span class="n">params</span><span class="p">,</span><span class="n">oWe</span><span class="p">,</span><span class="n">Wsem</span><span class="p">);</span>
<span class="w">    </span><span class="n">sentVecsCell</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">z</span><span class="p">;</span>
<span class="w">    </span><span class="n">sentTriplesCell</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">ts</span><span class="p">;</span>
<span class="w">    </span><span class="n">sentVecICell</span><span class="p">{</span><span class="nb">i</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="nb">i</span><span class="p">;</span>
<span class="k">end</span>
<span class="n">sentVecI</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">sentVecICell</span><span class="p">{:});</span>
<span class="n">allSentVecs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">sentVecsCell</span><span class="p">{:});</span>
<span class="n">Ns</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">size</span><span class="p">(</span><span class="n">allSentVecs</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>

<span class="c">% compute fragment scores</span>
<span class="n">dots</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">allImgVecs</span><span class="o">&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">allSentVecs</span><span class="p">;</span>

<span class="c">% compute local objective</span>
<span class="k">if</span><span class="w"> </span><span class="n">uselocal</span>

<span class="w">    </span><span class="n">MEQ</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">bsxfun</span><span class="p">(@</span><span class="nb">eq</span><span class="p">,</span><span class="w"> </span><span class="n">imgVecI</span><span class="p">,</span><span class="w"> </span><span class="n">sentVecI</span><span class="o">&#39;</span><span class="p">);</span><span class="w"> </span><span class="c">% indicator array for what should be high and low</span>
<span class="w">    </span><span class="n">Y</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="o">-</span><span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">MEQ</span><span class="p">));</span>
<span class="w">    </span><span class="n">Y</span><span class="p">(</span><span class="n">MEQ</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">domil</span>

<span class="w">        </span><span class="c">% miSVM formulation: we are minimizing over Y in the objective,</span>
<span class="w">        </span><span class="c">% what follows is a heuristic for it mentioned in miSVM paper.</span>
<span class="w">        </span><span class="n">fpos</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dots</span><span class="w"> </span><span class="o">.*</span><span class="w"> </span><span class="n">MEQ</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">9999</span><span class="w">  </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="o">~</span><span class="n">MEQ</span><span class="p">);</span><span class="w"> </span><span class="c">% simplifies things</span>
<span class="w">        </span><span class="n">Ypos</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">sign</span><span class="p">(</span><span class="n">fpos</span><span class="p">);</span>

<span class="w">        </span><span class="n">ixbad</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">find</span><span class="p">(</span><span class="o">~</span><span class="nb">any</span><span class="p">(</span><span class="n">Ypos</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="o">~</span><span class="nb">isempty</span><span class="p">(</span><span class="n">ixbad</span><span class="p">)</span>
<span class="w">            </span><span class="p">[</span><span class="o">~</span><span class="p">,</span><span class="w"> </span><span class="n">fmaxi</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">max</span><span class="p">(</span><span class="n">fpos</span><span class="p">(:,</span><span class="n">ixbad</span><span class="p">),</span><span class="w"> </span><span class="p">[],</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">            </span><span class="n">Ypos</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Ypos</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">sparse</span><span class="p">(</span><span class="n">fmaxi</span><span class="p">,</span><span class="w"> </span><span class="n">ixbad</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">Ni</span><span class="p">,</span><span class="w"> </span><span class="n">Ns</span><span class="p">);</span><span class="w"> </span><span class="c">% flip from -1 to 1: add 2</span>
<span class="w">        </span><span class="k">end</span>

<span class="w">        </span><span class="n">Y</span><span class="p">(</span><span class="n">MEQ</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Ypos</span><span class="p">(</span><span class="n">MEQ</span><span class="p">);</span><span class="w"> </span><span class="c">% augment Y in positive bags</span>
<span class="w">    </span><span class="k">end</span>

<span class="w">    </span><span class="c">% weighted fragment alignment objective</span>
<span class="w">    </span><span class="n">marg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">lmargin</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">.*</span><span class="w"> </span><span class="n">dots</span><span class="p">);</span><span class="w"> </span><span class="c">% compute margins</span>
<span class="w">    </span><span class="n">W</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">Ni</span><span class="p">,</span><span class="w"> </span><span class="n">Ns</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">Ns</span>
<span class="w">        </span><span class="n">ypos</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Y</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">;</span>
<span class="w">        </span><span class="n">yneg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Y</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="o">==-</span><span class="mi">1</span><span class="p">;</span>
<span class="w">        </span><span class="n">W</span><span class="p">(</span><span class="n">ypos</span><span class="p">,</span><span class="w"> </span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">ypos</span><span class="p">);</span>
<span class="w">        </span><span class="n">W</span><span class="p">(</span><span class="n">yneg</span><span class="p">,</span><span class="w"> </span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">yneg</span><span class="p">));</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="n">wmarg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">.*</span><span class="w"> </span><span class="n">marg</span><span class="p">;</span>
<span class="w">    </span><span class="n">lcost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">lscale</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">wmarg</span><span class="p">(:));</span>
<span class="w">    </span><span class="n">cost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lcost</span><span class="p">;</span>
<span class="k">end</span>

<span class="c">% compute global objective</span>
<span class="k">if</span><span class="w"> </span><span class="n">useglobal</span>
<span class="w">    </span>
<span class="w">    </span><span class="c">% forward scores in all regions</span>
<span class="w">    </span><span class="n">SG</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">SGN</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span><span class="w"> </span><span class="c">% the number of values (for mean)</span>
<span class="w">    </span><span class="n">accumsis</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">cell</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nb">j</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">            </span><span class="n">d</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dots</span><span class="p">(</span><span class="n">imgVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">i</span><span class="p">,</span><span class="w"> </span><span class="n">sentVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">j</span><span class="p">);</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">thrglobalscore</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">(</span><span class="n">d</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="k">end</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">maxaccum</span>
<span class="w">                </span><span class="p">[</span><span class="n">sv</span><span class="p">,</span><span class="w"> </span><span class="n">si</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">max</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="p">[],</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="c">% score will be max (i.e. we&#39;re finding support of each fragment in image)</span>
<span class="w">                </span><span class="n">accumsis</span><span class="p">{</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">}</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">si</span><span class="p">;</span><span class="w"> </span><span class="c">% remember switches for backprop</span>
<span class="w">                </span><span class="n">s</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">sv</span><span class="p">);</span>
<span class="w">            </span><span class="k">else</span>
<span class="w">                </span><span class="n">s</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">d</span><span class="p">(:));</span><span class="w"> </span><span class="c">% score is sum</span>
<span class="w">            </span><span class="k">end</span>
<span class="w">            </span><span class="n">nnorm</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">size</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="c">% number of sent fragments</span>
<span class="w">            </span><span class="n">nnorm</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">nnorm</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">smoothnum</span><span class="p">;</span>
<span class="w">            </span><span class="n">s</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">s</span><span class="o">/</span><span class="n">nnorm</span><span class="p">;</span>
<span class="w">            </span><span class="n">SG</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">s</span><span class="p">;</span>
<span class="w">            </span><span class="n">SGN</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">nnorm</span><span class="p">;</span>
<span class="w">        </span><span class="k">end</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span>
<span class="w">    </span><span class="c">% compute the cost</span>
<span class="w">    </span><span class="n">gcost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="n">cdiffs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">rdiffs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">        </span><span class="c">% i is the pivot. It should have higher score than col and row</span>
<span class="w">        </span>
<span class="w">        </span><span class="c">% col term</span>
<span class="w">        </span><span class="n">cdiff</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">SG</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">SG</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gmargin</span><span class="p">);</span>
<span class="w">        </span><span class="n">cdiff</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c">% nvm score with self</span>
<span class="w">        </span><span class="n">cdiffs</span><span class="p">(:,</span><span class="w"> </span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cdiff</span><span class="p">;</span><span class="w"> </span><span class="c">% useful in backprop</span>
<span class="w">        </span>
<span class="w">        </span><span class="c">% row term</span>
<span class="w">        </span><span class="n">rdiff</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">SG</span><span class="p">(</span><span class="nb">i</span><span class="p">,:)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">SG</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gmargin</span><span class="p">);</span>
<span class="w">        </span><span class="n">rdiff</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="n">rdiffs</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="w"> </span><span class="p">:)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">rdiff</span><span class="p">;</span><span class="w"> </span><span class="c">% useful in backprop</span>
<span class="w">        </span>
<span class="w">        </span><span class="n">gcost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">gcost</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">cdiff</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">rdiff</span><span class="p">);</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span>
<span class="w">    </span><span class="n">gcost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">gscale</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gcost</span><span class="p">;</span>
<span class="w">    </span><span class="n">cost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gcost</span><span class="p">;</span>
<span class="k">end</span>

<span class="n">ltop</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">Ni</span><span class="p">,</span><span class="w"> </span><span class="n">Ns</span><span class="p">);</span>

<span class="k">if</span><span class="w"> </span><span class="n">uselocal</span>
<span class="w">    </span><span class="c">% backprop local objective</span>
<span class="w">    </span><span class="n">ltop</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">ltop</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lscale</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">marg</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">.*</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">.*</span><span class="w"> </span><span class="n">W</span><span class="p">;</span>
<span class="k">end</span>

<span class="k">if</span><span class="w"> </span><span class="n">useglobal</span>
<span class="w">    </span><span class="c">% backprop global objective</span>
<span class="w">    </span>
<span class="w">    </span><span class="c">% backprop margin</span>
<span class="w">    </span><span class="n">dsg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">        </span><span class="nb">cd</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cdiffs</span><span class="p">(:,</span><span class="nb">i</span><span class="p">);</span>
<span class="w">        </span><span class="n">rd</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">rdiffs</span><span class="p">(</span><span class="nb">i</span><span class="p">,:);</span>
<span class="w">        </span>
<span class="w">        </span><span class="c">% col term backprop</span>
<span class="w">        </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="nb">cd</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="n">dsg</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dsg</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="nb">cd</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span>
<span class="w">        </span><span class="c">% row term backprop</span>
<span class="w">        </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">rd</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,:)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,:)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">rd</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span>
<span class="w">    </span><span class="c">% backprop into scores</span>
<span class="w">    </span><span class="n">ltopg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">ltop</span><span class="p">));</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nb">j</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span>
<span class="w">            </span>
<span class="w">            </span><span class="c">% backprop through the accumulation function</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">maxaccum</span>
<span class="w">                </span><span class="c">% route the gradient along in each column. bit messy...</span>
<span class="w">                </span><span class="n">gradroute</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">SGN</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">);</span>
<span class="w">                </span><span class="n">mji</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">find</span><span class="p">(</span><span class="n">sentVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">j</span><span class="p">);</span>
<span class="w">                </span><span class="n">mii</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">find</span><span class="p">(</span><span class="n">imgVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">i</span><span class="p">);</span>
<span class="w">                </span><span class="n">accumsi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">accumsis</span><span class="p">{</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">};</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="n">q</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="nb">length</span><span class="p">(</span><span class="n">mji</span><span class="p">)</span>
<span class="w">                    </span><span class="n">miy</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">mii</span><span class="p">(</span><span class="n">accumsi</span><span class="p">(</span><span class="n">q</span><span class="p">));</span>
<span class="w">                    </span><span class="n">mix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">mji</span><span class="p">(</span><span class="n">q</span><span class="p">);</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="n">thrglobalscore</span>
<span class="w">                        </span><span class="k">if</span><span class="w"> </span><span class="n">dots</span><span class="p">(</span><span class="n">miy</span><span class="p">,</span><span class="n">mix</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span>
<span class="w">                            </span><span class="n">ltopg</span><span class="p">(</span><span class="n">miy</span><span class="p">,</span><span class="w"> </span><span class="n">mix</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">gradroute</span><span class="p">;</span>
<span class="w">                        </span><span class="k">end</span>
<span class="w">                    </span><span class="k">else</span>
<span class="w">                        </span><span class="n">ltopg</span><span class="p">(</span><span class="n">miy</span><span class="p">,</span><span class="w"> </span><span class="n">mix</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">gradroute</span><span class="p">;</span>
<span class="w">                    </span><span class="k">end</span>
<span class="w">                </span><span class="k">end</span>
<span class="w">            </span><span class="k">else</span>
<span class="w">                </span><span class="n">d</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dots</span><span class="p">(</span><span class="n">imgVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">i</span><span class="p">,</span><span class="w"> </span><span class="n">sentVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">j</span><span class="p">);</span>
<span class="w">                </span><span class="n">dd</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">d</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dsg</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">SGN</span><span class="p">(</span><span class="nb">i</span><span class="p">,</span><span class="nb">j</span><span class="p">);</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">thrglobalscore</span>
<span class="w">                    </span><span class="n">dd</span><span class="p">(</span><span class="n">d</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">                </span><span class="k">end</span>
<span class="w">                </span><span class="n">ltopg</span><span class="p">(</span><span class="n">imgVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">i</span><span class="p">,</span><span class="w"> </span><span class="n">sentVecI</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">j</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">dd</span><span class="p">;</span>
<span class="w">            </span><span class="k">end</span>
<span class="w">        </span><span class="k">end</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="n">ltop</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">ltop</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gscale</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ltopg</span><span class="p">;</span>
<span class="k">end</span>

<span class="c">% backprop into fragment vectors</span>
<span class="n">allDeltasImg</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">allSentVecs</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ltop</span><span class="o">&#39;</span><span class="p">;</span>
<span class="n">allDeltasSent</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">allImgVecs</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ltop</span><span class="p">;</span>

<span class="c">% backprop image mapping</span>
<span class="n">df_Wi2s</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">allDeltasImg</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">imgVecs</span><span class="p">;</span>

<span class="k">if</span><span class="w"> </span><span class="n">finetuneCNN</span>
<span class="w">    </span><span class="c">% derivative wrt CNN data so that we can pass on gradient to RCNN</span>
<span class="w">    </span><span class="n">df_CNN</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">allDeltasImg</span><span class="o">&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Wi2s</span><span class="p">;</span>
<span class="k">end</span>

<span class="c">% backprop sentence mapping</span>
<span class="n">df_Wsem</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">BackwardSents</span><span class="p">(</span><span class="n">depTrees</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">oWe</span><span class="p">,</span><span class="n">Wsem</span><span class="p">,</span><span class="n">sentVecsCell</span><span class="p">,</span><span class="n">allDeltasSent</span><span class="p">);</span>

<span class="n">cost_struct</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">struct</span><span class="p">();</span>
<span class="n">cost_struct</span><span class="p">.</span><span class="n">raw_cost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cost</span><span class="p">;</span>
<span class="n">cost_struct</span><span class="p">.</span><span class="n">reg_cost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">regC</span><span class="o">/</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">sum</span><span class="p">(</span><span class="n">theta</span><span class="o">.^</span><span class="mi">2</span><span class="p">);</span>
<span class="n">cost_struct</span><span class="p">.</span><span class="n">cost</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">cost_struct</span><span class="p">.</span><span class="n">raw_cost</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">cost_struct</span><span class="p">.</span><span class="n">reg_cost</span><span class="p">;</span>

<span class="c">%[grad,~] = param2stack(df_Wi2s, df_Wsem);</span>
<span class="n">grad</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span><span class="n">df_Wi2s</span><span class="p">(:);</span><span class="w"> </span><span class="n">df_Wsem</span><span class="p">(:);];</span><span class="w"> </span><span class="c">% for speed hardcode param2stack</span>
<span class="n">grad</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">regC</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">theta</span><span class="p">;</span><span class="w"> </span><span class="c">% regularization term gradient</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Around <span class="math notranslate nohighlight">\(2014\)</span> it was standard to implement not just the cost but also the backward pass manually. So there he calculates the image embeddings sentence embeddings, sentence embeddings, the scores and the loss function. And after the loss function is calculated, he backwards through the <strong>nn</strong> and he appends a regularization term. So everything was done by hand manually, and you would just write out the backward pass. And then you would use a gradient checker to make sure that your numerical estimate of the gradient agrees with the one you calculated during <strong>backprop</strong>. This was very standard for a long time. Today, of course, it is standard to use an autograd engine. But it was definitely useful, and I think people sort of understood how these <strong>nn</strong>s work on a very intuitive level. Therefore, itâ€™s still a good exercise.</p>
</section>
<section id="manually-implementing-a-backward-pass">
<h2>Manually implementing a backward pass<a class="headerlink" href="#manually-implementing-a-backward-pass" title="Link to this heading">#</a></h2>
<p>Okay, so just as a reminder that weâ€™re going to keep everything the same from our previous lecture. So weâ€™re still going to have a two-layer multi-layer perceptron with a <strong>batchnorm</strong> layer. So the forward pass will be basically identical to this lecture. But here, weâ€™re going to get rid of <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>. And instead, weâ€™re going to write the backward pass manually. Apart from that, weâ€™re going to keep everything the same. So the forward pass will be basically identical to the previous lesson, etc. We are simply just going to write the backward pass manually. Hereâ€™s the starter code for this lecture:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span> <span class="c1"># for making figures</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> inline
<span class="k">else</span><span class="p">:</span>
    <span class="o">%</span><span class="k">matplotlib</span> ipympl
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2147483647</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># read in all the words</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;names.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32033
15
[&#39;emma&#39;, &#39;olivia&#39;, &#39;ava&#39;, &#39;isabella&#39;, &#39;sophia&#39;, &#39;charlotte&#39;, &#39;mia&#39;, &#39;amelia&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># build the vocabulary of characters and mappings to/from integers</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
<span class="n">ctoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">ctoi</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">itoc</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ctoi</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">itoc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">itoc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}
27
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">3</span>


<span class="k">def</span><span class="w"> </span><span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">w</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">:</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">ctoi</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span>
            <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>  <span class="c1"># crop and append</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">n2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="n">n1</span><span class="p">])</span>  <span class="c1"># 80%</span>
<span class="n">xval</span><span class="p">,</span> <span class="n">yval</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">])</span>  <span class="c1"># 10%</span>
<span class="n">xtest</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">n2</span><span class="p">:])</span>  <span class="c1"># 10%</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])
</pre></div>
</div>
</div>
</div>
<p>Now, here, weâ€™ll introduce a utility function that weâ€™re going to use later to compare the gradients. So in particular, we are going to have the gradients that we estimate manually ourselves. And weâ€™re going to have gradients that PyTorch calculates. And weâ€™re going to be checking for correctness, assuming, of course, that PyTorch is correct.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># utility function we will use later when comparing manual gradients to PyTorch gradients</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cmp</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">ex</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dt</span> <span class="o">==</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">app</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">maxdiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">dt</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">s</span><span class="si">:</span><span class="s2">15s</span><span class="si">}</span><span class="s2"> | exact: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span><span class="si">:</span><span class="s2">5s</span><span class="si">}</span><span class="s2"> | approximate: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">app</span><span class="p">)</span><span class="si">:</span><span class="s2">5s</span><span class="si">}</span><span class="s2"> | maxdiff: </span><span class="si">{</span><span class="n">maxdiff</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then, here, we have an initialization function that we are quite used to, where we create all the parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">define_nn</span><span class="p">(</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">w1_factor</span><span class="o">=</span><span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">b1_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">w2_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">b2_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bngain_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bnbias_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">global</span> <span class="n">g</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">block_size</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
    <span class="c1"># Layer 1</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">input_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
        <span class="o">*</span> <span class="n">w1_factor</span>
        <span class="o">/</span> <span class="p">((</span><span class="n">input_size</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b1_factor</span>
    <span class="p">)</span>  <span class="c1"># using b1 just for fun, it&#39;s useless because of batchnorm layer</span>
    <span class="c1"># Layer 2</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">w2_factor</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">b2_factor</span>
    <span class="c1"># BatchNorm parameters</span>
    <span class="n">bngain</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span> <span class="o">*</span> <span class="n">bngain_factor</span> <span class="o">+</span> <span class="mf">1.0</span>
    <span class="n">bnbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span> <span class="o">*</span> <span class="n">bnbias_factor</span>
    <span class="c1"># Note: We are initializating many of these parameters in non-standard ways</span>
    <span class="c1"># because sometimes initializating with e.g. all zeros could mask an incorrect</span>
    <span class="c1"># implementation of the backward pass.</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">C</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">bngain</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<p>Now, you will note that we changed the initialization a little bit to be small numbers. So normally you would set the biases to be all <span class="math notranslate nohighlight">\(0\)</span>. Here, Iâ€™m setting them to be small random numbers. And Iâ€™m doing this because if your variables are all <span class="math notranslate nohighlight">\(0\)</span>, or initialized to exactly <span class="math notranslate nohighlight">\(0\)</span>, sometimes what can happen is that can mask an incorrect implementation of a gradient. Because when everything is <span class="math notranslate nohighlight">\(0\)</span>, it sort of like simplifies and gives you a much simpler expression of the gradient than you would otherwise get. And so by making it small numbers, we are trying to unmask those potential errors in these calculations. You might also notice that we are using <code class="docutils literal notranslate"><span class="pre">b1</span></code> in the first layer. We are using a bias despite <strong>batchnorm</strong> right afterwards. So this would typically not be what youâ€™d do as we talked about the fact that you donâ€™t need a bias. But we are doing this here just for fun, because we are going to have a gradient <strong>w.r.t.</strong> it and we can check that we are still calculating it correctly even though this bias is spurious.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>


<span class="k">def</span><span class="w"> </span><span class="nf">construct_minibatch</span><span class="p">():</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">xtrain</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span> <span class="n">ytrain</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span>
</pre></div>
</div>
</div>
</div>
<p>So this function calculates a single batch. Then we will define a forward pass function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xb</span><span class="p">]</span>  <span class="c1"># embed the characters into vectors</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate the vectors</span>
    <span class="c1"># Linear layer 1</span>
    <span class="n">hprebn</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># BatchNorm layer</span>
    <span class="n">bnmeani</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">hprebn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bndiff</span> <span class="o">=</span> <span class="n">hprebn</span> <span class="o">-</span> <span class="n">bnmeani</span>
    <span class="n">bndiff2</span> <span class="o">=</span> <span class="n">bndiff</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">bnvar</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">bndiff2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>  <span class="c1"># note: Bessel&#39;s correction (dividing by n-1, not n)</span>
    <span class="n">bnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
    <span class="n">bnraw</span> <span class="o">=</span> <span class="n">bndiff</span> <span class="o">*</span> <span class="n">bnvar_inv</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="n">bnraw</span> <span class="o">+</span> <span class="n">bnbias</span>
    <span class="c1"># Non-linearity</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>
    <span class="c1"># Linear layer 2</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># output layer</span>
    <span class="c1"># cross entropy loss (same as F.cross_entropy(logits, Yb))</span>
    <span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span>  <span class="c1"># subtract max for numerical stability</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
    <span class="nb">globals</span><span class="p">()</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">locals</span><span class="p">())</span>  <span class="c1"># make variables defined so far globally-accessible</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">intermediate_values</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">logprobs</span><span class="p">,</span>
        <span class="n">probs</span><span class="p">,</span>
        <span class="n">counts</span><span class="p">,</span>
        <span class="n">counts_sum</span><span class="p">,</span>
        <span class="n">counts_sum_inv</span><span class="p">,</span>
        <span class="n">norm_logits</span><span class="p">,</span>
        <span class="n">logit_maxes</span><span class="p">,</span>
        <span class="n">logits</span><span class="p">,</span>
        <span class="n">h</span><span class="p">,</span>
        <span class="n">hpreact</span><span class="p">,</span>
        <span class="n">bnraw</span><span class="p">,</span>
        <span class="n">bnvar_inv</span><span class="p">,</span>
        <span class="n">bnvar</span><span class="p">,</span>
        <span class="n">bndiff2</span><span class="p">,</span>
        <span class="n">bndiff</span><span class="p">,</span>
        <span class="n">hprebn</span><span class="p">,</span>
        <span class="n">bnmeani</span><span class="p">,</span>
        <span class="n">embcat</span><span class="p">,</span>
        <span class="n">emb</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">intermediate_values</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">fp_intermed_values</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="c1"># PyTorch backward pass</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">fp_intermed_values</span><span class="p">:</span>
        <span class="n">t</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This <code class="docutils literal notranslate"><span class="pre">forward()</span></code> is now significantly expanded compared to what we are used to. The reason our forward pass is longer is for two reasons. Number one, before we just had an <code class="docutils literal notranslate"><span class="pre">f.cross_entropy()</span></code>, but here we are bringing back an explicit implementation of the <strong>loss</strong> function. And number two, we have broken up the implementation into manageable chunks, so we have a lot more intermediate tensors along the way in the forward pass. And thatâ€™s because we are about to go backwards and calculate the gradients in this <strong>backprop</strong>, from the bottom (<code class="docutils literal notranslate"><span class="pre">loss</span></code>) to the top (â€¦, <code class="docutils literal notranslate"><span class="pre">emb</span></code>, <code class="docutils literal notranslate"><span class="pre">C</span></code>, <code class="docutils literal notranslate"><span class="pre">xb</span></code>). So weâ€™re going to go upwards and just like we have for example the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> in this forward pass, in the backward pass we are going to have a <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code> which is going to store the derivative of the <strong>loss</strong> <strong>w.r.t.</strong> the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> tensor (<span class="math notranslate nohighlight">\(\dfrac{\partial **loss**}{\partial logprobs}\)</span>). And so in our manual implementation of <strong>backprop</strong>, we are going to be prepending <code class="docutils literal notranslate"><span class="pre">d</span></code> to every one of these tensors and calculating it along the way of this <strong>backprop</strong>. As an example, we have  a <code class="docutils literal notranslate"><span class="pre">bnraw</span></code> here, so weâ€™re going to be calculating a <code class="docutils literal notranslate"><span class="pre">dbnraw</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial **loss**}{\partial bnraw}\)</span>). Also, in the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function, before the <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> call, we are calling <code class="docutils literal notranslate"><span class="pre">t.retain_grad()</span></code> and thus telling PyTorch that we want to retain the graph of all of these intermediate values. This is because:</p>
<ul class="simple">
<li><p>in <strong>exercise 1</strong> (coming up) we are going to calculate the backward pass, so weâ€™re going to calculate all these <code class="docutils literal notranslate"><span class="pre">d</span></code> variables and use the <code class="docutils literal notranslate"><span class="pre">cmp</span></code> function weâ€™ve introduced above to check our correctness <strong>w.r.t.</strong> what PyTorch is telling us. This is going to be exercise 1, where we will <strong>backprop</strong> through this entire graph (starting from the <code class="docutils literal notranslate"><span class="pre">loss</span></code> and going upward through the intermediate values). Then,</p></li>
<li><p>in <strong>exercise 2</strong> we will fully break up the <strong>loss</strong> and <strong>backprop</strong> through it manually in all the little atomic pieces that make it up. But then weâ€™re going to collapse the <strong>loss</strong> into a single cross entropy call and instead weâ€™re going to analytically derive using math and paper and pencil the gradient of the <strong>loss</strong> <strong>w.r.t.</strong> the logits and instead of <strong>backprop</strong>ing through all of its little chunks, one at a time, weâ€™re just going to analytically derive what that gradient is and weâ€™re going to implement that, which is much more efficient as weâ€™ll see in a bit. Next,</p></li>
<li><p>for <strong>exercise 3</strong> weâ€™re going to do the exact same thing for <strong>batchnorm</strong>. Weâ€™re going to use pen and paper and calculus to derive the gradient through the <strong>batchnorm</strong> layer. So weâ€™re going to calculate the backward pass through the <strong>batchnorm</strong> layer in a much more efficient expression, instead of <strong>backprop</strong> through all of its little pieces independently. And then,</p></li>
<li><p>in <strong>exercise 4</strong> weâ€™re going to put it all together and this is the full code of training this two layer mlp and weâ€™re going to basically insert our manual <strong>backprop</strong> and weâ€™re going to take out <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and you will basically see that we can get all the same results using fully our own code and the only thing weâ€™re going to be using from PyTorch is <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> to make the calculations efficient. But otherwise, you will understand fully what it means to forward and backward through the <strong>nn</strong> and train it and I think thatâ€™ll be awesome so letâ€™s get to it.</p></li>
</ul>
<p>Okay, letâ€™s now define our <strong>nn</strong> and do one forward and one backward pass on it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">()</span>
<span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">construct_minibatch</span><span class="p">()</span>
<span class="n">fp_intermed_values</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
<span class="n">backward</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">fp_intermed_values</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4137
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-1-implementing-the-backward-pass">
<h2>Exercise 1: implementing the backward pass<a class="headerlink" href="#exercise-1-implementing-the-backward-pass" title="Link to this heading">#</a></h2>
<p>Now, <strong>exercise 1</strong>: implementing the backward pass:</p>
<p>Weâ€™ll start with <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code>, which basically means we are going to start with calculating the gradient of the <strong>loss</strong> <strong>w.r.t.</strong> all the elements of <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> tensor: <span class="math notranslate nohighlight">\(\dfrac{\partial \textbf{loss}}{\partial \textbf{logprobs}}\)</span>. So, <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code> will have the same shape as <code class="docutils literal notranslate"><span class="pre">logprobs</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 27])
</pre></div>
</div>
</div>
</div>
<p>Now, how does <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> influence the <code class="docutils literal notranslate"><span class="pre">loss</span></code>? Like this: <code class="docutils literal notranslate"><span class="pre">-logprobs[range(batch_size),</span> <span class="pre">yb].mean()</span></code>. Just as a reminder, <code class="docutils literal notranslate"><span class="pre">yb</span></code> is only just an array of the correct indeces:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,
        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])
</pre></div>
</div>
</div>
</div>
<p>And so by doing <code class="docutils literal notranslate"><span class="pre">logprobs[range(batch_size),</span> <span class="pre">yb]</span></code>, what we are essentially doing is for each row <code class="docutils literal notranslate"><span class="pre">i</span></code> of the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> tensor (of size <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>), we are plucking out the element at column index <code class="docutils literal notranslate"><span class="pre">yb[i]</span></code>. For example, from row <code class="docutils literal notranslate"><span class="pre">0</span></code> we would be getting the element from column <code class="docutils literal notranslate"><span class="pre">yb[0]</span> <span class="pre">==</span> <span class="pre">1</span></code>, from row <code class="docutils literal notranslate"><span class="pre">1</span></code> we would be getting the element from column <code class="docutils literal notranslate"><span class="pre">yb[1]</span> <span class="pre">==</span> <span class="pre">12</span></code>, and so on. Therefore, the elements inside the <code class="docutils literal notranslate"><span class="pre">logprobs[range(batch_size),</span> <span class="pre">yb]</span></code> could be alternatively calculated as such:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprobslist</span> <span class="o">=</span> <span class="p">[</span><span class="n">logprobs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">yb</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
</pre></div>
</div>
<p>Here is the proof by assertion that <code class="docutils literal notranslate"><span class="pre">logprobslist</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">logprobs</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logprobslist</span> <span class="o">=</span> <span class="p">[</span><span class="n">logprobs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">yb</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>
<span class="k">assert</span> <span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="n">logprobslist</span>
</pre></div>
</div>
</div>
</div>
<p>So these elements get plucked out from <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> and then the negative of their mean becomes the <strong>loss</strong>. It is always nice to work with simpler examples in order to understand the numerical form of derivatives. Whatâ€™s going on here is once weâ€™ve plucked out these examples, weâ€™re taking the mean and then the negative (<code class="docutils literal notranslate"><span class="pre">-logprobs[range(batch_size),</span> <span class="pre">yb].mean()</span></code>). So the <strong>loss</strong> basically, to write it in a simpler way, is the negative of the mean of e.g. <span class="math notranslate nohighlight">\(3\)</span> values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span><span class="o">/</span><span class="mi">3</span> <span class="o">-</span> <span class="n">b</span><span class="o">/</span><span class="mi">3</span> <span class="o">-</span> <span class="n">c</span><span class="o">/</span><span class="mi">3</span>
</pre></div>
</div>
<p>That would be how we achieve the mean of three numbers <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, <code class="docutils literal notranslate"><span class="pre">c</span></code> (although we actually have <span class="math notranslate nohighlight">\(32\)</span> numbers here). But therefore the derivative of this toy <strong>loss</strong> <strong>w.r.t.</strong> <code class="docutils literal notranslate"><span class="pre">a</span></code> would basically be just:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dloss</span><span class="o">/</span><span class="n">da</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span>
</pre></div>
</div>
<p>So you can see that if we donâ€™t just have <span class="math notranslate nohighlight">\(3\)</span> numbers (<code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, <code class="docutils literal notranslate"><span class="pre">c</span></code>), but we have <span class="math notranslate nohighlight">\(32\)</span> numbers, then <code class="docutils literal notranslate"><span class="pre">dloss</span></code> by <code class="docutils literal notranslate"><span class="pre">dk</span></code> (where <code class="docutils literal notranslate"><span class="pre">k</span></code> is every one of those <span class="math notranslate nohighlight">\(32\)</span> numbers) is going to be: <span class="math notranslate nohighlight">\(1/32\)</span> or just <span class="math notranslate nohighlight">\(\frac{1}{batch\_size}\)</span> more generally. Now, what about the other elements inside <code class="docutils literal notranslate"><span class="pre">logprobs</span></code>? Remember, <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> is a large array with a <code class="docutils literal notranslate"><span class="pre">logprobs.shape</span></code> of <span class="math notranslate nohighlight">\(32 \times 27\)</span> (<span class="math notranslate nohighlight">\(864\)</span> numbers in total) and only:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32])
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(32\)</span> of these numbers participate in the <strong>loss</strong> calculation. So, what about the derivative of all the other <span class="math notranslate nohighlight">\(832\)</span> numbers of the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> tensor, meaning the rest of the elements that do not get plucked out? Well, their gradient intuitively is <span class="math notranslate nohighlight">\(0\)</span>. Simply because they do not participate in the <strong>loss</strong> calculation. They are left out. So, since most of these numbers inside the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> tensor does not feed into the <strong>loss</strong>, if we were to change these numbers, then the value of <code class="docutils literal notranslate"><span class="pre">loss</span></code> wouldnâ€™t change. Which is the equivalent way of saying that the derivative of the <code class="docutils literal notranslate"><span class="pre">loss</span></code> <strong>w.r.t.</strong> them is zero. They donâ€™t impact it. So hereâ€™s a way to implement this derivative then: we create a tensor of zeros exactly in the shape of <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> and then fill in the derivative value <code class="docutils literal notranslate"><span class="pre">-1/batch_size</span></code> inside exactly these indeces: <code class="docutils literal notranslate"><span class="pre">[range(batch_size),</span> <span class="pre">yb]</span></code>. Therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
<span class="n">dlogprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
</div>
<p>Then this is the candidate derivative for <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code>. Letâ€™s call our assertion function to compare with the actual derivative computed by PyTorch and check whether our manually calculated derivative is correct:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logprobs&quot;</span><span class="p">,</span> <span class="n">dlogprobs</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logprobs        | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Cool! So letâ€™s look at the definition of the <code class="docutils literal notranslate"><span class="pre">cmp</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cmp</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">ex</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dt</span> <span class="o">==</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">app</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">maxdiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">dt</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">s</span><span class="si">:</span><span class="s1">15s</span><span class="si">}</span><span class="s1"> | exact: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span><span class="si">:</span><span class="s1">5s</span><span class="si">}</span><span class="s1"> | approximate: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">app</span><span class="p">)</span><span class="si">:</span><span class="s1">5s</span><span class="si">}</span><span class="s1"> | maxdiff: </span><span class="si">{</span><span class="n">maxdiff</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>What it does is that it basically receives the calculated value provided by us, which is <code class="docutils literal notranslate"><span class="pre">dt</span></code> and then checks whether it is exactly equal to <code class="docutils literal notranslate"><span class="pre">t.grad</span></code> as calculated by PyTorch. This makes sure that all of the elements are <em>exactly</em> equal. After that, it calculates for approximate equality because of floating point issues using <code class="docutils literal notranslate"><span class="pre">torch.allclose</span></code> which has a little bit of a wiggle available because sometimes you can get very, very close. But if you use a slightly different calculation, because of floating point, you canâ€™t get very, very close. Generally, because of floating point arithmetic, you can get a slightly different result. So this second call checks for an approximately equal result. And then it finds the maximum, basically the value that has the highest difference, and what is the difference, and the absolute value difference between those two. At the end it prints whether we have an exact equality, an approximate equality, and what is the largest difference. In our case, we have an exact equality. And so therefore, of course, we also have an approximate equality, and the maximum difference is exactly zero. So basically, our <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code> is exactly equal to what PyTorch calculated to be <code class="docutils literal notranslate"><span class="pre">logprobs.grad</span></code> in its backpropagation. So far, weâ€™re doing pretty well. Okay, so letâ€™s now continue our manual <strong>backprop</strong>. We know that <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> depends on <code class="docutils literal notranslate"><span class="pre">probs</span></code> through a <code class="docutils literal notranslate"><span class="pre">.log()</span></code> call. So a <code class="docutils literal notranslate"><span class="pre">log</span></code> is applied to all the elements of <code class="docutils literal notranslate"><span class="pre">probs</span></code>, element-wise. Now, if we want <code class="docutils literal notranslate"><span class="pre">dprobs</span></code>: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial probs} = \dfrac{\partial loss}{\partial logprobs}\cdot\dfrac{\partial logprobs}{\partial probs}\)</span>, then, well letâ€™s <a class="reference external" href="https://www.wolframalpha.com/input?i=d%2Fdx+log%28x%29">ask WolframAlpha what the derivative of <code class="docutils literal notranslate"><span class="pre">log(x)</span></code> is w.r.t. <code class="docutils literal notranslate"><span class="pre">x</span></code></a>: <span class="math notranslate nohighlight">\(\dfrac{d(log(x))}{dx} = \dfrac{1}{x}\)</span>. Therefore we write out the derivative and because we are doing <strong>backprop</strong> and thus applying the chain rule, we want to chain it to the previous result:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dprobs</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">dlogprobs</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;probs&quot;</span><span class="p">,</span> <span class="n">dprobs</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>probs           | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Now, notice what happens if your <code class="docutils literal notranslate"><span class="pre">probs</span></code> is very very close to <span class="math notranslate nohighlight">\(1.0\)</span>, that means that the <strong>nn</strong> is predicting the character correctly, then <code class="docutils literal notranslate"><span class="pre">dprobs</span> <span class="pre">=</span> <span class="pre">(1.0</span> <span class="pre">/</span> <span class="pre">1.0)</span> <span class="pre">*</span> <span class="pre">dlogprobs</span> <span class="pre">=</span> <span class="pre">dlogprobs</span></code> and thus <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code> just gets passed through. But if <code class="docutils literal notranslate"><span class="pre">probs</span></code> are very low, then the fraction part that is multiplied by <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code> will be boosted instead of <code class="docutils literal notranslate"><span class="pre">dlogprobs</span></code>. So what this line is doing intuitively is it is boosting the the gradient of the examples that have a very low probability currently assigned. Next up is <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code>. So we want <code class="docutils literal notranslate"><span class="pre">dcounts_sum_inv</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial counts\_sum\_inv}\)</span>). But letâ€™s just pause for a moment and explain what is happening until the calculation of <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code>, in case all this is a bit confusing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="c1"># Linear layer 2</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># output layer</span>
<span class="c1"># cross entropy loss (same as F.cross_entropy(logits, Yb))</span>
<span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> <span class="c1"># subtract max for numerical stability</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
<span class="o">...</span>
</pre></div>
</div>
<p>We have the <code class="docutils literal notranslate"><span class="pre">logits</span></code> that come out of the <strong>nn</strong>. Then we are finding the maximum in each row that we then subtract from the <code class="docutils literal notranslate"><span class="pre">logits</span></code> for purposes of numerical stability. Then, because some of the logits may take on too large values we end up exponentiating (this is done just for safety, numerically). This exponentiation yields our <code class="docutils literal notranslate"><span class="pre">counts</span></code>. And then, we take the sum of these <code class="docutils literal notranslate"><span class="pre">counts</span></code>. Using this <code class="docutils literal notranslate"><span class="pre">counts_sum</span></code>, we normalize the <code class="docutils literal notranslate"><span class="pre">counts</span></code> so that all of the <code class="docutils literal notranslate"><span class="pre">probs</span></code> sum to <span class="math notranslate nohighlight">\(1\)</span>. But basically, all thatâ€™s happening here is we got the logits, we want to exponentiate all of them, and we want to normalize the counts to create our probabilities, itâ€™s just that we have defined this set of intermediate calculations across multiple lines. Ok, so what should be <code class="docutils literal notranslate"><span class="pre">dcounts_sum_inv</span></code>? Now, we actually have to be careful here because we have to scrutinize and be careful with the shapes. So, <code class="docutils literal notranslate"><span class="pre">counts.shape</span></code> and <code class="docutils literal notranslate"><span class="pre">counts_sum_inv.shape</span></code> are different:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">counts</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">counts_sum_inv</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 27]) torch.Size([32, 1])
</pre></div>
</div>
</div>
</div>
<p>So during the multiplication <code class="docutils literal notranslate"><span class="pre">probs</span> <span class="pre">=</span> <span class="pre">counts</span> <span class="pre">*</span> <span class="pre">counts_sum_inv</span></code>, there is an implicit broadcasting that PyTorch does. Because what it needs to do is take one column of <span class="math notranslate nohighlight">\(32 \times 1\)</span> that is <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code> and replicate it horizontally <span class="math notranslate nohighlight">\(27\)</span> times in order to align these two tensors so it can do an element-wise multiplication. Using a toy example, this is what this looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># c = a * b, but with tensors:</span>
<span class="c1"># a[3x3] * b[3,1] ---&gt;</span>
<span class="c1"># a11*b1 a12*b1 a13*b1</span>
<span class="c1"># a21*b2 a22*b2 a23*b2</span>
<span class="c1"># a31*b3 a32*b3 a33*b3</span>
<span class="c1"># c[3x3]</span>
</pre></div>
</div>
</div>
</div>
<p>In this example, in order for <span class="math notranslate nohighlight">\(a[3\times3]\)</span> to multiply <span class="math notranslate nohighlight">\(b[3\times1]\)</span>, we have to replicate <span class="math notranslate nohighlight">\(3\)</span> times the vertical column <span class="math notranslate nohighlight">\([b1, b2, b3]^T\)</span> horizontally. And then do the actual multiplication. And this is exactly what PyTorch does in this situation. Meaning, in order to do <span class="math notranslate nohighlight">\(c = a \cdot b\)</span> (<span class="math notranslate nohighlight">\(c[3\times3] = a[3\times3] \cdot b[3\times1]\)</span>) it:</p>
<ol class="arabic simple">
<li><p>first replicates <span class="math notranslate nohighlight">\(b\)</span> across the column dimension (<span class="math notranslate nohighlight">\(1\)</span>): <span class="math notranslate nohighlight">\(b \rightarrow b\_replicated\)</span> (<span class="math notranslate nohighlight">\(b[3\times1] \rightarrow b[3\times3]\)</span>)</p></li>
<li><p>then it does the multiplication <span class="math notranslate nohighlight">\(c = a \cdot b\_replicated\)</span> (<span class="math notranslate nohighlight">\(c[3\times3] = a[3\times3] \cdot b[3\times3]\)</span>)</p></li>
</ol>
<p>Therefore, if we wanted to find <span class="math notranslate nohighlight">\(\dfrac{\partial c}{\partial a}\)</span>, then that would equal be <span class="math notranslate nohighlight">\(a\_replicated\)</span> (and <strong>not</strong> <span class="math notranslate nohighlight">\(a\)</span>). What the replication operation means in terms of the computational graph is that <span class="math notranslate nohighlight">\(a\)</span> in this case is being fed to multiple outputs. Thus, as we discussed in <strong>micrograd</strong> (lesson 1), during the backward pass, the correct thing to do in this case is to sum all the gradients that arrive at any one node  (i.e. <span class="math notranslate nohighlight">\(a\)</span>). So if a node is used multiple times, the gradients for all of its uses sum during <strong>backprop</strong>. This means that since e.g. <span class="math notranslate nohighlight">\(b1\)</span> is used multiple times in each column of <span class="math notranslate nohighlight">\(c\)</span>, therefore the right thing to do here would be to sum horizontally across all the columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">da</span> <span class="o">=</span> <span class="n">a_replicated</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Equivalently, <code class="docutils literal notranslate"><span class="pre">probs</span> <span class="pre">=</span> <span class="pre">counts</span> <span class="pre">*</span> <span class="pre">counts_sum_inv</span></code>, is broken up into to operations as well:</p>
<ol class="arabic simple">
<li><p>first, the replication: <span class="math notranslate nohighlight">\(counts\_sum\_inv \rightarrow counts\_sum\_inv\_replicated\)</span> (<span class="math notranslate nohighlight">\(counts\_sum\_inv[32\times1] \rightarrow counts\_sum\_inv[32\times27]\)</span>)</p></li>
<li><p>then, the multiplication: <span class="math notranslate nohighlight">\(probs = counts \cdot counts\_sum\_inv\_replicated\)</span> (<span class="math notranslate nohighlight">\(probs[32\times27] = counts[32\times27] \cdot counts\_sum\_inv[32\times27]\)</span>)</p></li>
</ol>
<p>And therefore <code class="docutils literal notranslate"><span class="pre">dcounts_sum_inv</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial counts\_sum\_inv} = \dfrac{\partial loss}{\partial probs} \dfrac{\partial probs}{\partial counts\_sum\_inv}\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dcounts_sum_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span> <span class="o">*</span> <span class="n">dprobs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># multiply by `dprobs` because of chain rule and keepdim=True to keep the column dimension</span>
<span class="k">assert</span> <span class="n">dcounts_sum_inv</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">counts_sum_inv</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># shapes are equals</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;counts_sum_inv&quot;</span><span class="p">,</span> <span class="n">dcounts_sum_inv</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Again, this derivative is exactly correct. Letâ€™s also <strong>backprop</strong> into <code class="docutils literal notranslate"><span class="pre">counts</span></code>. Similarly, <code class="docutils literal notranslate"><span class="pre">dcounts</span></code> is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dcounts</span> <span class="o">=</span> <span class="n">counts_sum_inv</span> <span class="o">*</span> <span class="n">dprobs</span>
<span class="k">assert</span> <span class="n">dcounts</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">counts</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<p>with no additional summation required here since there is a broadcasting going on that will yield a derivative with a shape that is equivalent to the shape of the <code class="docutils literal notranslate"><span class="pre">counts</span></code> tensor. But we canâ€™t call <code class="docutils literal notranslate"><span class="pre">cmp</span></code> and check the correctness of <code class="docutils literal notranslate"><span class="pre">dcounts</span></code> just yet. The reason is that besides being used to calculate <code class="docutils literal notranslate"><span class="pre">probs</span></code>, it is also being used in another branch to calculate <code class="docutils literal notranslate"><span class="pre">counts_sum_inv</span></code>. So, even though we have calculated the first contribution of it: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial probs}\cdot\dfrac{\partial probs}{\partial counts}\)</span>, we still have to calculate the second contribution of it: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial counts\_sum}\cdot\dfrac{\partial counts\_sum}{\partial counts}\)</span>. To continue with this branch, we now want to calculate <code class="docutils literal notranslate"><span class="pre">dcounts_sum</span></code>: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial counts\_sum} = \dfrac{\partial counts\_sum\_inv}{\partial counts\_sum}\cdot\dfrac{\partial loss}{\partial counts\_sum\_inv}\)</span>. With the <a class="reference external" href="https://www.wolframalpha.com/input?i=d%2Fdx+x**-1">help of WolframAlpha</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dcounts_sum</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">counts_sum</span><span class="o">**-</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dcounts_sum_inv</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;counts_sum&quot;</span><span class="p">,</span> <span class="n">dcounts_sum</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Cool. Next, moving along, we want to <strong>backprop</strong> through the preceding operation: <code class="docutils literal notranslate"><span class="pre">counts_sum</span> <span class="pre">=</span> <span class="pre">counts.sum(1,</span> <span class="pre">keepdims=True)</span></code>. This means we want to now calculate the additional <code class="docutils literal notranslate"><span class="pre">dcounts</span></code> component: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial counts} = \dfrac{\partial loss}{\partial counts\_sum}\cdot\dfrac{\partial counts \_sum}{\partial counts}\)</span>. If we remember from <strong>micrograd</strong>, the sum operation basically acts just like a router, allowing the gradient (<code class="docutils literal notranslate"><span class="pre">dcounts_sum</span></code>, in our case) to distribute equally to all the input during the backward flow. In other words, to use a toy example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># b = sum(a), but with tensors:</span>
<span class="c1"># b[3,1] = a[3x3].sum(1) ---&gt;</span>
<span class="c1"># a11 a12 a13 ---&gt; b1 (= a11 + a12 + a13)</span>
<span class="c1"># a21 a22 a23 ---&gt; b2 (= a21 + a22 + a23)</span>
<span class="c1"># a31 a32 a33 ---&gt; b3 (= a31 + a32 + a33)</span>
</pre></div>
</div>
</div>
</div>
<p>Therefore considering that <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are multi-dimensional tensors: <span class="math notranslate nohighlight">\(\dfrac{\partial b_{3\times1}}{\partial a_{3\times3}} = \dfrac{\partial{\sum a_{3\times3}}}{\partial a_{3\times3}} = \dfrac{\partial \textbf{1}_{3\times3}^T a_{3\times3}}{\partial a_{3\times3}} = \textbf{1}_{3\times3}\)</span>. Therefore, similarly, <span class="math notranslate nohighlight">\(\dfrac{\partial counts\_sum}{\partial counts} = \textbf{1}\)</span>, where the matrix <span class="math notranslate nohighlight">\(\textbf{1}\)</span> has the shape of <code class="docutils literal notranslate"><span class="pre">counts</span></code>. Therefore, we can now add the calculated second component <span class="math notranslate nohighlight">\(\dfrac{\partial counts\_sum}{\partial counts}\dfrac{\partial loss}{\partial counts\_sum}\)</span> to <code class="docutils literal notranslate"><span class="pre">dcounts</span></code>. And since there is no extra component that makes up <code class="docutils literal notranslate"><span class="pre">dcounts</span></code> we can do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dcounts</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="o">*</span> <span class="n">dcounts_sum</span>
</pre></div>
</div>
</div>
</div>
<p>And now <code class="docutils literal notranslate"><span class="pre">dcounts</span></code> is equal to: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial counts} = \dfrac{\partial loss}{\partial probs}\cdot\dfrac{\partial probs}{\partial counts} + \dfrac{\partial loss}{\partial counts\_sum}\cdot\dfrac{\partial counts\_sum}{\partial counts}\)</span>. And now, we can check for correctness:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">dcounts</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>counts          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Awesome! The match is exact, which of course means that once again we have correctly calculated the gradient of yet-another node, and our smooth manual <strong>backprop</strong>ing journey continues! So hopefully we should be getting a hang of this now. Now, <code class="docutils literal notranslate"><span class="pre">dnorm_logits</span></code> is easy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dnorm_logits</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">dcounts</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;norm_logits&quot;</span><span class="p">,</span> <span class="n">dnorm_logits</span><span class="p">,</span> <span class="n">norm_logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>since <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial norm\_logits} = \dfrac{\partial counts}{\partial norm\_logits}\cdot\dfrac{\partial loss}{\partial counts} = \dfrac{\partial e^{norm\_logits}}{\partial norm\_logits}\cdot\dfrac{\partial loss}{\partial counts} = e^{norm\_logits}\cdot\dfrac{\partial loss}{\partial counts} = counts\cdot\dfrac{\partial loss}{\partial counts}\)</span>. Letâ€™s continue. Next up is <code class="docutils literal notranslate"><span class="pre">norm_logits</span> <span class="pre">=</span> <span class="pre">logits</span> <span class="pre">-</span> <span class="pre">logit_maxes</span></code>. So now we care about finding <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> and <code class="docutils literal notranslate"><span class="pre">dlogit_maxes</span></code>. We have to be careful here again as the shapes are not the same:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">norm_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))
</pre></div>
</div>
</div>
</div>
<p>This means that when the subtraction happens there is going to be an implicit broadcasting happening:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># c = a - b, but with tensors: a[3x3], b[3x1], c[3x3]</span>
<span class="c1"># c11 c12 c13 = a11 a12 a13   b1</span>
<span class="c1"># c21 c22 c23 = a21 a22 a23 - b2</span>
<span class="c1"># c31 c32 c33 = a31 a32 a33   b3</span>

<span class="c1"># so e.g. c32 = a32 - b3</span>
</pre></div>
</div>
</div>
</div>
<p>For calculating <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> we care about finding the (first) component <span class="math notranslate nohighlight">\(\dfrac{\partial norm\_logits}{\partial logits}\dfrac{\partial loss}{\partial norm\_logits} = \dfrac{\partial loss}{\partial norm\_logits}\)</span>. And therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogits</span> <span class="o">=</span> <span class="n">dnorm_logits</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now, like for <code class="docutils literal notranslate"><span class="pre">dcounts</span></code> previously, we cannot yet check for the correctness of <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> because there is still a second component that should be added to the first one to make up the complete <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> gradient <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial logits}\)</span>. Before we calculate that letâ€™s first find <code class="docutils literal notranslate"><span class="pre">dlogit_maxes</span></code>: <span class="math notranslate nohighlight">\(\dfrac{\partial loss}{\partial logit\_maxes} = \dfrac{\partial norm\_logits}{\partial logit\_maxes}\dfrac{\partial loss}{\partial norm\_logits} = -\dfrac{\partial loss}{\partial norm\_logits}\)</span>, which essentially means: <code class="docutils literal notranslate"><span class="pre">dlogit_maxes</span> <span class="pre">=</span> <span class="pre">-dnorm_logits</span></code>. Now remember, because we are dealing with tensors and <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code> has a shape of <span class="math notranslate nohighlight">\(32\times1\)</span>, that means that <code class="docutils literal notranslate"><span class="pre">-dnorm_logits</span></code>, whose shape is <span class="math notranslate nohighlight">\(32\times27\)</span>, must be summed along the column axis in order for <code class="docutils literal notranslate"><span class="pre">dlogits_maxes</span></code> to become a <span class="math notranslate nohighlight">\(32\times1\)</span> tensor (the same shape as <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code>). Therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogit_maxes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">dnorm_logits</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logit_maxes&quot;</span><span class="p">,</span> <span class="n">dlogit_maxes</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Next up is: <code class="docutils literal notranslate"><span class="pre">logit_maxes</span> <span class="pre">=</span> <span class="pre">logits.max(1,</span> <span class="pre">keepdim=True).values</span></code>. Now, this is the line out of which we will find the second derivative component of <code class="docutils literal notranslate"><span class="pre">dlogits</span></code>: <span class="math notranslate nohighlight">\(\dfrac{\partial logit\_maxes}{\partial logits}\cdot\dfrac{\partial loss}{\partial logit\_maxes}\)</span>. But what is <span class="math notranslate nohighlight">\(\dfrac{\partial logit\_maxes}{\partial logits}\)</span> lossin this case? Before we answer this question, letâ€™s pause here briefly and look at these <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code>, and especially their gradients. Weâ€™ve talked previously in the previous lesson that the only reason weâ€™re doing this is for the numerical stability of the softmax that we are implementing here. And we talked about how if you take these <code class="docutils literal notranslate"><span class="pre">logits</span></code> for any one of these examples, so one row of this logits tensor, if you add or subtract any value equally to all the elements, then the value of the <code class="docutils literal notranslate"><span class="pre">probs</span></code> will be unchanged. Youâ€™re not changing the softmax. The only thing that this is doing is itâ€™s making sure that the subsequent <code class="docutils literal notranslate"><span class="pre">exp()</span></code> operation doesnâ€™t overflow. And the reason weâ€™re using a <code class="docutils literal notranslate"><span class="pre">max</span></code> is because then we are guaranteed that each row of logits, the highest number, is <span class="math notranslate nohighlight">\(0\)</span>. Basically, that has repercussions. If it is the case that changing <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code> does not change the <code class="docutils literal notranslate"><span class="pre">probs</span></code>, and therefore does not change the <code class="docutils literal notranslate"><span class="pre">loss</span></code>, then the gradient on <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code> should be <span class="math notranslate nohighlight">\(0\)</span>. Are they?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogit_maxes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-4.6566e-09],
        [-9.3132e-10],
        [-1.8626e-09],
        [-4.6566e-09],
        [-9.3132e-10],
        [-1.6298e-09],
        [-3.7253e-09],
        [-4.6566e-09],
        [-5.5879e-09],
        [-1.8626e-09],
        [ 1.3970e-09],
        [-9.3132e-10],
        [ 3.4925e-09],
        [-1.1642e-09],
        [ 2.3283e-10],
        [ 6.2864e-09],
        [ 0.0000e+00],
        [-6.9849e-10],
        [ 5.3551e-09],
        [ 1.1642e-09],
        [ 0.0000e+00],
        [-2.0955e-09],
        [ 6.7521e-09],
        [ 9.3132e-10],
        [ 1.8626e-09],
        [ 9.3132e-10],
        [-3.7253e-09],
        [-9.3132e-10],
        [ 4.6566e-09],
        [ 4.6566e-10],
        [-2.3283e-09],
        [ 3.0268e-09]], grad_fn=&lt;SumBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<p>Not exactly. But regardless, these are very very tiny floating point numbers that are close to <span class="math notranslate nohighlight">\(0\)</span>. And so this is telling us that the values of <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code> are not impacting the <code class="docutils literal notranslate"><span class="pre">loss</span></code>, as they should not. It feels kind of weird to <strong>backprop</strong> through this branch, honestly, because if you have any implementation of <code class="docutils literal notranslate"><span class="pre">f.cross_entropy</span></code> in PyTorch, and you block together all of these elements, and youâ€™re not doing <strong>backprop</strong> piece by piece, then you would probably assume that the derivative through here is exactly <span class="math notranslate nohighlight">\(0\)</span>. So you would be sort of skipping this branch. Because itâ€™s only done for numerical stability. But itâ€™s interesting to see that even if you break up everything into the full atoms, and you still do the computation as youâ€™d like <strong>w.r.t.</strong> numerical stability, the correct thing happens. And you still get very, very small gradients here. Basically reflecting the fact that the values of these do not matter <strong>w.r.t.</strong> the final <strong>loss</strong>. Okay, so letâ€™s now continue <strong>backprop</strong> through this line here. Weâ€™ve just calculated the <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code>, and now we want to <strong>backprop</strong> into <code class="docutils literal notranslate"><span class="pre">logits</span></code> through this second branch. Here we took the max along all the rows and then looked at its values <code class="docutils literal notranslate"><span class="pre">logits.max(1,</span> <span class="pre">keepdim=True).values</span></code>. In PyTorch, max returns both the max values but also the indices of those values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.return_types.max(
values=tensor([[1.0230],
        [0.8773],
        [1.2418],
        [0.6361],
        [1.7365],
        [0.9537],
        [0.8150],
        [1.4842],
        [1.0861],
        [1.1175],
        [1.7868],
        [2.0955],
        [1.1189],
        [0.9202],
        [0.5922],
        [0.8290],
        [0.9719],
        [0.7808],
        [1.1189],
        [0.8777],
        [0.8088],
        [1.0604],
        [1.1189],
        [1.2769],
        [1.6185],
        [1.0034],
        [1.2943],
        [1.0409],
        [1.0030],
        [0.7977],
        [1.1389],
        [0.9284]], grad_fn=&lt;MaxBackward0&gt;),
indices=tensor([[ 1],
        [ 2],
        [19],
        [15],
        [15],
        [25],
        [16],
        [ 3],
        [19],
        [ 8],
        [15],
        [ 3],
        [22],
        [18],
        [ 7],
        [ 5],
        [ 2],
        [ 1],
        [22],
        [19],
        [15],
        [19],
        [22],
        [22],
        [23],
        [ 5],
        [22],
        [20],
        [24],
        [ 8],
        [24],
        [13]]))
</pre></div>
</div>
</div>
</div>
<p>Although we just keep the values, in the backward pass, it is extremely useful to know about where those maximum values occured. And these indices will help us with the <strong>backprop</strong>. Again, we ask, what is <span class="math notranslate nohighlight">\(\dfrac{\partial logit\_maxes}{\partial logits}\)</span> in this case? We have the <code class="docutils literal notranslate"><span class="pre">logits</span></code> whose shape is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 27])
</pre></div>
</div>
</div>
</div>
<p>and in each row we find the maximum value. And then that value gets plucked out into <code class="docutils literal notranslate"><span class="pre">logit_maxes</span></code>. And so intuitively, the derivative we are looking for (that is flowing through) <span class="math notranslate nohighlight">\(1\)</span> (for the appropriate entry that was plucked out) times <code class="docutils literal notranslate"><span class="pre">dlogit_maxes</span></code>. So if you think about it, what we want here is we need to take the <code class="docutils literal notranslate"><span class="pre">dlogit_maxes</span></code> tensor and scatter it into the correct positions in <code class="docutils literal notranslate"><span class="pre">logits</span></code>, meaning the indices of the max values. One way to do this is create a tensor with <span class="math notranslate nohighlight">\(1s\)</span> at the indices we care about and <span class="math notranslate nohighlight">\(0s\)</span> elsewhere and multiply this tensor by <code class="docutils literal notranslate"><span class="pre">dlogit_maxes</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ones_maxes_indeces</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">ones_maxes_indeces</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3859a73b4fce4c36a28ae95aaeecc886", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This is basically what we want. And array of <span class="math notranslate nohighlight">\(1s\)</span> at one position (the index where the maxes came from) in each row of our <code class="docutils literal notranslate"><span class="pre">logits</span></code> tensor. Therefore, letâ€™s now add the second derivative component to <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> and check for correctness.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogits</span> <span class="o">+=</span> <span class="n">ones_maxes_indeces</span> <span class="o">*</span> <span class="n">dlogit_maxes</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">dlogits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logits          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Next up: <code class="docutils literal notranslate"><span class="pre">logits</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">w2</span> <span class="pre">+</span> <span class="pre">b2</span></code>. Letâ€™s look at the shapes of all the intermediate tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 27]),
 torch.Size([32, 64]),
 torch.Size([64, 27]),
 torch.Size([27]))
</pre></div>
</div>
</div>
</div>
<p>Now letâ€™s see the shape of <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">w2</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">h</span> <span class="o">@</span> <span class="n">w2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 27])
</pre></div>
</div>
</div>
</div>
<p>This tells us that since the shape of <code class="docutils literal notranslate"><span class="pre">b2</span></code> is <span class="math notranslate nohighlight">\(27\)</span>, the <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">w2</span> <span class="pre">+</span> <span class="pre">b2</span></code> will first broadcast the <code class="docutils literal notranslate"><span class="pre">b2</span></code> tensor into a shape of <span class="math notranslate nohighlight">\(1\times27\)</span> and then it will be replicated vertically along the columns (across the <span class="math notranslate nohighlight">\(0\)</span> dimension) in order to give rise to a <code class="docutils literal notranslate"><span class="pre">b2_replicated</span></code> with a shape of <span class="math notranslate nohighlight">\(32\times27\)</span>. But the question now is how do we <strong>backprop</strong> from <code class="docutils literal notranslate"><span class="pre">logits</span></code> to the hidden states <code class="docutils literal notranslate"><span class="pre">h</span></code>, the weight matrix <code class="docutils literal notranslate"><span class="pre">w2</span></code> and the bias matrix <code class="docutils literal notranslate"><span class="pre">b2</span></code>? Now you might think that we need to go to some matrix calculus textbook and look up the derivative for matrix multiplication. But actually first principles suffice here to derive this yourself on a piece of paper. Specifically, what works great in these types of situations is to find a specific small example that you then fully write out and then in the process of analyzing how that individual small example works out, you will understand a broader pattern and youâ€™ll be able to generalize and write out the full general formula for how derivatives flow in an expression like this one. So letâ€™s try that out:</p>
<p><span class="math notranslate nohighlight">\(d = a \cdot b + c\\
\begin{bmatrix}
d_{11} &amp; d_{12} \\
d_{21} &amp; d_{22}
\end{bmatrix} =
\begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}
\end{bmatrix} \cdot
\begin{bmatrix}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22}
\end{bmatrix} + 
\begin{bmatrix}
c_{1} &amp; c_{2}
\end{bmatrix} \Rightarrow \\
d_{11} = a_{11}b_{11} + a_{12}b_{21} + c_1 \\
d_{12} = a_{11}b_{12} + a_{12}b_{22} + c_2 \\
d_{21} = a_{21}b_{11} + a_{22}b_{21} + c_1 \\
d_{22} = a_{21}b_{12} + a_{22}b_{22} + c_2
\)</span></p>
<p>where
<span class="math notranslate nohighlight">\(\begin{bmatrix}
c_{1} &amp; c_{2}
\end{bmatrix}\)</span>
gets implicitly replicated vertically into a
<span class="math notranslate nohighlight">\(\begin{bmatrix}
c_{1} &amp; c_{2} \\
c_{1} &amp; c_{2}
\end{bmatrix}\)</span>
in order for the addition to be a valid one. This is something that PyTorch also does. So as we know <code class="docutils literal notranslate"><span class="pre">dlogits</span></code>, by analogy, assume we also know <code class="docutils literal notranslate"><span class="pre">dd</span></code>. And since we are looking for <code class="docutils literal notranslate"><span class="pre">dh</span></code>, <code class="docutils literal notranslate"><span class="pre">dw2</span></code> and <code class="docutils literal notranslate"><span class="pre">db2</span></code>, in this simple example, by analogy, we are looking for <code class="docutils literal notranslate"><span class="pre">da</span></code>, <code class="docutils literal notranslate"><span class="pre">db</span></code>, <code class="docutils literal notranslate"><span class="pre">dc</span></code>. Letâ€™s write them out by hand:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(
\dfrac{\partial L}{\partial a} = 
\begin{bmatrix}
\dfrac{\partial L}{\partial a_{11}} &amp; \dfrac{\partial L}{\partial a_{12}} \\
\dfrac{\partial L}{\partial a_{21}} &amp; \dfrac{\partial L}{\partial a_{22}}
\end{bmatrix} = 
\begin{bmatrix}
\dfrac{\partial L}{\partial d_{11}}b_{11} + \dfrac{\partial L}{\partial d_{12}}b_{12} &amp; \dfrac{\partial L}{\partial d_{11}}b_{21} + \dfrac{\partial L}{\partial d_{12}}b_{22} \\
\dfrac{\partial L}{\partial d_{21}}b_{11} + \dfrac{\partial L}{\partial d_{22}}b_{12} &amp; \dfrac{\partial L}{\partial d_{21}}b_{21} + \dfrac{\partial L}{\partial d_{22}}b_{12}
\end{bmatrix} =
\begin{bmatrix}
\dfrac{\partial L}{\partial d_{11}} &amp; \dfrac{\partial L}{\partial d_{12}} \\
\dfrac{\partial L}{\partial d_{21}} &amp; \dfrac{\partial L}{\partial d_{22}}
\end{bmatrix}
\begin{bmatrix}
b_{11} &amp; b_{21} \\
b_{12} &amp; b_{22}
\end{bmatrix} = 
\dfrac{\partial L}{\partial d} \cdot b^T
\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial b} =
\begin{bmatrix}
\dfrac{\partial L}{\partial b_{11}} &amp; \dfrac{\partial L}{\partial b_{12}} \\
\dfrac{\partial L}{\partial b_{21}} &amp; \dfrac{\partial L}{\partial b_{22}}
\end{bmatrix} =
\begin{bmatrix}
a_{11}\dfrac{\partial L}{\partial d_{11}} + a_{21}\dfrac{\partial L}{\partial d_{21}} &amp; a_{11}\dfrac{\partial L}{\partial d_{12}} + a_{21}\dfrac{\partial L}{\partial d_{22}} \\
a_{12}\dfrac{\partial L}{\partial d_{11}} + a_{22}\dfrac{\partial L}{\partial d_{21}} &amp; a_{12}\dfrac{\partial L}{\partial d_{12}} + a_{22}\dfrac{\partial L}{\partial d_{22}}
\end{bmatrix} = 
\begin{bmatrix}
a_{11} &amp; a_{21} \\
a_{12} &amp; a_{22}
\end{bmatrix} 
\begin{bmatrix}
\dfrac{\partial L}{\partial d_{11}} &amp; \dfrac{\partial L}{\partial d_{12}} \\
\dfrac{\partial L}{\partial d_{21}} &amp; \dfrac{\partial L}{\partial d_{22}}
\end{bmatrix} = 
a^T \cdot \dfrac{\partial L}{\partial d}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c} =
\begin{bmatrix}
\dfrac{\partial L}{\partial c_{1}} &amp; \dfrac{\partial L}{\partial c_{2}}
\end{bmatrix} =
\begin{bmatrix}
\dfrac{\partial L}{\partial c_{1}} &amp; \dfrac{\partial L}{\partial c_{2}} \\
\end{bmatrix} =
\begin{bmatrix}
1\dfrac{\partial L}{\partial d_{11}} + 1\dfrac{\partial L}{\partial d_{21}} &amp; 1\dfrac{\partial L}{\partial d_{12}} + 1\dfrac{\partial L}{\partial d_{22}}
\end{bmatrix} =
(\dfrac{\partial L}{\partial d}).sum(0)\)</span></p></li>
</ul>
<p>After calculating these derivatives, long story short, the backward pass of a matrix multiply is a matrix multiply. Therefore, just like we had <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span> <span class="pre">+</span> <span class="pre">c</span></code> (scalar case), so does <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">&#64;</span> <span class="pre">b</span> <span class="pre">+</span> <span class="pre">c</span></code> (matrix case) lead us to something very very similar but now with a matrix multiplication instead of a scalar multiplication. In both cases: <code class="docutils literal notranslate"><span class="pre">da</span> <span class="pre">=</span> <span class="pre">dd</span> <span class="pre">&#64;</span> <span class="pre">b.T</span></code> and <code class="docutils literal notranslate"><span class="pre">db</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">&#64;</span> <span class="pre">dd</span></code> we have matrix multiplication with both <code class="docutils literal notranslate"><span class="pre">dd</span></code> and <code class="docutils literal notranslate"><span class="pre">da</span></code> or <code class="docutils literal notranslate"><span class="pre">db</span></code> terms involved, whereas <code class="docutils literal notranslate"><span class="pre">dc</span></code> is simply a sum: <code class="docutils literal notranslate"><span class="pre">dc</span> <span class="pre">=</span> <span class="pre">dd.sum(0)</span></code>. Now hereâ€™s a dirty little secret: you donâ€™t need to remember the formulas we just derived for <strong>backprop</strong>agating through matrix multiplication. You can <strong>backprop</strong> through these expressions just fine. And the reason this works is because the dimensions have to work out. Letâ€™s see an example. Consider <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">w2</span> <span class="pre">+</span> <span class="pre">b2</span></code>. What is <code class="docutils literal notranslate"><span class="pre">dh</span></code>? The shape of <code class="docutils literal notranslate"><span class="pre">dh</span></code> must be the same as the shape of h:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 64])
</pre></div>
</div>
</div>
</div>
<p>And then the other piece of information we now know is that <code class="docutils literal notranslate"><span class="pre">dh</span></code> must be some kind of matrix multiplication of <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> with <code class="docutils literal notranslate"><span class="pre">w2</span></code>. Letâ€™s see the shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 27]),
 torch.Size([32, 64]),
 torch.Size([64, 27]),
 torch.Size([27]))
</pre></div>
</div>
</div>
</div>
<p>So since <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> is <span class="math notranslate nohighlight">\(32\times27\)</span>, <code class="docutils literal notranslate"><span class="pre">w2</span></code> is <span class="math notranslate nohighlight">\(64\times27\)</span> and <code class="docutils literal notranslate"><span class="pre">h</span></code> is <span class="math notranslate nohighlight">\(32\times64\)</span> there is only a single way to make the shape work out in this case. Namely, the only way to achieve <code class="docutils literal notranslate"><span class="pre">dh</span></code> with size <span class="math notranslate nohighlight">\(32\times64\)</span> (the size of <code class="docutils literal notranslate"><span class="pre">h</span></code>) is to matrix multiply <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> with the transpose of <code class="docutils literal notranslate"><span class="pre">w2</span></code> (in order to make the dimensions work out):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dh</span> <span class="o">=</span> <span class="n">dlogits</span> <span class="o">@</span> <span class="n">w2</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>And it is the only way to make these to matrix multiply those two pieces to make the shapes work out. And that turns out to be the correct formula, since, by analogy, as we saw in our simple example: <code class="docutils literal notranslate"><span class="pre">da</span> <span class="pre">=</span> <span class="pre">dd</span> <span class="pre">&#64;</span> <span class="pre">b.T</span></code>. So thereâ€™s no real need to remember these formulas. Similarly, knowing that each parameter derivative must end up with the target shape of the parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dw2</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlogits</span>  <span class="c1"># 64x32 @ 32x27 -&gt; 64x27</span>
<span class="n">db2</span> <span class="o">=</span> <span class="n">dlogits</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 32x27 -&gt; 27</span>
</pre></div>
</div>
</div>
</div>
<p>So thatâ€™s the intuitive, hacky way of findings matrix parameter derivatives. Letâ€™s check if we got <code class="docutils literal notranslate"><span class="pre">dh</span></code>, <code class="docutils literal notranslate"><span class="pre">dw2</span></code> and <code class="docutils literal notranslate"><span class="pre">db2</span></code> correct:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">dh</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;w2&quot;</span><span class="p">,</span> <span class="n">dw2</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;b2&quot;</span><span class="p">,</span> <span class="n">db2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>h               | exact: True  | approximate: True  | maxdiff: 0.0
w2              | exact: True  | approximate: True  | maxdiff: 0.0
b2              | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Exactly! Nice. We have <strong>backprop</strong>ed through a linear layer. Woohoo! Next up for <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">=</span> <span class="pre">torch.tanh(hpreact)</span></code>, we already have <code class="docutils literal notranslate"><span class="pre">dh</span></code> and we now need to backpropagate through <code class="docutils literal notranslate"><span class="pre">tanh</span></code> into <code class="docutils literal notranslate"><span class="pre">hpreact</span></code>, which means we are looking for <code class="docutils literal notranslate"><span class="pre">dhpreact</span></code>. If you remember, we have already done something similar in <a class="reference internal" href="#1.-micrograd"><span class="xref myst"><strong>micrograd</strong></span></a> and we remember that <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Derivatives"><span class="math notranslate nohighlight">\(tanh\)</span> has a very simple derivative</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dhpreact</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">h</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dh</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hpreact&quot;</span><span class="p">,</span> <span class="n">dhpreact</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10
</pre></div>
</div>
</div>
</div>
<p>Here, we notice that there only an approximate (and <strong>not</strong> an exact) equality between the the PyTorch gradient values and our own. This can be attributed to tiny floating point imprecision or perhaps a bug that has to do with specific library versions. Since the <code class="docutils literal notranslate"><span class="pre">maxdiff</span></code> is negligible, we can ignore the slight difference and safely assume that they are indeed exactly equal and make the assignment (in order to avoid any potential accumulation of slight differences and further exact inequalities):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dhpreact</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">grad</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hpreact&quot;</span><span class="p">,</span> <span class="n">dhpreact</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hpreact         | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Okay, next up we have <code class="docutils literal notranslate"><span class="pre">dhpreact</span></code> and now we want to <strong>backprop</strong> through the <code class="docutils literal notranslate"><span class="pre">bngain</span></code>, the <code class="docutils literal notranslate"><span class="pre">bnraw</span></code> and the <code class="docutils literal notranslate"><span class="pre">bnbias</span></code> (<code class="docutils literal notranslate"><span class="pre">hpreact</span> <span class="pre">=</span> <span class="pre">bngain</span> <span class="pre">*</span> <span class="pre">bnraw</span> <span class="pre">+</span> <span class="pre">bnbias</span></code>). So here these are the <strong>batchnorm</strong> parameters: <code class="docutils literal notranslate"><span class="pre">bngain</span></code> and <code class="docutils literal notranslate"><span class="pre">bnbias</span></code>  that take the <code class="docutils literal notranslate"><span class="pre">bnraw</span></code> that is exact unit Gaussian and they scale it and shift it. Here, we have a multiplication (<code class="docutils literal notranslate"><span class="pre">*</span></code>), but itâ€™s worth noting that this multiply is very very different from the matrix multiply <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> (e.g. in the <code class="docutils literal notranslate"><span class="pre">logits</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">&#64;</span> <span class="pre">w2</span> <span class="pre">+</span> <span class="pre">b2</span></code> we saw previously). matrix multiply means dot products between rows and columns of the involved matrices (e.g. <code class="docutils literal notranslate"><span class="pre">h</span></code> and <code class="docutils literal notranslate"><span class="pre">w2</span></code>). Whereas, <code class="docutils literal notranslate"><span class="pre">*</span></code> is an element wise multiply so things are quite a bit simpler. Yet, still, we do have to be careful with some of the broadcasting happening in this line of code though: <code class="docutils literal notranslate"><span class="pre">hpreact</span> <span class="pre">=</span> <span class="pre">bngain</span> <span class="pre">*</span> <span class="pre">bnraw</span> <span class="pre">+</span> <span class="pre">bnbias</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hpreact</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bngain</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bnraw</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bnbias</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 64]),
 torch.Size([1, 64]),
 torch.Size([32, 64]),
 torch.Size([1, 64]))
</pre></div>
</div>
</div>
</div>
<p>You see how <code class="docutils literal notranslate"><span class="pre">bngain</span></code> and <code class="docutils literal notranslate"><span class="pre">bnbias</span></code> are <span class="math notranslate nohighlight">\(1\times64\)</span> but <code class="docutils literal notranslate"><span class="pre">hpreact</span></code> and <code class="docutils literal notranslate"><span class="pre">bnraw</span></code> are <span class="math notranslate nohighlight">\(32\times64\)</span>. We have to be careful with that and make sure that all the shapes work out fine and that the broadcasting is correctly <strong>backprop</strong>agated. Letâ€™s start with calculating <code class="docutils literal notranslate"><span class="pre">dbngain</span></code>. Using the chain rule:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dbngain</span> <span class="o">=</span> <span class="n">bnraw</span> <span class="o">*</span> <span class="n">dhpreact</span>
</pre></div>
</div>
</div>
</div>
<p>But this is incorrect. We have to be careful, since <code class="docutils literal notranslate"><span class="pre">bngain</span></code> is of size <span class="math notranslate nohighlight">\(1\times64\)</span>, which is the size <code class="docutils literal notranslate"><span class="pre">dbngain</span></code> has to have. However, <code class="docutils literal notranslate"><span class="pre">bnraw</span> <span class="pre">*</span> <span class="pre">dhpreact</span></code> has a size of <span class="math notranslate nohighlight">\(32x64\)</span>. So the correct thing to do in this case of course is to sum across the examples/rows dimension while being careful to keep the number of dimensions the same:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dbngain</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnraw</span> <span class="o">*</span> <span class="n">dhpreact</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And now the dimensions are correct! Similarly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dbnraw</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="n">dhpreact</span>  <span class="c1"># replication occurs automatically, no summing required</span>
<span class="n">dbnbias</span> <span class="o">=</span> <span class="n">dhpreact</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># sum to reduce the batch size dimension</span>
</pre></div>
</div>
</div>
</div>
<p>Letâ€™s check for correctness:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bngain&quot;</span><span class="p">,</span> <span class="n">dbngain</span><span class="p">,</span> <span class="n">bngain</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnraw&quot;</span><span class="p">,</span> <span class="n">dbnraw</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnbias&quot;</span><span class="p">,</span> <span class="n">dbnbias</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bngain          | exact: True  | approximate: True  | maxdiff: 0.0
bnraw           | exact: True  | approximate: True  | maxdiff: 0.0
bnbias          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Awesome! Now we get to the <strong>batchnorm</strong> layer. We see how here <code class="docutils literal notranslate"><span class="pre">bngain</span></code> and <code class="docutils literal notranslate"><span class="pre">bnbias</span></code> are parameters, so the backpropagation ends. But <code class="docutils literal notranslate"><span class="pre">bnraw</span> <span class="pre">=</span> <span class="pre">bndiff</span> <span class="pre">*</span> <span class="pre">bnvar_inv</span></code> is the output of the standardization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bnmeani</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">hprebn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">bndiff</span> <span class="o">=</span> <span class="n">hprebn</span> <span class="o">-</span> <span class="n">bnmeani</span>
<span class="n">bndiff2</span> <span class="o">=</span> <span class="n">bndiff</span><span class="o">**</span><span class="mi">2</span>
<span class="n">bnvar</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">bndiff2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># note: Bessel&#39;s correction (dividing by n-1, not n)</span>
<span class="n">bnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span><span class="o">**-</span><span class="mf">0.5</span>
<span class="n">bnraw</span> <span class="o">=</span> <span class="n">bndiff</span> <span class="o">*</span> <span class="n">bnvar_inv</span>
</pre></div>
</div>
<p>with an equivalence to the <a class="reference external" href="https://arxiv.org/abs/1502.03167"><strong>batchnorm</strong> paper</a> assignments:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;batchnorm_algorithm1.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7b9c6c8e713aca7856ccf8da56df923843d51901d12e0638eb6d84c659820634.png" src="../_images/7b9c6c8e713aca7856ccf8da56df923843d51901d12e0638eb6d84c659820634.png" />
</div>
</div>
<p>So, next up, on the line <code class="docutils literal notranslate"><span class="pre">bnraw</span> <span class="pre">=</span> <span class="pre">bndiff</span> <span class="pre">*</span> <span class="pre">bnvar_inv</span></code> we have to backpropagate into <code class="docutils literal notranslate"><span class="pre">bndiff</span></code> and <code class="docutils literal notranslate"><span class="pre">bnvar_inv</span></code>, having already calculated <code class="docutils literal notranslate"><span class="pre">dbnraw</span></code>. Letâ€™s see the shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnraw</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bndiff</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))
</pre></div>
</div>
</div>
</div>
<p>We can infer that there is broadcasting of <code class="docutils literal notranslate"><span class="pre">bnvar_inv</span></code> happening here that we have to be careful with. Other than that, this is just an element-wise multiplication. By now we should be pretty comfortable with that. Therefore:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dbndiff</span> <span class="o">=</span> <span class="n">bnvar_inv</span> <span class="o">*</span> <span class="n">dbnraw</span>
<span class="c1"># we sum since dbnvar_inv must have the same shape as `bnvar_inv`</span>
<span class="n">dbnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bndiff</span> <span class="o">*</span> <span class="n">dbnraw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Are these correct? Letâ€™s see.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnvar_inv&quot;</span><span class="p">,</span> <span class="n">dbnvar_inv</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bndiff&quot;</span><span class="p">,</span> <span class="n">dbndiff</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0
bndiff          | exact: False | approximate: False | maxdiff: 0.0011080320691689849
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">dbnvar_inv</span></code> is correct. But, oh no! <code class="docutils literal notranslate"><span class="pre">dbndiff</span></code> isnâ€™tâ€¦ Well, this is actually expected, because we are not yet done with <code class="docutils literal notranslate"><span class="pre">bndiff</span></code>, since it not only contributes to <code class="docutils literal notranslate"><span class="pre">bnraw</span></code> but also indirectly to <code class="docutils literal notranslate"><span class="pre">bnvar_inv</span></code> (through <code class="docutils literal notranslate"><span class="pre">bndiff2</span></code> and so on). So basically, <code class="docutils literal notranslate"><span class="pre">bndiff</span></code> branches out into two branches of which we have only <strong>backprop</strong>agated through one of them. So we have to continue our <strong>backprop</strong> until we get to the second branch of <code class="docutils literal notranslate"><span class="pre">bndiff</span></code>, where we can calculate and add the second partial derivative component to <code class="docutils literal notranslate"><span class="pre">dbndiff</span></code> using a <code class="docutils literal notranslate"><span class="pre">+=</span></code> (like we have done before for previous variables). Letâ€™s do so! Next up is: <code class="docutils literal notranslate"><span class="pre">bnvar_inv</span> <span class="pre">=</span> <span class="pre">(bnvar</span> <span class="pre">+</span> <span class="pre">1e-5)**-0.5</span></code>. <a class="reference external" href="https://www.wolframalpha.com/input?i=dx%5En%2Fdx">This is just</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dbnvar</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnvar_inv</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnvar&quot;</span><span class="p">,</span> <span class="n">dbnvar</span><span class="p">,</span> <span class="n">bnvar</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bnvar           | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>which is the correct result! Now before we move on here, letâ€™s talk a bit about Besselâ€™s correction from: <code class="docutils literal notranslate"><span class="pre">bnvar</span> <span class="pre">=</span> <span class="pre">1/(batch_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">*</span> <span class="pre">(bndiff2).sum(0,</span> <span class="pre">keepdim=True)</span> <span class="pre">#</span> <span class="pre">note:</span> <span class="pre">Bessel's</span> <span class="pre">correction</span> <span class="pre">(dividing</span> <span class="pre">by</span> <span class="pre">n-1,</span> <span class="pre">not</span> <span class="pre">n)</span></code>. Youâ€™ll notice that dividing by <code class="docutils literal notranslate"><span class="pre">n-1</span></code> instead of <code class="docutils literal notranslate"><span class="pre">n</span></code> is a departure from the paper (where <code class="docutils literal notranslate"><span class="pre">m</span></code> == <code class="docutils literal notranslate"><span class="pre">n</span></code> == <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>). So it turns out that there are two ways of estimating variance of an array. One is the biased estimate, which is <code class="docutils literal notranslate"><span class="pre">1/n</span></code> and the other one is the unbiased estimate which is <code class="docutils literal notranslate"><span class="pre">1/n-1</span></code>. Now confusingly in the paper itâ€™s not very clearly described and also itâ€™s a detail that kind of matters. They essentially use the biased version at training time but later when they are talking about the inference, they are mentioning that when they do the inference they are using the unbiased estimate (which is <code class="docutils literal notranslate"><span class="pre">1/n-1</span></code> version) to calibrate the running mean and the running variance basically. And so they actually introduce a train test mismatch where in training they use the biased version and in test time they use the unbiased version. This of course is very confusing. You can read more about the Besselâ€™s correction: <a class="reference external" href="https://mathcenter.oxford.emory.edu/site/math117/besselCorrection/">https://mathcenter.oxford.emory.edu/site/math117/besselCorrection/</a> and why dividing by <code class="docutils literal notranslate"><span class="pre">n-1</span></code> gives you a better estimate of the variance in the case where you have population sizes or samples from a population that are very small. And that is indeed the case for us because we are dealing with mini-matches and these mini-matches are a small sample of a larger population which is the entire training set. And it turns out that if you just estimate it using <code class="docutils literal notranslate"><span class="pre">1/n</span></code> that actually almost always underestimates the variance and it is a biased estimator and it is advised that you use the unbiased version and divide by <code class="docutils literal notranslate"><span class="pre">n-1</span></code>. The documentation of <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.var.html"><code class="docutils literal notranslate"><span class="pre">torch.var</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BatchNorm1d</span></code></a> are less confusing. So, long story short, since PyTorch uses Besselâ€™s correction, we will too. Ok, so letâ€™s now <strong>backprop</strong> through the next line: <code class="docutils literal notranslate"><span class="pre">bnvar</span> <span class="pre">=</span> <span class="pre">1/(batch_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">*</span> <span class="pre">(bndiff2).sum(0,</span> <span class="pre">keepdim=True)</span></code>. As you might have realized until now, it is good practice to scrutinize the shapes first.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnvar</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bndiff2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([1, 64]), torch.Size([32, 64]))
</pre></div>
</div>
</div>
</div>
<p>Therefore the sum operation in this line is squashing the first dimension <span class="math notranslate nohighlight">\(32\)</span> into <span class="math notranslate nohighlight">\(1\)</span>. This hints to us that will be some kind of replication or broadcasting in the backward pass. And maybe youâ€™re noticing a pattern here: everytime you have a <code class="docutils literal notranslate"><span class="pre">sum</span></code> in the forward pass, that turns into a replication or broadcasting in the backward pass along the same dimension. And conversely, when we have a replication or broadcasting in the forward pass, that indicates a variable reuse and so in the backward pass that turns into a <code class="docutils literal notranslate"><span class="pre">sum</span></code> over the exact same dimension. And so we are noticing a duality, where these two operations are kind of like the opposites of each other in the forward and the backward pass. Now, usually once we have understood the shapes, the next thing to look at is a toy example in order to roughly understand how the variable dependencies go in the mathematical formula. In the line we are interested in we have an array (<code class="docutils literal notranslate"><span class="pre">bndiff2</span></code>) on which we are summing vertically over the columns (<code class="docutils literal notranslate"><span class="pre">(bndiff2).sum(0,</span> <span class="pre">keepdim=True)</span></code>) and that we are scaling (by <code class="docutils literal notranslate"><span class="pre">1/(batch_size</span> <span class="pre">-</span> <span class="pre">1)</span></code>). So if we have a <span class="math notranslate nohighlight">\(2\times2\)</span> matrix <span class="math notranslate nohighlight">\(a\)</span> that we then sum over the columns and then scale, we would get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a11 a12</span>
<span class="c1"># a21 a22</span>
<span class="c1"># -&gt; </span>
<span class="c1"># b1 b2, where:</span>
<span class="c1"># b1 = 1/(n-1)*(a11 + a21)</span>
<span class="c1"># b2 = 1/(n-1)*(a12 + a22)</span>
</pre></div>
</div>
</div>
</div>
<p>Looking at this simple example, what we want basically is we want to <strong>backpro</strong> the derivative of <code class="docutils literal notranslate"><span class="pre">b</span></code>, <code class="docutils literal notranslate"><span class="pre">db1</span></code> and <code class="docutils literal notranslate"><span class="pre">db2</span></code> into all the elements of <code class="docutils literal notranslate"><span class="pre">a</span></code>. And so itâ€™s clear that, by just differentiating in your head, the local derivatives of <code class="docutils literal notranslate"><span class="pre">b</span></code> are simply <code class="docutils literal notranslate"><span class="pre">(1/(n-1))*1</span></code> for each one of these elements of <code class="docutils literal notranslate"><span class="pre">a</span></code>. Basically, each local derivative will flow through each column of <code class="docutils literal notranslate"><span class="pre">a</span></code> and be scaled by <code class="docutils literal notranslate"><span class="pre">1/(n-1)</span></code>. Therefore, intuitively:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># basically a large array of ones the size of bndiff2 times dbnvar (chain rule), scaled</span>
<span class="n">dbndiff2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bndiff2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnvar</span>
</pre></div>
</div>
</div>
</div>
<p>Notice here how we are multiplying a scaled ones array of shape <span class="math notranslate nohighlight">\(32x64\)</span> with <code class="docutils literal notranslate"><span class="pre">dbnvar</span></code>, an array of <span class="math notranslate nohighlight">\(1\times64\)</span>. Basically we are just letting PyTorch do the replication, so that we end up with a <code class="docutils literal notranslate"><span class="pre">dbndiff2</span></code> array of <span class="math notranslate nohighlight">\(32\times64\)</span> (same size as <code class="docutils literal notranslate"><span class="pre">bndiff2</span></code>). And indeed we see that this derivative is correct:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bndiff2&quot;</span><span class="p">,</span> <span class="n">dbndiff2</span><span class="p">,</span> <span class="n">bndiff2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Next up, letâ€™s differentiate here: <code class="docutils literal notranslate"><span class="pre">bndiff2</span> <span class="pre">=</span> <span class="pre">bndiff**2</span></code> into <code class="docutils literal notranslate"><span class="pre">bndiff</span></code>. This is a simple one, but donâ€™t forget the addition assignment, as this will now be adding the second partial derivative component to the first one we already calculated before. So this completes the backpropagation through the second branch of <code class="docutils literal notranslate"><span class="pre">bndiff</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dbndiff</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bndiff</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbndiff2</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bndiff&quot;</span><span class="p">,</span> <span class="n">dbndiff</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bndiff          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>As you can see now, the derivative of <code class="docutils literal notranslate"><span class="pre">bndiff</span></code> is now exact and correct. Thatâ€™s comforting! Next up: <code class="docutils literal notranslate"><span class="pre">bndiff</span> <span class="pre">=</span> <span class="pre">hprebn</span> <span class="pre">-</span> <span class="pre">bnmeani</span></code>. Letâ€™s check the shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bndiff</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">hprebn</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bnmeani</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))
</pre></div>
</div>
</div>
</div>
<p>Therefore, the minus sign here (<span class="math notranslate nohighlight">\(-\)</span>) is actually doing broadcasting. Letâ€™s be careful about that as this hints us to the duality we previously mentioned. A broadcasting in the forward pass means variable reuse and therefore translates to a sum operation in the backward pass. Letâ€™s find the derivatives:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dhprebn</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">dbndiff</span>
<span class="n">dbnmeani</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">dbndiff</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hprebn&quot;</span><span class="p">,</span> <span class="n">dhprebn</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnmeani&quot;</span><span class="p">,</span> <span class="n">dbnmeani</span><span class="p">,</span> <span class="n">bnmeani</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hprebn          | exact: False | approximate: False | maxdiff: 0.001130998833104968
bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>And <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code> is wrongâ€¦ Damn! Haha, well, donâ€™t sweat it. This is supposed to be wrong! Can you guess why? Like happened for <code class="docutils literal notranslate"><span class="pre">bndiff</span></code>, this is also not the only branch we have to backpropagate to for calculating <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code>, since <code class="docutils literal notranslate"><span class="pre">bnmeani</span></code> also depends on <code class="docutils literal notranslate"><span class="pre">hprebn</span></code>. Therefore, there will be a second derivative component coming from this second branch. So we are not done yet. This we will find now since the next line is: <code class="docutils literal notranslate"><span class="pre">bnmeani</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">batch_size</span> <span class="pre">*</span> <span class="pre">hprebn.sum(0,</span> <span class="pre">keepdim=True)</span></code>. Now here again we have to be careful since there is a sum along dimension <span class="math notranslate nohighlight">\(0\)</span> so this will turn into broadcasting in the backward pass. So, similarly to the <code class="docutils literal notranslate"><span class="pre">bnvar</span></code> line and the <code class="docutils literal notranslate"><span class="pre">dbndiff2</span></code> calculation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dhprebn</span> <span class="o">+=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">hprebn</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnmeani</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hprebn&quot;</span><span class="p">,</span> <span class="n">dhprebn</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hprebn          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Awesome! So that completes the <strong>backprop</strong> of the <strong>batchnorm</strong> layer. Next up we will <strong>backprop</strong>agate through the linear layer: <code class="docutils literal notranslate"><span class="pre">hprebn</span> <span class="pre">=</span> <span class="pre">embcat</span> <span class="pre">&#64;</span> <span class="pre">w1</span> <span class="pre">+</span> <span class="pre">b1</span></code>. Like we have already mentioned, <strong>backprop</strong>agating through linear layers is pretty easy. So letâ€™s inspect the shapes and begin.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hprebn</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embcat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 64]),
 torch.Size([32, 30]),
 torch.Size([30, 64]),
 torch.Size([64]))
</pre></div>
</div>
</div>
</div>
<p>Now we find <code class="docutils literal notranslate"><span class="pre">dembcat</span></code> to be <span class="math notranslate nohighlight">\(32x30\)</span>. We need to take <code class="docutils literal notranslate"><span class="pre">w1</span></code> of size <span class="math notranslate nohighlight">\(30x64\)</span> (the tensor that is matrix-multiplied by <code class="docutils literal notranslate"><span class="pre">embcat</span></code>) and matrix-multiply it with <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code> (think chain rule). Therefore, we need to do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dembcat</span> <span class="o">=</span> <span class="n">dhprebn</span> <span class="o">@</span> <span class="n">w1</span><span class="o">.</span><span class="n">T</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;embcat&quot;</span><span class="p">,</span> <span class="n">dembcat</span><span class="p">,</span> <span class="n">embcat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>embcat          | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>And similarly for the remaining variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dw1</span> <span class="o">=</span> <span class="n">embcat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dhprebn</span>
<span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">*</span> <span class="n">dhprebn</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">,</span> <span class="n">dw1</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;b1&quot;</span><span class="p">,</span> <span class="n">db1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w1              | exact: True  | approximate: True  | maxdiff: 0.0
b1              | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Cool, everything is correct! Moving onto <code class="docutils literal notranslate"><span class="pre">embcat</span> <span class="pre">=</span> <span class="pre">emb.view(emb.shape[0],</span> <span class="pre">-1)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embcat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 30]), torch.Size([32, 3, 10]))
</pre></div>
</div>
</div>
</div>
<p>As you can see, the view operation squashes the last two dimensions of the <code class="docutils literal notranslate"><span class="pre">emb</span></code> tensor (<code class="docutils literal notranslate"><span class="pre">3</span></code>, <code class="docutils literal notranslate"><span class="pre">10</span></code>) into one dimension (<code class="docutils literal notranslate"><span class="pre">30</span></code>). So here we are dealing with a concatenation of dimensions in the forward pass, therefore we want to undo that in the backward pass. We can do this again using the view operation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">demb</span> <span class="o">=</span> <span class="n">dembcat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;emb&quot;</span><span class="p">,</span> <span class="n">demb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>emb             | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Simple, right? Now the only operation that is left to <strong>backprop</strong> into is the initial indexing operation: <code class="docutils literal notranslate"><span class="pre">emb</span> <span class="pre">=</span> <span class="pre">C[xb]</span></code>. Letâ€™s look at the shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 3, 10]), torch.Size([27, 10]), torch.Size([32, 3]))
</pre></div>
</div>
</div>
</div>
<p>So what is happening in this indexing operation? <code class="docutils literal notranslate"><span class="pre">xb</span></code> (<span class="math notranslate nohighlight">\(32\)</span> examples, <span class="math notranslate nohighlight">\(3\)</span> character indeces per example) is being used to index the lookup table <code class="docutils literal notranslate"><span class="pre">C</span></code> (<span class="math notranslate nohighlight">\(27\)</span> possible character indeces, <span class="math notranslate nohighlight">\(10\)</span> embedding dimensions per character index) to yield <code class="docutils literal notranslate"><span class="pre">emb</span></code> (<span class="math notranslate nohighlight">\(32\)</span> examples, <span class="math notranslate nohighlight">\(3\)</span> character indeces per example, <span class="math notranslate nohighlight">\(10\)</span> embedding dimensions per character index). In other words, by indexing <code class="docutils literal notranslate"><span class="pre">C</span></code> using <code class="docutils literal notranslate"><span class="pre">xb</span></code> we are using each integer in <code class="docutils literal notranslate"><span class="pre">xb</span></code> to specify which row of <code class="docutils literal notranslate"><span class="pre">C</span></code> we want to pluck out. This indexing operation yields an embeddings tensor <code class="docutils literal notranslate"><span class="pre">emb</span></code> of <span class="math notranslate nohighlight">\(32\)</span> examples (same as the indexing tensor <code class="docutils literal notranslate"><span class="pre">xb</span></code>), <span class="math notranslate nohighlight">\(3\)</span> plucked-out character rows per example (from <code class="docutils literal notranslate"><span class="pre">C</span></code>), each of which contains <span class="math notranslate nohighlight">\(10\)</span> embedding dimesions. So now, for each one of these plucked-out rows we have their gradients <code class="docutils literal notranslate"><span class="pre">demb</span></code> (arranged in a <span class="math notranslate nohighlight">\(32\times3\times10\)</span> tensor). All we have to do now is to route this gradients backwards through this indexing operation. So first we need to find the specific rows of <code class="docutils literal notranslate"><span class="pre">C</span></code> that every one of these <span class="math notranslate nohighlight">\(10\)</span>-dimensional embeddings comes from. And then, into the corresponding <code class="docutils literal notranslate"><span class="pre">dC</span></code> row indices we need to deposit the <code class="docutils literal notranslate"><span class="pre">demb</span></code> gradients. Basically we need to undo the indexing. And of course, if any of these rows of <code class="docutils literal notranslate"><span class="pre">C</span></code> was used multiple times, then we have to remember that the gradients that arrive there have to add (accumulate through an addition operation). The PyTorch operation that will help us do this is <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html"><code class="docutils literal notranslate"><span class="pre">index_add_</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define `dC` tensor (same size as `C`)</span>
<span class="n">dC</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="c1"># Plucked-out `C` row indices tensor</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># shape: 32 * 3 = 96, range of index values: 0-26</span>
<span class="c1"># `demb` derivative values tensor</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">demb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># shape: (32 * 3)x10 = 96x10</span>
<span class="c1"># Add `demb` values to specific indices</span>
<span class="n">dC</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">dC</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C               | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
<p>Make sure to take some time to understand what we just did. Hereâ€™s a bare-minimum example (produced by <a class="reference external" href="https://chat.openai.com/"><code class="docutils literal notranslate"><span class="pre">ChatGPT</span></code></a>) of what the <code class="docutils literal notranslate"><span class="pre">index_add_</span></code> operation does exactly, in case youâ€™re confused:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># tensor to add values into</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Base tensor before index_add_ operation:&quot;</span><span class="p">,</span> <span class="n">base_tensor</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># indices where we want to add values</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>  <span class="c1"># values to add at those indices</span>
<span class="n">base_tensor</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>  <span class="c1"># add values at specified indices</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Base tensor after index_add_ operation:&quot;</span><span class="p">,</span> <span class="n">base_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Base tensor before index_add_ operation: tensor([0., 0., 0., 0., 0.])
Base tensor after index_add_ operation: tensor([1., 2., 0., 3., 0.])
</pre></div>
</div>
</div>
</div>
<p>What remains now isâ€¦ Well, nothing. Yey :D! We have backpropagated through this entire beast. This was our first exercise. Letâ€™s clearly write out all the gradients we manually calculated, along with their correctness calls:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 1: backprop through the whole thing manually,</span>
<span class="c1"># backpropagating through exactly all of the variables</span>
<span class="c1"># as they are defined in the forward pass above, one by one</span>

<span class="n">dlogprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
<span class="n">dlogprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">batch_size</span>
<span class="n">dprobs</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">dlogprobs</span>
<span class="n">dcounts_sum_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span> <span class="o">*</span> <span class="n">dprobs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dcounts</span> <span class="o">=</span> <span class="n">counts_sum_inv</span> <span class="o">*</span> <span class="n">dprobs</span>
<span class="n">dcounts_sum</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">counts_sum</span><span class="o">**-</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">dcounts_sum_inv</span>
<span class="n">dcounts</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="o">*</span> <span class="n">dcounts_sum</span>
<span class="n">dnorm_logits</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">dcounts</span>
<span class="n">dlogits</span> <span class="o">=</span> <span class="n">dnorm_logits</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">dlogit_maxes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">dnorm_logits</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dlogits</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">dlogit_maxes</span>
<span class="n">dh</span> <span class="o">=</span> <span class="n">dlogits</span> <span class="o">@</span> <span class="n">w2</span><span class="o">.</span><span class="n">T</span>
<span class="n">dw2</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlogits</span>
<span class="n">db2</span> <span class="o">=</span> <span class="n">dlogits</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># FIXME: PyTorch version-specific precision issue</span>
<span class="n">dhpreact</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># correct: dhpreact = (1.0 - h**2) * dh</span>
<span class="n">dbngain</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnraw</span> <span class="o">*</span> <span class="n">dhpreact</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dbnraw</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="n">dhpreact</span>
<span class="n">dbnbias</span> <span class="o">=</span> <span class="n">dhpreact</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dbndiff</span> <span class="o">=</span> <span class="n">bnvar_inv</span> <span class="o">*</span> <span class="n">dbnraw</span>
<span class="n">dbnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bndiff</span> <span class="o">*</span> <span class="n">dbnraw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dbnvar</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnvar_inv</span>
<span class="n">dbndiff2</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">bndiff2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnvar</span>
<span class="n">dbndiff</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bndiff</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbndiff2</span>
<span class="n">dhprebn</span> <span class="o">=</span> <span class="n">dbndiff</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">dbnmeani</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">dbndiff</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dhprebn</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">hprebn</span><span class="p">)</span> <span class="o">*</span> <span class="n">dbnmeani</span><span class="p">)</span>
<span class="n">dembcat</span> <span class="o">=</span> <span class="n">dhprebn</span> <span class="o">@</span> <span class="n">w1</span><span class="o">.</span><span class="n">T</span>
<span class="n">dw1</span> <span class="o">=</span> <span class="n">embcat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dhprebn</span>
<span class="n">db1</span> <span class="o">=</span> <span class="n">dhprebn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">demb</span> <span class="o">=</span> <span class="n">dembcat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">dC</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">dC</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">demb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logprobs&quot;</span><span class="p">,</span> <span class="n">dlogprobs</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;probs&quot;</span><span class="p">,</span> <span class="n">dprobs</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;counts_sum_inv&quot;</span><span class="p">,</span> <span class="n">dcounts_sum_inv</span><span class="p">,</span> <span class="n">counts_sum_inv</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;counts_sum&quot;</span><span class="p">,</span> <span class="n">dcounts_sum</span><span class="p">,</span> <span class="n">counts_sum</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">dcounts</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;norm_logits&quot;</span><span class="p">,</span> <span class="n">dnorm_logits</span><span class="p">,</span> <span class="n">norm_logits</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logit_maxes&quot;</span><span class="p">,</span> <span class="n">dlogit_maxes</span><span class="p">,</span> <span class="n">logit_maxes</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">dlogits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">dh</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;w2&quot;</span><span class="p">,</span> <span class="n">dw2</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;b2&quot;</span><span class="p">,</span> <span class="n">db2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hpreact&quot;</span><span class="p">,</span> <span class="n">dhpreact</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bngain&quot;</span><span class="p">,</span> <span class="n">dbngain</span><span class="p">,</span> <span class="n">bngain</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnbias&quot;</span><span class="p">,</span> <span class="n">dbnbias</span><span class="p">,</span> <span class="n">bnbias</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnraw&quot;</span><span class="p">,</span> <span class="n">dbnraw</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnvar_inv&quot;</span><span class="p">,</span> <span class="n">dbnvar_inv</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnvar&quot;</span><span class="p">,</span> <span class="n">dbnvar</span><span class="p">,</span> <span class="n">bnvar</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bndiff2&quot;</span><span class="p">,</span> <span class="n">dbndiff2</span><span class="p">,</span> <span class="n">bndiff2</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bndiff&quot;</span><span class="p">,</span> <span class="n">dbndiff</span><span class="p">,</span> <span class="n">bndiff</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;bnmeani&quot;</span><span class="p">,</span> <span class="n">dbnmeani</span><span class="p">,</span> <span class="n">bnmeani</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hprebn&quot;</span><span class="p">,</span> <span class="n">dhprebn</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;embcat&quot;</span><span class="p">,</span> <span class="n">dembcat</span><span class="p">,</span> <span class="n">embcat</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">,</span> <span class="n">dw1</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;b1&quot;</span><span class="p">,</span> <span class="n">db1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;emb&quot;</span><span class="p">,</span> <span class="n">demb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">dC</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logprobs        | exact: True  | approximate: True  | maxdiff: 0.0
probs           | exact: True  | approximate: True  | maxdiff: 0.0
counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0
counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0
counts          | exact: True  | approximate: True  | maxdiff: 0.0
norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0
logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0
logits          | exact: True  | approximate: True  | maxdiff: 0.0
h               | exact: True  | approximate: True  | maxdiff: 0.0
w2              | exact: True  | approximate: True  | maxdiff: 0.0
b2              | exact: True  | approximate: True  | maxdiff: 0.0
hpreact         | exact: True  | approximate: True  | maxdiff: 0.0
bngain          | exact: True  | approximate: True  | maxdiff: 0.0
bnbias          | exact: True  | approximate: True  | maxdiff: 0.0
bnraw           | exact: True  | approximate: True  | maxdiff: 0.0
bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0
bnvar           | exact: True  | approximate: True  | maxdiff: 0.0
bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0
bndiff          | exact: True  | approximate: True  | maxdiff: 0.0
bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0
hprebn          | exact: True  | approximate: True  | maxdiff: 0.0
embcat          | exact: True  | approximate: True  | maxdiff: 0.0
w1              | exact: True  | approximate: True  | maxdiff: 0.0
b1              | exact: True  | approximate: True  | maxdiff: 0.0
emb             | exact: True  | approximate: True  | maxdiff: 0.0
C               | exact: True  | approximate: True  | maxdiff: 0.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-backprop-through-cross-entropy-but-all-in-one-go">
<h2>Exercise 2: <strong>backprop</strong> through cross_entropy but all in one go<a class="headerlink" href="#exercise-2-backprop-through-cross-entropy-but-all-in-one-go" title="Link to this heading">#</a></h2>
<p>Now we come to exercise 2. It basically turns out that in this first exercise we were doing way too much work. We were <strong>backprop</strong>agating way too much and it was all good practice and so on but itâ€™s not what you would do in practice.
The reason for that is, for example at some point we separated out a <strong>loss</strong> calculation over multiple lines and broke it up all to its smallest atomic pieces and we <strong>backprop</strong>agated through all of those individually. But it turns out that if you just look at the mathematical expression for the <strong>loss</strong> then actually you can do the differentiation on pen and paper and a lot of terms cancel and simplify and the mathematical expression you end up with is significantly shorter and easier to implement than <strong>backprop</strong>agating through all the little pieces of everything youâ€™ve done. So before we had this complicated forward pass going from logits to the <strong>loss</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># cross entropy loss (same as F.cross_entropy(logits, Yb))</span>
<span class="n">logit_maxes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">norm_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logit_maxes</span> <span class="c1"># subtract max for numerical stability</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">norm_logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">counts_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">counts_sum_inv</span> <span class="o">=</span> <span class="n">counts_sum</span><span class="o">**-</span><span class="mi">1</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">counts_sum_inv</span>
<span class="n">logprobs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">logprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>But in PyTorch everything can just be glued together into a single call at that cross entropy. You just pass in logits and the labels and you get the exact same <strong>loss</strong> as we verify here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 2: backprop through cross_entropy but all in one go</span>
<span class="c1"># to complete this challenge look at the mathematical expression of the loss,</span>
<span class="c1"># take the derivative, simplify the expression, and just write it out</span>

<span class="n">loss_fast</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_fast</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s2">&quot;diff:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">loss_fast</span> <span class="o">-</span> <span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.348198175430298 diff: 0.0
</pre></div>
</div>
</div>
</div>
<p>So our previous <strong>loss</strong> and the fast <strong>loss</strong> coming from the chunk of operations as a single mathematical expression is much faster than the whole forward pass we did previously. Itâ€™s also much much faster in the backward pass. And the reason for that is if you just look at the mathematical form of <code class="docutils literal notranslate"><span class="pre">F.cross_entropy(logits,</span> <span class="pre">yb)</span></code> and differentiate again you will end up with a very small and short expression. So thatâ€™s what we want to do here. In a single operation or in a single go or like very quickly, we want to go directly into <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> and we need to implement <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> as a function of <code class="docutils literal notranslate"><span class="pre">logits</span></code> and <code class="docutils literal notranslate"><span class="pre">yb</span></code>. But it will be significantly shorter that going through all these intermediate operations to get to it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dlogprobs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
<span class="n">dlogprobs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">batch_size</span>
<span class="n">dprobs</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">probs</span><span class="p">)</span> <span class="o">*</span> <span class="n">dlogprobs</span>
<span class="n">dcounts_sum_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span> <span class="o">*</span> <span class="n">dprobs</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dcounts</span> <span class="o">=</span> <span class="n">counts_sum_inv</span> <span class="o">*</span> <span class="n">dprobs</span>
<span class="n">dcounts_sum</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">counts_sum</span><span class="o">**-</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dcounts_sum_inv</span>
<span class="n">dcounts</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span> <span class="o">*</span> <span class="n">dcounts_sum</span>
<span class="n">dnorm_logits</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">*</span> <span class="n">dcounts</span>
<span class="n">dlogits</span> <span class="o">=</span> <span class="n">dnorm_logits</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">dlogit_maxes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">dnorm_logits</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dlogits</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">dlogit_maxes</span>
</pre></div>
</div>
<p>Basically all of this above work can be skipped in a much much simpler mathematical expression that we will now implement. Letâ€™s give it a shot. The question we are asking is: what is the mathematical expression of the <strong>loss</strong>? Once, we have the answer we can differentiate <strong>w.r.t</strong> the logits. Hereâ€™s whatâ€™s happening: inputs go through the <strong>nn</strong> which yields logits. If these are passed through a softmax, we get the probabilities:</p>
<p><span class="math notranslate nohighlight">\(x \rightarrow nn \rightarrow logits \rightarrow softmax \rightarrow probs\)</span></p>
<p>Then, we are using the indentity <code class="docutils literal notranslate"><span class="pre">y</span></code> of the correct next character to pluck out a row of probabilities <code class="docutils literal notranslate"><span class="pre">probs[y]</span></code> and then take the negative log to get the negative log probability: <code class="docutils literal notranslate"><span class="pre">-log(probs[y])</span></code>. And then we average up all the negative log probabilities to get our <strong>loss</strong>.</p>
<p><span class="math notranslate nohighlight">\(probs[y] \rightarrow -log(probs[y]) \rightarrow average\ \textbf{loss}\)</span></p>
<p>Basically, given a logit vector <span class="math notranslate nohighlight">\(l\)</span>, a softmax/probability vector <span class="math notranslate nohighlight">\(P\)</span> and a target label <span class="math notranslate nohighlight">\(y\)</span>:</p>
<p><span class="math notranslate nohighlight">\(loss = -\log \mathbf{P}_y = -\log \left(\frac{e^{l_y}}{\sum_j e^{l_j}}\right)\)</span></p>
<p>And we are basically interested in:</p>
<p><span class="math notranslate nohighlight">\(\nabla_{l_i} loss = \dfrac{\partial loss}{\partial l_i} = \dfrac{\partial}{\partial l_i} \left[-\log \left(\dfrac{e^{l_y}}{\sum_j e^{l_j}}\right)\right]\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \dfrac{\partial}{\partial l_i} \left(\dfrac{e^{l_y}}{\sum_j e^{l_j}}\right)\)</span> (using <span class="math notranslate nohighlight">\(\dfrac{d\log x}{dx} = \dfrac{1}{x}\)</span>)</p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \dfrac{\partial}{\partial l_i} \left(e^{l_y} \dfrac{1}{\sum_j e^{l_j}}\right)\)</span></p>
<p><strong>if</strong> <span class="math notranslate nohighlight">\(i \neq y\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \left(\dfrac{\partial e^{l_y}}{\partial l_i} \cdot \dfrac{1}{\sum_j e^{l_j}} +e^{l_y} \cdot \dfrac{\partial}{\partial l_i} (\dfrac{1}{\sum_j e^{l_j}}) \right)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \left(0 \cdot \dfrac{1}{\sum_j e^{l_j}} +e^{l_y} \cdot \dfrac{\partial}{\partial l_i} (\dfrac{1}{\sum_j e^{l_j}}) \right)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \left(-\dfrac{e^{l_y} e^{l_i}}{(\sum_j e^{l_j})^2} \right)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = \dfrac{e^{l_i}}{\sum_j e^{l_j}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = P_i\)</span></p>
<p><strong>if</strong> <span class="math notranslate nohighlight">\(i = y\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \left[\dfrac{\partial e^{l_y}}{\partial l_i} \cdot \dfrac{1}{\sum_j e^{l_j}} +e^{l_y} \cdot \dfrac{\partial}{\partial l_i} (\dfrac{1}{\sum_j e^{l_j}}) \right]\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\sum_j e^{l_j}}{e^{l_y}} \left[\dfrac{e^{l_y}}{\sum_j e^{l_j}} - \dfrac{e^{l_y} e^{l_i}}{(\sum_j e^{l_j})^2} \right]\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = -1 + \dfrac{e^{l_i}}{\sum_j e^{l_j}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ = P_i - 1\)</span></p>
<p>Sweet! By separating the casing of whether the <span class="math notranslate nohighlight">\(ith\)</span> index of the logits is equal or not equal to the label <span class="math notranslate nohighlight">\(y\)</span>, and using basic rules of calculus such as the product rule and the power rule, we have derived the mathematical expression for the <strong>loss</strong> analytically. What we found is that for the first case we get <span class="math notranslate nohighlight">\(P_i\)</span> and for the other case <span class="math notranslate nohighlight">\(P_i - 1\)</span>. And that is the form that the gradient <span class="math notranslate nohighlight">\(\nabla_{l_i} loss\)</span> takes analytically. Now itâ€™s time to implement it. Keep in mind, that we derived the analytical <strong>loss</strong> only for a single example, but here we will be working with batches of examples. Thus the <strong>loss</strong> for a batch will be the average <strong>loss</strong> over all the examples, meaning the sum of all the individual losses per example, divided by the number of examples. And so we have to <strong>backprop</strong>agate through that operation as well and be careful with it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># backward pass</span>
<span class="n">dlogits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># apply softmax along the rows</span>
<span class="n">dlogits</span><span class="p">[</span>
    <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span>
<span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>  <span class="c1"># at the correct/target positions (where i == y) subtract 1</span>
<span class="n">dlogits</span> <span class="o">/=</span> <span class="n">batch_size</span>  <span class="c1"># divide by batch_size since the loss is also (during averaging across examples)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">dlogits</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>  <span class="c1"># we can only get approximate correctness ~6e-9</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>logits          | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09
</pre></div>
</div>
</div>
</div>
<p>As you can see, we get an approximately-correct but not an exactly-correct answer, with a slight deviation of ~<code class="docutils literal notranslate"><span class="pre">6e-9</span></code>. This is due to floating point wonkiness, but this is basically the correct answer, approximately. Now, before we move on to the next exercise, letâ€™s pause for a little bit in order to gain an intuitive sense of what <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> is, because it has a beautiful and surprisingly simple explanation honestly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dlogits</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3859a73b4fce4c36a28ae95aaeecc886", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Here, we are visualizing the <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> tensor that contains <span class="math notranslate nohighlight">\(32\)</span> examples and <span class="math notranslate nohighlight">\(27\)</span> characters per example. What is <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> intuitively? <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> is the softmax/probabilities matrix of the forward pass! The black squares are the positions of the correct indices where we subtracted a <code class="docutils literal notranslate"><span class="pre">1</span></code>. So what does this mean? Letâ€™s look at the values of the first row:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0731, 0.0872, 0.0182, 0.0518, 0.0192, 0.0845, 0.0231, 0.0355, 0.0173,
        0.0319, 0.0364, 0.0370, 0.0371, 0.0285, 0.0345, 0.0132, 0.0085, 0.0190,
        0.0148, 0.0545, 0.0513, 0.0209, 0.0247, 0.0721, 0.0584, 0.0259, 0.0215],
       grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>So these are the probabilities of the first row. Whereas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0731,  0.0872,  0.0182,  0.0518,  0.0192,  0.0845,  0.0231,  0.0355,
        -0.9827,  0.0319,  0.0364,  0.0370,  0.0371,  0.0285,  0.0345,  0.0132,
         0.0085,  0.0190,  0.0148,  0.0545,  0.0513,  0.0209,  0.0247,  0.0721,
         0.0584,  0.0259,  0.0215], grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>gives us the derivatives. Which is exactly equal to the probabilities, except that the value at the position of the correct index which has the probability value <code class="docutils literal notranslate"><span class="pre">p</span> <span class="pre">-</span> <span class="pre">1</span></code>! Also if we sum <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> of this row, we find out they sum to <code class="docutils literal notranslate"><span class="pre">0</span></code> (or at least very close to it):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dlogits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1.3970e-09, grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>We can think of the gradients of each row here as a <em>force</em> that pulls down on the probabilities of the incorrect characters and pulls up the probability at the correct (black dotted) index. And thatâ€™s whatâ€™s basically happening in each row. And the amount of push and pull is exactly equalized because the sum is <span class="math notranslate nohighlight">\(0\)</span>. So the amount to which we pull down in the probabilities and the amount that we push up on the probability of the correct character is equal. So the repulsion and the attraction are equal. So you can think of the <strong>nn</strong> now as a massive pulley system, or something like that, in which at the level of <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> weâ€™re pulling down the probabilities of incorrect and pulling up the probability of the correct ones. And in this complicated pulley system, we think of it as sort of like this tension translating to this complicating pulley mechanism and then eventually we get a tug on the weights and the biases. And basically in each update we just kind of like tug in the direction that weâ€™d like for each of these elements and the parameters are slowly given in to the tug and thatâ€™s what training the <strong>nn</strong> kind of looks like on a high level. And so the forces of push and pull in these gradients (e.g. see the values of <code class="docutils literal notranslate"><span class="pre">dlogits[0]</span> <span class="pre">*</span> <span class="pre">batch_size</span></code>) are actually very intuitive here weâ€™re pushing and pulling on the correct answer and the amount of force that weâ€™re applying is actually proportional to the probabilities that came out in the forward pass. And so for example if our probabilities came out exactly correct, so they would have had zero everywhere except for one at the correct position then the <code class="docutils literal notranslate"><span class="pre">dlogits</span></code> would be all a row of zeros for that example: there would be no push and pull. So the amount to which your prediction is incorrect is exactly the amount by which youâ€™re going to get a pull or a push in that dimension. So if you have for example a very confidently mispredicted element, then whatâ€™s going to happen is that element is going to be pulled down very heavily and the correct answer is going to be pulled up to the same amount and the other characters are not going to be influenced too much. So the amount to which you mispredict is then proportional to the strength of the pull and thatâ€™s happening independently in all the dimensions of this tensor and itâ€™s sort of very intuitive and very easy to think through and thatâ€™s basically the magic of the cross entropy loss and what itâ€™s doing dynamically in the backward pass of the <strong>nn</strong>.</p>
</section>
<section id="exercise-3-backprop-through-batchnorm-but-all-in-one-go">
<h2>Exercise 3: <strong>backprop</strong> through <strong>batchnorm</strong> but all in one go<a class="headerlink" href="#exercise-3-backprop-through-batchnorm-but-all-in-one-go" title="Link to this heading">#</a></h2>
<p>We now get to exercise number three, which is a very fun exercise depending on your definition of fun. We are going to do for <strong>batchnorm</strong> exactly what we did for cross entropy <strong>loss</strong> in exercise 2. That is that is we are going to consider it as a glued single mathematical expression and <strong>backprop</strong> through it in a very efficient manner because we are going to derive a much simpler formula for the backward pass of <strong>batchnorm</strong> and weâ€™re going to do that using pen and paper. So previously weâ€™ve broken up <strong>batchnorm</strong> into all of the little intermediate pieces and all the atomic operations inside it and then we <strong>backprop</strong>agated through it one by one. Now, we just have a single forward pass and itâ€™s all glued together:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 3: backprop through batchnorm but all in one go</span>
<span class="c1"># to complete this challenge look at the mathematical expression of the output of batchnorm,</span>
<span class="c1"># take the derivative w.r.t. its input, simplify the expression, and just write it out</span>

<span class="c1"># forward pass</span>

<span class="c1"># before:</span>
<span class="c1"># bnmeani = 1/n*hprebn.sum(0, keepdim=True)</span>
<span class="c1"># bndiff = hprebn - bnmeani</span>
<span class="c1"># bndiff2 = bndiff**2</span>
<span class="c1"># bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel&#39;s correction (dividing by n-1, not n)</span>
<span class="c1"># bnvar_inv = (bnvar + 1e-5)**-0.5</span>
<span class="c1"># bnraw = bndiff * bnvar_inv</span>
<span class="c1"># hpreact = bngain * bnraw + bnbias</span>

<span class="c1"># now:</span>
<span class="n">hpreact_fast</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">bngain</span>
    <span class="o">*</span> <span class="p">(</span><span class="n">hprebn</span> <span class="o">-</span> <span class="n">hprebn</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hprebn</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">bnbias</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;max diff:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">hpreact_fast</span> <span class="o">-</span> <span class="n">hpreact</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>max diff: tensor(4.7684e-07, grad_fn=&lt;MaxBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<p>And we see that we get the exact same result as before: an almost-zero difference. Now for the backward pass, weâ€™d like to also implement a single formula basically for backpropagating through this entire operation that is the <strong>batchnorm</strong>. So in the forward pass previously we took <code class="docutils literal notranslate"><span class="pre">hprebn</span></code>, the hidden states of the pre batch normalization and created <code class="docutils literal notranslate"><span class="pre">hpreact</span></code> which is the hidden states just before the activation. In the <strong>batchnorm</strong> paper:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;batchnorm_algorithm1.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7b9c6c8e713aca7856ccf8da56df923843d51901d12e0638eb6d84c659820634.png" src="../_images/7b9c6c8e713aca7856ccf8da56df923843d51901d12e0638eb6d84c659820634.png" />
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">hprebn</span></code> is <span class="math notranslate nohighlight">\(x\)</span> and <code class="docutils literal notranslate"><span class="pre">hpreact</span></code> is <span class="math notranslate nohighlight">\(y\)</span>. So in the backward pass what weâ€™d like to do now is we have <code class="docutils literal notranslate"><span class="pre">dhpreact</span></code> and weâ€™d like to produce <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code> and weâ€™d like to do that in a very efficient manner. So thatâ€™s the name of the game. Calculate <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code> given <code class="docutils literal notranslate"><span class="pre">dhpreact</span></code> and for the purposes of this exercise, weâ€™re going to ignore <code class="docutils literal notranslate"><span class="pre">gamma</span></code> and <code class="docutils literal notranslate"><span class="pre">beta</span></code> and their derivatives, because they take on a very simple form in a very similar way to what we did up above. So letâ€™s calculate <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial x_i}\)</span>) given <code class="docutils literal notranslate"><span class="pre">dhpreact</span></code> (<span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial y_i}\)</span>) (i.e. <strong>backprop</strong> through the <strong>batchnorm</strong>). According to <strong>Algorithm 1</strong> above (from the paper):</p>
<div class="highlight-ascii notranslate"><div class="highlight"><pre><span></span>       ____Î¼______(3)_______
      â•±     â•²               â”‚     
    (4)      â•²              â”‚
    â•±         â•²             â†“  
  [x]---â†’ÏƒÂ²---(2)---â†’[xÌ‚]---(1)-----â†’[y]
    â•²                       â†‘       â•±â”‚
     â•²______________________â”‚      Î³ Î²        
</pre></div>
</div>
<p>the bracketed variables represent vectors and the numbering represents the order in which we are going to <strong>backprop</strong>, going from <code class="docutils literal notranslate"><span class="pre">(1)</span></code> (right) all the way to <code class="docutils literal notranslate"><span class="pre">(4)</span></code> (left). Therefore, given <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial y_i}\)</span> we want to calculate <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial x_i}\)</span>:</p>
<p><strong>1</strong>. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \hat{x}_i} = \gamma \dfrac{\partial L}{\partial y_i}\)</span>, easy enough :)</p>
<p><strong>2</strong>. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \sigma^2} = \sum_i \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial \sigma^2}\)</span>, we sum the product of the local and global derivative because <span class="math notranslate nohighlight">\(\sigma^2\)</span> feeds into each <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> of the <span class="math notranslate nohighlight">\(\hat{x}\)</span> vector (so basically there are <span class="math notranslate nohighlight">\(m\)</span> (<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, e.g. <span class="math notranslate nohighlight">\(32\)</span>) number of arrows feeding from <span class="math notranslate nohighlight">\(\hat{x}\)</span> back into <span class="math notranslate nohighlight">\(\sigma^2\)</span>)</p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ = \gamma \sum_i \dfrac{\partial L}{\partial y_i} \dfrac{\partial}{\partial \sigma^2} \left[ ( x_i - \mu ) (\sigma^2 + \epsilon)^{-1/2} \right]\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ = -\dfrac{\gamma}{2} \sum_i \dfrac{\partial L}{\partial y_i} ( x_i - \mu ) (\sigma^2 + \epsilon)^{-3/2}\)</span></p>
<p><strong>3</strong>. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \mu} = \sum_i \dfrac{\partial L}{\partial \hat{x}_i} \dfrac{\partial \hat{x}_i}{\partial \mu} + \dfrac{\partial L}{\partial \sigma^2} \dfrac{\partial \sigma^2}{\partial \mu}\)</span>, here we also add a second component because as you can see above there are two branches emerging from <span class="math notranslate nohighlight">\(\mu\)</span>, one into <span class="math notranslate nohighlight">\(\sigma^2\)</span> and one into <span class="math notranslate nohighlight">\(\hat{x}\)</span>. To keep it simple, letâ€™s solve each partial derivative separately:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial \hat{x}_i}{\partial \mu} = \dfrac{\partial}{\partial \mu} \left[ ( x_i - \mu )(\sigma^2 + \epsilon)^{-1/2} \right] = -(\sigma^2 + \epsilon)^{-1/2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial \sigma^2}{\partial \mu} = \dfrac{\partial}{\partial \mu}\left[\dfrac{1}{m - 1}\sum_{i=1}^{m} (x_i - \mu)^2 \right]\)</span>, notice <span class="math notranslate nohighlight">\(\dfrac{1}{m - 1}\)</span> which is Besselâ€™s correction</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ = \dfrac{-2}{m - 1}\sum_{i=1}^{m} (x_i - \mu)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ = \dfrac{-2}{m - 1}(\sum_{i=1}^{m}x_i - \sum_{i=1}^{m}\mu)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ = \dfrac{-2}{m - 1}(m\mu - m\mu)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ = 0\)</span>, <strong>WHOA</strong> :)!</p>
<p>Therefore: <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \mu} = \sum_i \dfrac{\partial L}{\partial \hat{x}_i} [-(\sigma^2 + \epsilon)^{-1/2}] + \dfrac{\partial L}{\partial \sigma^2} \cdot 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = -\gamma\sum_i\dfrac{\partial L}{\partial y_i} (\sigma^2 + \epsilon)^{-1/2}\)</span></p>
<p><strong>4</strong>. <span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial x_i} = \dfrac{\partial L}{\partial \hat{x}_i}\dfrac{\partial \hat{x}_i}{\partial x_i} + \dfrac{\partial L}{\partial \mu}\dfrac{\partial \mu}{\partial x_i} + \dfrac{\partial L}{\partial \sigma^2}\dfrac{\partial \sigma^2}{\partial x_i}\)</span>, three components (partial derivative products), one for every path exiting reaching <code class="docutils literal notranslate"><span class="pre">x</span></code> from <code class="docutils literal notranslate"><span class="pre">L</span></code> (one through <span class="math notranslate nohighlight">\(\hat{x}\)</span>, one through <span class="math notranslate nohighlight">\(\mu\)</span> and one through <span class="math notranslate nohighlight">\(\sigma^2\)</span>). Similarly, letâ€™s solve each partial derivative that we do not yet know separately:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial \hat{x}_i}{\partial x_i} = \dfrac{\partial}{\partial x_i} \left[ ( x_i - \mu )(\sigma^2 + \epsilon)^{-1/2} \right] = (\sigma^2 + \epsilon)^{-1/2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial \mu}{\partial x_i} = \dfrac{\partial}{\partial x_i}\left(\dfrac{1}{m}\sum_j x_j\right) = \dfrac{1}{m}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial \sigma^2}{\partial x_i} = \dfrac{\partial }{\partial x_i}\left[\dfrac{1}{m - 1}\sum_j (x_j - \mu)^2 \right] = \dfrac{2}{m - 1} (x_i - \mu)\)</span></p></li>
</ul>
<p>Now that we have all the individual derivatives, letâ€™s find the three derivative products, one by one:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \hat{x}_i}\dfrac{\partial \hat{x}_i}{\partial x_i} = (\sigma^2 + \epsilon)^{-1/2} \gamma \dfrac{\partial L}{\partial y_i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \mu}\dfrac{\partial \mu}{\partial x_i} = -\gamma\sum_j\dfrac{\partial L}{\partial y_j} (\sigma^2 + \epsilon)^{-1/2} \dfrac{1}{m}\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = -(\sigma^2 + \epsilon)^{-1/2}\dfrac{\gamma}{m}\sum_j\dfrac{\partial L}{\partial y_j}\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial \sigma^2}\dfrac{\partial \sigma^2}{\partial x_i} = \left[-\dfrac{\gamma}{2} \sum_j \dfrac{\partial L}{\partial y_j} ( x_j - \mu ) (\sigma^2 + \epsilon)^{-3/2}\right] \left[\dfrac{2}{m - 1} (x_i - \mu)\right]\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\( \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = (\sigma^2 + \epsilon)^{-1/2}\left[-\gamma \sum_j \dfrac{\partial L}{\partial y_j} ( x_j - \mu ) (\sigma^2 + \epsilon)^{-1/2}\right] \left[\dfrac{1}{m - 1} (x_i - \mu)(\sigma^2 + \epsilon)^{-1/2}\right]\)</span></p>
<p><span class="math notranslate nohighlight">\( \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = (\sigma^2 + \epsilon)^{-1/2}\left[-\gamma \sum_j \dfrac{\partial L}{\partial y_j} \dfrac{x_j - \mu}{\sqrt{\sigma^2 + \epsilon}}\right] \left[\dfrac{1}{m - 1} \dfrac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\right]\)</span></p>
<p><span class="math notranslate nohighlight">\( \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = (\sigma^2 + \epsilon)^{-1/2}\left[-\gamma \sum_j \dfrac{\partial L}{\partial y_j} \hat{x}_j \right] \left[\dfrac{1}{m - 1} \hat{x}_i\right]\)</span></p>
<p>And finally, letâ€™s add up all the product terms to get the final expression for</p>
<p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial x_i} = (\sigma^2 + \epsilon)^{-1/2} \gamma \dfrac{\partial L}{\partial y_i} - (\sigma^2 + \epsilon)^{-1/2}\dfrac{\gamma}{m}\sum_j\dfrac{\partial L}{\partial y_j} + (\sigma^2 + \epsilon)^{-1/2}\left(-\gamma \sum_j \dfrac{\partial L}{\partial y_j} \hat{x}_j \right) \left(\dfrac{1}{m - 1} \hat{x}_i\right)\)</span></p>
<p><span class="math notranslate nohighlight">\( \ \ \ \ \ \ \ =  \dfrac{\gamma}{m}(\sigma^2 + \epsilon)^{-1/2} \left( m\dfrac{\partial L}{\partial y_i} - \sum_j\dfrac{\partial L}{\partial y_j} - \dfrac{m}{m - 1} \hat{x}_i \sum_j \dfrac{\partial L}{\partial y_j} \hat{x}_j \right)\)</span></p>
<p>And what we end up with at the end is a fairly simple mathematical expression over here that we cannot simplify further. But basically youâ€™ll notice that it only uses the stuff we have (<span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial y_i}\)</span>, <span class="math notranslate nohighlight">\(\hat{x}_i\)</span>) and it gives us the thing we need (<span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial x_i}\)</span>). Letâ€™s now implement this final result:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dhprebn</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">bngain</span>
    <span class="o">/</span> <span class="n">batch_size</span>
    <span class="o">*</span> <span class="n">bnvar_inv</span>
    <span class="o">*</span> <span class="p">(</span>
        <span class="n">batch_size</span> <span class="o">*</span> <span class="n">dhpreact</span>
        <span class="o">-</span> <span class="n">dhpreact</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">batch_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnraw</span> <span class="o">*</span> <span class="p">(</span><span class="n">dhpreact</span> <span class="o">*</span> <span class="n">bnraw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">cmp</span><span class="p">(</span><span class="s2">&quot;hprebn&quot;</span><span class="p">,</span> <span class="n">dhprebn</span><span class="p">,</span> <span class="n">hprebn</span><span class="p">)</span>  <span class="c1"># we can only get approximate correctness ~9e-10</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hprebn</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dhprebn</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bngain</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dbnraw</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dbnraw</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="mi">0</span>
<span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([32, 64]),
 torch.Size([32, 64]),
 torch.Size([1, 64]),
 torch.Size([1, 64]),
 torch.Size([32, 64]),
 torch.Size([64]))
</pre></div>
</div>
</div>
</div>
<p>Remember the mathematical expression we analytically calculated applies only to one neuron, whereas the programmatical <code class="docutils literal notranslate"><span class="pre">dhprebn</span></code> expression applies to all the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> number of neurons we have, in parallel. This is non-trivial, so take your time to see how it is written out with the  broadcasting happening appropriately, so that the shapes work out.</p>
</section>
<section id="exercise-4-training-the-mlp-with-our-own-manual-backward-pass">
<h2>Exercise 4: training the <strong>mlp</strong> with our own manual backward pass<a class="headerlink" href="#exercise-4-training-the-mlp-with-our-own-manual-backward-pass" title="Link to this heading">#</a></h2>
<p>For the final exercise 4, we want to put it all together. Meaning, we want to train the <strong>mlp</strong> with our own manual backward pass. So first, letâ€™s define a new, simpler forward pass function, a manual backward pass function, a training function and a function to print the loss:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xb</span><span class="p">]</span>  <span class="c1"># embed the characters into vectors</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concatenate the vectors</span>
    <span class="c1"># Linear layer</span>
    <span class="n">hprebn</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># hidden layer pre-activation</span>
    <span class="c1"># BatchNorm layer</span>
    <span class="c1"># -------------------------------------------------------------</span>
    <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hprebn</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bnvar</span> <span class="o">=</span> <span class="n">hprebn</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bnvar_inv</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
    <span class="n">bnraw</span> <span class="o">=</span> <span class="p">(</span><span class="n">hprebn</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnvar_inv</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="n">bnraw</span> <span class="o">+</span> <span class="n">bnbias</span>
    <span class="c1"># -------------------------------------------------------------</span>
    <span class="c1"># Non-linearity</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>  <span class="c1"># hidden layer</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># output layer</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>  <span class="c1"># loss function</span>
    <span class="k">return</span> <span class="n">emb</span><span class="p">,</span> <span class="n">embcat</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">manual_backward</span><span class="p">(</span>
    <span class="n">parameters</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">xb</span><span class="p">,</span>
    <span class="n">yb</span><span class="p">,</span>
    <span class="n">h</span><span class="p">,</span>
    <span class="n">hpreact</span><span class="p">,</span>
    <span class="n">bnraw</span><span class="p">,</span>
    <span class="n">bngain</span><span class="p">,</span>
    <span class="n">bnvar_inv</span><span class="p">,</span>
    <span class="n">embcat</span><span class="p">,</span>
    <span class="n">emb</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">dlogits</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">dlogits</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">yb</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">dlogits</span> <span class="o">/=</span> <span class="n">batch_size</span>
    <span class="c1"># 2nd layer backprop</span>
    <span class="n">dh</span> <span class="o">=</span> <span class="n">dlogits</span> <span class="o">@</span> <span class="n">w2</span><span class="o">.</span><span class="n">T</span>
    <span class="n">dw2</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dlogits</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="n">dlogits</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># tanh</span>
    <span class="n">dhpreact</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">h</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dh</span>
    <span class="c1"># batchnorm backprop</span>
    <span class="n">dbngain</span> <span class="o">=</span> <span class="p">(</span><span class="n">bnraw</span> <span class="o">*</span> <span class="n">dhpreact</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dbnbias</span> <span class="o">=</span> <span class="n">dhpreact</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dhprebn</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">bngain</span>
        <span class="o">/</span> <span class="n">batch_size</span>
        <span class="o">*</span> <span class="n">bnvar_inv</span>
        <span class="o">*</span> <span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">*</span> <span class="n">dhpreact</span>
            <span class="o">-</span> <span class="n">dhpreact</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="o">-</span> <span class="n">batch_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bnraw</span> <span class="o">*</span> <span class="p">(</span><span class="n">dhpreact</span> <span class="o">*</span> <span class="n">bnraw</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># 1st layer</span>
    <span class="n">dembcat</span> <span class="o">=</span> <span class="n">dhprebn</span> <span class="o">@</span> <span class="n">w1</span><span class="o">.</span><span class="n">T</span>
    <span class="n">dw1</span> <span class="o">=</span> <span class="n">embcat</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dhprebn</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="n">dhprebn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># embedding</span>
    <span class="n">demb</span> <span class="o">=</span> <span class="n">dembcat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dC</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">dC</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">demb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">dC</span><span class="p">,</span> <span class="n">dw1</span><span class="p">,</span> <span class="n">db1</span><span class="p">,</span> <span class="n">dw2</span><span class="p">,</span> <span class="n">db2</span><span class="p">,</span> <span class="n">dbngain</span><span class="p">,</span> <span class="n">dbnbias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">,</span>
    <span class="n">maxsteps</span><span class="o">=</span><span class="mi">200000</span><span class="p">,</span>
    <span class="n">batchsize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">run_torch_backward</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">break_at_step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">lossi</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># use this context manager to enable or disable gradient tracking</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">run_torch_backward</span><span class="p">):</span>
        <span class="c1"># kick off optimization</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxsteps</span><span class="p">):</span>
            <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">construct_minibatch</span><span class="p">()</span>
            <span class="n">emb</span><span class="p">,</span> <span class="n">embcat</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">run_torch_backward</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="n">emb</span><span class="p">,</span> <span class="n">embcat</span><span class="p">,</span> <span class="n">bnvar_inv</span><span class="p">,</span> <span class="n">bnraw</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">hpreact</span><span class="p">,</span> <span class="n">logits</span><span class="p">]:</span>
                    <span class="n">t</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">manual_backward</span><span class="p">(</span>
                <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">xb</span><span class="o">=</span><span class="n">xb</span><span class="p">,</span>
                <span class="n">yb</span><span class="o">=</span><span class="n">yb</span><span class="p">,</span>
                <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
                <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">,</span>
                <span class="n">hpreact</span><span class="o">=</span><span class="n">hpreact</span><span class="p">,</span>
                <span class="n">bnraw</span><span class="o">=</span><span class="n">bnraw</span><span class="p">,</span>
                <span class="n">bngain</span><span class="o">=</span><span class="n">bngain</span><span class="p">,</span>
                <span class="n">bnvar_inv</span><span class="o">=</span><span class="n">bnvar_inv</span><span class="p">,</span>
                <span class="n">embcat</span><span class="o">=</span><span class="n">embcat</span><span class="p">,</span>
                <span class="n">emb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># update</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">initial_lr</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100000</span> <span class="k">else</span> <span class="n">initial_lr</span> <span class="o">/</span> <span class="mi">10</span>
            <span class="p">)</span>  <span class="c1"># step learning rate decay</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="k">if</span> <span class="n">run_torch_backward</span> <span class="k">else</span> <span class="n">grad</span><span class="p">)</span>
            <span class="c1"># track stats</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># print every once in a while</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">maxsteps</span><span class="si">:</span><span class="s2">7d</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">lossi</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">break_at_step</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">break_at_step</span><span class="p">:</span>
                <span class="k">break</span>
    <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">lossi</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>  <span class="c1"># this decorator disables gradient tracking</span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>Cool. Now, we will simply train our <strong>nn</strong> using the backprop expressions we calculated manually (<code class="docutils literal notranslate"><span class="pre">manual_backward()</span></code>) instead of having PyTorch do it for us (i.e. <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>). Basically, we have plucked out all the operations that PyTorch would calculate and we have instead implemented them manually. Of course, our manual results are identical to the automated ones, as we have verified already. So, letâ€™s train and do both classic PyTorch backpropagation and our manual propagation and break the training loop at <span class="math notranslate nohighlight">\(100\)</span> epochs and check whether gradients match the PyTorch ones:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 4: putting it all together!</span>
<span class="c1"># Train the MLP neural net with your own backward pass</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">grads</span><span class="p">,</span> <span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span>
    <span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">run_torch_backward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">break_at_step</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="c1"># check gradients against PyTorch</span>
<span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
    <span class="n">cmp</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)),</span> <span class="n">g</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12297
      0/ 200000: 3.8279
(27, 10)        | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09
(30, 200)       | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09
(200,)          | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09
(200, 27)       | exact: False | approximate: True  | maxdiff: 2.2351741790771484e-08
(27,)           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09
(1, 200)        | exact: False | approximate: True  | maxdiff: 1.979060471057892e-09
(1, 200)        | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09
</pre></div>
</div>
</div>
</div>
<p>So here we see that our gradients are mostly, but not exactly, equal. Most are approximately equal with only tiny differences of the <code class="docutils literal notranslate"><span class="pre">e-9</span></code> magnitude. Which is totally fine. Can you figure out why we donâ€™t get exact correctness for all the gradients? If we now redefine our <strong>nn</strong> and train without breaking and only do our manual <strong>backprop</strong> (feels amazing to realize we donâ€™t need the automatic backward pass anymore!):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">define_nn</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_embd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">grads</span><span class="p">,</span> <span class="n">lossi</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12297
      0/ 200000: 3.8279
  10000/ 200000: 2.1609
  20000/ 200000: 2.4227
  30000/ 200000: 2.4362
  40000/ 200000: 2.0088
  50000/ 200000: 2.4084
  60000/ 200000: 2.4508
  70000/ 200000: 2.1090
  80000/ 200000: 2.3592
  90000/ 200000: 2.2353
 100000/ 200000: 1.9750
 110000/ 200000: 2.3438
 120000/ 200000: 2.0156
 130000/ 200000: 2.4772
 140000/ 200000: 2.3107
 150000/ 200000: 2.1108
 160000/ 200000: 1.9497
 170000/ 200000: 1.8004
 180000/ 200000: 2.0284
 190000/ 200000: 1.8848
</pre></div>
</div>
</div>
</div>
<p>After training, we will calibrate the <strong>batchnorm</strong> parameters as we did not keep track of the running mean and variance in the training loop:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calibrate the batch norm at the end of training</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># pass the training set through</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">xtrain</span><span class="p">]</span>
    <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="c1"># measure the mean/std over the entire training set</span>
    <span class="n">bnmean</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bnvar</span> <span class="o">=</span> <span class="n">hpreact</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And print both the training and validation losses:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train 2.0673816204071045
val 2.1060986518859863
</pre></div>
</div>
</div>
</div>
<p>We achieve a pretty good loss, very similar to what we achieved in our previous lecture! Now, all that remains is to sample from our model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample from the model</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>  <span class="c1"># initialize with all ...</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># ------------</span>
        <span class="c1"># forward pass:</span>
        <span class="c1"># Embedding</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">context</span><span class="p">])]</span>  <span class="c1"># (1,block_size,d)</span>
        <span class="n">embcat</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># concat into (N, block_size * n_embd)</span>
        <span class="n">hpreact</span> <span class="o">=</span> <span class="n">embcat</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">hpreact</span> <span class="o">=</span> <span class="n">bngain</span> <span class="o">*</span> <span class="p">(</span><span class="n">hpreact</span> <span class="o">-</span> <span class="n">bnmean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">bnvar</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">bnbias</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hpreact</span><span class="p">)</span>  <span class="c1"># (N, n_hidden)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>  <span class="c1"># (N, vocab_size)</span>
        <span class="c1"># ------------</span>
        <span class="c1"># Sample</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">itoc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mora.
mayah.
see.
mad.
ryla.
ren.
ruthadraegen.
chedielin.
shi.
jenleigh.
sananaraelyn.
malaia.
noshubergihirael.
kindreth.
konnie.
cayu.
zayven.
jamyleyeh.
yuma.
myston.
</pre></div>
</div>
</div>
</div>
<p>And we see the sort of name-like gibberish that we have been used to. Which of course means that the model works, as before ðŸ˜ƒ!</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Everything is the same except that we didnâ€™t use <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> or <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> and we estimated the gradients ourselves, by hand. And so hopefully youâ€™re looking at the the manual backward pass we did through this <strong>nn</strong> and youâ€™re thinking to yourself â€œactually thatâ€™s not too complicatedâ€. Each one of these layers is like three lines of code or something like that and most of it is fairly straightforward potentially with the notable exception of the <strong>batchnorm</strong> backward pass. And thatâ€™s everything we wanted to cover in this lecture. So hopefully you found this interesting and what is noteworthy about it honestly is that it gave us a very nice diversity of layers to backpropagate through. It should give us a pretty nice and comprehensive sense of how these backward passes are implemented and how they work and youâ€™d be able to derive them yourself. But of course in practice you probably donâ€™t want to: you want to use <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> instead. But hopefully now you have some intuition about how gradients flow backwards through the <strong>nn</strong> starting at the <strong>loss</strong> and how they flow through all the variables. And if you understood a good chunk of it and if you now have a sense of that, you should proudly count yourself as one of these buff doges on the left instead of the sorry little one on the right here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;backwardmemelol.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0c3966abe84e8c54ed777ddab4e97015cdf431108b640fa74f406effb540467e.png" src="../_images/0c3966abe84e8c54ed777ddab4e97015cdf431108b640fa74f406effb540467e.png" />
</div>
</div>
</section>
<section id="outro">
<h2>Outro<a class="headerlink" href="#outro" title="Link to this heading">#</a></h2>
<p>Now in the next lecture weâ€™re actually going to go to LSTMs and all the other variants of RNNs and weâ€™re going to start to complexify the architecture and start to achieve better log likelihoods and what not. Now thereâ€™s something to look forward to! Time for new adventures! See you there.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./micrograduate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="makemore3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</p>
      </div>
    </a>
    <a class="right-next"
       href="makemore5.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backprop-back-in-the-day"><strong>Backprop</strong> back in the day</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manually-implementing-a-backward-pass">Manually implementing a backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-implementing-the-backward-pass">Exercise 1: implementing the backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-backprop-through-cross-entropy-but-all-in-one-go">Exercise 2: <strong>backprop</strong> through cross_entropy but all in one go</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-backprop-through-batchnorm-but-all-in-one-go">Exercise 3: <strong>backprop</strong> through <strong>batchnorm</strong> but all in one go</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-training-the-mlp-with-our-own-manual-backward-pass">Exercise 4: training the <strong>mlp</strong> with our own manual backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outro">Outro</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christos Karaneen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <style>
  button#theme-switch-button {
    display: none !important;
  }
</style>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>