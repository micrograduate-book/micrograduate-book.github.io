
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7. picoGPT: implementing a tiny GPT from scratch &#8212; microgra∇uate</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'micrograduate/picogpt';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="6. makemore (part 5): building a WaveNet" href="makemore5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="microgra∇uate - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="microgra∇uate - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    <div align="center">  </div>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="micrograd.html">1. <strong>micrograd</strong>: implementing an autograd engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore1.html">2. <strong>makemore</strong> (part 1): implementing a bigram character-level language model</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore2.html">3. <strong>makemore</strong> (part 2): mlp</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore3.html">4. <strong>makemore</strong> (part 3): activations &amp; gradients, batchnorm</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore4.html">5. <strong>makemore</strong> (part 4): becoming a backprop ninja</a></li>
<li class="toctree-l1"><a class="reference internal" href="makemore5.html">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ckaraneen/micrograduate/issues/new?title=Issue%20on%20page%20%2Fmicrograduate/picogpt.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/micrograduate/picogpt.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>7. picoGPT: implementing a tiny GPT from scratch</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-gpt">What is a <strong>GPT</strong>?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-output">Input / Output</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input">Input</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output">Output</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-text">Generating text</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive">Autoregressive</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling">Sampling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">Prompting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-layers">Basic Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gelu">GELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-architecture"><strong>GPT</strong> Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings">Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embeddings">Token Embeddings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings">Positional Embeddings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#combined">Combined</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-stack">Decoder Stack</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-to-vocab">Projection to Vocab</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-block">Decoder Block</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-wise-feed-forward-network">Position-wise Feed Forward Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-causal-self-attention">Multi-Head Causal Self Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-lookups">Key-Value Lookups</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-lookups-based-on-meaning">Key-Value Lookups based on Meaning</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#word-vectors-and-similarity">Word Vectors and Similarity</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-scores-using-the-dot-product">Attention Scores using the Dot Product</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled Dot Product Attention</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self">Self</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#causal">Causal</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head">Multi-Head</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting it All Together</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-next">What Next?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-tpu-support">GPU/TPU Support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backprop"><strong>Backprop</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batching">Batching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-test">JAX test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-optimization">Inference Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-improvements">Architecture Improvements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-generation">Stopping Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-fine-tuning">Classification Fine-tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-fine-tuning">Generative Fine-tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-fine-tuning">Instruction Fine-tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning">Parameter Efficient Fine-tuning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="picogpt-implementing-a-tiny-gpt-from-scratch">
<h1>7. <strong>picoGPT</strong>: implementing a tiny <strong>GPT</strong> from scratch<a class="headerlink" href="#picogpt-implementing-a-tiny-gpt-from-scratch" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>
<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cloning repo...&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>--quiet<span class="w"> </span>https://github.com/ckarageorgkaneen/micrograduate.git<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
    <span class="o">%</span><span class="k">cd</span> micrograduate
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Installing requirements...&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>uv
    <span class="o">!</span>uv<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--system<span class="w"> </span>--quiet<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</div>
</div>
<section id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Link to this heading">#</a></h2>
<p>In this lesson, we’ll implement a tiny <strong>GPT</strong> from scratch in just a few lines of NumPy. To keep things simple, we’ll then load the trained <strong>GPT</strong>-2 model weights released by OpenAI into our implementation and generate some text. Sounds straightforward, right? Well, that’s because it is! Let’s get started.</p>
</section>
<section id="what-is-a-gpt">
<h2>What is a <strong>GPT</strong>?<a class="headerlink" href="#what-is-a-gpt" title="Link to this heading">#</a></h2>
<p><strong>GPT</strong> stands for <em>Generative Pre-trained Transformer</em>. It’s a type of <strong>nn</strong> architecture based on the <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">Transformer</a>. <a class="reference external" href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">Jay Alammar’s How <strong>GPT</strong>3 Works</a> is an excellent introduction to <strong>GPT</strong>s at a high level, but here’s the tl;dr:</p>
<ul class="simple">
<li><p>Generative: A <strong>GPT</strong> generates text.</p></li>
<li><p>Pre-trained: A <strong>GPT</strong> is trained on lots of text from books, the internet, etc …</p></li>
<li><p>Transformer: A <strong>GPT</strong> is a decoder-only transformer <strong>nn</strong>.</p></li>
</ul>
<p>Large Language Models (LLMs), like <a class="reference external" href="https://en.wikipedia.org/wiki/GPT-3">OpenAI’s <strong>GPT</strong>-3</a>, are just <strong>GPT</strong>s under the hood. What makes them special is they happen to be:</p>
<ol class="arabic simple">
<li><p>very big (billions of parameters) and</p></li>
<li><p>trained on lots of data (hundreds of gigabytes of text).</p></li>
</ol>
<p>Fundamentally, a <strong>GPT</strong> generates text given a prompt. Even with this very simple API (input = text, output = text), a well-trained <strong>GPT</strong> can do some pretty awesome stuff like <a class="reference external" href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Demo-of-Drafting-an-Email.png?lossy=0&amp;amp;strip=1&amp;amp;webp=1&amp;amp;ezimgfmt=ng:webp/ngcb1">write your emails</a>, <a class="reference external" href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Example-Book-Summarization.png?lossy=0&amp;amp;strip=1&amp;amp;webp=1&amp;amp;ezimgfmt=ng:webp/ngcb1">summarize a book</a>, <a class="reference external" href="https://khrisdigital.com/wp-content/uploads/2022/12/image-1.png">give you instagram caption ideas</a>, <a class="reference external" href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Examples-Explaining-Black-Holes.png?lossy=0&amp;amp;strip=1&amp;amp;webp=1&amp;amp;ezimgfmt=ng:webp/ngcb1">explain black holes to a 5 year old</a>, <a class="reference external" href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Demo-of-Writing-SQL-Queries.png?lossy=0&amp;amp;strip=1&amp;amp;webp=1&amp;amp;ezimgfmt=ng:webp/ngcb1">code in SQL</a>, and even <a class="reference external" href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/Chat-GPT-Example-Writing-a-Will.png?lossy=0&amp;amp;strip=1&amp;amp;webp=1&amp;amp;ezimgfmt=ng:webp/ngcb1">write your will</a>. So that’s a high-level overview of <strong>GPT</strong>s and their capabilities. Let’s dig into some more specifics.</p>
<section id="input-output">
<h3>Input / Output<a class="headerlink" href="#input-output" title="Link to this heading">#</a></h3>
<p>The function signature for a <strong>GPT</strong> looks roughly like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
    <span class="c1"># inputs has shape [n_seq]</span>
    <span class="c1"># output has shape [n_seq, n_vocab]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="c1"># beep boop neural network magic</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<section id="input">
<h4>Input<a class="headerlink" href="#input" title="Link to this heading">#</a></h4>
<p>The input is some text represented by a sequence of integers that map to tokens in the text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># integers represent tokens in our text, for example:</span>
<span class="c1"># text   = &quot;not all heroes wear capes&quot;:</span>
<span class="c1"># tokens = &quot;not&quot;  &quot;all&quot; &quot;heroes&quot; &quot;wear&quot; &quot;capes&quot;</span>
<span class="n">inputs</span> <span class="o">=</span>   <span class="p">[</span><span class="mi">1</span><span class="p">,</span>     <span class="mi">0</span><span class="p">,</span>    <span class="mi">2</span><span class="p">,</span>      <span class="mi">4</span><span class="p">,</span>     <span class="mi">6</span><span class="p">]</span>
</pre></div>
</div>
<p>Tokens are sub-pieces of the text, which are produced using some kind of tokenizer. We can map tokens to integers using a vocabulary:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the index of a token in the vocab represents the integer id for that token</span>
<span class="c1"># i.e. the integer id for &quot;heroes&quot; would be 2, since vocab[2] = &quot;heroes&quot;</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;not&quot;</span><span class="p">,</span> <span class="s2">&quot;heroes&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;wear&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;capes&quot;</span><span class="p">]</span>

<span class="c1"># a pretend tokenizer that tokenizes on whitespace</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">WhitespaceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="c1"># the encode() method converts a str -&gt; list[int]</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;not all heroes wear&quot;</span><span class="p">)</span> <span class="c1"># ids = [1, 0, 2, 4]</span>

<span class="c1"># we can see what the actual tokens are via our vocab mapping</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">]</span> <span class="c1"># tokens = [&quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;]</span>

<span class="c1"># the decode() method converts back a list[int] -&gt; str</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="c1"># text = &quot;not all heroes wear&quot;</span>
</pre></div>
</div>
<p>In short:</p>
<ul class="simple">
<li><p>We have a string.</p></li>
<li><p>We use a tokenizer to break it down into smaller pieces called tokens.</p></li>
<li><p>We use a vocabulary to map those tokens to integers.</p></li>
</ul>
<p>In practice, we use more advanced methods of tokenization than simply splitting by whitespace, such as <a class="reference external" href="https://huggingface.co/course/chapter6/5?fw=pt">Byte-Pair Encoding</a> or <a class="reference external" href="https://huggingface.co/course/chapter6/6?fw=pt">WordPiece</a>, but the principle is the same:</p>
<ul class="simple">
<li><p>There is a <code class="docutils literal notranslate"><span class="pre">vocab</span></code> that maps string tokens to integer indices</p></li>
<li><p>There is an <code class="docutils literal notranslate"><span class="pre">encode</span></code> method that converts <code class="docutils literal notranslate"><span class="pre">str</span> <span class="pre">-&gt;</span> <span class="pre">list[int]</span></code></p></li>
<li><p>There is a <code class="docutils literal notranslate"><span class="pre">decode</span></code> method that converts <code class="docutils literal notranslate"><span class="pre">list[int]</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code></p></li>
</ul>
<p><em>Note</em>: For certain applications, the tokenizer doesn’t require a decode method. For example, if you want to classify if a movie review is saying the movie was good or bad, you only need to be able to encode the text and do a forward pass of the model, there is no need for decode. For generating text however, decode is a requirement. ↩︎</p>
</section>
<section id="output">
<h4>Output<a class="headerlink" href="#output" title="Link to this heading">#</a></h4>
<p>The output is a <span class="math notranslate nohighlight">\(2D\)</span> array, where <code class="docutils literal notranslate"><span class="pre">output[i][j]</span></code> is the model’s predicted probability that the token at <code class="docutils literal notranslate"><span class="pre">vocab[j]</span></code> is the next token <code class="docutils literal notranslate"><span class="pre">inputs[i+1]</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;not&quot;</span><span class="p">,</span> <span class="s2">&quot;heroes&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;wear&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;capes&quot;</span><span class="p">]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="c1"># &quot;not&quot; &quot;all&quot; &quot;heroes&quot; &quot;wear&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c1">#              [&quot;all&quot;, &quot;not&quot;, &quot;heroes&quot;, &quot;the&quot;, &quot;wear&quot;, &quot;.&quot;, &quot;capes&quot;]</span>
<span class="c1"># output[0] =  [0.75    0.1     0.0       0.15    0.0   0.0    0.0  ]</span>
<span class="c1"># given just &quot;not&quot;, the model predicts the word &quot;all&quot; with the highest probability</span>

<span class="c1">#              [&quot;all&quot;, &quot;not&quot;, &quot;heroes&quot;, &quot;the&quot;, &quot;wear&quot;, &quot;.&quot;, &quot;capes&quot;]</span>
<span class="c1"># output[1] =  [0.0     0.0      0.8     0.1    0.0    0.0   0.1  ]</span>
<span class="c1"># given the sequence [&quot;not&quot;, &quot;all&quot;], the model predicts the word &quot;heroes&quot; with the highest probability</span>

<span class="c1">#              [&quot;all&quot;, &quot;not&quot;, &quot;heroes&quot;, &quot;the&quot;, &quot;wear&quot;, &quot;.&quot;, &quot;capes&quot;]</span>
<span class="c1"># output[-1] = [0.0     0.0     0.0     0.1     0.0    0.05  0.85  ]</span>
<span class="c1"># given the whole sequence [&quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;], the model predicts the word &quot;capes&quot; with the highest probability</span>
</pre></div>
</div>
<p>To get a next token prediction for the whole sequence, we simply take the token with the highest probability in <code class="docutils literal notranslate"><span class="pre">output[-1]</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;not&quot;</span><span class="p">,</span> <span class="s2">&quot;heroes&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;wear&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;capes&quot;</span><span class="p">]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="c1"># &quot;not&quot; &quot;all&quot; &quot;heroes&quot; &quot;wear&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># next_token_id = 6</span>
<span class="n">next_token</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">next_token_id</span><span class="p">]</span> <span class="c1"># next_token = &quot;capes&quot;</span>
</pre></div>
</div>
<p>Taking the token with the highest probability as our prediction is known as <a class="reference external" href="https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#1-pick-the-top-token-greedy-decoding">greedy decoding</a> or greedy sampling. The task of predicting the next logical word in a sequence is called language modeling. As such, we can call a <strong>GPT</strong> a language model. Generating a single word is cool and all, but what about entire sentences, paragraphs, etc …?</p>
</section>
</section>
<section id="generating-text">
<h3>Generating text<a class="headerlink" href="#generating-text" title="Link to this heading">#</a></h3>
<section id="autoregressive">
<h4>Autoregressive<a class="headerlink" href="#autoregressive" title="Link to this heading">#</a></h4>
<p>We can generate full sentences by iteratively getting the next token prediction from our model. At each iteration, we append the predicted token back into the input:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_tokens_to_generate</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens_to_generate</span><span class="p">):</span> <span class="c1"># auto-regressive decode loop</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># model forward pass</span>
        <span class="n">next_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># greedy sampling</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">next_id</span><span class="p">))</span> <span class="c1"># append prediction to input</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">n_tokens_to_generate</span> <span class="p">:]</span>  <span class="c1"># only return generated ids</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># &quot;not&quot; &quot;all&quot;</span>
<span class="n">output_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># output_ids = [2, 4, 6]</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">output_ids</span><span class="p">]</span> <span class="c1"># &quot;heroes&quot; &quot;wear&quot; &quot;capes&quot;</span>
</pre></div>
</div>
<p>This process of predicting a future value (regression), and adding it back into the input (auto), is why you might see a <strong>GPT</strong> described as autoregressive.</p>
</section>
<section id="sampling">
<h4>Sampling<a class="headerlink" href="#sampling" title="Link to this heading">#</a></h4>
<p>We can introduce some stochasticity (randomness) to our generations by sampling from the probability distribution instead of being greedy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="c1"># &quot;not&quot; &quot;all&quot; &quot;heroes&quot; &quot;wear&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># capes</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># hats</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># capes</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># capes</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># pants</span>
</pre></div>
</div>
<p>This allows us to generate different sentences given the same input. When combined with techniques like <a class="reference external" href="https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k">top-k</a>, <a class="reference external" href="https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#3-pick-from-amongst-the-top-tokens-whose-probabilities-add-up-to-15-top-p">top-p</a>, and <a class="reference external" href="https://docs.cohere.ai/docs/temperature">temperature</a>, which modify the distribution prior to sampling, the quality of our outputs is greatly increased. These techniques also introduce some hyperparameters that we can play around with to get different generation behaviors (for example, increasing temperature makes our model take more risks and thus be more “creative”.</p>
</section>
</section>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h3>
<p>We train a <strong>GPT</strong> like any other <strong>nn</strong>, using gradient descent <strong>w.r.t.</strong> some <strong>loss</strong> function. In the case of a <strong>GPT</strong>, we take the cross entropy <strong>loss</strong> over the language modeling task:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lm_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">params</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="c1"># the labels y are just the input shifted 1 to the left</span>
    <span class="c1">#</span>
    <span class="c1"># inputs = [not,     all,   heros,   wear,   capes]</span>
    <span class="c1">#      x = [not,     all,   heroes,  wear]</span>
    <span class="c1">#      y = [all,  heroes,     wear,  capes]</span>
    <span class="c1">#</span>
    <span class="c1"># of course, we don&#39;t have a label for inputs[-1], so we exclude it from x</span>
    <span class="c1">#</span>
    <span class="c1"># as such, for N inputs, we have N - 1 language modeling example pairs</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># both have shape [num_tokens_in_seq - 1]</span>

    <span class="c1"># forward pass</span>
    <span class="c1"># all the predicted next token probability distributions at each position</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="c1"># has shape [num_tokens_in_seq - 1, num_tokens_in_vocab]</span>

    <span class="c1"># cross entropy loss</span>
    <span class="c1"># we take the average over all N-1 examples</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)),</span> <span class="n">y</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">params</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">lm_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">compute_gradients_via_backpropagation</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">gradient_descent_update_step</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
<p>This is a heavily simplified training setup, but it illustrates the point. Notice the addition of <code class="docutils literal notranslate"><span class="pre">params</span></code> to our <code class="docutils literal notranslate"><span class="pre">gpt</span></code> function signature (we left this out in the previous sections for simplicity). During each iteration of the training loop:</p>
<ol class="arabic simple">
<li><p>We compute the language modeling <strong>loss</strong> for the given input text example</p></li>
<li><p>The <strong>loss</strong> determines our gradients, which we compute via <strong>backprop</strong></p></li>
<li><p>We use the gradients to update our model parameters such that the <strong>loss</strong> is minimized (gradient descent)</p></li>
</ol>
<p>Notice, we don’t use explicitly labelled data. Instead, we are able to produce the input/label pairs from just the raw text itself. This is referred to as <a class="reference external" href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised learning</a>. Self-supervision enables us to massively scale training data. Just get our hands on as much raw text as possible and throw it at the model. For example, <strong>GPT</strong>-3 was trained on 300 billion tokens of text from the internet and books:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;table_2.2_gpt3_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4076a77a03f46afa40d101ec47eef3e6f62964a13c0336614fe4ec3ad223dbbe.png" src="../_images/4076a77a03f46afa40d101ec47eef3e6f62964a13c0336614fe4ec3ad223dbbe.png" />
</div>
</div>
<p>Of course, you need a sufficiently large model to be able to learn from all this data, which is why <strong>GPT</strong>-3 has 175 billion parameters and probably cost between <a class="reference external" href="https://twitter.com/eturner303/status/1266264358771757057">$1m-10m in compute cost to train</a>. This self-supervised training step is called pre-training, since we can reuse the “pre-trained” models weights to further train the model on downstream tasks, such as classifying if a tweet is toxic or not. Pre-trained models are also sometimes called foundation models. Training the model on downstream tasks is called fine-tuning, since the model weights have already been pre-trained to understand language, it’s just being fine-tuned to the specific task at hand. The “pre-training on a general task + fine-tuning on a specific task” strategy is called <a class="reference external" href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>.</p>
<p><em>Note</em>: Although, with the <a class="reference external" href="https://arxiv.org/pdf/2210.11416.pdf">InstructGPT</a> and <a class="reference external" href="https://arxiv.org/pdf/2203.15556.pdf">Chinchilla</a> papers, we’ve realized that we don’t actually need to train models that big. An optimally trained and instruction fine-tuned <strong>GPT</strong> at 1.3B parameters can outperform <strong>GPT</strong>-3 at 175B parameters.</p>
</section>
<section id="prompting">
<h3>Prompting<a class="headerlink" href="#prompting" title="Link to this heading">#</a></h3>
<p>In principle, the original <a class="reference external" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"><strong>GPT</strong></a> paper was only about the benefits of pre-training a transformer model for transfer learning. The paper showed that pre-training a 117M <strong>GPT</strong> achieved state-of-the-art performance on various NLP (natural language processing) tasks when fine-tuned on labelled datasets. It wasn’t until the <a class="reference external" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"><strong>GPT</strong>-2</a> and <a class="reference external" href="https://arxiv.org/abs/2005.14165"><strong>GPT</strong>-3</a> papers that we realized a <strong>GPT</strong> model pre-trained on enough data with enough parameters was capable of performing any arbitrary task by itself, no fine-tuning needed. Just prompt the model, perform autoregressive language modeling, and voila, the model magically gives us an appropriate response. This is referred to as in-context learning, because the model is using just the context of the prompt to perform the task. In-context learning can be zero shot, one shot, or few shot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;fig_2.1_gpt3_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/786f719441c1e7afdad3bd9d71fec87dd6d251ef18a0ff9759664989a8996824.png" src="../_images/786f719441c1e7afdad3bd9d71fec87dd6d251ef18a0ff9759664989a8996824.png" />
</div>
</div>
<p>Generating text given a prompt is also sometimes referred to as conditional generation, since our model is generating some output conditioned on some input. <strong>GPT</strong>s are not limited to NLP tasks. You can condition the model on anything you want. For example, you can turn a <strong>GPT</strong> into a chatbot (i.e. <a class="reference external" href="https://openai.com/index/chatgpt/">Chat<strong>GPT</strong></a>) by conditioning it on the conversation history. You can also further condition the chatbot to behave a certain way by prepending the prompt with some kind of description (i.e. “You are a chatbot. Be polite, speak in full sentences, don’t say harmful things, etc …”). Conditioning the model like this can even give your <a class="reference external" href="https://imgur.com/a/AbDFcgk">chatbot a persona</a>. This is often referred to as a system prompt. However, this is not robust, you can still <a class="reference external" href="https://twitter.com/zswitten/status/1598380220943593472">“jailbreak” the model and make it misbehave</a>. With that out of the way, let’s finally get to the actual implementation.</p>
</section>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<p>Let’s dive right into the <strong>GPT</strong> implementation. First though, let’s define the necessary functions for downloading the model of our choice and the tokenizer files for  loading <code class="docutils literal notranslate"><span class="pre">encoder</span></code>, <code class="docutils literal notranslate"><span class="pre">hparams</span></code>, and <code class="docutils literal notranslate"><span class="pre">params</span></code> into our code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Byte pair encoding utilities.</span>

<span class="sd">Contains the code for OpenAI&#39;s BPE Tokenizer, taken straight from their gpt-2 repo: https://github.com/openai/gpt-2/blob/master/src/encoder.py.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">lru_cache</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">regex</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">re</span>


<span class="nd">@lru_cache</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bytes_to_unicode</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns list of utf-8 byte and a corresponding list of unicode strings.</span>
<span class="sd">    The reversible bpe codes work on unicode strings.</span>
<span class="sd">    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.</span>
<span class="sd">    When you&#39;re at something like a 10B token dataset you end up needing around 5K for decent coverage.</span>
<span class="sd">    This is a significant percentage of your normal, say, 32K bpe vocab.</span>
<span class="sd">    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.</span>
<span class="sd">    And avoids mapping to whitespace/control characters the bpe code barfs on.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="s2">&quot;!&quot;</span><span class="p">),</span> <span class="nb">ord</span><span class="p">(</span><span class="s2">&quot;~&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="s2">&quot;¡&quot;</span><span class="p">),</span> <span class="nb">ord</span><span class="p">(</span><span class="s2">&quot;¬&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="s2">&quot;®&quot;</span><span class="p">),</span> <span class="nb">ord</span><span class="p">(</span><span class="s2">&quot;ÿ&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="n">bs</span><span class="p">[:]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">b</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bs</span><span class="p">:</span>
            <span class="n">bs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
            <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">8</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">chr</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">cs</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">cs</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return set of symbol pairs in a word.</span>
<span class="sd">    Word is represented as tuple of symbols (symbols being variable-length strings).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">prev_char</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">word</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">pairs</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">prev_char</span><span class="p">,</span> <span class="n">char</span><span class="p">))</span>
        <span class="n">prev_char</span> <span class="o">=</span> <span class="n">char</span>
    <span class="k">return</span> <span class="n">pairs</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Encoder</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">bpe_merges</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s2">&quot;replace&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="n">errors</span>  <span class="c1"># how to handle errors in decoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">byte_encoder</span> <span class="o">=</span> <span class="n">bytes_to_unicode</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">byte_decoder</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">byte_encoder</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bpe_ranks</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">bpe_merges</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bpe_merges</span><span class="p">))))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pat</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
<span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">bpe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
        <span class="n">word</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">get_pairs</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">token</span>

        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">bigram</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">pair</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bpe_ranks</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">bigram</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bpe_ranks</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">first</span><span class="p">,</span> <span class="n">second</span> <span class="o">=</span> <span class="n">bigram</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">j</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">first</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
                    <span class="n">new_word</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">j</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">=</span> <span class="n">j</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:])</span>
                    <span class="k">break</span>

                <span class="k">if</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">first</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">second</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">first</span> <span class="o">+</span> <span class="n">second</span><span class="p">)</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">new_word</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_word</span><span class="p">)</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">new_word</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">get_pairs</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">word</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
        <span class="k">return</span> <span class="n">word</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="n">bpe_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pat</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
            <span class="n">token</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">byte_encoder</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">token</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
            <span class="n">bpe_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">[</span><span class="n">bpe_token</span><span class="p">]</span> <span class="k">for</span> <span class="n">bpe_token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bpe</span><span class="p">(</span><span class="n">token</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">bpe_tokens</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">])</span>
        <span class="n">text</span> <span class="o">=</span> <span class="nb">bytearray</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">byte_decoder</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">text</span><span class="p">])</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
            <span class="s2">&quot;utf-8&quot;</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">text</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_encoder</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">models_dir</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">models_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="s2">&quot;encoder.json&quot;</span><span class="p">),</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">encoder</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">models_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="s2">&quot;vocab.bpe&quot;</span><span class="p">),</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">bpe_data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">bpe_merges</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">merge_str</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">merge_str</span> <span class="ow">in</span> <span class="n">bpe_data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span> <span class="n">bpe_merges</span><span class="o">=</span><span class="n">bpe_merges</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;GPT-2 utilities.</span>

<span class="sd">Contains the code to download and load the GPT-2 model weights, tokenizer, and hyperparameters.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>


<span class="k">def</span><span class="w"> </span><span class="nf">download_gpt2_files</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">model_size</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;124M&quot;</span><span class="p">,</span> <span class="s2">&quot;355M&quot;</span><span class="p">,</span> <span class="s2">&quot;774M&quot;</span><span class="p">,</span> <span class="s2">&quot;1558M&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;checkpoint&quot;</span><span class="p">,</span>
        <span class="s2">&quot;encoder.json&quot;</span><span class="p">,</span>
        <span class="s2">&quot;hparams.json&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model.ckpt.data-00000-of-00001&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model.ckpt.index&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model.ckpt.meta&quot;</span><span class="p">,</span>
        <span class="s2">&quot;vocab.bpe&quot;</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://openaipublic.blob.core.windows.net/gpt-2/models&quot;</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_size</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">),</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">file_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">headers</span><span class="p">[</span><span class="s2">&quot;content-length&quot;</span><span class="p">])</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">1000</span>
            <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="n">ncols</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fetching &quot;</span> <span class="o">+</span> <span class="n">filename</span><span class="p">,</span>
                <span class="n">total</span><span class="o">=</span><span class="n">file_size</span><span class="p">,</span>
                <span class="n">unit_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
                <span class="c1"># 1k for chunk_size, since Ethernet packet size is around 1500 bytes</span>
                <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">r</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">):</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
                    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">load_gpt2_params_from_tf_ckpt</span><span class="p">(</span><span class="n">tf_ckpt_path</span><span class="p">,</span> <span class="n">hparams</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_in_nested_dict</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">keys</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">val</span>
        <span class="k">if</span> <span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">d</span><span class="p">:</span>
            <span class="n">d</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">d</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">set_in_nested_dict</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">d</span>

    <span class="n">init_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">list_variables</span><span class="p">(</span><span class="n">tf_ckpt_path</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;blocks&quot;</span><span class="p">:</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;n_layer&quot;</span><span class="p">])]}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">init_vars</span><span class="p">:</span>
        <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">load_variable</span><span class="p">(</span><span class="n">tf_ckpt_path</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">removeprefix</span><span class="p">(</span><span class="s2">&quot;model/&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;h&quot;</span><span class="p">):</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;h([0-9]+)/(.*)&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">sub_name</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">set_in_nested_dict</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">n</span><span class="p">],</span> <span class="n">sub_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">),</span> <span class="n">array</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">set_in_nested_dict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">),</span> <span class="n">array</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span>


<span class="k">def</span><span class="w"> </span><span class="nf">load_encoder_hparams_and_params</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">models_dir</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">model_size</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;124M&quot;</span><span class="p">,</span> <span class="s2">&quot;355M&quot;</span><span class="p">,</span> <span class="s2">&quot;774M&quot;</span><span class="p">,</span> <span class="s2">&quot;1558M&quot;</span><span class="p">]</span>

    <span class="n">model_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">models_dir</span><span class="p">,</span> <span class="n">model_size</span><span class="p">)</span>
    <span class="n">tf_ckpt_path</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tf_ckpt_path</span><span class="p">:</span>  <span class="c1"># download files if necessary</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">download_gpt2_files</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">)</span>
        <span class="n">tf_ckpt_path</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

    <span class="n">encoder</span> <span class="o">=</span> <span class="n">get_encoder</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">models_dir</span><span class="p">)</span>
    <span class="n">hparams</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="s2">&quot;hparams.json&quot;</span><span class="p">)))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">load_gpt2_params_from_tf_ckpt</span><span class="p">(</span><span class="n">tf_ckpt_path</span><span class="p">,</span> <span class="n">hparams</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">hparams</span><span class="p">,</span> <span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-02-18 12:27:24.663897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739899644.722397 1472589 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739899644.738680 1472589 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-18 12:27:24.870881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
<p>Cool. Having defined the necessary utilities, we will now define the prompt and generation functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="k">def</span><span class="w"> </span><span class="nf">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">wte</span><span class="p">,</span> <span class="n">wpe</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">ln_f</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
    <span class="k">pass</span>  <span class="c1"># TODO: implement this</span>


<span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">n_tokens_to_generate</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
        <span class="nb">range</span><span class="p">(</span><span class="n">n_tokens_to_generate</span><span class="p">),</span> <span class="s2">&quot;generating&quot;</span>
    <span class="p">):</span>  <span class="c1"># auto-regressive decode loop</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>  <span class="c1"># model forward pass</span>
        <span class="n">next_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># greedy sampling</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">next_id</span><span class="p">))</span>  <span class="c1"># append prediction to input</span>

    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">n_tokens_to_generate</span> <span class="p">:]</span>  <span class="c1"># only return generated ids</span>


<span class="k">def</span><span class="w"> </span><span class="nf">prompt_gpt</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">n_tokens_to_generate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span>
    <span class="n">model_size</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;124M&quot;</span><span class="p">,</span>
    <span class="n">models_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;models&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># load encoder, hparams, and params from the released open-ai gpt-2 files</span>
    <span class="n">encoder</span><span class="p">,</span> <span class="n">hparams</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">load_encoder_hparams_and_params</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">models_dir</span><span class="p">)</span>
    <span class="c1"># map numpy arrays to jax arrays if jax is installed (in case saved params contain numpy arrays)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;jax.numpy&quot;</span><span class="p">:</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">jax.tree_util</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jtu</span>

        <span class="n">params</span> <span class="o">=</span> <span class="n">jtu</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span>
        <span class="p">)</span>
    <span class="c1"># encode the input string using the BPE tokenizer</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="c1"># make sure we are not surpassing the max sequence length of our model</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">+</span> <span class="n">n_tokens_to_generate</span> <span class="o">&lt;</span> <span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;n_ctx&quot;</span><span class="p">]</span>
    <span class="c1"># generate output ids</span>
    <span class="n">output_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;n_head&quot;</span><span class="p">],</span> <span class="n">n_tokens_to_generate</span><span class="p">)</span>
    <span class="c1"># decode the ids back into a string</span>
    <span class="n">output_text</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_text</span>
</pre></div>
</div>
</div>
</div>
<p>Breaking down each of the <span class="math notranslate nohighlight">\(4\)</span> sections:</p>
<ol class="arabic simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> function is the actual <strong>GPT</strong> code we’ll be implementing. You’ll notice that the function signature includes some extra stuff in addition to <code class="docutils literal notranslate"><span class="pre">inputs</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">wte</span></code>, <code class="docutils literal notranslate"><span class="pre">wpe</span></code>, <code class="docutils literal notranslate"><span class="pre">blocks</span></code>, and <code class="docutils literal notranslate"><span class="pre">ln_f</span></code> are the parameters of our model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_head</span></code> is a hyperparameter that is needed during the forward pass.</p></li>
</ul>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">generate</span></code> function is the autoregressive decoding algorithm we saw earlier. We use greedy sampling for simplicity. <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> is a progress bar to help us visualize the decoding process as it generates tokens one at a time.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">prompt_gpt</span></code> function handles:</p>
<ul class="simple">
<li><p>Loading the tokenizer (<code class="docutils literal notranslate"><span class="pre">encoder</span></code>), model weights (<code class="docutils literal notranslate"><span class="pre">params</span></code>), and hyperparameters (<code class="docutils literal notranslate"><span class="pre">hparams</span></code>)</p></li>
<li><p>Encoding the input prompt into token IDs using the tokenizer</p></li>
<li><p>Calling the <code class="docutils literal notranslate"><span class="pre">generate</span></code> function</p></li>
<li><p>Decoding the output IDs into a string</p></li>
</ul>
</li>
</ol>
<section id="encoder">
<h3>Encoder<a class="headerlink" href="#encoder" title="Link to this heading">#</a></h3>
<p>Take a closer look at the following call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span><span class="p">,</span> <span class="n">hparams</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">load_encoder_hparams_and_params</span><span class="p">(</span><span class="s2">&quot;124M&quot;</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This will download the necessary model and tokenizer files into <code class="docutils literal notranslate"><span class="pre">models/124M</span></code> and load <code class="docutils literal notranslate"><span class="pre">encoder</span></code>, <code class="docutils literal notranslate"><span class="pre">hparams</span></code>, and <code class="docutils literal notranslate"><span class="pre">params</span></code> into our code. Let’s give it a try (may take some minutes, depending on your connection speed):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span><span class="p">,</span> <span class="n">hparams</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">load_encoder_hparams_and_params</span><span class="p">(</span><span class="s2">&quot;124M&quot;</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-02-18 12:27:28.665923: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.
</pre></div>
</div>
</div>
</div>
<p>Now, let’s encode a prompt and see the <code class="docutils literal notranslate"><span class="pre">ids</span></code> it returns. Let’s then decode and verify that we get the same prompt back:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Not all heroes wear capes.&quot;</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
<span class="n">prompt_decoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_decoded</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">prompt</span> <span class="o">==</span> <span class="n">prompt_decoded</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[3673, 477, 10281, 5806, 1451, 274, 13]
Not all heroes wear capes.
</pre></div>
</div>
</div>
</div>
<p>We do, indeed. Cool! Using the vocabulary of the tokenizer (stored in <code class="docutils literal notranslate"><span class="pre">encoder.decoder</span></code>), we can take a peek at what the actual tokens look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">encoder</span><span class="o">.</span><span class="n">decoder</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Not&#39;, &#39;Ġall&#39;, &#39;Ġheroes&#39;, &#39;Ġwear&#39;, &#39;Ġcap&#39;, &#39;es&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<p>Interesting. Notice how sometimes our tokens are words (e.g. <code class="docutils literal notranslate"><span class="pre">Not</span></code>), sometimes they are words but with a space in front of them (e.g. <code class="docutils literal notranslate"><span class="pre">Ġall</span></code>, the <a class="reference external" href="https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/bpe.py#L22-L33">Ġ represents a space</a>), sometimes there are part of a word (e.g. capes is split into <code class="docutils literal notranslate"><span class="pre">Ġcap</span></code> and <code class="docutils literal notranslate"><span class="pre">es</span></code>), and sometimes they are punctuation (e.g. <code class="docutils literal notranslate"><span class="pre">.</span></code>). One nice thing about BPE is that it can encode any arbitrary string. If it encounters something that is not present in the vocabulary, it just breaks it down into substrings it does understand:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">encoder</span><span class="o">.</span><span class="n">decoder</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;zjqfl&quot;</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;z&#39;, &#39;j&#39;, &#39;q&#39;, &#39;fl&#39;]
</pre></div>
</div>
</div>
</div>
<p>We can also check the size of the vocabulary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">decoder</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>50257
</pre></div>
</div>
</div>
</div>
<p>The vocabulary, as well as the byte-pair merges which determines how strings are broken down, is obtained by <em>training</em> the tokenizer. When we load the tokenizer, we’re loading the already trained vocab and byte-pair merges from some files, which were downloaded alongside the model files when we ran <code class="docutils literal notranslate"><span class="pre">load_encoder_hparams_and_params</span></code>. See the files <code class="docutils literal notranslate"><span class="pre">models/124M/encoder.json</span></code> (the vocabulary) and <code class="docutils literal notranslate"><span class="pre">models/124M/vocab.bpe</span></code> (byte-pair merges).</p>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">hparams</span></code> is a dictionary that contains the hyper-parameters of our model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hparams</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;n_vocab&#39;: 50257, &#39;n_ctx&#39;: 1024, &#39;n_embd&#39;: 768, &#39;n_head&#39;: 12, &#39;n_layer&#39;: 12}
</pre></div>
</div>
</div>
</div>
<p>Here’s what each key refers to:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_vocab</span></code>: number of tokens in our vocabulary</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_ctx</span></code>: maximum possible sequence length of the input</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_embd</span></code>: embedding dimension (determines the “width” of the network)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_head</span></code>: number of attention heads (n_embd must be divisible by n_head)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_layer</span></code>: number of layers (determines the “depth” of the network)</p></li>
</ul>
<p>We’ll use these symbols in our code’s comments to show the underlying shape of things. We’ll also use <code class="docutils literal notranslate"><span class="pre">n_seq</span></code> to denote the length of our input sequence (i.e. <code class="docutils literal notranslate"><span class="pre">n_seq</span> <span class="pre">=</span> <span class="pre">len(inputs)</span></code>).</p>
</section>
<section id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">params</span></code> is a nested json dictionary that hold the trained weights of our model. The leaf nodes of the json are NumPy arrays. If we print <code class="docutils literal notranslate"><span class="pre">params</span></code>, replacing the arrays with their shapes, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="k">def</span><span class="w"> </span><span class="nf">shape_tree</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">shape_tree</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">shape_tree</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;uh oh&quot;</span><span class="p">)</span>


<span class="n">shape_tree</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;blocks&#39;: [{&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}},
  {&#39;attn&#39;: {&#39;c_attn&#39;: {&#39;b&#39;: [2304], &#39;w&#39;: [768, 2304]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [768, 768]}},
   &#39;ln_1&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;ln_2&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
   &#39;mlp&#39;: {&#39;c_fc&#39;: {&#39;b&#39;: [3072], &#39;w&#39;: [768, 3072]},
    &#39;c_proj&#39;: {&#39;b&#39;: [768], &#39;w&#39;: [3072, 768]}}}],
 &#39;ln_f&#39;: {&#39;b&#39;: [768], &#39;g&#39;: [768]},
 &#39;wpe&#39;: [1024, 768],
 &#39;wte&#39;: [50257, 768]}
</pre></div>
</div>
</div>
</div>
<p>where each dictionary inside the <code class="docutils literal notranslate"><span class="pre">'blocks'</span></code> list contains the weights information for each layer (<code class="docutils literal notranslate"><span class="pre">n_layers</span></code> total). These weights are loaded from the original OpenAI tensorflow checkpoint:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>

<span class="n">tf_ckpt_path</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="s2">&quot;models/124M&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">list_variables</span><span class="p">(</span><span class="n">tf_ckpt_path</span><span class="p">):</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">load_variable</span><span class="p">(</span><span class="n">tf_ckpt_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>model/h0/attn/c_attn/b: (2304,)
model/h0/attn/c_attn/w: (768, 2304)
model/h0/attn/c_proj/b: (768,)
model/h0/attn/c_proj/w: (768, 768)
model/h0/ln_1/b: (768,)
model/h0/ln_1/g: (768,)
model/h0/ln_2/b: (768,)
model/h0/ln_2/g: (768,)
model/h0/mlp/c_fc/b: (3072,)
model/h0/mlp/c_fc/w: (768, 3072)
model/h0/mlp/c_proj/b: (768,)
model/h0/mlp/c_proj/w: (3072, 768)
model/h1/attn/c_attn/b: (2304,)
model/h1/attn/c_attn/w: (768, 2304)
model/h1/attn/c_proj/b: (768,)
model/h1/attn/c_proj/w: (768, 768)
model/h1/ln_1/b: (768,)
model/h1/ln_1/g: (768,)
model/h1/ln_2/b: (768,)
model/h1/ln_2/g: (768,)
model/h1/mlp/c_fc/b: (3072,)
model/h1/mlp/c_fc/w: (768, 3072)
model/h1/mlp/c_proj/b: (768,)
model/h1/mlp/c_proj/w: (3072, 768)
model/h10/attn/c_attn/b: (2304,)
model/h10/attn/c_attn/w: (768, 2304)
model/h10/attn/c_proj/b: (768,)
model/h10/attn/c_proj/w: (768, 768)
model/h10/ln_1/b: (768,)
model/h10/ln_1/g: (768,)
model/h10/ln_2/b: (768,)
model/h10/ln_2/g: (768,)
model/h10/mlp/c_fc/b: (3072,)
model/h10/mlp/c_fc/w: (768, 3072)
model/h10/mlp/c_proj/b: (768,)
model/h10/mlp/c_proj/w: (3072, 768)
model/h11/attn/c_attn/b: (2304,)
model/h11/attn/c_attn/w: (768, 2304)
model/h11/attn/c_proj/b: (768,)
model/h11/attn/c_proj/w: (768, 768)
model/h11/ln_1/b: (768,)
model/h11/ln_1/g: (768,)
model/h11/ln_2/b: (768,)
model/h11/ln_2/g: (768,)
model/h11/mlp/c_fc/b: (3072,)
model/h11/mlp/c_fc/w: (768, 3072)
model/h11/mlp/c_proj/b: (768,)
model/h11/mlp/c_proj/w: (3072, 768)
model/h2/attn/c_attn/b: (2304,)
model/h2/attn/c_attn/w: (768, 2304)
model/h2/attn/c_proj/b: (768,)
model/h2/attn/c_proj/w: (768, 768)
model/h2/ln_1/b: (768,)
model/h2/ln_1/g: (768,)
model/h2/ln_2/b: (768,)
model/h2/ln_2/g: (768,)
model/h2/mlp/c_fc/b: (3072,)
model/h2/mlp/c_fc/w: (768, 3072)
model/h2/mlp/c_proj/b: (768,)
model/h2/mlp/c_proj/w: (3072, 768)
model/h3/attn/c_attn/b: (2304,)
model/h3/attn/c_attn/w: (768, 2304)
model/h3/attn/c_proj/b: (768,)
model/h3/attn/c_proj/w: (768, 768)
model/h3/ln_1/b: (768,)
model/h3/ln_1/g: (768,)
model/h3/ln_2/b: (768,)
model/h3/ln_2/g: (768,)
model/h3/mlp/c_fc/b: (3072,)
model/h3/mlp/c_fc/w: (768, 3072)
model/h3/mlp/c_proj/b: (768,)
model/h3/mlp/c_proj/w: (3072, 768)
model/h4/attn/c_attn/b: (2304,)
model/h4/attn/c_attn/w: (768, 2304)
model/h4/attn/c_proj/b: (768,)
model/h4/attn/c_proj/w: (768, 768)
model/h4/ln_1/b: (768,)
model/h4/ln_1/g: (768,)
model/h4/ln_2/b: (768,)
model/h4/ln_2/g: (768,)
model/h4/mlp/c_fc/b: (3072,)
model/h4/mlp/c_fc/w: (768, 3072)
model/h4/mlp/c_proj/b: (768,)
model/h4/mlp/c_proj/w: (3072, 768)
model/h5/attn/c_attn/b: (2304,)
model/h5/attn/c_attn/w: (768, 2304)
model/h5/attn/c_proj/b: (768,)
model/h5/attn/c_proj/w: (768, 768)
model/h5/ln_1/b: (768,)
model/h5/ln_1/g: (768,)
model/h5/ln_2/b: (768,)
model/h5/ln_2/g: (768,)
model/h5/mlp/c_fc/b: (3072,)
model/h5/mlp/c_fc/w: (768, 3072)
model/h5/mlp/c_proj/b: (768,)
model/h5/mlp/c_proj/w: (3072, 768)
model/h6/attn/c_attn/b: (2304,)
model/h6/attn/c_attn/w: (768, 2304)
model/h6/attn/c_proj/b: (768,)
model/h6/attn/c_proj/w: (768, 768)
model/h6/ln_1/b: (768,)
model/h6/ln_1/g: (768,)
model/h6/ln_2/b: (768,)
model/h6/ln_2/g: (768,)
model/h6/mlp/c_fc/b: (3072,)
model/h6/mlp/c_fc/w: (768, 3072)
model/h6/mlp/c_proj/b: (768,)
model/h6/mlp/c_proj/w: (3072, 768)
model/h7/attn/c_attn/b: (2304,)
model/h7/attn/c_attn/w: (768, 2304)
model/h7/attn/c_proj/b: (768,)
model/h7/attn/c_proj/w: (768, 768)
model/h7/ln_1/b: (768,)
model/h7/ln_1/g: (768,)
model/h7/ln_2/b: (768,)
model/h7/ln_2/g: (768,)
model/h7/mlp/c_fc/b: (3072,)
model/h7/mlp/c_fc/w: (768, 3072)
model/h7/mlp/c_proj/b: (768,)
model/h7/mlp/c_proj/w: (3072, 768)
model/h8/attn/c_attn/b: (2304,)
model/h8/attn/c_attn/w: (768, 2304)
model/h8/attn/c_proj/b: (768,)
model/h8/attn/c_proj/w: (768, 768)
model/h8/ln_1/b: (768,)
model/h8/ln_1/g: (768,)
model/h8/ln_2/b: (768,)
model/h8/ln_2/g: (768,)
model/h8/mlp/c_fc/b: (3072,)
model/h8/mlp/c_fc/w: (768, 3072)
model/h8/mlp/c_proj/b: (768,)
model/h8/mlp/c_proj/w: (3072, 768)
model/h9/attn/c_attn/b: (2304,)
model/h9/attn/c_attn/w: (768, 2304)
model/h9/attn/c_proj/b: (768,)
model/h9/attn/c_proj/w: (768, 768)
model/h9/ln_1/b: (768,)
model/h9/ln_1/g: (768,)
model/h9/ln_2/b: (768,)
model/h9/ln_2/g: (768,)
model/h9/mlp/c_fc/b: (3072,)
model/h9/mlp/c_fc/w: (768, 3072)
model/h9/mlp/c_proj/b: (768,)
model/h9/mlp/c_proj/w: (3072, 768)
model/ln_f/b: (768,)
model/ln_f/g: (768,)
model/wpe: (1024, 768)
model/wte: (50257, 768)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-02-18 12:27:29.143545: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">load_gpt2_params_from_tf_ckpt</span></code> function converts the above tensorflow variables into our <code class="docutils literal notranslate"><span class="pre">params</span></code> dictionary. For reference, here’s the shapes of <code class="docutils literal notranslate"><span class="pre">params</span></code> but with the numbers replaced by the <code class="docutils literal notranslate"><span class="pre">hparams</span></code> they represent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;wpe&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_ctx</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">],</span>
    <span class="s2">&quot;wte&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_vocab</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">],</span>
    <span class="s2">&quot;ln_f&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;g&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">]},</span>
    <span class="s2">&quot;blocks&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;attn&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;c_attn&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">n_embd</span><span class="p">]},</span>
                <span class="s2">&quot;c_proj&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">]},</span>
            <span class="p">},</span>
            <span class="s2">&quot;ln_1&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;g&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">]},</span>
            <span class="s2">&quot;ln_2&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;g&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">]},</span>
            <span class="s2">&quot;mlp&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;c_fc&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">]},</span>
                <span class="s2">&quot;c_proj&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">]},</span>
            <span class="p">},</span>
        <span class="p">},</span>
        <span class="o">...</span> <span class="c1"># repeat for n_layers</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>You’ll probably want to come back to reference this dictionary to check the shape of the weights as we implement our <strong>GPT</strong>. We’ll match the variable names in our code with the keys of this dictionary for consistency.</p>
</section>
</section>
<section id="basic-layers">
<h2>Basic Layers<a class="headerlink" href="#basic-layers" title="Link to this heading">#</a></h2>
<p>Last thing before we get into the actual <strong>GPT</strong> architecture itself, let’s implement some of the more basic <strong>nn</strong> layers that are non-specific to <strong>GPT</strong>s.</p>
<section id="gelu">
<h3>GELU<a class="headerlink" href="#gelu" title="Link to this heading">#</a></h3>
<p>The non-linearity (activation function) of choice for <strong>GPT</strong>-2 is GELU (Gaussian Error Linear Units), an alternative for ReLU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;fig_1_from_gelu_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/868338dd92a0b6586db08fba73c819417b9c44b3f4d7e4525abf18d53858c2ca.png" src="../_images/868338dd92a0b6586db08fba73c819417b9c44b3f4d7e4525abf18d53858c2ca.png" />
</div>
</div>
<p>It is approximated by the following function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>Like ReLU, GELU operates element-wise on the input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gelu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.84119199,  1.95459769],
       [-0.04540231,  0.34571401]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="softmax">
<h3>Softmax<a class="headerlink" href="#softmax" title="Link to this heading">#</a></h3>
<p>Good ole <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the <a class="reference external" href="https://jaykmody.com/blog/stable-softmax/">max(x) trick for numerical stability</a>. Softmax is used to a convert set of real numbers (between <span class="math notranslate nohighlight">\(-\infty\)</span> and <span class="math notranslate nohighlight">\(\infty\)</span>) to probabilities (between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, with the numbers all summing to <span class="math notranslate nohighlight">\(1\)</span>). We apply <code class="docutils literal notranslate"><span class="pre">softmax</span></code> over the last axis of the input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[2.74878501e-43, 1.00000000e+00],
       [6.69285092e-03, 9.93307149e-01]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1., 1.])
</pre></div>
</div>
</div>
</div>
</section>
<section id="layer-normalization">
<h3>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1607.06450.pdf">Layer normalization</a> standardizes values to have a mean of <code class="docutils literal notranslate"><span class="pre">0</span></code> and a variance of <code class="docutils literal notranslate"><span class="pre">1</span></code>:</p>
<div class="math notranslate nohighlight">
\[\text{LayerNorm}(x) = \gamma\cdot\frac{x - \mu}{\sigma} + \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean of <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(\sigma^2\)</span> is the variance of <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
        <span class="n">variance</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="p">)</span>  <span class="c1"># normalize x to have mean=0 and var=1 over last axis</span>
    <span class="k">return</span> <span class="n">g</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># scale and offset with gamma/beta params</span>
</pre></div>
</div>
</div>
</div>
<p>Layer normalization ensures that the inputs for each layer are always within a consistent range, which is supposed to speed up and stabilize the training process. Like <a class="reference external" href="https://arxiv.org/pdf/1502.03167.pdf"><strong>batchnorm</strong></a>, the normalized output is then scaled and offset with two learnable vectors <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. The small epsilon term in the denominator (<code class="docutils literal notranslate"><span class="pre">eps</span></code>) is used to avoid a division by zero error. Layer norm is used instead of batch norm in the transformer for <a class="reference external" href="https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm">various reasons</a>. The differences between various normalization techniques is outlined in <a class="reference external" href="https://tungmphung.com/deep-learning-normalization-methods/">this excellent blog post</a>. We apply layer normalization over the last axis of the input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;var:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># yields floating point shenanigans</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mean:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># same here</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-0.70709087 -0.70709087  1.41418174]
 [-1.39700038  0.50800014  0.88900024]]
var: [0.999955   0.99999855]
mean: [-2.96059473e-16 -3.70074342e-17]
</pre></div>
</div>
</div>
</div>
</section>
<section id="linear">
<h3>Linear<a class="headerlink" href="#linear" title="Link to this heading">#</a></h3>
<p>Your standard matrix multiplication + bias:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>  <span class="c1"># [m, in], [in, out], [out] -&gt; [m, out]</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<p>Linear layers are often referred to as projections (since they are projecting from one vector space to another vector space):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>  <span class="c1"># input dim = 784, batch/sequence dim = 64</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>  <span class="c1"># output dim = 10</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># shape before linear projection</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># shape after linear projection</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(64, 784)
(64, 10)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="gpt-architecture">
<h2><strong>GPT</strong> Architecture<a class="headerlink" href="#gpt-architecture" title="Link to this heading">#</a></h2>
<p>The <strong>GPT</strong> architecture follows that of the transformer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;fig_1_from_attention_is_all_you_need_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2c1e295ca0fe7dbdc8adf25dc2edec35cb9d01d93a83bcd1b9f60bbb1b9bcc2c.png" src="../_images/2c1e295ca0fe7dbdc8adf25dc2edec35cb9d01d93a83bcd1b9f60bbb1b9bcc2c.png" />
</div>
</div>
<p>But uses only the decoder stack (the right part of the diagram):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;transformer_decoder_stack.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ee3b9239d4d9beebb72dc0729c67edbc31ea0383d9c6c70c6744afedbe8edd0f.png" src="../_images/ee3b9239d4d9beebb72dc0729c67edbc31ea0383d9c6c70c6744afedbe8edd0f.png" />
</div>
</div>
<p>Note, the middle “cross-attention” layer is also removed since we got rid of the encoder. At a high level, the <strong>GPT</strong> architecture has three sections:</p>
<ul class="simple">
<li><p>Text + positional embeddings</p></li>
<li><p>A transformer decoder stack</p></li>
<li><p>A projection to vocab step</p></li>
</ul>
<p>In code, it looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">wte</span><span class="p">,</span> <span class="n">wpe</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">ln_f</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_vocab]</span>
    <span class="c1"># token + positional embeddings</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">wte</span><span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">wpe</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># forward pass through n_layer transformer blocks</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">block</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span>
        <span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># projection to vocab</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_f</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">wte</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s break down each of these three sections into more detail.</p>
<section id="embeddings">
<h3>Embeddings<a class="headerlink" href="#embeddings" title="Link to this heading">#</a></h3>
<section id="token-embeddings">
<h4>Token Embeddings<a class="headerlink" href="#token-embeddings" title="Link to this heading">#</a></h4>
<p>Token IDs by themselves are not very good representations for a <strong>nn</strong>. For one, the relative magnitudes of the token IDs falsely communicate information (for example, if <code class="docutils literal notranslate"><span class="pre">Apple</span> <span class="pre">=</span> <span class="pre">5</span></code> and <code class="docutils literal notranslate"><span class="pre">Table</span> <span class="pre">=</span> <span class="pre">10</span></code> in our vocab, then we are implying that <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">Apple</span> <span class="pre">=</span> <span class="pre">Table</span></code>). Secondly, a single number is not a lot of dimensionality for a <strong>nn</strong> to work with. To address these limitations, we’ll take advantage of <a class="reference external" href="https://jaykmody.com/blog/attention-intuition/#word-vectors-and-similarity">word vectors</a>, specifically via a learned embedding matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wte</span><span class="p">[</span><span class="n">inputs</span><span class="p">]</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_embd]</span>
</pre></div>
</div>
<p>Recall, <code class="docutils literal notranslate"><span class="pre">wte</span></code> is a <code class="docutils literal notranslate"><span class="pre">[n_vocab,</span> <span class="pre">n_embd]</span></code> matrix. It acts as a lookup table, where the <span class="math notranslate nohighlight">\(ith\)</span> row in the matrix corresponds to the learned vector for the <span class="math notranslate nohighlight">\(ith\)</span> token in our vocabulary. <code class="docutils literal notranslate"><span class="pre">wte[inputs]</span></code> uses <a class="reference external" href="https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing">integer array indexing</a> to retrieve the vectors corresponding to each token in our input. Like any other parameter in our network, <code class="docutils literal notranslate"><span class="pre">wte</span></code> is learned. That is, it is randomly initialized at the start of training and then updated via gradient descent.</p>
</section>
<section id="positional-embeddings">
<h4>Positional Embeddings<a class="headerlink" href="#positional-embeddings" title="Link to this heading">#</a></h4>
<p>One quirk of the transformer architecture is that it doesn’t take into account position. That is, if we randomly shuffled our input and then accordingly unshuffled the output, the output would be the same as if we never shuffled the input in the first place (the ordering of inputs doesn’t have any effect on the output). Of course, the ordering of words is a crucial part of language (duh), so we need some way to encode positional information into our inputs. For this, we can just use another learned embedding matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wpe</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_embd]</span>
</pre></div>
</div>
<p>Recall, <code class="docutils literal notranslate"><span class="pre">wpe</span></code> is a <code class="docutils literal notranslate"><span class="pre">[n_ctx,</span> <span class="pre">n_embd]</span></code> matrix. The <span class="math notranslate nohighlight">\(ith\)</span> row of the matrix contains a vector that encodes information about the <span class="math notranslate nohighlight">\(ith\)</span> position in the input. Similar to <span class="math notranslate nohighlight">\(wte\)</span>, this matrix is learned during gradient descent. Notice, this restricts our model to a maximum sequence length of <code class="docutils literal notranslate"><span class="pre">n_ctx</span></code>. That is, <code class="docutils literal notranslate"><span class="pre">len(inputs)</span> <span class="pre">&lt;=</span> <span class="pre">n_ctx</span></code> must hold.</p>
<p><em>Note</em>: The original transformer paper used a <a class="reference external" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding">calculated positional embedding</a> which they found performed just as well as learned positional embeddings, but has the distinct advantage that you can input any arbitrarily long sequence (you are not restricted by a maximum sequence length). However, in practice, your model is only going to be as the good sequence lengths that it was trained on. You can’t just train a <strong>GPT</strong> on sequences that are <span class="math notranslate nohighlight">\(1024\)</span> long and then expect it to perform well at 16k tokens long. Recently however, there has been some success with relative positional embeddings, such as <a class="reference external" href="https://arxiv.org/pdf/2108.12409.pdf">Alibi</a> and <a class="reference external" href="https://arxiv.org/pdf/2104.09864v4.pdf">RoPE</a>.</p>
</section>
<section id="combined">
<h4>Combined<a class="headerlink" href="#combined" title="Link to this heading">#</a></h4>
<p>We can add our token and positional embeddings to get a combined embedding that encodes both token and positional information:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># token + positional embeddings</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">wte</span><span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">wpe</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>  <span class="c1"># [n_seq] -&gt; [n_seq, n_embd]</span>

<span class="c1"># x[i] represents the word embedding for the ith word + the positional</span>
<span class="c1"># embedding for the ith position</span>
</pre></div>
</div>
</section>
</section>
<section id="decoder-stack">
<h3>Decoder Stack<a class="headerlink" href="#decoder-stack" title="Link to this heading">#</a></h3>
<p>This is where all the magic happens and the “deep” in deep learning comes in. We pass our embedding through a stack of <code class="docutils literal notranslate"><span class="pre">n_layer</span></code> transformer decoder blocks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward pass through n_layer transformer blocks</span>
<span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">block</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span>
    <span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
</pre></div>
</div>
<p>Stacking more layers is what allows us to control how deep our network is. <strong>GPT</strong>-3 for example, has a whopping <span class="math notranslate nohighlight">\(96\)</span> layers. On the other hand, choosing a larger <code class="docutils literal notranslate"><span class="pre">n_embd</span></code> value allows us to control how <em>wide</em> our network is (for example, <strong>GPT</strong>-3 uses an embedding size of <span class="math notranslate nohighlight">\(12288\)</span>).</p>
</section>
<section id="projection-to-vocab">
<h3>Projection to Vocab<a class="headerlink" href="#projection-to-vocab" title="Link to this heading">#</a></h3>
<p>In our final step, we project the output of the final transformer block to a probability distribution over our vocab:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># projection to vocab</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_f</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
<span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">wte</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_vocab]</span>
</pre></div>
</div>
<p>Couple things to note here:</p>
<ol class="arabic simple">
<li><p>We first pass <code class="docutils literal notranslate"><span class="pre">x</span></code> through a final layer normalization layer before doing the projection to vocab. This is specific to the <strong>GPT</strong>-2 architecture (this is not present in the original <strong>GPT</strong> and Transformer papers).</p></li>
<li><p>We are reusing the embedding matrix <code class="docutils literal notranslate"><span class="pre">wte</span></code> for the projection.  Other <strong>GPT</strong> implementations may choose to use a separate learned weight matrix for the projection, however sharing the embedding matrix has a couple of advantages:</p>
<ul class="simple">
<li><p>You save some parameters (although at <strong>GPT</strong>-3 scale, this is negligible).</p></li>
<li><p>Since the matrix is both responsible for mapping both <em>to</em> words and <em>from</em> words, in theory, it <em>may</em> learn a richer representation compared to having two separate matrixes.</p></li>
</ul>
</li>
<li><p>We don’t apply <code class="docutils literal notranslate"><span class="pre">softmax</span></code> at the end, so our outputs will be logits instead of probabilities between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. This is done for several reasons:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">softmax</span></code> is <a class="reference external" href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic</a>, so for greedy sampling <code class="docutils literal notranslate"><span class="pre">np.argmax(logits)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">np.argmax(softmax(logits))</span></code> making <code class="docutils literal notranslate"><span class="pre">softmax</span></code> redundant</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">softmax</span></code> is irreversible, meaning we can always go from logits to probabilities by applying <code class="docutils literal notranslate"><span class="pre">softmax</span></code>, but we can’t go back to logits from probabilities, so for maximum flexibility, we output the logits</p></li>
<li><p>Numerically stability (for example, to compute cross entropy <strong>loss</strong>, taking <code class="docutils literal notranslate"><span class="pre">log(softmax(logits))</span></code> is numerically unstable compared to <code class="docutils literal notranslate"><span class="pre">log_softmax(logits)</span></code></p></li>
</ul>
</li>
</ol>
<p>The projection to vocab step is also sometimes called the language modeling head. What does “head” mean? Once your <strong>GPT</strong> is pre-trained, you can swap out the language modeling head with some other kind of projection, like a classification head for fine-tuning the model on some classification task. So your model can have multiple heads, kind of like a <a class="reference external" href="https://en.wikipedia.org/wiki/Lernaean_Hydra">hydra</a>. So that’s the <strong>GPT</strong> architecture at a high level, let’s actually dig a bit deeper into what the decoder blocks are doing.</p>
</section>
<section id="decoder-block">
<h3>Decoder Block<a class="headerlink" href="#decoder-block" title="Link to this heading">#</a></h3>
<p>The transformer decoder block consists of two sublayers:</p>
<ol class="arabic simple">
<li><p>Multi-head causal self attention</p></li>
<li><p>Position-wise feed forward <strong>nn</strong></p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">transformer_block</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">ln_1</span><span class="p">,</span> <span class="n">ln_2</span><span class="p">,</span> <span class="n">n_head</span>
<span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># multi-head causal self attention</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">mha</span><span class="p">(</span>
        <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_1</span><span class="p">),</span> <span class="o">**</span><span class="n">attn</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span>
    <span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># position-wise feed forward network</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">ffn</span><span class="p">(</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_2</span><span class="p">),</span> <span class="o">**</span><span class="n">mlp</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Each sublayer utilizes layer normalization on their inputs as well as a residual connection (i.e. add the input of the sublayer to the output of the sublayer). Some things to note:</p>
<ol class="arabic simple">
<li><p>Multi-head causal self attention is what facilitates the communication between the inputs. Nowhere else in the network does the model allow inputs to “see” each other. The embeddings, position-wise feed forward network, layer norms, and projection to vocab all operate on our inputs position-wise. Modeling relationships between inputs is tasked solely to attention.</p></li>
<li><p>The Position-wise feed forward <strong>nn</strong> is just a regular 2 layer fully connected <strong>nn</strong>. This just adds a bunch of learnable parameters for our model to work with to facilitate learning.</p></li>
<li><p>In the original transformer paper, layer norm is placed on the output <code class="docutils literal notranslate"><span class="pre">layer_norm(x</span> <span class="pre">+</span> <span class="pre">sublayer(x))</span></code> while we place layer norm on the input <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">sublayer(layer_norm(x))</span></code> to match <strong>GPT</strong>-2. This is referred to as pre-norm and has been shown to be <a class="reference external" href="https://arxiv.org/pdf/2002.04745.pdf">important in improving the performance of the transformer</a>.</p></li>
<li><p>Residual connections (popularized by ResNet) serve a couple of different purposes:</p>
<ul class="simple">
<li><p>Makes it easier to optimize <strong>nn</strong>s that are deep (i.e. networks that have lots of layers). The idea here is that we are providing “shortcuts” for the gradients to flow back through the network, making it easier to optimize the earlier layers in the network.</p></li>
<li><p>Without residual connections, deeper models see a degradation in performance when adding more layers (possibly because it’s hard for the gradients to flow all the way back through a deep network without losing information). Residual connections seem to give a bit of an accuracy boost for deeper networks.</p></li>
<li><p>Can help with the <a class="reference external" href="https://programmathically.com/understanding-the-exploding-and-vanishing-gradients-problem/">vanishing/exploding gradients problem</a>.</p></li>
</ul>
</li>
</ol>
<p>Let’s dig a little deeper into the 2 sublayers.</p>
</section>
<section id="position-wise-feed-forward-network">
<h3>Position-wise Feed Forward Network<a class="headerlink" href="#position-wise-feed-forward-network" title="Link to this heading">#</a></h3>
<p>This is just a simple multi-layer perceptron with 2 layers:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c_fc</span><span class="p">,</span> <span class="n">c_proj</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># project up</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_fc</span><span class="p">))</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, 4*n_embd]</span>

    <span class="c1"># project back down</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">**</span><span class="n">c_proj</span><span class="p">)</span>  <span class="c1"># [n_seq, 4*n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Nothing super fancy here, we just project from <code class="docutils literal notranslate"><span class="pre">n_embd</span></code> up to a higher dimension <code class="docutils literal notranslate"><span class="pre">4*n_embd</span></code> and then back down to <code class="docutils literal notranslate"><span class="pre">n_embd[4]</span></code>.</p>
<p><em>Note</em>: Different <strong>GPT</strong> models may choose a different hidden width that is not <code class="docutils literal notranslate"><span class="pre">4*n_embd</span></code>, however this is the common practice for <strong>GPT</strong> models. Also, we give the multi-head attention layer a lot of <em>attention</em> (pun intended) for driving the success of the transformer, but at the scale of <strong>GPT</strong>-3, <span class="math notranslate nohighlight">\(80\%\)</span> of the model parameters are contained in the feed forward layer. Just something to think about.</p>
<p>Recall, from our params dictionary, that our mlp params look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;mlp&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;c_fc&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">]},</span>
    <span class="s2">&quot;c_proj&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">]},</span>
<span class="p">},</span>
</pre></div>
</div>
</section>
<section id="multi-head-causal-self-attention">
<h3>Multi-Head Causal Self Attention<a class="headerlink" href="#multi-head-causal-self-attention" title="Link to this heading">#</a></h3>
<p>This layer is probably the most difficult part of the transformer to understand. So let’s work our way up to “Multi-Head Causal Self Attention” by breaking each word down into its own section:</p>
<ol class="arabic simple">
<li><p>Attention</p></li>
<li><p>Self</p></li>
<li><p>Causal</p></li>
<li><p>Multi-Head</p></li>
</ol>
<section id="attention">
<h4>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h4>
<p>ChatGPT and other large language models use a special type of <strong>nn</strong> called the transformer. The transformer defining feature is the attention mechanism. Attention is defined by the equation:</p>
<div class="math notranslate nohighlight">
\[\text{attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</div>
<p>Attention can come in different forms, but this version of attention (known as scaled dot product attention) was first proposed in the <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">original transformer paper</a>. In this post, we’ll build an intuition for the above equation by deriving it from the ground up.</p>
<section id="key-value-lookups">
<h5>Key-Value Lookups<a class="headerlink" href="#key-value-lookups" title="Link to this heading">#</a></h5>
<p>A key-value (kv) lookup involves three components:</p>
<ol class="arabic simple">
<li><p>A list of <span class="math notranslate nohighlight">\(n_k\)</span> <strong>keys</strong></p></li>
<li><p>A list of <span class="math notranslate nohighlight">\(n_k\)</span> <strong>values</strong> (that map 1-to-1 with the keys, forming key-value pairs)</p></li>
<li><p>A <strong>query</strong>, for which we want to <em>match</em> with the keys and get some value based on the match</p></li>
</ol>
<p>You’re probably familiar with this concept as a dictionary or hash map:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;apple&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;banana&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;chair&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;apple&quot;</span>
<span class="n">d</span><span class="p">[</span><span class="n">query</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;apple&#39;, &#39;banana&#39;, &#39;chair&#39;])
dict_values([10, 5, 2])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10
</pre></div>
</div>
</div>
</div>
<p>Dictionaries let us perform lookups based on an exact string match. What if instead we wanted to do a lookup based on the meaning of a word?</p>
</section>
<section id="key-value-lookups-based-on-meaning">
<h5>Key-Value Lookups based on Meaning<a class="headerlink" href="#key-value-lookups-based-on-meaning" title="Link to this heading">#</a></h5>
<p>Say we wanted to look up the word “fruit” in our previous example, how do we choose which key is the best match? It’s obviously not “chair”, but both “apple” and “banana” seem like a good match. It’s hard to choose one or the other, fruit feels more like a combination of apple and banana rather than a strict match for either. So, let’s not choose. Instead, we’ll do exactly that, take a combination of apple and banana. For example, say we assign a <span class="math notranslate nohighlight">\(60\%\)</span> meaning based match for apple, a <span class="math notranslate nohighlight">\(40\%\)</span> match for banana, and <span class="math notranslate nohighlight">\(0\%\)</span> match for chair. We compute our final output value as the weighted sum of the values with the percentages:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;fruit&quot;</span>
<span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;apple&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;banana&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;chair&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="mf">0.6</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;apple&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;banana&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.0</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;chair&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8.0
</pre></div>
</div>
</div>
</div>
<p>In a sense, we are determining how much attention our query should be paying to each key-value pair based on <code class="docutils literal notranslate"><span class="pre">meaning</span></code>. The amount of “attention” is represented as a decimal percentage, called an attention score. Mathematically, we can define our output as a simple weighted sum:</p>
<div class="math notranslate nohighlight">
\[\sum_{i} \alpha_i v_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_i\)</span> is our attention score for the <span class="math notranslate nohighlight">\(ith\)</span> kv pair and <span class="math notranslate nohighlight">\(v_i\)</span> is the <span class="math notranslate nohighlight">\(ith\)</span> value. Remember, the attention scores are decimal percentages, that is they must be between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> inclusive (<span class="math notranslate nohighlight">\(0 \leq \alpha_i \leq 1\)</span>) and their sum must be <span class="math notranslate nohighlight">\(1\)</span> (<span class="math notranslate nohighlight">\(\sum_{i} \alpha_i = 1\)</span>). Okay, but where did we get these attention scores from? In our example, we kind of chose them based on what we felt. While we did a pretty good job, this approach doesn’t seem sustainable. Instead, let’s take a look at how word vectors can help solve our problem of determining attention scores.</p>
</section>
<section id="word-vectors-and-similarity">
<h5>Word Vectors and Similarity<a class="headerlink" href="#word-vectors-and-similarity" title="Link to this heading">#</a></h5>
<p>Imagine we represent a word with a vector of numbers. Ideally, the values in the vector should in some way capture the <em>meaning</em> of the word it represents. For example, imagine we have the following word vectors (visualized in 2D space):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;2d_word_vectors_example.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/89ac9cf722c804502a8bae9968cea9e30acb14c221b316f167ecd86695e7a54e.png" src="../_images/89ac9cf722c804502a8bae9968cea9e30acb14c221b316f167ecd86695e7a54e.png" />
</div>
</div>
<p>You can see that words that are <em>similar</em> are clustered together. Fruits are clustered at the top right, vegetables are clustered at the top left, and furniture is clustered at the bottom. In fact, you can even see that the vegetable and fruit clusters are closer to each other than they are to the furniture cluster, since they are more closely related things. You can even imagine doing arithmetic on word vectors. For example, given the words “king”, “queen”, “man”, and “woman” and their respective vector representations <span class="math notranslate nohighlight">\(v_{king}\)</span>, <span class="math notranslate nohighlight">\(v_{queen}\)</span>, <span class="math notranslate nohighlight">\(v_{man}\)</span>, and <span class="math notranslate nohighlight">\(v_{woman}\)</span>, we can imagine that:</p>
<div class="math notranslate nohighlight">
\[v_{queen} - v_{woman} + v_{man} \sim v_{king}\]</div>
<p>That is, the vector for “queen” minus “woman” plus “man” should result in a vector that is <em>similar</em> to the vector for “king”. But what does it exactly mean for two vectors to be <em>similar</em>? In the fruits/vegetables example, we were using distance as a measure of similarity (in particular, <a class="reference external" href="https://en.wikipedia.org/wiki/Euclidean_distance">euclidean distance</a>). There are also <a class="reference external" href="https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa">other ways to measure similarity between two vectors</a>, each with its own advantages and disadvantages. Possibly the simplest measure of similarity between two vectors is their dot product:</p>
<div class="math notranslate nohighlight">
\[\textbf{v} \cdot \textbf{w} = \sum_{i} v_i w_i\]</div>
<p><a class="reference external" href="https://www.youtube.com/watch?v=LyGKycYT2v0">3blue1brown has a great video on the intuition behind dot product</a>, but for our purposes all we need to know is:</p>
<ul class="simple">
<li><p>If two vectors are pointing in the same direction, the dot product will be <span class="math notranslate nohighlight">\(&gt; 0\)</span> (i.e. <em>similar</em>)</p></li>
<li><p>If they are pointing in opposing directions, the dot product will be <span class="math notranslate nohighlight">\(&lt; 0\)</span> (i.e. <em>dissimilar</em>)</p></li>
<li><p>If they are exactly perpendicular, the dot product will be <span class="math notranslate nohighlight">\(0\)</span> (i.e. <em>neutral</em>)</p></li>
</ul>
<p>Using this information, we can define a simple heuristic to determine the similarity between two word vectors: The greater the dot product, the more similar two words are in <em>meaning</em>.</p>
<p><em>Note</em>: You’ll note that the magnitude of the vectors have an influence on the output of dot product. For example, given 3 vectors, <span class="math notranslate nohighlight">\(a = [1, 1, 1]\)</span>, <span class="math notranslate nohighlight">\(b = [1000, 0, 0]\)</span>, <span class="math notranslate nohighlight">\(c = [2, 2, 2]\)</span>, our dot product heuristic would tell us that because <span class="math notranslate nohighlight">\(a \cdot b &gt; a \cdot c\)</span>, that <span class="math notranslate nohighlight">\(a\)</span> is more similar to <span class="math notranslate nohighlight">\(c\)</span> than <span class="math notranslate nohighlight">\(a\)</span> is to <span class="math notranslate nohighlight">\(b\)</span>. This doesn’t seem right, since <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are pointing in the exact same direction, while <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(c\)</span> are not. <a class="reference external" href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a> accounts for this normalizing the vectors to unit vectors before taking the dot product, essentially ignoring the magnitudes and only caring about the direction. So why don’t we take the cosine similarity? In a deep learning setting, the magnitude of a vector might actually contain information we care about (and we shouldn’t get rid of it). Also, if we regularize our networks properly, outlier examples like the above should not occur.</p>
<p>Okay cool, but where do these word vectors actually come from? They usually come from some kind of learned embedding or latent representation. That is, initially the word vectors are just random numbers, but as the <strong>nn</strong> is trained, their values are adjusted to become better and better representations for words.</p>
</section>
<section id="attention-scores-using-the-dot-product">
<h5>Attention Scores using the Dot Product<a class="headerlink" href="#attention-scores-using-the-dot-product" title="Link to this heading">#</a></h5>
<p>Let’s return to our example of fruits, but this time around using word vectors to represent our words. That is <span class="math notranslate nohighlight">\(\textbf{q} = \textbf{v}_{fruit}\)</span> and <span class="math notranslate nohighlight">\(\textbf{k} = [\textbf{v}_{apple} \textbf{v}_{banana} \textbf{v}_{chair}]\)</span>, such that <span class="math notranslate nohighlight">\(\textbf{v} \in \mathbb{R}^{d_k}\)</span> (that is each vector has the same dimensionality of <span class="math notranslate nohighlight">\(d_k\)</span>, which is a value we choose when training a <strong>nn</strong>). Using our new dot product similarity measure, we can compute the similarity between the query and the <span class="math notranslate nohighlight">\(ith\)</span> key as:</p>
<div class="math notranslate nohighlight">
\[\textbf{x}_i = \textbf{q} \cdot \textbf{k}_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\textbf{a}_i\)</span> is the attention score for the <span class="math notranslate nohighlight">\(ith\)</span> key-value pair.  Generalizing this further, we can compute the dot product for all <span class="math notranslate nohighlight">\(n_k\)</span> keys with:</p>
<div class="math notranslate nohighlight">
\[\textbf{x} = \textbf{q} \cdot \textbf{K}^T\]</div>
<p>where <span class="math notranslate nohighlight">\(\textbf{x}\)</span> is our vector of dot products <span class="math notranslate nohighlight">\(\textbf{x} = [x_1, x_2, ..., x_{n_k - 1}, x_{n_k}]\)</span> and <span class="math notranslate nohighlight">\(K\)</span> is a row-wise matrix of our key vectors (i.e. our key vectors stacked on-top of each-other to form a <span class="math notranslate nohighlight">\(n_k \times d_k\)</span> matrix such that <span class="math notranslate nohighlight">\(k_i\)</span> is the <span class="math notranslate nohighlight">\(ith\)</span> row of <span class="math notranslate nohighlight">\(K\)</span>). If you’re having trouble understanding this, here’s an explanation:</p>
<p>Basically, instead of computing each dot product separately:</p>
<div class="math notranslate nohighlight">
\[x_1 = \textbf{q} \cdot \textbf{k}_1 = [2, 1, 3] \cdot [-1, 2, -1] = -3\]</div>
<div class="math notranslate nohighlight">
\[x_2 = \textbf{q} \cdot \textbf{k}_2 = [2, 1, 3] \cdot [1.5, 0, -1] = 0\]</div>
<div class="math notranslate nohighlight">
\[x_3 = \textbf{q} \cdot \textbf{k}_3 = [2, 1, 3] \cdot [4, -2, -1] = 3\]</div>
<p>You compute it all at once:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\textbf{x} = \textbf{q} \cdot \textbf{K}^T = [2, 1, 3] \cdot \begin{bmatrix} -1 &amp; 2 &amp; -1 \\ 1.5 &amp; 0 &amp; -1 \\ 4 &amp; -2 &amp; -1 \end{bmatrix}^T = [2, 1, 3] \cdot \begin{bmatrix} -1 &amp; 1.5 &amp; 4 \\ 2 &amp; 0 &amp; -2 \\ -1 &amp; -1 &amp; -1 \end{bmatrix} = [-3, 0, 3] = [x_1, x_2, x_3]\end{split}\]</div>
<p>Now, recall that our attention scores need to be decimal percentages (between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> and sum to <span class="math notranslate nohighlight">\(1\)</span>). Our dot product values however can be any real number (i.e. between <span class="math notranslate nohighlight">\(-\infty\)</span> and <span class="math notranslate nohighlight">\(\infty\)</span>). To transform our dot product values to decimal percentages, we’ll use the softmax function:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\]</div>
<p>e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">softmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.86482256, 0.00582713, 0.12935032])
</pre></div>
</div>
</div>
</div>
<p>Notice:</p>
<ul class="simple">
<li><p>✅ Each number is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span></p></li>
<li><p>✅ The numbers sum to <span class="math notranslate nohighlight">\(1\)</span></p></li>
<li><p>✅ The larger valued inputs get more “weight”</p></li>
<li><p>✅ The sorted order is preserved (i.e. the <span class="math notranslate nohighlight">\(4.0\)</span> is still the largest after softmax, and <span class="math notranslate nohighlight">\(-1.0\)</span> is still the lowest), this is because softmax is a <a class="reference external" href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic function</a></p></li>
</ul>
<p>This satisfies all the desired properties of an attention scores. Thus, we can compute the attention score for the <span class="math notranslate nohighlight">\(ith\)</span> key-value pair with:</p>
<div class="math notranslate nohighlight">
\[a_i = \text{softmax}(x)_i = \text{softmax}(\textbf{q} \textbf{K}^T)_i\]</div>
<p>Plugging this into our weighted sum we get:</p>
<div class="math notranslate nohighlight">
\[\sum_{i} a_i v_i = \sum_{i} \text{softmax}(\textbf{x})_i v_i = \sum_{i} \text{softmax}(\textbf{q} \textbf{K}^T)_i v_i = \text{softmax}(\textbf{q} \textbf{K}^T) \textbf{v}\]</div>
<p>Note: In the last step, we pack our values into a vector <span class="math notranslate nohighlight">\(\textbf{v} = [v_1, v_2, ..., v_{n_k - 1}, v_{n_k}]\)</span>, which allows us to get rid of the summation notation in favor of a dot product. And that’s it, we have a full working definition for attention:</p>
<div class="math notranslate nohighlight">
\[\text{attention}(\textbf{q}, K, \textbf{v}) = \text{softmax}(\textbf{qK}^T)\textbf{v}\]</div>
<p>In code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_word_vector</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">d_k</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hypothetical mapping that returns a word vector of size</span>
<span class="sd">    d_k for the given word. For demonstrative purposes, we initialize</span>
<span class="sd">    this vector randomly, but in practice this would come from a learned</span>
<span class="sd">    embedding or some kind of latent representation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d_k</span><span class="p">,))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># assumes q is a vector of shape (d_k)</span>
    <span class="c1"># assumes K is a matrix of shape (n_k, d_k)</span>
    <span class="c1"># assumes v is a vector of shape (n_k)</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>


<span class="k">def</span><span class="w"> </span><span class="nf">kv_lookup</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">attention</span><span class="p">(</span>
        <span class="n">q</span><span class="o">=</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">query</span><span class="p">),</span>
        <span class="n">K</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">]),</span>
        <span class="n">v</span><span class="o">=</span><span class="n">values</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># returns some float number</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kv_lookup</span><span class="p">(</span><span class="s2">&quot;fruit&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;apple&quot;</span><span class="p">,</span> <span class="s2">&quot;banana&quot;</span><span class="p">,</span> <span class="s2">&quot;chair&quot;</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.589311199486881
</pre></div>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention">
<h5>Scaled Dot Product Attention<a class="headerlink" href="#scaled-dot-product-attention" title="Link to this heading">#</a></h5>
<p>In principle, the attention equation we derived in the last section is complete. However, we’ll need to make a couple of changes to match the version in <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a>.</p>
<p><strong>Values as Vectors</strong></p>
<p>Currently, our values in the key-value pairs are just numbers. However, we could also instead replace them with vectors of some size <span class="math notranslate nohighlight">\(d_v\)</span>. For example, with <span class="math notranslate nohighlight">\(d_v = 4\)</span>, you might have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;apple&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
    <span class="s2">&quot;banana&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
    <span class="s2">&quot;chair&quot;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When we compute our output via a weighted sum, we’d be doing a weighted sum over vectors instead of numbers (i.e. scalar-vector multiplication instead of scalar-scalar multiplication). This is desirable because vectors let us hold/convey more information than just a single number. To adjust for this change in our equation, instead of multiplying our attention scores by a vector <span class="math notranslate nohighlight">\(v\)</span>, we multiply it by the row-wise matrix of our value vectors <span class="math notranslate nohighlight">\(V\)</span> (similar to how we stacked our keys to form <span class="math notranslate nohighlight">\(K\)</span>):</p>
<div class="math notranslate nohighlight">
\[\text{attention}(\textbf{q}, K, V) = \text{softmax}(\textbf{qK}^T)V\]</div>
<p>Of course, our output is no longer a scalar, instead it would be a vector of dimensionality <span class="math notranslate nohighlight">\(d_v\)</span>.</p>
<p><strong>Scaling</strong></p>
<p>The dot product between our query and keys can get really large in magnitude if <span class="math notranslate nohighlight">\(d_k\)</span> is large. This makes the output of softmax more <em>extreme</em>. For example, <code class="docutils literal notranslate"><span class="pre">softmax([3,</span> <span class="pre">2,</span> <span class="pre">1])</span> <span class="pre">=</span> <span class="pre">[0.665,</span> <span class="pre">0.244,</span> <span class="pre">0.090]</span></code>, but with larger values <code class="docutils literal notranslate"><span class="pre">softmax([30,</span> <span class="pre">20,</span> <span class="pre">10])</span> <span class="pre">=</span> <span class="pre">[9.99954600e-01,</span> <span class="pre">4.53978686e-05,</span> <span class="pre">2.06106005e-09]</span></code>. When training a <strong>nn</strong>, this would mean the gradients would become really small which is undesirable. As a solution, we scale our pre-softmax scores by <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_k}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{attention}(\textbf{q}, K, V) = \text{softmax}(\frac{\textbf{qK}^T}{\sqrt{d_k}})V\]</div>
<p><strong>Multiple Queries</strong></p>
<p>In practice, we often want to perform multiple lookups for <span class="math notranslate nohighlight">\(n_q\)</span> different queries rather than just a single query. Of course, we could always do this one at a time, plugging each query individually into the above equation. However, if we stack of query vectors row-wise as a matrix <span class="math notranslate nohighlight">\(Q\)</span> (in the same way we did for <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span>), we can compute our output as an <span class="math notranslate nohighlight">\(n_q \times d_v\)</span> matrix where row <span class="math notranslate nohighlight">\(i\)</span> is the output vector for the attention on the <span class="math notranslate nohighlight">\(ith\)</span> query:</p>
<div class="math notranslate nohighlight">
\[\text{attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</div>
<p>that is, $<span class="math notranslate nohighlight">\(\text{attention}(Q, K, V)_i = \text{attention}(q_i, K, V)\)</span>$</p>
<p>This makes computation faster than if we ran attention for each query sequentially (say, in a for loop) since we can parallelize calculations (particularly when using a GPU). Note, our input to softmax becomes a matrix instead of a vector. When we write softmax here, we mean that we are taking the softmax along each row of the matrix independently, as if we were doing things sequentially.</p>
<p><strong>Result</strong></p>
<p>With that, we have our final equation for scaled dot product attention as it’s written in the <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">original transformer paper</a>:</p>
<div class="math notranslate nohighlight">
\[\text{attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</div>
<p>In code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>  <span class="c1"># [n_q, d_k], [n_k, d_k], [n_k, d_v] -&gt; [n_q, d_v]</span>
    <span class="c1"># assumes q is a matrix of shape [n_q, d_k]</span>
    <span class="c1"># assumes k is a matrix of shape [n_k, d_k]</span>
    <span class="c1"># assumes v is a matrix of shape [n_k, d_v]</span>
    <span class="c1"># output is a matrix of shape [n_q, d_v]</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">))</span> <span class="o">@</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="self">
<h4>Self<a class="headerlink" href="#self" title="Link to this heading">#</a></h4>
<p>When <code class="docutils literal notranslate"><span class="pre">q</span></code>, <code class="docutils literal notranslate"><span class="pre">k</span></code>, and <code class="docutils literal notranslate"><span class="pre">v</span></code> all come from the same source, we are performing <a class="reference external" href="https://lilianweng.github.io/posts/2018-06-24-attention/#self-attention">self-attention</a> (i.e. letting our input sequence attend to itself):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="k">return</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For example, if our input is <code class="docutils literal notranslate"><span class="pre">Jay</span> <span class="pre">went</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">store,</span> <span class="pre">he</span> <span class="pre">bought</span> <span class="pre">10</span> <span class="pre">apples.</span></code>, we would be letting the word <code class="docutils literal notranslate"><span class="pre">he</span></code> attend to all the other words, including <code class="docutils literal notranslate"><span class="pre">Jay</span></code>, meaning the model can learn to recognize that <code class="docutils literal notranslate"><span class="pre">he</span></code> is referring to <code class="docutils literal notranslate"><span class="pre">Jay</span></code>. We can enhance self attention by introducing projections for <code class="docutils literal notranslate"><span class="pre">q</span></code>, <code class="docutils literal notranslate"><span class="pre">k</span></code>, <code class="docutils literal notranslate"><span class="pre">v</span></code> and the attention output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_k</span><span class="p">,</span> <span class="n">w_q</span><span class="p">,</span> <span class="n">w_v</span><span class="p">,</span> <span class="n">w_proj</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># qkv projections</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_q</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_k</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_v</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># perform self attention</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># out projection</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_proj</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>This enables our model to learn a mapping for <code class="docutils literal notranslate"><span class="pre">q</span></code>, <code class="docutils literal notranslate"><span class="pre">k</span></code>, and <code class="docutils literal notranslate"><span class="pre">v</span></code> that best helps attention distinguish relationships between inputs. We can reduce the number of matrix multiplication from <span class="math notranslate nohighlight">\(4\)</span> to just <span class="math notranslate nohighlight">\(2\)</span> if we combine <code class="docutils literal notranslate"><span class="pre">w_q</span></code>, <code class="docutils literal notranslate"><span class="pre">w_k</span></code> and <code class="docutils literal notranslate"><span class="pre">w_v</span></code> into a single matrix <code class="docutils literal notranslate"><span class="pre">w_fc</span></code>, perform the projection, and then split the result:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_fc</span><span class="p">,</span> <span class="n">w_proj</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># qkv projections</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_fc</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, 3*n_embd] -&gt; [n_seq, 3*n_embd]</span>

    <span class="c1"># split into qkv</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]</span>

    <span class="c1"># perform self attention</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># out projection</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_proj</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>This is a bit more efficient as modern accelerators (GPUs) can take better advantage of one large matrix multiplication rather than <span class="math notranslate nohighlight">\(3\)</span> separate small ones happening sequentially. Finally, we add bias vectors to match the implementation of <strong>GPT</strong>-2, use our <code class="docutils literal notranslate"><span class="pre">linear</span></code> function, and rename our parameters to match our <code class="docutils literal notranslate"><span class="pre">params</span></code> dictionary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c_attn</span><span class="p">,</span> <span class="n">c_proj</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># qkv projections</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_attn</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>

    <span class="c1"># split into qkv</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]</span>

    <span class="c1"># perform self attention</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># out projection</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_proj</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Recall, from our <code class="docutils literal notranslate"><span class="pre">params</span></code> dictionary, our <code class="docutils literal notranslate"><span class="pre">attn</span></code> params look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;attn&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;c_attn&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">n_embd</span><span class="p">]},</span>
    <span class="s2">&quot;c_proj&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">],</span> <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">]},</span>
<span class="p">},</span>
</pre></div>
</div>
</section>
<section id="causal">
<h4>Causal<a class="headerlink" href="#causal" title="Link to this heading">#</a></h4>
<p>There is a bit of an issue with our current self-attention setup, our inputs can see into the future! For example, if our input is <code class="docutils literal notranslate"><span class="pre">[&quot;not&quot;,</span> <span class="pre">&quot;all&quot;,</span> <span class="pre">&quot;heroes&quot;,</span> <span class="pre">&quot;wear&quot;,</span> <span class="pre">&quot;capes&quot;]</span></code>, during self attention we are allowing <code class="docutils literal notranslate"><span class="pre">&quot;wear&quot;</span></code> to see <code class="docutils literal notranslate"><span class="pre">&quot;capes&quot;</span></code>. This means our output probabilities for <code class="docutils literal notranslate"><span class="pre">&quot;wear&quot;</span></code> will be biased since the model already knows the correct answer is <code class="docutils literal notranslate"><span class="pre">&quot;capes&quot;</span></code>. This is no good since our model will just learn that the correct answer for input <span class="math notranslate nohighlight">\(i\)</span> can be taken from input <span class="math notranslate nohighlight">\(i+1\)</span>. To prevent this, we need to somehow modify our attention matrix to <em>hide</em> or <strong>mask</strong> our inputs from being able to see into the future. For example, let’s pretend our attention matrix looks like this:</p>
<div class="highlight-js notranslate"><div class="highlight"><pre><span></span><span class="w">       </span><span class="nx">not</span><span class="w">    </span><span class="nx">all</span><span class="w">    </span><span class="nx">heroes</span><span class="w"> </span><span class="nx">wear</span><span class="w">   </span><span class="nx">capes</span>
<span class="w">   </span><span class="nx">not</span><span class="w"> </span><span class="mf">0.116</span><span class="w">  </span><span class="mf">0.159</span><span class="w">  </span><span class="mf">0.055</span><span class="w">  </span><span class="mf">0.226</span><span class="w">  </span><span class="mf">0.443</span>
<span class="w">   </span><span class="nx">all</span><span class="w"> </span><span class="mf">0.180</span><span class="w">  </span><span class="mf">0.397</span><span class="w">  </span><span class="mf">0.142</span><span class="w">  </span><span class="mf">0.106</span><span class="w">  </span><span class="mf">0.175</span>
<span class="nx">heroes</span><span class="w"> </span><span class="mf">0.156</span><span class="w">  </span><span class="mf">0.453</span><span class="w">  </span><span class="mf">0.028</span><span class="w">  </span><span class="mf">0.129</span><span class="w">  </span><span class="mf">0.234</span>
<span class="w">  </span><span class="nx">wear</span><span class="w"> </span><span class="mf">0.499</span><span class="w">  </span><span class="mf">0.055</span><span class="w">  </span><span class="mf">0.133</span><span class="w">  </span><span class="mf">0.017</span><span class="w">  </span><span class="mf">0.295</span>
<span class="w"> </span><span class="nx">capes</span><span class="w"> </span><span class="mf">0.089</span><span class="w">  </span><span class="mf">0.290</span><span class="w">  </span><span class="mf">0.240</span><span class="w">  </span><span class="mf">0.228</span><span class="w">  </span><span class="mf">0.153</span>
</pre></div>
</div>
<p>Each row corresponds to a query and the columns to a key. In this case, looking at the row for <code class="docutils literal notranslate"><span class="pre">wear</span></code>, you can see that it is attending to <code class="docutils literal notranslate"><span class="pre">capes</span></code> in the last column with a weight of <span class="math notranslate nohighlight">\(0.295\)</span>. To prevent this, we want to set that entry to <span class="math notranslate nohighlight">\(0.000\)</span>:</p>
<div class="highlight-js notranslate"><div class="highlight"><pre><span></span><span class="w">        </span><span class="nx">not</span><span class="w">    </span><span class="nx">all</span><span class="w">    </span><span class="nx">heroes</span><span class="w"> </span><span class="nx">wear</span><span class="w">   </span><span class="nx">capes</span>
<span class="w">   </span><span class="nx">not</span><span class="w"> </span><span class="mf">0.116</span><span class="w">  </span><span class="mf">0.159</span><span class="w">  </span><span class="mf">0.055</span><span class="w">  </span><span class="mf">0.226</span><span class="w">  </span><span class="mf">0.443</span>
<span class="w">   </span><span class="nx">all</span><span class="w"> </span><span class="mf">0.180</span><span class="w">  </span><span class="mf">0.397</span><span class="w">  </span><span class="mf">0.142</span><span class="w">  </span><span class="mf">0.106</span><span class="w">  </span><span class="mf">0.175</span>
<span class="nx">heroes</span><span class="w"> </span><span class="mf">0.156</span><span class="w">  </span><span class="mf">0.453</span><span class="w">  </span><span class="mf">0.028</span><span class="w">  </span><span class="mf">0.129</span><span class="w">  </span><span class="mf">0.234</span>
<span class="w">  </span><span class="nx">wear</span><span class="w"> </span><span class="mf">0.499</span><span class="w">  </span><span class="mf">0.055</span><span class="w">  </span><span class="mf">0.133</span><span class="w">  </span><span class="mf">0.017</span><span class="w">  </span><span class="mf">0.000</span>
<span class="w"> </span><span class="nx">capes</span><span class="w"> </span><span class="mf">0.089</span><span class="w">  </span><span class="mf">0.290</span><span class="w">  </span><span class="mf">0.240</span><span class="w">  </span><span class="mf">0.228</span><span class="w">  </span><span class="mf">0.153</span>
</pre></div>
</div>
<p>In general, to prevent all the queries in our input from looking into the future, we set all positions <span class="math notranslate nohighlight">\(i, j\)</span> where <span class="math notranslate nohighlight">\(j &gt; i\)</span> to <span class="math notranslate nohighlight">\(0.000\)</span>:</p>
<div class="highlight-js notranslate"><div class="highlight"><pre><span></span><span class="w">        </span><span class="nx">not</span><span class="w">    </span><span class="nx">all</span><span class="w">    </span><span class="nx">heroes</span><span class="w"> </span><span class="nx">wear</span><span class="w">   </span><span class="nx">capes</span>
<span class="w">   </span><span class="nx">not</span><span class="w"> </span><span class="mf">0.116</span><span class="w">  </span><span class="mf">0.000</span><span class="w">  </span><span class="mf">0.000</span><span class="w">  </span><span class="mf">0.000</span><span class="w">  </span><span class="mf">0.000</span>
<span class="w">   </span><span class="nx">all</span><span class="w"> </span><span class="mf">0.180</span><span class="w">  </span><span class="mf">0.397</span><span class="w">  </span><span class="mf">0.000</span><span class="w">  </span><span class="mf">0.000</span><span class="w">  </span><span class="mf">0.000</span>
<span class="nx">heroes</span><span class="w"> </span><span class="mf">0.156</span><span class="w">  </span><span class="mf">0.453</span><span class="w">  </span><span class="mf">0.028</span><span class="w">  </span><span class="mf">0.000</span><span class="w">  </span><span class="mf">0.000</span>
<span class="w">  </span><span class="nx">wear</span><span class="w"> </span><span class="mf">0.499</span><span class="w">  </span><span class="mf">0.055</span><span class="w">  </span><span class="mf">0.133</span><span class="w">  </span><span class="mf">0.017</span><span class="w">  </span><span class="mf">0.000</span>
<span class="w"> </span><span class="nx">capes</span><span class="w"> </span><span class="mf">0.089</span><span class="w">  </span><span class="mf">0.290</span><span class="w">  </span><span class="mf">0.240</span><span class="w">  </span><span class="mf">0.228</span><span class="w">  </span><span class="mf">0.153</span>
</pre></div>
</div>
<p>We call this <strong>masking</strong>. One issue with our above <strong>masking</strong> approach is our rows no longer sum to <span class="math notranslate nohighlight">\(1\)</span> (since we are setting them to <span class="math notranslate nohighlight">\(0\)</span> after the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> has been applied). To make sure our rows still sum to <span class="math notranslate nohighlight">\(1\)</span>, we need to modify our attention matrix before the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> is applied. This can be achieved by setting entries that are to be masked to <span class="math notranslate nohighlight">\(-\infty\)</span> prior to the <code class="docutils literal notranslate"><span class="pre">softmax</span></code>.</p>
<p><em>Note</em>: If you’re not convinced, stare at the softmax equation and convince yourself this is true (maybe even pull out a pen and paper):</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(\vec{x})_{i} = \frac{e^{x_i}}{\sum_{j} e^{x_j}}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span>
<span class="p">):</span>  <span class="c1"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">mask</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">mask</span></code> is the matrix (for <code class="docutils literal notranslate"><span class="pre">n_seq=5</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="o">-</span><span class="mf">1e10</span> <span class="o">-</span><span class="mf">1e10</span> <span class="o">-</span><span class="mf">1e10</span> <span class="o">-</span><span class="mf">1e10</span>
<span class="mi">0</span>   <span class="mi">0</span>   <span class="o">-</span><span class="mf">1e10</span> <span class="o">-</span><span class="mf">1e10</span> <span class="o">-</span><span class="mf">1e10</span>
<span class="mi">0</span>   <span class="mi">0</span>     <span class="mi">0</span>   <span class="o">-</span><span class="mf">1e10</span> <span class="o">-</span><span class="mf">1e10</span>
<span class="mi">0</span>   <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>   <span class="o">-</span><span class="mf">1e10</span>
<span class="mi">0</span>   <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
</pre></div>
</div>
<p>We use <code class="docutils literal notranslate"><span class="pre">-1e10</span></code> instead of <code class="docutils literal notranslate"><span class="pre">-np.inf</span></code> as <code class="docutils literal notranslate"><span class="pre">-np.inf</span></code> can cause <code class="docutils literal notranslate"><span class="pre">nan</span></code>s. Adding <code class="docutils literal notranslate"><span class="pre">mask</span></code> to our attention matrix instead of just explicitly setting the values to <code class="docutils literal notranslate"><span class="pre">-1e10</span></code> works because practically, any number plus <code class="docutils literal notranslate"><span class="pre">-inf</span></code> is just <code class="docutils literal notranslate"><span class="pre">-inf</span></code>. We can compute the mask matrix in NumPy with <code class="docutils literal notranslate"><span class="pre">(1</span> <span class="pre">-</span> <span class="pre">np.tri(n_seq))</span> <span class="pre">*</span> <span class="pre">-1e10</span></code>. Putting it all together, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span>
<span class="p">):</span>  <span class="c1"># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">mask</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>


<span class="k">def</span><span class="w"> </span><span class="nf">causal_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c_attn</span><span class="p">,</span> <span class="n">c_proj</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># qkv projections</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_attn</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>

    <span class="c1"># split into qkv</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]</span>

    <span class="c1"># causal mask to hide future inputs from being attended to</span>
    <span class="n">causal_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tri</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e10</span>  <span class="c1"># [n_seq, n_seq]</span>

    <span class="c1"># perform causal self attention</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># out projection</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_proj</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="multi-head">
<h4>Multi-Head<a class="headerlink" href="#multi-head" title="Link to this heading">#</a></h4>
<p>We can further improve our implementation by performing <code class="docutils literal notranslate"><span class="pre">n_head</span></code> separate attention computations, splitting our queries, keys, and values into heads:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c_attn</span><span class="p">,</span> <span class="n">c_proj</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>
    <span class="c1"># qkv projection</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_attn</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]</span>

    <span class="c1"># split into qkv</span>
    <span class="n">qkv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]</span>

    <span class="c1"># split into heads</span>
    <span class="n">qkv_heads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">qkv</span><span class="p">)</span>
    <span class="p">)</span>  <span class="c1"># [3, n_seq, n_embd] -&gt; [3, n_head, n_seq, n_embd/n_head]</span>

    <span class="c1"># causal mask to hide future inputs from being attended to</span>
    <span class="n">causal_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tri</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e10</span>  <span class="c1"># [n_seq, n_seq]</span>

    <span class="c1"># perform attention over each head</span>
    <span class="n">out_heads</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">qkv_heads</span><span class="p">)</span>
    <span class="p">]</span>  <span class="c1"># [3, n_head, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span>

    <span class="c1"># merge heads</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">out_heads</span><span class="p">)</span>  <span class="c1"># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span>

    <span class="c1"># out projection</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">c_proj</span><span class="p">)</span>  <span class="c1"># [n_seq, n_embd] -&gt; [n_seq, n_embd]</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>There are three steps added here:</p>
<ol class="arabic simple">
<li><p>Split <code class="docutils literal notranslate"><span class="pre">q</span></code>, <code class="docutils literal notranslate"><span class="pre">k</span></code>, <code class="docutils literal notranslate"><span class="pre">v</span></code> into <code class="docutils literal notranslate"><span class="pre">n_head</span></code> heads:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># split into heads</span>
<span class="n">qkv_heads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">qkv</span><span class="p">))</span>  <span class="c1"># [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Compute attention for each head:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># perform attention over each head</span>
<span class="n">out_heads</span> <span class="o">=</span> <span class="p">[</span><span class="n">attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">qkv_heads</span><span class="p">)]</span>  <span class="c1"># [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Merge the outputs of each head:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># merge heads</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">out_heads</span><span class="p">)</span>  <span class="c1"># [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]</span>
</pre></div>
</div>
<p>Notice, this reduces the dimension from <code class="docutils literal notranslate"><span class="pre">n_embd</span></code> to <code class="docutils literal notranslate"><span class="pre">n_embd/n_head</span></code> for each attention computation. This is a tradeoff. For reduced dimensionality, our model gets additional subspaces to work when modeling relationships via attention. For example, maybe one attention head is responsible for connecting pronouns to the person the pronoun is referencing. Maybe another might be responsible for grouping sentences by periods. Another could simply be identifying which words are entities, and which are not. Although, it’s probably just another <strong>nn</strong> black box. The code we wrote performs the attention computations over each head sequentially in a loop (one at a time), which is not very efficient. In practice, you’d want to do these in parallel. For simplicity, we’ll just leave this sequential. With that, we’re finally done our <strong>GPT</strong> implementation! Now, all that’s left to do is put it all together and run our code.</p>
</section>
</section>
</section>
<section id="putting-it-all-together">
<h2>Putting it All Together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h2>
<p>Having put everything together, we get the equivalent of <a class="reference external" href="https://github.com/jaymody/picoGPT/blob/main/gpt2.py">gpt2.py</a>, which in its entirety is a mere <span class="math notranslate nohighlight">\(120\)</span> lines of code (<a class="reference external" href="https://github.com/jaymody/picoGPT/blob/a750c145ba4d09d5764806a6c78c71ffaff88e64/gpt2_pico.py#L3-L58"><span class="math notranslate nohighlight">\(60\)</span> lines if you remove comments and whitespace!</a>). All that remains, it to test our implementation by prompting our tiny <strong>GPT</strong>-2 model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">prompt_gpt</span><span class="p">(</span><span class="s2">&quot;Alan Turing theorized that computers would one day become&quot;</span><span class="p">,</span> <span class="n">n_tokens_to_generate</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">output</span> <span class="o">==</span> <span class="s2">&quot; the most powerful machines on the planet.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-02-18 12:27:30.196142: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.
generating: 100%|██████████| 8/8 [00:07&lt;00:00,  1.03it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> the most powerful machines on the planet.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>It works! We can also test that our implementation gives identical results to OpenAI’s official <strong>GPT</strong>-2 repo by building and executing the Docker container as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span><span class="s2">&quot;openai-gpt-2&quot;</span><span class="w"> </span>-<span class="w"> </span><span class="s">&lt;&lt;EOF</span>
<span class="s">FROM tensorflow/tensorflow:1.13.2-py3</span>

<span class="s">ENV DEBIAN_FRONTEND=noninteractive</span>
<span class="s">RUN apt update -y &amp;&amp; apt upgrade -y &amp;&amp; apt install git -y</span>

<span class="s">RUN git clone https://github.com/openai/gpt-2 /gpt-2</span>
<span class="s">WORKDIR /gpt-2</span>

<span class="s">RUN python3 -m pip install --upgrade pip &amp;&amp; python3 -m pip install -r requirements.txt</span>
<span class="s">RUN python3 download_model.py 124M</span>
<span class="s">RUN python3 download_model.py 355M</span>
<span class="s">RUN python3 download_model.py 774M</span>
<span class="s">RUN python3 download_model.py 1558M</span>
<span class="s">EOF</span>

docker<span class="w"> </span>run<span class="w"> </span>-dt<span class="w"> </span>--name<span class="w"> </span><span class="s2">&quot;openai-gpt-2-app&quot;</span><span class="w"> </span>openai-gpt-2
docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span><span class="s2">&quot;openai-gpt-2-app&quot;</span><span class="w"> </span>/bin/bash<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;python3 src/interactive_conditional_samples.py --length 8 --model_type 124M --top_k 1&#39;</span>
</pre></div>
</div>
<p>and then pasting the following when prompted:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;Alan Turing theorized that computers would one day become&quot;</span>
</pre></div>
</div>
<p>This should yield an identical result:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">the</span> <span class="n">most</span> <span class="n">powerful</span> <span class="n">machines</span> <span class="n">on</span> <span class="n">the</span> <span class="n">planet</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="what-next">
<h2>What Next?<a class="headerlink" href="#what-next" title="Link to this heading">#</a></h2>
<p>This implementation is cool and all, but it’s missing a ton of bells and whistles:</p>
<section id="gpu-tpu-support">
<h3>GPU/TPU Support<a class="headerlink" href="#gpu-tpu-support" title="Link to this heading">#</a></h3>
<p>Replace NumPy with <a class="reference external" href="https://github.com/google/jax">JAX</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
<p>That’s it. You can now use the code with GPUs and even <a class="reference external" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">TPUs</a>! Just make sure you <a class="reference external" href="https://github.com/google/jax#installation">install JAX correctly</a>.</p>
</section>
<section id="backprop">
<h3><strong>Backprop</strong><a class="headerlink" href="#backprop" title="Link to this heading">#</a></h3>
<p>Again, if we replace NumPy with JAX:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
<p>Then computing the gradients is as easy as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">lm_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">gpt2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)[</span><span class="n">y</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">lm_loss</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">n_head</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="batching">
<h3>Batching<a class="headerlink" href="#batching" title="Link to this heading">#</a></h3>
<p>Once again, if we replace NumPy with JAX:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
<p>Then, making our <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> function batched is as easy as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_batched</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">gpt2</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
<span class="n">gpt2_batched</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">)</span>  <span class="c1"># [batch, seq_len] -&gt; [batch, seq_len, vocab]</span>
</pre></div>
</div>
</section>
<section id="jax-test">
<h3>JAX test<a class="headerlink" href="#jax-test" title="Link to this heading">#</a></h3>
<p>Let’s verify that switching to JAX is indeed as easy as described:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># all references to np are now references to jax.numpy</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">prompt_gpt</span><span class="p">(</span><span class="s2">&quot;Alan Turing theorized that computers would one day become&quot;</span><span class="p">,</span> <span class="n">n_tokens_to_generate</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">output</span> <span class="o">==</span> <span class="s2">&quot; the most powerful machines on the planet.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-02-18 12:27:38.750777: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.
generating: 100%|██████████| 8/8 [00:10&lt;00:00,  1.30s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> the most powerful machines on the planet.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="inference-optimization">
<h3>Inference Optimization<a class="headerlink" href="#inference-optimization" title="Link to this heading">#</a></h3>
<p>Our implementation is quite inefficient. The quickest and most impactful optimization you can make (outside of GPU + batching support) would be to implement a <a class="reference external" href="https://kipp.ly/blog/transformer-inference-arithmetic/#kv-cache">kv cache</a>. Also, we implemented our attention head computations sequentially, when we should really be doing it in parallel. Using JAX, this is as simple as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
</pre></div>
</div>
<p>There’s many many more inference optimizations. Here’s two recommendations as a starting point:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Lillian Weng’s Large Transformer Model Inference Optimization</a></p></li>
<li><p><a class="reference external" href="https://kipp.ly/blog/transformer-inference-arithmetic/">Kipply’s Transformer Inference Arithmetic</a></p></li>
</ol>
</section>
<section id="id1">
<h3>Training<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Training a <strong>GPT</strong> is pretty standard for a <strong>nn</strong> (gradient descent <strong>w.r.t</strong> a <strong>loss</strong> function). Of course, you also need to use the standard bag of tricks when training a <strong>GPT</strong> (i.e. use the Adam optimizer, find the optimal learning rate, regularization via dropout and/or weight decay, use a learning rate scheduler, use the correct weight initialization, batching, etc …). Though the real challenge to training a good <strong>GPT</strong> model is the ability to scale the data and the model. For scaling data, you’ll want a corpus of text that is big, high quality, and diverse.</p>
<ul class="simple">
<li><p>Big means billions of tokens (terabytes of data). For example, check out <a class="reference external" href="https://pile.eleuther.ai/">The Pile</a>, which is an open source pre-training dataset for large language models.</p></li>
<li><p>High quality means you want to filter out duplicate examples, unformatted text, incoherent text, garbage text, etc …</p></li>
<li><p>Diverse means varying sequence lengths, about lots of different topics, from different sources, with differing perspectives, etc … Of course, if there are any biases in the data, it will reflect in the model, so you need to be careful of that as well.</p></li>
</ul>
<p>Scaling the model to billions of parameters involves a cr*p ton of engineering (and money lol). Training frameworks can get <a class="reference external" href="https://github.com/NVIDIA/Megatron-LM">absurdly long and complex</a>. A good place to start would be <a class="reference external" href="https://lilianweng.github.io/posts/2021-09-25-train-large/">Lillian Weng’s How to Train Really Large Models on Many GPUs</a>. On the topic there’s also the <a class="reference external" href="https://arxiv.org/pdf/1909.08053.pdf">NVIDIA’s Megatron Framework</a>, <a class="reference external" href="https://arxiv.org/pdf/2204.06514.pdf">Cohere’s Training Framework</a>, <a class="reference external" href="https://arxiv.org/pdf/2204.02311.pdf">Google’s PALM</a>, the open source <a class="reference external" href="https://github.com/kingoflolz/mesh-transformer-jax">mesh-transformer-jax</a> (used to train EleutherAI’s open source models), and <a class="reference external" href="https://arxiv.org/pdf/2203.15556.pdf">many</a> <a class="reference external" href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">many</a> <a class="reference external" href="https://arxiv.org/pdf/2005.14165.pdf">more</a>.</p>
</section>
<section id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h3>
<p>Oh boy, how does one even evaluate LLMs? Honestly, it’s really hard problem. <a class="reference external" href="https://arxiv.org/abs/2211.09110">HELM</a> is pretty comprehensive and a good place to start, but you should always be skeptical of <a class="reference external" href="https://en.wikipedia.org/wiki/Goodhart%27s_law">benchmarks and evaluation metrics</a>.</p>
</section>
<section id="architecture-improvements">
<h3>Architecture Improvements<a class="headerlink" href="#architecture-improvements" title="Link to this heading">#</a></h3>
<p>You can also take a look at <a class="reference external" href="https://github.com/lucidrains/x-transformers">Phil Wang’s X-Transformer’s</a>. <a class="reference external" href="https://arxiv.org/pdf/2102.11972.pdf">This paper</a> is also a pretty good summary (see Table 1). Facebook’s <a class="reference external" href="https://arxiv.org/pdf/2302.13971.pdf">LLaMA paper</a> is also probably a good reference for standard architecture improvements (at least as of February 2023 it was).</p>
</section>
<section id="stopping-generation">
<h3>Stopping Generation<a class="headerlink" href="#stopping-generation" title="Link to this heading">#</a></h3>
<p>Our current implementation requires us to specify the exact number of tokens we’d like to generate ahead of time. This is not a very good approach as our generations end up being too long, too short, or cutoff mid-sentence. To resolve this, we can introduce a special end of sentence (EOS) token. During pre-training, we append the EOS token to the end of our input (i.e. <code class="docutils literal notranslate"><span class="pre">tokens</span> <span class="pre">=</span> <span class="pre">[&quot;not&quot;,</span> <span class="pre">&quot;all&quot;,</span> <span class="pre">&quot;heroes&quot;,</span> <span class="pre">&quot;wear&quot;,</span> <span class="pre">&quot;capes&quot;,</span> <span class="pre">&quot;.&quot;,</span> <span class="pre">&quot;&lt;|EOS|&gt;&quot;]</span></code>). During generation, we simply stop whenever we encounter the EOS token (or if we hit some maximum sequence length):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">eos_id</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">):</span>
	<span class="n">prompt_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
	<span class="k">while</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">eos_id</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_seq_len</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">next_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">next_id</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="n">prompt_len</span><span class="p">:]</span>
</pre></div>
</div>
<p><strong>GPT</strong>-2 was not pre-trained with an EOS token, so we can’t use this approach in our code, but most LLMs nowadays use an EOS token.</p>
</section>
<section id="fine-tuning">
<h3>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h3>
<p>We briefly touched on fine-tuning in the training section. Recall, fine-tuning is when we re-use the pre-trained weights to train the model on some downstream task. We call this process transfer-learning. In theory, we could use zero-shot or few-shot prompting to get the model to complete our task, however, if you have access to a labelled dataset, fine-tuning a <strong>GPT</strong> is going to yield better results (results that can scale given additional data and higher quality data). There are a couple different topics related to fine-tuning, as described below:</p>
<section id="classification-fine-tuning">
<h4>Classification Fine-tuning<a class="headerlink" href="#classification-fine-tuning" title="Link to this heading">#</a></h4>
<p>In classification fine-tuning, we give the model some text and we ask it to predict which class it belongs to. For example, consider the <a class="reference external" href="https://huggingface.co/datasets/imdb">IMDB dataset</a>, which contains movie reviews that rate the movie as either good, or bad:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>--- Example 1 ---
Text: I wouldn&#39;t rent this one even on dollar rental night.
Label: Bad
--- Example 2 ---
Text: I don&#39;t know why I like this movie so well, but I never get tired of watching it.
Label: Good
--- Example 3 ---
...
</pre></div>
</div>
<p>To fine-tune our model, we replace the language modeling head with a classification head, which we apply to the last token output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gpt2</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">wte</span><span class="p">,</span> <span class="n">wpe</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">ln_f</span><span class="p">,</span> <span class="n">cls_head</span><span class="p">,</span> <span class="n">n_head</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">wte</span><span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">wpe</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">blocks</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">block</span><span class="p">,</span> <span class="n">n_head</span><span class="o">=</span><span class="n">n_head</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_f</span><span class="p">)</span>

	<span class="c1"># project to n_classes</span>
	<span class="c1"># [n_embd] @ [n_embd, n_classes] -&gt; [n_classes]</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">@</span> <span class="n">cls_head</span>
</pre></div>
</div>
<p>We only use the last token output <code class="docutils literal notranslate"><span class="pre">x[-1]</span></code> because we only need to produce a single probability distribution for the entire input instead of <code class="docutils literal notranslate"><span class="pre">n_seq</span></code> distributions as in the case of language modeling. We take the last token in particular (instead of say the first token or a combination of all the tokens) because the last token is the only token that is allowed to attend to the entire sequence and thus has information about the input text as a whole. As per usual, we optimize <strong>w.r.t.</strong> the cross entropy <strong>loss</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">singe_example_loss_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">label</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">gpt</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">label</span><span class="p">])</span> <span class="c1"># cross entropy loss</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="generative-fine-tuning">
<h4>Generative Fine-tuning<a class="headerlink" href="#generative-fine-tuning" title="Link to this heading">#</a></h4>
<p>Some tasks can’t be neatly categorized into classes. For example, consider the task of summarization. We can fine-tune these types of task by simply performing language modeling on the input concatenated with the label. For example, here’s what a single summarization training sample might look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>--- Article ---
This is an article I would like to summarize.
--- Summary ---
This is the summary.
</pre></div>
</div>
<p>We train the model as we do during pre-training (optimize <strong>w.r.t</strong> language modeling <strong>loss</strong>). At predict time, we feed the model the everything up to <code class="docutils literal notranslate"><span class="pre">---</span> <span class="pre">Summary</span> <span class="pre">---</span></code> and then perform auto-regressive language modeling to generate the summary. The choice of the delimiters <code class="docutils literal notranslate"><span class="pre">---</span> <span class="pre">Article</span> <span class="pre">---</span></code> and <code class="docutils literal notranslate"><span class="pre">---</span> <span class="pre">Summary</span> <span class="pre">---</span></code> are arbitrary. How you choose to format the text is up to you, as long as it is consistent between training and inference. Notice, we can also formulate classification tasks as generative tasks (for example with IMDB):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>--- Text ---
I wouldn&#39;t rent this one even on dollar rental night.
--- Label ---
Bad
</pre></div>
</div>
<p>However, this will probably perform worse than doing classification fine-tuning directly (<strong>loss</strong> includes language modeling on the entire sequence, not just the final prediction, so the <strong>loss</strong> specific to the prediction will get diluted)</p>
</section>
<section id="instruction-fine-tuning">
<h4>Instruction Fine-tuning<a class="headerlink" href="#instruction-fine-tuning" title="Link to this heading">#</a></h4>
<p>Most state-of-the-art large language models these days also undergo an additional instruction fine-tuning step after being pre-trained. In this step, the model is fine-tuned (generative) on thousands of instruction prompt + completion pairs that were human labeled. Instruction fine-tuning can also be referred to as supervised fine-tuning, since the data is human labelled (i.e. supervised). So what’s the benefit of instruction fine-tuning? While predicting the next word in a wikipedia article makes the model is good at continuing sentences, it doesn’t make it particularly good at following instructions, or having a conversation, or summarizing a document (all the things we would like a <strong>GPT</strong> to do). Fine-tuning them on human labelled instruction + completion pairs is a way to teach the model how it can be more useful, and make them easier to interact with. This call this AI alignment, as we are aligning the model to do and behave as we want it to. Alignment is an active area of research, and includes more than just following instructions (bias, safety, intent, etc …). What does this instruction data look like exactly? Google’s <a class="reference external" href="https://arxiv.org/pdf/2109.01652.pdf">FLAN</a> models were trained on various academic NLP datasets (which are already human labelled):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;fig_3_from_flan_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/81f19fc5371cc3ff6576b51b00206e90541123e3f66b76376effc049e4bef9ae.png" src="../_images/81f19fc5371cc3ff6576b51b00206e90541123e3f66b76376effc049e4bef9ae.png" />
</div>
</div>
<p>OpenAI’s <a class="reference external" href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT</a> on the other hand was trained on prompts collected from their own API. They then paid workers to write completions for those prompts. Here’s a breakdown of the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;table_1_and_2_from_instructgpt_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7101c3b9d0b97bd5f86093702482f391e135f7d3b4630deeaac224afeae405d4.png" src="../_images/7101c3b9d0b97bd5f86093702482f391e135f7d3b4630deeaac224afeae405d4.png" />
</div>
</div>
</section>
<section id="parameter-efficient-fine-tuning">
<h4>Parameter Efficient Fine-tuning<a class="headerlink" href="#parameter-efficient-fine-tuning" title="Link to this heading">#</a></h4>
<p>When we talk about fine-tuning in the above sections, it is assumed that we are updating all of the model parameters. While this yields the best performance, it is costly both in terms of compute (need to back propagate over the entire model) and in terms of storage (for each fine-tuned model, you need to store a completely new copy of the parameters). For instruction fine-tuning, this is fine, we want maximum performance, but if you then wanted to fine-tune 100 different models for various downstream tasks, then you’d have a problem. The most simple approach to this problem is to only update the head and freeze (i.e. make untrainable) the rest of the model. This would speed up training and greatly reduce the number of new parameters, however it would not perform nearly as well as a full fine-tune (we are lacking the deep in deep learning). We could instead selectively freeze specific layers (i.e. freeze all layers except the last 4, or freeze every other layer, or freeze all parameters except multi-head attention parameters), which would help restore some of the depth. This will perform a lot better, but we become a lot less parameter efficient and reduce our training speed ups. Instead, we can utilize parameter-efficient fine-tuning (PEFT) methods. PEFT is active area of research, and there are <a class="reference external" href="https://aclanthology.org/2021.emnlp-main.243.pdf">lots</a> <a class="reference external" href="https://arxiv.org/pdf/2110.07602.pdf">of</a> <a class="reference external" href="https://arxiv.org/pdf/2101.00190.pdf">different</a> <a class="reference external" href="https://arxiv.org/pdf/2103.10385.pdf">methods</a> <a class="reference external" href="https://arxiv.org/pdf/2106.09685.pdf">to</a> <a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">choose</a> <a class="reference external" href="https://arxiv.org/abs/2205.05638">from</a>. As an example, take the <a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">Adapters paper</a>. In this approach, we add an additional “adapter” layer after the FFN and MHA layers in the transformer block. The adapter layer is just a simple 2 layer fully connected <strong>nn</strong>, where the input and output dimensions are <code class="docutils literal notranslate"><span class="pre">n_embd</span></code>, and the hidden dimension is smaller than <code class="docutils literal notranslate"><span class="pre">n_embd</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;fig_2_from_the_adapters_paper.png&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/979b1b2fafbef671ce9ad3c5fc0ff681724ed7739884e333e7958021382b23cf.png" src="../_images/979b1b2fafbef671ce9ad3c5fc0ff681724ed7739884e333e7958021382b23cf.png" />
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./micrograduate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="makemore5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">6. <strong>makemore</strong> (part 5): building a <strong>WaveNet</strong></p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro">Intro</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-gpt">What is a <strong>GPT</strong>?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-output">Input / Output</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input">Input</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output">Output</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-text">Generating text</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive">Autoregressive</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling">Sampling</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">Prompting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-layers">Basic Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gelu">GELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-architecture"><strong>GPT</strong> Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings">Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embeddings">Token Embeddings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings">Positional Embeddings</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#combined">Combined</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-stack">Decoder Stack</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-to-vocab">Projection to Vocab</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-block">Decoder Block</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#position-wise-feed-forward-network">Position-wise Feed Forward Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-causal-self-attention">Multi-Head Causal Self Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-lookups">Key-Value Lookups</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-value-lookups-based-on-meaning">Key-Value Lookups based on Meaning</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#word-vectors-and-similarity">Word Vectors and Similarity</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-scores-using-the-dot-product">Attention Scores using the Dot Product</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled Dot Product Attention</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#self">Self</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#causal">Causal</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head">Multi-Head</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting it All Together</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-next">What Next?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-tpu-support">GPU/TPU Support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backprop"><strong>Backprop</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batching">Batching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jax-test">JAX test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-optimization">Inference Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-improvements">Architecture Improvements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-generation">Stopping Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-fine-tuning">Classification Fine-tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-fine-tuning">Generative Fine-tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#instruction-fine-tuning">Instruction Fine-tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning">Parameter Efficient Fine-tuning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christos Karaneen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>