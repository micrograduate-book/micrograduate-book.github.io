{"version":3,"kind":"Notebook","sha256":"368ea204c15533451af249185ed405308b9b8c5bc6868922427213442e93bb46","slug":"micrograduate.makemore2","location":"/micrograduate/makemore2.ipynb","dependencies":[],"frontmatter":{"title":"3. makemore (part 2): mlp","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"micrograduate-env","language":"python"},"github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","source_url":"https://github.com/ckaraneen/micrograduate/blob/main/micrograduate/makemore2.ipynb","edit_url":"https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore2.ipynb","exports":[{"format":"ipynb","filename":"makemore2.ipynb","url":"/build/makemore2-5200ced13f3b4ff9e6870611b09307f6.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt","key":"ehVVBToHIw"},{"type":"outputs","id":"egp5P9uM6nppgDSw22caI","children":[],"key":"L1g4gtwJxB"}],"key":"jfnsnsWW4q"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Intro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uoYshtIQ9c"}],"identifier":"intro","label":"Intro","html_id":"intro","implicit":true,"key":"rYm2BYXya8"}],"key":"MsORjiXDFy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Time to make more out of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ho3yoC7urv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yNSmNe14HE"}],"key":"edOued5pW1"},{"type":"text","value":"! In the last lesson, we implemented the bigram language model, both using counts and a super simple, 1-linear-layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T98MSMwHHe"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"is0Y54avco"}],"key":"XUDQmMYi2e"},{"type":"text","value":". How we approach training is that we looked only at the single previous character and we predicted a distribution for the character coming next in the sequence. We did that by taking counts and normalizing them into probabilities so that each row in the count matrix sums to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oeDZAeiuuH"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qapg70ipgP"},{"type":"text","value":". This method is great if you only have one character of previous context. The problem with that model though is that predictions are not very good. Another problem, if we are to take more context into account, is that the counts in the matrix grow exponentially as we increase the length of the context. For just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FKRPsXsiK6"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gGS3QrGawE"},{"type":"text","value":" character of context we have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tqfKCYFX0c"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IkrmRGGIMt"},{"type":"text","value":" rows, each representing the next possible character. For ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FjpsxTOJbh"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FfCENHDdkV"},{"type":"text","value":" characters, the number of rows would grow to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vqgjaXxCVK"},{"type":"inlineMath","value":"27 \\cdot 27 = 729","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>27</mn><mo>⋅</mo><mn>27</mn><mo>=</mo><mn>729</mn></mrow><annotation encoding=\"application/x-tex\">27 \\cdot 27 = 729</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">729</span></span></span></span>","key":"UUO7aYqPpH"},{"type":"text","value":". Whereas for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mXXRLtFvZc"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ENZENKAAx7"},{"type":"text","value":" characters, it would explode to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JbU4obsycR"},{"type":"inlineMath","value":"27 \\cdot 27 \\cdot 27 = 19683","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>27</mn><mo>⋅</mo><mn>27</mn><mo>⋅</mo><mn>27</mn><mo>=</mo><mn>19683</mn></mrow><annotation encoding=\"application/x-tex\">27 \\cdot 27 \\cdot 27 = 19683</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">27</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">19683</span></span></span></span>","key":"JIruTO5eHQ"},{"type":"text","value":", and so on. This solution simply doesn’t scale well and explodes. That is why we are going to move on and instead implement an ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DfoLYhAseK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Zo6awTnfK2"}],"key":"lEiQIN3lES"},{"type":"text","value":" model to predict the next character in a sequence.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oDJ1pnwcFQ"}],"key":"DqcbkvjfGY"}],"key":"dY8v7nl3Zz"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Building a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v2ssHeItqg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UslcCr8i1V"}],"key":"KUKRgsw1T8"},{"type":"text","value":" language model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"glaAh9QTGg"}],"identifier":"building-a-mlp-language-model","label":"Building a mlp language model","html_id":"building-a-mlp-language-model","implicit":true,"key":"SkAqzLpEKt"}],"key":"WGnE188n4a"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The modeling approach we are going to adopt follows ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i51X3aj7iU"},{"type":"link","url":"https://doi.org/10.5555/944919.944966","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Bengio et al. 2003","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ClA6QhL8ip"}],"urlSource":"https://dl.acm.org/doi/10.5555/944919.944966","data":{"doi":"10.5555/944919.944966"},"internal":false,"protocol":"doi","key":"Fmnal8hTfG"},{"type":"text","value":", an important paper that we are going to implement. Although they implement a word-level language model, we are going to stick to our character-level language model, but follow the same approach. The authors propose associating each and every word (out of e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V7Jsr7Ol4O"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EMWqnd8lHt"},{"type":"text","value":") with a feature vector (e.g. of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OKUDiA4l3d"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WPhasFfrCB"},{"type":"text","value":" dimensions). In other words, every word is a point that is embedded into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WIAzCd30FY"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ySQxOwm4Yl"},{"type":"text","value":"-dimensional space. You can think of it this way. We have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"am8BEW5pry"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I3JfIC4keO"},{"type":"text","value":" point-vectors in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yM5SlsC1pV"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v8kHe08tI5"},{"type":"text","value":"-dimensional space. As you can imagine, that is very crowded, that’s lots of points for a very small space. Now, in the beginning, these words are initialized completely randomly: they are spread out at random. But, then we are going to tune these embeddings of these words using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ScVVkBzKzK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OqUPdfOAN1"}],"key":"ywu9xz7QBK"},{"type":"text","value":". So during the course of training of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Oi1mMBOENS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KtO8cAynvU"}],"key":"hxCJ5ZbD4a"},{"type":"text","value":", these point-vectors are going to basically be moved around in this space. And you might imagine that, for example, words that have very similar meanings or that are indeed synonyms of each other might end up in a very similar part of the vector space, and, conversely, words with very different meanings will go somewhere else in that space. Now, their modeling approach otherwise is identical to what ours has been so far. They are using a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y7Wt4FKUfh"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SQVWYT7ALA"}],"key":"pYFsNKbpu6"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YYh0p68TEG"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gU233LTjWy"}],"key":"LPPSFky8Cl"},{"type":"text","value":" to predict the next word, given the previous words and to train the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rB4XoHVWIv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KiHThhwNZq"}],"key":"SSx4ZM1JJO"},{"type":"text","value":" they are maximizing the log-likehood of the training data, just like we did. Here, is their example of this intuition: suppose the exact phrase ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AJfD3dd1MG"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a dog was running in a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gkxMzjQMPW"}],"key":"itQr4EAoHT"},{"type":"text","value":" has never occured and at test time we want our model to complete the sentence by predicting the word that might follow it (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uGOAYSRFv2"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"room","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t3WuC01lX9"}],"key":"uxQsDsSCec"},{"type":"text","value":"). Because the model has never encountered this exact phrase in the training set, it is out of distribution, as we say. Meaning, you don’t have fundamentally any reason to suspect what might come next. However, the approach we are following allows you to get around such suspicion. Maybe we haven’t seen the exact phrase, but maybe we have seen similar phrases like: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i80DbIQcr4"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"the dog was running in a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tb184kFkoD"}],"key":"EU0PQjikqY"},{"type":"text","value":" and maybe your ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IHsnhUjU36"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Vlw6r3IGz1"}],"key":"IqdkIsim5s"},{"type":"text","value":" has learned that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TQNKUNcidp"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YfZ2zY4475"}],"key":"odoYwg7JwM"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FMS2uzK8jv"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"the","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sx39CICx0s"}],"key":"jVE7XqH0X2"},{"type":"text","value":" are frequently interchangeble with each other. So maybe our model took the embeddings for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wTjCHRe1xm"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OceyeipJni"}],"key":"MyjSuppZGB"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fzA9azCkJ4"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"the","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JJ7rMnLpAV"}],"key":"HDLhQKtxjH"},{"type":"text","value":" and it actually put them nearby eachother in the vector space. Thus, you can transfer knowledge through such an embedding and generalize in that way. Similarly, it can do the same with other similar words such that a phrase such as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Di3x8dKnqd"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"The cat is walking in the bedroom","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QTzsJx7flq"}],"key":"nG4daAfOsy"},{"type":"text","value":" can help us generalize to a diserable or at least valid sentence like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d82vPN8mFS"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"a dog was running in a","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b8oaH9Zf6s"}],"key":"SJuWpgMTjR"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ADH4xalmly"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"room","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZYScm0TRvI"}],"key":"KZTNgRolo2"},{"type":"text","value":" by merit of the magic of feature vector similarity after training! To put it more simply, manipulating the embedding space allows us to transfer knowledge, predict and generalize to novel scenarios even when fed inputs like the sequence of words mentioned that we have not trained on. If you scroll down the paper, you will see the following diagram:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NtXdGNZLMg"}],"key":"f72CUuQH5T"}],"key":"B6fhk68tic"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\ndisplay(Image(filename='bengio2003nn.jpeg'))","key":"xnYT8Fo5rp"},{"type":"outputs","id":"Zn1eZKq_1Hg2aiOH4btGg","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"a21abcc7498c74c85d4a3cd5f51b3817","path":"/build/a21abcc7498c74c85d4a3cd5f51b3817.jpeg"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"Nr0xc3v5vz"}],"key":"tBHmqfVFX8"}],"key":"nan64Ztmm6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This is the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Jt8zKy2zqE"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xJJGw8eZbX"}],"key":"TObIJySctV"},{"type":"text","value":" where we are taking e.g. three previous words and we are trying to predict the fourth word in a sequence, where ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dGfawLdNIk"},{"type":"inlineMath","value":"w_{t-3}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>3</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{t-3}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">3</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span>","key":"laHdiKSlR8"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oobjk8EijW"},{"type":"inlineMath","value":"w_{t-2}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{t-2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span>","key":"y3hO11Os9w"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q2Cg4IwRz8"},{"type":"inlineMath","value":"w_{t-1}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{t-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span>","key":"iuQ3dgP1N0"},{"type":"text","value":" are the indeces of each incoming word. Since there are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IZbUkfyRko"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oKI5myQWyA"},{"type":"text","value":" possible words, these indeces are integers between ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Aa7gX3azWD"},{"type":"inlineMath","value":"0-16999","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>−</mo><mn>16999</mn></mrow><annotation encoding=\"application/x-tex\">0-16999</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">16999</span></span></span></span>","key":"sqQOUvey4E"},{"type":"text","value":". There’s also a lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"obDWdwYV2H"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"gHFnQfaYGy"},{"type":"text","value":", a matrix that has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pTfqlyhmOH"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"exzJbwwVPu"},{"type":"text","value":" rows (one for each word embedding) and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JQOB63CBYI"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mOGYOg3OFy"},{"type":"text","value":" columns (one for each feature vector/embedding dimension). Every index basically plucks out a row of this embedding matrix so that each index is converted to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JHNfarSBPj"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LbxOwNxJ96"},{"type":"text","value":"-dimensional embedding vector corresponding to that word. Therefore, each word index corresponds to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RpYcuKzfs9"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MVNjfFe49k"},{"type":"text","value":" neuron activations exiting the first layer: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pit2jJUOwG"},{"type":"inlineMath","value":"w_{t-3} \\rightarrow C(w_{t-3})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>3</mn></mrow></msub><mo>→</mo><mi>C</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>3</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w_{t-3} \\rightarrow C(w_{t-3})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">3</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">3</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>","key":"F8M1ubURvI"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VOc9nuzwEE"},{"type":"inlineMath","value":"w_{t-2} \\rightarrow C(w_{t-2})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub><mo>→</mo><mi>C</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w_{t-2} \\rightarrow C(w_{t-2})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>","key":"bpQmNPYU3b"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WfQlJ6K9Gj"},{"type":"inlineMath","value":"w_{t-1} \\rightarrow C(w_{t-1})","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>→</mo><mi>C</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w_{t-1} \\rightarrow C(w_{t-1})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6389em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>","key":"i1QpnnG490"},{"type":"text","value":". Thus, the first layer contains ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uqKBl5hPpe"},{"type":"text","value":"90","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h3hGDCNa9L"},{"type":"text","value":" neurons in total. Notice how the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wKi2VB36yu"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"ZJO68b8x8l"},{"type":"text","value":" matrix is shared, which means that we are indexing the same matrix over and over. Next up is the hidden layer of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GMa3NElwTK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XeolemlrsP"}],"key":"mDeXcmKWUH"},{"type":"text","value":" whose size is a hyperparameter, meaning that it is up to the choice of the designer how wide, aka how many neurons, it is going to have. For example it could have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sHrh0dakm8"},{"type":"text","value":"100","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PmuPhPMm9q"},{"type":"text","value":" or any other number that endows the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eDCREet4Lz"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GOjZFgrfIQ"}],"key":"PIzPJS7K2P"},{"type":"text","value":" with the best performance, after evaluation. This hidden layer is fully connected to the input layer of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h3jvxnhShc"},{"type":"text","value":"90","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gad4yMCLdL"},{"type":"text","value":" neurons, meaning each neuron is connected to each one of this layer’s neurons. Then there’s a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qvrEzh9Faz"},{"type":"inlineMath","value":"\\tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>tanh</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mop\">tanh</span></span></span></span>","key":"BHfbwg0uy2"},{"type":"text","value":" non-linearity, and then there’s an output layer. And because of course we want the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cBBJxDGXMx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lNcv3WzdyV"}],"key":"hgoDlHnivl"},{"type":"text","value":" to give us the next word, the output layer has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SOpbnsqogw"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sPVQMeLGr0"},{"type":"text","value":" neurons that are also fully connected to the previous (hidden) layer’s neurons. So, there’s a lot of parameters, as there are a lot of words, so most computation happens in the output layer. Each of this layer’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B9qjSrfMxJ"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vAot6EsjZv"},{"type":"text","value":" logits is passed through a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fidvTakHt9"},{"type":"inlineMath","value":"softmax","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">softmax</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">so</span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">ma</span><span class=\"mord mathnormal\">x</span></span></span></span>","key":"MxgPhvVlHD"},{"type":"text","value":" function, meaning they are all exponentiated and then everything is normalized to sum to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qT5lTpZiNS"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ayMFy8O4a8"},{"type":"text","value":", so that we have a nice probability distribution ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ss1Cud3kgq"},{"type":"inlineMath","value":"P(w_{t}=i\\ |\\ context)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>=</mo><mi>i</mi><mtext> </mtext><mi mathvariant=\"normal\">∣</mi><mtext> </mtext><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(w_{t}=i\\ |\\ context)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mspace\"> </span><span class=\"mord\">∣</span><span class=\"mspace\"> </span><span class=\"mord mathnormal\">co</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">x</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span></span></span></span>","key":"hbjQcmNeO7"},{"type":"text","value":" for the next word ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"teblmjPoSx"},{"type":"inlineMath","value":"w_{t}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{t}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"jCg3HO45O2"},{"type":"text","value":" in the sequence. During training of course, we have the label or target index: the index of the next word in the sequence which we use to pluck out the probability of that word from that distribution. The point of training is to maximize the probability of that word ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RwW2yiXj2J"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"w.r.t.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VDnffL2tt0"}],"key":"QZ4wOSXKJa"},{"type":"text","value":" the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eJw8cEShHu"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"afUjDrMYBw"}],"key":"F8CuxfxyYJ"},{"type":"text","value":" parameters, meaning the weights and biases of the output layer, of the hidden layer and of the embedding lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u6Z9KZMl3h"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"lFk2SYGW4u"},{"type":"text","value":". All of these parameters are optimized using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dPhISyt2zV"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"REooGDzZ4j"}],"key":"cDsxefufPQ"},{"type":"text","value":". Ignore the green dashed arrows in the diagram, they represent a variation of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HgejDSRpY4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B6q5TxUaxr"}],"key":"PDfXy6mIcV"},{"type":"text","value":" we are not going to explore in this lesson. So, what we  described is the setup. Now, let’s implement it!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ub9Bu4lLEQ"}],"key":"gjIsSIlJe5"}],"key":"PMxrXHlQeX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl","key":"rMfdZHGHjh"},{"type":"outputs","id":"LoaXwkD2tbWBQHDY3hNhD","children":[],"key":"OXtbHpNSNn"}],"key":"WkxsbA9x3i"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nwords[:8]","key":"UdYkyGxhhQ"},{"type":"outputs","id":"eiys32nCX3rQRwrx0OoId","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']","content_type":"text/plain"}}},"children":[],"key":"TftknT9Fze"}],"key":"vdqJQN3rZ8"}],"key":"f33Wq3LyI0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(words)","key":"AWRNOxqpny"},{"type":"outputs","id":"eMhxfoc4Wp32u6CqalXHf","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":5,"metadata":{},"data":{"text/plain":{"content":"32033","content_type":"text/plain"}}},"children":[],"key":"hmguMNfSsC"}],"key":"CQwfKq4rja"}],"key":"NeOwbcEgep"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {c: i + 1 for i, c in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: c for c, i in ctoi.items()}\nprint(itoc)","key":"RXf5vDZ2SB"},{"type":"outputs","id":"9WU6WC9ZeuOZz4ZPWBKfq","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"},"children":[],"key":"qzFMj2GjI6"}],"key":"rgucHeVu5d"}],"key":"rKLhAl3X58"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, we are reading ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R14zqAob6g"},{"type":"inlineCode","value":"32033","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rvEl4ynEhA"},{"type":"text","value":" words into a list and we are creating character-to/from-index mappings. From here, the first thing we want to do is compile the dataset for the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gSMomF3Svj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RF09VjAB4G"}],"key":"JUGaYqFngj"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Az0wLe949v"}],"key":"CJRfOdm1WG"}],"key":"YLhadwpxf5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# context length: how many characters do we take to predict the next one?\nblock_size = 3\n\n\ndef build_dataset(words, verbose=False):\n    x, y = [], []\n    for w in words:\n        if verbose:\n            print(w)\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            if verbose:\n                print(\"\".join(itoc[i] for i in context), \"--->\", itoc[ix])\n            context = context[1:] + [ix]  # crop and append\n            x.append(context)\n            y.append(ix)\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(f\"{x.shape=}, {y.shape=}\")\n    print(f\"{x.dtype=}, {y.dtype=}\")\n    return x, y\n\n\nx, y = build_dataset(words[:5], verbose=True)","key":"M0WHxkoKup"},{"type":"outputs","id":"_fNTQET5DGM_2hJIaMkOP","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"emma\n... ---> e\n..e ---> m\n.em ---> m\nemm ---> a\nmma ---> .\nolivia\n... ---> o\n..o ---> l\n.ol ---> i\noli ---> v\nliv ---> i\nivi ---> a\nvia ---> .\nava\n... ---> a\n..a ---> v\n.av ---> a\nava ---> .\nisabella\n... ---> i\n..i ---> s\n.is ---> a\nisa ---> b\nsab ---> e\nabe ---> l\nbel ---> l\nell ---> a\nlla ---> .\nsophia\n... ---> s\n..s ---> o\n.so ---> p\nsop ---> h\noph ---> i\nphi ---> a\nhia ---> .\nx.shape=torch.Size([32, 3]), y.shape=torch.Size([32])\nx.dtype=torch.int64, y.dtype=torch.int64\n"},"children":[],"key":"kYZzTCCOce"}],"key":"xoDFz1F6RY"}],"key":"DTXDKQmbPC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We first define the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RU4CYOOs5Y"},{"type":"inlineCode","value":"block_size","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f6kh2HJPPF"},{"type":"text","value":" which is how many characters we need to predict the next one. In the example I just described, we used ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aC9YXo6jwC"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dqOljL37Pb"},{"type":"text","value":" words to predict the next one. Here, we also use a block size of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"co2E1qcDSz"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AC54tBYnnt"},{"type":"text","value":" and do the same thing, but remember, instead of words we expect characters as inputs and predictions. After defining the block size, we construct ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kFdEEhS2WX"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rg8kWXzmVL"},{"type":"text","value":": a feature list of word index triplets (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CZ3xRTRsat"},{"type":"inlineCode","value":"[[ 0,  0,  5], [ 0,  5, 13], ...]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l6K9bF5Ts6"},{"type":"text","value":") that represent the context inputs, and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FiAyLEHtz1"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TEuaLfD4yO"},{"type":"text","value":": a list of corresponding target word indeces (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JQbmuYV6Ub"},{"type":"inlineCode","value":"[ 5, 13, ...]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tQmV1vWcjv"},{"type":"text","value":"). In the printout above, you can see for each word’s context character triplet, the corresponding target character. E.g. for an input of  ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PZXnRajrnE"},{"type":"inlineCode","value":"...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iwdnPiOIj5"},{"type":"text","value":" the target character is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vSe1poeYeL"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nvGqI1tsxD"},{"type":"text","value":", for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bmZFTBDTyi"},{"type":"inlineCode","value":"..e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tCSRfhHc29"},{"type":"text","value":" it’s ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kAiZSNCdIj"},{"type":"inlineCode","value":"m","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uiWsWKPeOO"},{"type":"text","value":", and so on! Change the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WklYDR5ZTR"},{"type":"inlineCode","value":"block_size","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mArWIcaI9X"},{"type":"text","value":" and see the print out for yourself. Notice how we are using dots as padding. After building the dataset, inputs and targets look like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WW0PqkyNr2"}],"key":"ONiDKDaweV"}],"key":"AuRisf1Bt6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"x, y","key":"nJMXiduZbV"},{"type":"outputs","id":"xldReeIRdVIYCVugjGwRr","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":8,"metadata":{},"data":{"text/plain":{"content":"(tensor([[ 0,  0,  5],\n         [ 0,  5, 13],\n         [ 5, 13, 13],\n         [13, 13,  1],\n         [13,  1,  0],\n         [ 0,  0, 15],\n         [ 0, 15, 12],\n         [15, 12,  9],\n         [12,  9, 22],\n         [ 9, 22,  9],\n         [22,  9,  1],\n         [ 9,  1,  0],\n         [ 0,  0,  1],\n         [ 0,  1, 22],\n         [ 1, 22,  1],\n         [22,  1,  0],\n         [ 0,  0,  9],\n         [ 0,  9, 19],\n         [ 9, 19,  1],\n         [19,  1,  2],\n         [ 1,  2,  5],\n         [ 2,  5, 12],\n         [ 5, 12, 12],\n         [12, 12,  1],\n         [12,  1,  0],\n         [ 0,  0, 19],\n         [ 0, 19, 15],\n         [19, 15, 16],\n         [15, 16,  8],\n         [16,  8,  9],\n         [ 8,  9,  1],\n         [ 9,  1,  0]]),\n tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n          1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))","content_type":"text/plain"}}},"children":[],"key":"EQh8q3x4CD"}],"key":"g7qC2ENeKA"}],"key":"aKaLa3yunM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Given these, let’s write a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OHzWLLo03X"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FLpNjUrXYk"}],"key":"F2zycZsh74"},{"type":"text","value":" that takes the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZIvVnl7mpV"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D8ZpoSTGhm"},{"type":"text","value":" and predicts ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UPVGZqHLbS"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YZRPTB0Ilw"},{"type":"text","value":". First, let’s build the embedding lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LrSaTcmuB9"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VkC7jp4975"},{"type":"text","value":". In the paper, they have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l6bb3ivsGR"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zkwLjP96hw"},{"type":"text","value":" words and embed them in spaces as low-dimensional as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nCIn4Lsms8"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lXdZpZ93wZ"},{"type":"text","value":", so they cram ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qD5rrdN3um"},{"type":"text","value":"17000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aO7fqctPIq"},{"type":"text","value":" into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ck8eWFtRwY"},{"type":"text","value":"30","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v4x7Il9iIU"},{"type":"text","value":"-dimensional space. In our case, we have only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dfX8ZFfxvF"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bM8QxYck4f"},{"type":"text","value":" possible characters, so let’s cram them into as small as -let’s say- a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xocZUtfzrW"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dpHyVfSVbI"},{"type":"text","value":"-dimensional space:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RETPBARvik"}],"key":"PjqDBgxTuc"}],"key":"kYV3Yia6GA"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"SEED = 2147483647\ng = torch.Generator().manual_seed(SEED)\nC = torch.randn((27, 2), generator=g)\nC","key":"bAu9YjSM2O"},{"type":"outputs","id":"CsvtGzjExZNv0qpJLb7Su","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":9,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 1.5674, -0.2373],\n        [-0.0274, -1.1008],\n        [ 0.2859, -0.0296],\n        [-1.5471,  0.6049],\n        [ 0.0791,  0.9046],\n        [-0.4713,  0.7868],\n        [-0.3284, -0.4330],\n        [ 1.3729,  2.9334],\n        [ 1.5618, -1.6261],\n        [ 0.6772, -0.8404],\n        [ 0.9849, -0.1484],\n        [-1.4795,  0.4483],\n        [-0.0707,  2.4968],\n        [ 2.4448, -0.6701],\n        [-1.2199,  0.3031],\n        [-1.0725,  0.7276],\n        [ 0.0511,  1.3095],\n        [-0.8022, -0.8504],\n        [-1.8068,  1.2523],\n        [ 0.1476, -1.0006],\n        [-0.5030, -1.0660],\n        [ 0.8480,  2.0275],\n        [-0.1158, -1.2078],\n        [-1.0406, -1.5367],\n        [-0.5132,  0.2961],\n        [-1.4904, -0.2838],\n        [ 0.2569,  0.2130]])","content_type":"text/plain"}}},"children":[],"key":"kSYTJ48AoY"}],"key":"dbYXR0cHH5"}],"key":"bQ6oiQhMQK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Each of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wZvvu6QosO"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R4HyfRy0gw"},{"type":"text","value":" characters will have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zPeBQmas7R"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SrjihUljOS"},{"type":"text","value":"-dimensional embedding. Therefore, our table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ca16YphTpr"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CV2KRZOkTY"},{"type":"text","value":" will have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k4aNDIeLjP"},{"type":"text","value":"27","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bVvwZnV2ZI"},{"type":"text","value":" rows (one for each character) and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EcdZvn09X9"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n6gSwGf67U"},{"type":"text","value":" columns (number of dimensions per character embedding). Before we embed all the integers inside input ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sxMvaqve2l"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nNTgkjkMTV"},{"type":"text","value":" using this lookup table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TBfasDObGa"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PHeu0l4dnL"},{"type":"text","value":", let’s first embed a single, individual character, let’s say, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VocBu8TAhH"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mbHZ4ZagH0"},{"type":"text","value":", so we get a sense of how this works. One way to do it is to simply index the table using the character index:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x8OTug4XTj"}],"key":"CdYTt5NvsV"}],"key":"IB1xcctT16"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"C[5]","key":"s6fhQ2nRqx"},{"type":"outputs","id":"r97W-7LbBq0jElWzjTNYq","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":10,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.4713,  0.7868])","content_type":"text/plain"}}},"children":[],"key":"r8Dqnnfpeh"}],"key":"CoM4GFB8Bd"}],"key":"zJxxBq2nRW"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Another way, as we saw in the previous lesson, is to one-hot encode the character:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OlosKcSFT3"}],"key":"KHR7y6qSdN"}],"key":"pbejtWIUq2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ohv = F.one_hot(torch.tensor(5), num_classes=27)\nprint(ohv)\nprint(ohv.shape)","key":"VRIs23HlRF"},{"type":"outputs","id":"AeaH6VeVU13o4aic7Kiyo","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0])\ntorch.Size([27])\n"},"children":[],"key":"tfMWUQOzKe"}],"key":"sXBrFjf2zD"}],"key":"qarFXiy9u6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"With this way we get a one-hot encoded representation whose ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YVc6darkn4"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RWXyrLFpUT"},{"type":"text","value":"-th element is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZHcb7jtLzs"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KAb1MoEJGh"},{"type":"text","value":" and all the rest are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UkuK2DjDin"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Fn8QDV5hZp"},{"type":"text","value":". Now, notice how, just as we previously alluded to in the previous lesson, if we take this one-hot vector and we multiply it by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ds1ySD9lfs"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gmJaYGqWAk"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qpa2Jnuy1T"}],"key":"v2dDjHuAol"}],"key":"YvXBF2DGuJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"ohv_matmul_C = ohv.float() @ C\nohv_matmul_C","key":"H1arQHcm65"},{"type":"outputs","id":"k2KDTfIAlbkD6AIWeiq5I","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":12,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.4713,  0.7868])","content_type":"text/plain"}}},"children":[],"key":"FWrTQfDdX3"}],"key":"ds8vE3gpd2"}],"key":"uLJ8dDeepE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"as you can see, they are identical:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dLGQ4lP6Rm"}],"key":"CZGoLglNx6"}],"key":"hANyFGvPZo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert C[5].equal(ohv_matmul_C)","key":"x23UFmORP8"},{"type":"outputs","id":"WJSyNOF2eZzkTw-Rw2LWp","children":[],"key":"xi11WYTomh"}],"key":"gwDqov0bYm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Multiplying a one-hot encoded vector and an appropriate matrix acts like indexing that matrix with the index of the vector that points to element ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U4KJcQ5Nmv"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fQyTfLjBKG"},{"type":"text","value":"! And so, we actually arrive at the same result. This is interesting since it points out how the first layer of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"owROoYKgTc"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ponU1zALTC"}],"key":"z6wN8J7iSM"},{"type":"text","value":" (see diagram above) can be thought of as a set of neurons whose weight matrix is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c0ZpI9o8gm"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ATbJmKXWdf"},{"type":"text","value":", when the inputs (the integer character indeces) are one-hot encoded. Note aside, in order to embed a character, we are just going to simply index the table as it’s much faster:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J83AAKvWOi"}],"key":"DUeyZlsTJr"}],"key":"iEJpj7REoX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"C[5]","key":"MfzDCN4wHQ"},{"type":"outputs","id":"kgpAUpBdpKIYA6jtK-Yj9","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":14,"metadata":{},"data":{"text/plain":{"content":"tensor([-0.4713,  0.7868])","content_type":"text/plain"}}},"children":[],"key":"FhgKslXDJB"}],"key":"bJRTeAzxNO"}],"key":"BW8USEmBEc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"which is easy for a single character. But what if we want to index more simultaneously? That’s also easy:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"brHFFMpBUm"}],"key":"sssGrdIJTL"}],"key":"J1S5biAl87"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"C[[5, 6, 7]]","key":"Izp05bvFiI"},{"type":"outputs","id":"fwMT9KnwYlGje7rCih9w-","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":15,"metadata":{},"data":{"text/plain":{"content":"tensor([[-0.4713,  0.7868],\n        [-0.3284, -0.4330],\n        [ 1.3729,  2.9334]])","content_type":"text/plain"}}},"children":[],"key":"gQ57IqUOqq"}],"key":"eBFjNqzB6V"}],"key":"jhrQyEeRG1"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Cool! You can actually index a PyTorch tensor with a list or another tensor. Therefore, to easily get all character embeddings, we can simply do:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QWER3OFQ48"}],"key":"y5Lim2uUwY"}],"key":"FNxmol4lqO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"emb = C[x]\nprint(emb)\nprint(emb.shape)","key":"KD2GU0bFBk"},{"type":"outputs","id":"hb7jk_se-y1F9TcVqLujq","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([[[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [-0.4713,  0.7868]],\n\n        [[ 1.5674, -0.2373],\n         [-0.4713,  0.7868],\n         [ 2.4448, -0.6701]],\n\n        [[-0.4713,  0.7868],\n         [ 2.4448, -0.6701],\n         [ 2.4448, -0.6701]],\n\n        [[ 2.4448, -0.6701],\n         [ 2.4448, -0.6701],\n         [-0.0274, -1.1008]],\n\n        [[ 2.4448, -0.6701],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [-1.0725,  0.7276]],\n\n        [[ 1.5674, -0.2373],\n         [-1.0725,  0.7276],\n         [-0.0707,  2.4968]],\n\n        [[-1.0725,  0.7276],\n         [-0.0707,  2.4968],\n         [ 0.6772, -0.8404]],\n\n        [[-0.0707,  2.4968],\n         [ 0.6772, -0.8404],\n         [-0.1158, -1.2078]],\n\n        [[ 0.6772, -0.8404],\n         [-0.1158, -1.2078],\n         [ 0.6772, -0.8404]],\n\n        [[-0.1158, -1.2078],\n         [ 0.6772, -0.8404],\n         [-0.0274, -1.1008]],\n\n        [[ 0.6772, -0.8404],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [-0.0274, -1.1008]],\n\n        [[ 1.5674, -0.2373],\n         [-0.0274, -1.1008],\n         [-0.1158, -1.2078]],\n\n        [[-0.0274, -1.1008],\n         [-0.1158, -1.2078],\n         [-0.0274, -1.1008]],\n\n        [[-0.1158, -1.2078],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [ 0.6772, -0.8404]],\n\n        [[ 1.5674, -0.2373],\n         [ 0.6772, -0.8404],\n         [ 0.1476, -1.0006]],\n\n        [[ 0.6772, -0.8404],\n         [ 0.1476, -1.0006],\n         [-0.0274, -1.1008]],\n\n        [[ 0.1476, -1.0006],\n         [-0.0274, -1.1008],\n         [ 0.2859, -0.0296]],\n\n        [[-0.0274, -1.1008],\n         [ 0.2859, -0.0296],\n         [-0.4713,  0.7868]],\n\n        [[ 0.2859, -0.0296],\n         [-0.4713,  0.7868],\n         [-0.0707,  2.4968]],\n\n        [[-0.4713,  0.7868],\n         [-0.0707,  2.4968],\n         [-0.0707,  2.4968]],\n\n        [[-0.0707,  2.4968],\n         [-0.0707,  2.4968],\n         [-0.0274, -1.1008]],\n\n        [[-0.0707,  2.4968],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]],\n\n        [[ 1.5674, -0.2373],\n         [ 1.5674, -0.2373],\n         [ 0.1476, -1.0006]],\n\n        [[ 1.5674, -0.2373],\n         [ 0.1476, -1.0006],\n         [-1.0725,  0.7276]],\n\n        [[ 0.1476, -1.0006],\n         [-1.0725,  0.7276],\n         [ 0.0511,  1.3095]],\n\n        [[-1.0725,  0.7276],\n         [ 0.0511,  1.3095],\n         [ 1.5618, -1.6261]],\n\n        [[ 0.0511,  1.3095],\n         [ 1.5618, -1.6261],\n         [ 0.6772, -0.8404]],\n\n        [[ 1.5618, -1.6261],\n         [ 0.6772, -0.8404],\n         [-0.0274, -1.1008]],\n\n        [[ 0.6772, -0.8404],\n         [-0.0274, -1.1008],\n         [ 1.5674, -0.2373]]])\ntorch.Size([32, 3, 2])\n"},"children":[],"key":"dyCzeaJr20"}],"key":"oIQgGfeErF"}],"key":"gUjHUfVjGJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Notice the shape: ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UGo64LM8xH"},{"type":"inlineCode","value":"[<number of character input sets>, <input size>, <number of character embedding dimensions>]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FdXW2IeKR5"},{"type":"text","value":". Indexing as following, we can assert that both ways of representation are valid:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gGvmkQ0vd7"}],"key":"xJUSO2pGXt"}],"key":"yrKsmLjfWi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"assert emb[13, 2].equal(C[x[13, 2]])","key":"QRBnQa6XiO"},{"type":"outputs","id":"T7w_eFs4ek5J8bYmce-Bt","children":[],"key":"bqQyfE3mN3"}],"key":"LBc2cTLPnL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Long story short, PyTorch indexing is awesome and tensors such as embedding tables can be indexed by other tensors, e.g. inputs. One last thing, as far as the first layer is concerned. Since each embedding of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fkDS70EBzh"},{"type":"inlineCode","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MMSY8yaM2o"},{"type":"text","value":" inputs has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RbXTovhnKZ"},{"type":"inlineCode","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y1lEj4RrtS"},{"type":"text","value":" dimensions, the output dimension of our first layer is basically ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k49bKuFUfm"},{"type":"inlineMath","value":"3 \\cdot 2 = 6","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mo>⋅</mo><mn>2</mn><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">3 \\cdot 2 = 6</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">6</span></span></span></span>","key":"xJxFfKCBzB"},{"type":"text","value":". Usually, a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t3PovFrZUt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ocDojp0YGw"}],"key":"uAmeb9r2aK"},{"type":"text","value":" layer is described by a pair of input and output dimensions. The input dimension of our first, embeddings layer is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hhz02DteLv"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MyfYkgNsQT"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yYIV0LX54W"},{"type":"inlineCode","value":"<number of character inputs>","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wqkwxwzIMB"},{"type":"text","value":"). To get the output dimension we have to concatenate the following ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lXEsh4IGGf"},{"type":"inlineCode","value":"<inputs size>","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x76JvgBuqa"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H03qTfHqDA"},{"type":"inlineCode","value":"<number of character embedding dimensions>","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lRleXpHV16"},{"type":"text","value":" tensor dimensions into one dimension:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GsWLRQiVE1"}],"key":"lnzqiPdX0j"}],"key":"xKKaJVk1ID"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"last_dims = emb.shape[1:]  # get dimensions after the first one\nlast_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\nemb_proper = emb.view(-1, last_dims_product)\nemb_proper.shape  # tada!","key":"QVrt9STL1X"},{"type":"outputs","id":"ElCeFowaRVG2nXXKRdtTM","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":18,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 6])","content_type":"text/plain"}}},"children":[],"key":"XpSqXWUQY5"}],"key":"bBaO3HR7tD"}],"key":"g6yf2UUJ02"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We have prepared the dimensionality of the first layer. Now, let’s implement the hidden, second layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dlaIvKyirI"}],"key":"YgHfZiPLjh"}],"key":"Id0OxvfWVa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"l0out = emb_proper.shape[-1]\nl1in = l0out\nl1out = 100  # neurons of hidden layer\nw1 = torch.randn(l1in, l1out, generator=g)\nb1 = torch.randn(l1out, generator=g)\nprint(w1.shape)\nprint(b1.shape)","key":"YAZ3JjfLuy"},{"type":"outputs","id":"N-hG0GDROmgHG912EiLqs","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([6, 100])\ntorch.Size([100])\n"},"children":[],"key":"CygXOs121j"}],"key":"GVCuxS3pTl"}],"key":"d7Gln2QFK9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"h = torch.tanh(emb_proper @ w1 + b1)\nh","key":"eYFb1GHxOz"},{"type":"outputs","id":"nqPjAfGPG4Z1mn3se4bcR","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":20,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0.2797,  0.9997,  0.7675,  ...,  0.9929,  0.9992,  0.9981],\n        [-0.9960,  1.0000, -0.8694,  ..., -0.5159, -1.0000, -0.0069],\n        [-0.9968,  1.0000,  0.9878,  ...,  0.4976, -0.9297, -0.8616],\n        ...,\n        [-0.9043,  1.0000,  0.9868,  ..., -0.7859, -0.4819,  0.9981],\n        [-0.9048,  1.0000,  0.9553,  ...,  0.9866,  1.0000,  0.9907],\n        [-0.9868,  1.0000,  0.5264,  ...,  0.9843,  0.0223, -0.1655]])","content_type":"text/plain"}}},"children":[],"key":"YMRAZQguYC"}],"key":"caOs7xa8aJ"}],"key":"grydxgwy4z"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"h.shape","key":"ug23CMfWq7"},{"type":"outputs","id":"Q7eXcmEfWp3xwTGqYWkBX","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":21,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 100])","content_type":"text/plain"}}},"children":[],"key":"Kk6ajd8git"}],"key":"ogxgA0tdGE"}],"key":"RFeBwQP741"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Done! And now, to create the output layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zKqxcnPv0x"}],"key":"wJmIW4AbvE"}],"key":"brdPxgzHSw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"l2in = l1out\nl2out = 27  # number of characters\nw2 = torch.randn(l2in, l2out, generator=g)\nb2 = torch.randn(l2out, generator=g)\nprint(w2.shape)\nprint(b2.shape)","key":"AvGGhKtd1f"},{"type":"outputs","id":"d1eq7Ck_maDiSizEZSYhn","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([100, 27])\ntorch.Size([27])\n"},"children":[],"key":"HyV10E4M0l"}],"key":"kef0bNOu8T"}],"key":"MNZAbct0dh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"logits = h @ w2 + b2\nlogits.shape","key":"uG19P1jhnR"},{"type":"outputs","id":"Ehu-Soa8FA6Y_nzHkMXjU","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":23,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 27])","content_type":"text/plain"}}},"children":[],"key":"UCXZftEXYk"}],"key":"YmKYOicRpf"}],"key":"hmneHvRv1O"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exactly as we saw in the previous lesson, we want to take these logits and we want to first exponentiate them to get our fake counts. Then, we want to normalize them to get the probabilities of how likely it is for each character to come next:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UjvyrhXL7J"}],"key":"l34J2uLJto"}],"key":"fcX1xQFK6K"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"counts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nprob.shape","key":"H9oYPV8V9c"},{"type":"outputs","id":"bztG3b4jsh1aV6MzdhKvL","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":24,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 27])","content_type":"text/plain"}}},"children":[],"key":"Ax2E2ovwJh"}],"key":"tVOYFAZ1MB"}],"key":"HPAVW1UbDM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Remember, we also have the the target values ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GWnPVx0ecO"},{"type":"inlineCode","value":"y","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sFV0GKzNTE"},{"type":"text","value":", the actual characters that come next that we would like our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LmC4EI6mNk"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HrQ555flvK"}],"key":"CVHgRafqv6"},{"type":"text","value":" to be able to predict:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eql1yXYZam"}],"key":"jBE2ERQmns"}],"key":"EJmVT3fE26"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"y","key":"MUZDi8nfcY"},{"type":"outputs","id":"_4t05arCshX3DPwbAjHjv","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":25,"metadata":{},"data":{"text/plain":{"content":"tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])","content_type":"text/plain"}}},"children":[],"key":"VqMTpeulnk"}],"key":"GaJFQ29PIs"}],"key":"vCAA22yVhg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, what we would like to do now is index into the rows of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"giIRDBpLgU"},{"type":"inlineCode","value":"prob","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ekaMcZnVcS"},{"type":"text","value":" and for each row to pluck out the probability given to the correct character:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r10xnRWw4g"}],"key":"JvfZIfSs4M"}],"key":"PeDv56TpPt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"prob[range(len(y)), y]","key":"ehO7gWzXpq"},{"type":"outputs","id":"ENJI3JSi1Z4T2dQymd-Cz","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":26,"metadata":{},"data":{"text/plain":{"content":"tensor([9.9994e-12, 1.9647e-08, 4.0322e-07, 3.0845e-09, 4.6517e-11, 7.4238e-12,\n        2.0297e-09, 9.9179e-01, 1.7138e-02, 3.2410e-03, 2.8552e-06, 1.0565e-06,\n        2.6391e-09, 4.1804e-06, 3.5843e-08, 7.7737e-07, 3.5022e-02, 2.7425e-10,\n        1.7086e-08, 6.3572e-02, 1.1315e-08, 1.6961e-09, 2.1885e-11, 1.5201e-10,\n        1.0528e-03, 3.6704e-08, 9.5847e-02, 3.1954e-12, 8.5695e-17, 2.5576e-03,\n        9.1782e-12, 1.0565e-06])","content_type":"text/plain"}}},"children":[],"key":"hnHMfdWirt"}],"key":"YloVGk9xGF"}],"key":"ENeGvgVbOo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This gives the current probabilities for these specific correct, target characters that come next after each character sequence, given the current ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YOfNCm5eHQ"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dijlvqWdFZ"}],"key":"l0tcUjDJ5R"},{"type":"text","value":" configuration (weights and biases). Currently these probabilities are pretty bad and most characters are pretty unlikely to occur next. Of course, we haven’t trained the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OqRgldH4JW"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NwAeTFmUi7"}],"key":"Lptq7dcLVQ"},{"type":"text","value":" yet. So, we want to train it so that each probability approximates ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gu4wJ0PiM8"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A7lqJSrtGQ"},{"type":"text","value":". As we saw previously, to do so, we have to define the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eKOhyHEnaD"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dYlTycVgaU"}],"key":"oQZKoZMEoV"},{"type":"text","value":" and then minimize it:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yyMeCTf9Mf"}],"key":"Tjc2LUZICZ"}],"key":"N0G0iguxpV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss = -prob[range(len(y)), y].log().mean()\nloss","key":"wsEicnKoh4"},{"type":"outputs","id":"My6kuwUw4oGkp_Litn2PB","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":27,"metadata":{},"data":{"text/plain":{"content":"tensor(16.0342)","content_type":"text/plain"}}},"children":[],"key":"IiEwn8QDcD"}],"key":"znBO8CMMzJ"}],"key":"WYZUUI5NyS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Pretty big loss! Haha. Now we will minimize it so our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W9gkkvsSGP"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nRs29UC5Oa"}],"key":"PJOkohEEE8"},{"type":"text","value":" able to predict the next character in each sequence correctly. To do so, we have to optimize the parameters. Let’s define a function that defines them and collects them all into a list just so we have easy access:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o8Tic8GTSK"}],"key":"jp5ojBUqS2"}],"key":"QaD39kC8Sa"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import random\n\n# context length: how many characters do we take to predict the next one?\nblock_size = 3\n\n\n# build the dataset\ndef build_dataset(words):\n    X, Y = [], []\n    for w in words:\n        # print(w)\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            X.append(context)\n            Y.append(ix)\n            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n            context = context[1:] + [ix]  # crop and append\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\n\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\n\ndef define_nn(l1out=100, embsize=2):\n    global C, w1, b1, w2, b2\n    g = torch.Generator().manual_seed(SEED)\n    C = torch.randn((27, embsize), generator=g)\n    l1in = embsize * block_size\n    # l1out: neurons of hidden layer\n    w1 = torch.randn(l1in, l1out, generator=g)\n    b1 = torch.randn(l1out, generator=g)\n    l2in = l1out\n    l2out = 27  # neurons of output layer, number of characters\n    w2 = torch.randn(l2in, l2out, generator=g)\n    b2 = torch.randn(l2out, generator=g)\n    parameters = [C, w1, b1, w2, b2]\n    return parameters\n\n\nparameters = define_nn()\nsum(p.nelement() for p in parameters)","key":"cciOVLoYjZ"},{"type":"outputs","id":"w8yi6I5ch9NrZpi7CujbE","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n"},"children":[],"key":"iV3squWlM5"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":28,"metadata":{},"data":{"text/plain":{"content":"3481","content_type":"text/plain"}}},"children":[],"key":"WHvjYxdQ76"}],"key":"xYGBLSa4mg"}],"key":"sJEXj2wrFt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To recap the forward pass:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IXjIMnHId8"}],"key":"ockquoyjQU"}],"key":"BWO46iwSix"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"emb = C[x]  # [32, 3, 2]\nlast_dims = emb.shape[1:]  # get dimensions after the first one\nlast_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\nemb_proper = emb.view(-1, last_dims_product)  # [32, 6]\nh = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]\nlogits = h @ w2 + b2  # [32, 27]\ncounts = logits.exp()\nprob = counts / counts.sum(1, keepdims=True)\nloss = -prob[range(len(y)), y].log().mean()\nloss","key":"M56nfNRYWe"},{"type":"outputs","id":"GfuBQ6OM3rQRDm_GjoYVy","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":29,"metadata":{},"data":{"text/plain":{"content":"tensor(16.0342)","content_type":"text/plain"}}},"children":[],"key":"cZLNnmQLOh"}],"key":"gs6UaHJB53"}],"key":"RNxZGuZy05"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"A better and more efficient way to calculate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dGz4XONUX4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YhwQ9sZG5k"}],"key":"xsf2Q3wQk0"},{"type":"text","value":" from logits and targets is through the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LsVWh918oU"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"cross entropy loss function","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pTEeCRIk3T"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html","key":"ZZZVAnCgh6"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LgomiPUF12"}],"key":"bd30J7qMEx"}],"key":"LGQ8CbmHCU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"F.cross_entropy(logits, y)","key":"KjXwslmK0M"},{"type":"outputs","id":"655egS3MFOPJJ6aGukYHg","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":30,"metadata":{},"data":{"text/plain":{"content":"tensor(16.0342)","content_type":"text/plain"}}},"children":[],"key":"GyuhESZld4"}],"key":"q7DOF45iLz"}],"key":"CJNflW2KWA"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s tidy up the forward pass:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oUM6HWndlt"}],"key":"FS11a9WJIG"}],"key":"ipTcCm2DjU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward_pass(x, y):\n    emb = C[x]  # [32, 3, 2]\n    last_dims = emb.shape[1:]  # get dimensions after the first one\n    last_dims_product = torch.prod(torch.tensor(last_dims)).item()  # multiply them\n    emb_proper = emb.view(-1, last_dims_product)  # [32, 6]\n    h = torch.tanh(emb_proper @ w1 + b1)  # [32, 100]\n    logits = h @ w2 + b2  # [32, 27]\n    loss = F.cross_entropy(logits, y)\n    return loss","key":"I1kMqWX0yi"},{"type":"outputs","id":"c4Gcg1JXVeDx8G9MRG-4j","children":[],"key":"g3v77BjosB"}],"key":"bTsEXHHN5J"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Training the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s9voNJUEzq"}],"identifier":"training-the-model","label":"Training the model","html_id":"training-the-model","implicit":true,"key":"k1YA4aQtlF"}],"key":"e6INoU67gp"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And now, let’s train our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ilvjhyMGmt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OkWXp0PGx6"}],"key":"raMddiCFr2"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mbHDXv9Lo5"}],"key":"KA3NowOM5r"}],"key":"ifspnona0j"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def train(x, y, epochs=10):\n    parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for _ in range(epochs):\n        loss = forward_pass(x, y)\n        print(loss.item())\n        for p in parameters:\n            p.grad = None\n        loss.backward()\n        for p in parameters:\n            p.data += -0.1 * p.grad","visibility":"show","key":"EKZBSbVWfe"},{"type":"outputs","id":"K7tuORwB1vtW0TI-4laJR","children":[],"visibility":"show","key":"AiSt4rWoTS"}],"visibility":"show","key":"PoiVX2HnzG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x, y)","key":"IK1jLB3cIE"},{"type":"outputs","id":"Me5pWTtZQ1aPrZRcTdJq4","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"16.034189224243164\n11.740133285522461\n9.195296287536621\n7.302927017211914\n5.805147647857666\n4.850736618041992\n4.184849739074707\n3.644319534301758\n3.207334518432617\n2.843700885772705\n"},"children":[],"key":"LF8PAABaQe"}],"key":"uRCxVQswnV"}],"key":"vYDQibcJYx"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LsaCov5Cj5"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pyEdbiPPv3"}],"key":"U77mfDTC51"},{"type":"text","value":" keeps decreasing, which means that the training process is working! Now, since we are only training using a dataset of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G5UvmQG8tY"},{"type":"text","value":"5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I9FyQL96E9"},{"type":"text","value":" words, and since our parameters are many more than the samples we are training on, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Bm4bR1xA7q"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WHwhspywKf"}],"key":"QMuCzB5fMy"},{"type":"text","value":" is probably overfitting. What we have to do now, is train on the whole dataset.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qUt79R88dY"}],"key":"SwG3z6ti0u"}],"key":"XbOSTrl89m"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"x_all, y_all = build_dataset(words)","key":"mNWJLaN0Us"},{"type":"outputs","id":"xyFofqc0eK7GniuW2ksSx","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([228146, 3]) torch.Size([228146])\n"},"children":[],"key":"eK2Sqt3Ed4"}],"key":"CD38juvf7r"}],"key":"q4G1q24e0k"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all)","key":"yOl1lHUeO8"},{"type":"outputs","id":"rGb3Ycm4eDUUB0fncJ3ry","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"19.505229949951172\n17.084487915039062\n15.776533126831055\n14.83334732055664\n14.002612113952637\n13.253267288208008\n12.579923629760742\n11.983108520507812\n11.470502853393555\n11.05186653137207\n"},"children":[],"key":"ToI9g7au20"}],"key":"bjHSBxqC2d"}],"key":"EdJYF041w8"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Same, the loss for all input samples also keeps decreasing. But, you’ll notice that training takes longer now. This is happening because we are doing a lot of work, forward and backward passing on ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OgEgug00lf"},{"type":"text","value":"228146","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OMdMC16mvT"},{"type":"text","value":" examples. That’s way too much work! In practice, what people usually do in such cases is they train on minibatches of the whole dataset. So, what we want to do, is we want to randomly select some portion of the dataset, and that’s a minibatch! And then, only forward, backward and update on that minibatch, likewise iterate and train on those minibatches. A simple way to implement minibatching is to set a batch size, e.g.:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VXKbZDgtgh"}],"key":"Z75x7pVXh3"}],"key":"A3qSVua33S"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"batchsize = 32","key":"mALWjiWtUX"},{"type":"outputs","id":"qzl9bF8UlI9vG9PtEMEV9","children":[],"key":"WUoUEPrmuf"}],"key":"ogEqk0s8Jy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"and then to randomly select ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zMBAJL8kAN"},{"type":"inlineCode","value":"batchsize","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J9Xsc08Vgt"},{"type":"text","value":" number of indeces referencing the subset of input data to be used for minibatch training. To get the indeces you can do something like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T2vBo3Ac9q"}],"key":"IC61ZIfW4I"}],"key":"sWW23SfPX5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"batchix = torch.randint(0, x_all.shape[0], (batchsize,))\nprint(batchix)","key":"htv7OdDely"},{"type":"outputs","id":"cMkyLg-UqfIjf6JktAxvb","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor([ 74679, 122216,  57092, 133769, 226181,  38045, 126099,  23446, 218663,\n         17662, 225764, 199486, 185049,  64041, 217855, 198821, 192633,  84825,\n         44722,  46171, 182390,  99196, 102624,    409, 168159, 182770, 142590,\n        173184,  86521,   1596, 158516, 206175])\n"},"children":[],"key":"YCq99OzZyN"}],"key":"ZBsbzB3gre"}],"key":"dU4p2CDPUI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then, to actually get a minibatch per epoch, just create a new, random set of indeces and index the samples and targets from the dataset before each forward pass. Like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iXSGUvpCN2"}],"key":"uEi3NGWo9s"}],"key":"uRF8oldooX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(x, y, lr=0.1, epochs=10, print_all_losses=True):\n    parameters = define_nn()\n    for p in parameters:\n        p.requires_grad = True\n    for _ in range(epochs):\n        batchix = torch.randint(0, x.shape[0], (batchsize,))\n        bx, by = x[batchix], y[batchix]\n        loss = forward_pass(bx, by)\n        if print_all_losses:\n            print(loss.item())\n        for p in parameters:\n            p.grad = None\n        loss.backward()\n        for p in parameters:\n            p.data += -lr * p.grad\n    if not print_all_losses:\n        print(loss.item())\n    return loss.item()","key":"vC4Qs57Mge"},{"type":"outputs","id":"twlfE-F4UMQOpDCyX3xN9","children":[],"key":"ycj3P1SHXv"}],"key":"GgPd2UcFnU"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, if we train using minibatches...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"etGJQ9cibh"}],"key":"XF7GvTJp3l"}],"key":"qfBIacrxth"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all)","key":"SGaYrXfYKM"},{"type":"outputs","id":"d1SiImMHpF2W-ReYLxUg7","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"17.94289779663086\n15.889695167541504\n15.060649871826172\n13.832050323486328\n16.023155212402344\n14.010979652404785\n16.336170196533203\n13.788375854492188\n11.292967796325684\n13.045702934265137\n"},"children":[],"key":"jo7qLEpkya"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"13.045702934265137","content_type":"text/plain"}}},"children":[],"key":"gVwYV9yEg2"}],"key":"FYSe25Velz"}],"key":"QIvCbUgVCG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"training is much much faster, almost instant! However, since we are dealing with minibatches, the quality of our gradient is lower, so the direction is not as reliable. It’s not the actual exact gradient direction, but the gradient direction is good enough even though it’s being estimated for only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cq6vm5ukVz"},{"type":"inlineCode","value":"batchsize","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ybSX1Z8ylw"},{"type":"text","value":" (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EHWyWInuqg"},{"type":"inlineCode","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"neZ11RSltl"},{"type":"text","value":") examples. In general, it is better to have an approximate gradient and just make more steps than it is to compute the exact gradient and take fewer steps. And that is why in practice, minibatching works quite well.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jjdJcKQjsg"}],"key":"Sy3Br9rHSI"}],"key":"YLAx56xh8Y"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Finding a good learning rate","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HmnJLqP8F0"}],"identifier":"finding-a-good-learning-rate","label":"Finding a good learning rate","html_id":"finding-a-good-learning-rate","implicit":true,"key":"l93oEBXJJN"}],"key":"CV7PnoBFwc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, one issue that has popped up as you may have noticed, is that during minibatch training the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X9MKWxsO9u"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hBvTJR6BqY"}],"key":"yAG1ICIyxb"},{"type":"text","value":" seems to fluctuate. For some epochs it decreases, but then it increases again, and vice versa. That question that arises from this observation is this: are we stepping too slow or too fast? Meaning, are we updating the parameters by a fraction of their gradients that is too small or too large? Such magnitude is determined by the step size, aka the learning rate. Therefore, the overarching question is: how do you determine this learning rate? How do we gain confidence that we are stepping with the right speed? Let’s see one way to determine the learning rate. We basically want to find a reasonable search range, if you will. What people usually do is they pick different learning rate values until they find a satisfactory one. Let’s try to find one that is better. We see for example if it is very small:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wCaseWWxWt"}],"key":"DSokjLMe7Y"}],"key":"DlGV1tUnzi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=0.0001, epochs=100)","key":"CF3epTEwND"},{"type":"outputs","id":"Kc39CUFE5i7bZBshXktlx","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"21.48345375061035\n17.693952560424805\n20.993595123291016\n19.23766326904297\n17.807458877563477\n19.426368713378906\n20.208740234375\n21.426673889160156\n20.107091903686523\n16.44301986694336\n16.98691749572754\n16.693296432495117\n16.654979705810547\n18.207632064819336\n20.281587600708008\n19.277870178222656\n19.782976150512695\n20.056819915771484\n19.198200225830078\n15.863028526306152\n18.550344467163086\n19.435653686523438\n19.682138442993164\n17.305164337158203\n21.181236267089844\n19.23027992248535\n18.67540168762207\n18.95297622680664\n21.35270881652832\n21.46781349182129\n21.018564224243164\n20.318994522094727\n21.243608474731445\n19.62767791748047\n19.476289749145508\n17.74656867980957\n20.23328399658203\n20.085819244384766\n16.801542282104492\n18.122915267944336\n19.09043312072754\n19.84799575805664\n20.199235916137695\n16.658361434936523\n19.510778427124023\n19.398319244384766\n18.517004013061523\n19.53419303894043\n22.490541458129883\n20.45920753479004\n17.721420288085938\n18.58787727355957\n20.76034927368164\n20.696556091308594\n18.54053497314453\n19.546337127685547\n15.577354431152344\n18.100522994995117\n15.600821495056152\n21.15610122680664\n20.79819107055664\n18.512712478637695\n17.394367218017578\n15.756057739257812\n21.389039993286133\n16.85922622680664\n13.484357833862305\n19.010683059692383\n18.83637046813965\n19.841796875\n18.28095054626465\n20.777664184570312\n19.818172454833984\n18.778358459472656\n20.82563591003418\n19.217248916625977\n18.208587646484375\n19.463356018066406\n16.181228637695312\n16.927345275878906\n18.849687576293945\n19.017803192138672\n18.24212074279785\n20.15293312072754\n19.38414764404297\n19.442598342895508\n22.70920181274414\n19.071269989013672\n17.25360679626465\n16.035856246948242\n19.327434539794922\n20.848506927490234\n19.198562622070312\n18.62538719177246\n20.031288146972656\n22.616220474243164\n18.733247756958008\n20.26487159729004\n18.593149185180664\n24.60611343383789\n"},"children":[],"key":"uaiheH7DLa"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":40,"metadata":{},"data":{"text/plain":{"content":"24.60611343383789","content_type":"text/plain"}}},"children":[],"key":"mm1xOsNknE"}],"key":"DWNI4czFC1"}],"key":"NsDEyVEWFZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The loss barely decreases. So this value is too low. Let’s try something bigger, e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RhpEdIx83T"}],"key":"a2epUHyacs"}],"key":"CwfC3NlZGZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=0.001, epochs=100)","key":"m2CRFZJTCb"},{"type":"outputs","id":"Y223r2iPstUVE32mMy5w1","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"20.843355178833008\n21.689167022705078\n19.067970275878906\n17.899921417236328\n20.831283569335938\n21.015792846679688\n21.317489624023438\n18.584075927734375\n19.30166244506836\n17.16819190979004\n20.15867805480957\n18.009313583374023\n22.017822265625\n18.148967742919922\n17.53749656677246\n19.633005142211914\n17.75607681274414\n17.64638900756836\n18.511669158935547\n20.743165969848633\n18.751705169677734\n18.893756866455078\n19.241785049438477\n19.387001037597656\n18.413681030273438\n20.803842544555664\n18.513309478759766\n20.150976181030273\n19.927082061767578\n20.02385711669922\n19.09459686279297\n16.731922149658203\n19.16921615600586\n19.426868438720703\n17.71548843383789\n17.51811408996582\n18.333765029907227\n20.96685028076172\n19.99691390991211\n19.345508575439453\n19.923913955688477\n16.887836456298828\n17.372751235961914\n18.805681228637695\n18.897857666015625\n16.86487579345703\n17.781234741210938\n20.2587833404541\n17.451217651367188\n18.460481643676758\n17.9292049407959\n20.8989315032959\n20.129817962646484\n17.018564224243164\n19.071075439453125\n15.609376907348633\n18.350452423095703\n14.199233055114746\n18.659013748168945\n17.954235076904297\n17.826528549194336\n18.58924102783203\n17.669662475585938\n16.46000862121582\n15.66697883605957\n17.6021785736084\n17.65107536315918\n16.883989334106445\n14.59417724609375\n16.17646026611328\n18.381986618041992\n19.10284423828125\n15.856918334960938\n18.458749771118164\n18.598033905029297\n17.683555603027344\n17.749269485473633\n17.12112808227539\n20.2098445892334\n18.316301345825195\n16.487417221069336\n14.472514152526855\n16.50566864013672\n19.501144409179688\n18.444271087646484\n17.818748474121094\n12.876835823059082\n17.16472816467285\n15.761727333068848\n18.426593780517578\n17.760990142822266\n18.603355407714844\n16.690837860107422\n16.553945541381836\n15.75294303894043\n17.358163833618164\n16.67896842956543\n17.08140754699707\n18.213592529296875\n17.534658432006836\n"},"children":[],"key":"IQE454Taa2"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":41,"metadata":{},"data":{"text/plain":{"content":"17.534658432006836","content_type":"text/plain"}}},"children":[],"key":"hnSce5yd1X"}],"key":"q7RkWcxcIL"}],"key":"mfXsf20m8X"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This is ok, but still not good enough. The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M41W5UdfMF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NBNNoB3r6Q"}],"key":"oYYTfAenTn"},{"type":"text","value":" value decreases but not steadily and fluctuates a lot. For an even bigger value:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fMnfx6Lupx"}],"key":"PaDKA5XS3z"}],"key":"yShhiLiPJ9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=1, epochs=100)","key":"lxQyZT273d"},{"type":"outputs","id":"8l4ek7Zi1_MW0_K-C4JSE","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"18.0602970123291\n15.639816284179688\n14.974983215332031\n16.521020889282227\n10.994373321533203\n19.934722900390625\n13.230535507202148\n10.555554389953613\n11.227700233459473\n8.685490608215332\n11.373151779174805\n9.253009796142578\n8.64297866821289\n9.108452796936035\n7.178742408752441\n9.831550598144531\n5.762266635894775\n9.50097370147705\n10.957695960998535\n12.328742980957031\n8.328937530517578\n8.131488800048828\n11.364215850830078\n9.899446487426758\n7.836635589599609\n8.942032814025879\n11.561779022216797\n9.97337532043457\n8.2869291305542\n9.61953067779541\n9.630435943603516\n14.119514465332031\n12.620526313781738\n8.802383422851562\n8.957738876342773\n11.694437980651855\n16.162137985229492\n10.493476867675781\n7.7210798263549805\n10.860843658447266\n8.748751640319824\n13.449786186218262\n10.955209732055664\n8.923118591308594\n6.181601047515869\n8.725625991821289\n6.119848251342773\n11.221086502075195\n8.663549423217773\n9.03221607208252\n8.159632682800293\n11.553065299987793\n7.1041059494018555\n6.436527729034424\n9.19931697845459\n6.504988670349121\n8.564536094665527\n6.59806489944458\n8.718829154968262\n7.369975566864014\n11.306722640991211\n10.493293762207031\n7.680598735809326\n8.20093059539795\n7.427743911743164\n7.3400983810424805\n8.856118202209473\n7.980756759643555\n11.46378231048584\n8.093060493469238\n9.521681785583496\n6.227016925811768\n8.569214820861816\n8.454265594482422\n7.388335227966309\n6.649340629577637\n7.111802101135254\n7.661591053009033\n12.89154052734375\n8.51455020904541\n5.992252349853516\n6.762502193450928\n6.146595478057861\n8.050479888916016\n8.089849472045898\n7.87835168838501\n7.628716945648193\n7.732893943786621\n6.767331600189209\n8.324596405029297\n8.824007987976074\n8.258061408996582\n7.636016368865967\n6.856623649597168\n6.543000221252441\n7.319474697113037\n5.69791841506958\n5.777251243591309\n6.574363708496094\n5.4569573402404785\n"},"children":[],"key":"Td51FR2j4B"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":42,"metadata":{},"data":{"text/plain":{"content":"5.4569573402404785","content_type":"text/plain"}}},"children":[],"key":"cnZwD7fsO5"}],"key":"GBE97s6MwF"}],"key":"lzbz0MO8GM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Better! How about:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dt9EfiI2yp"}],"key":"Bm1a84kWrP"}],"key":"IeUkYV5ETk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"train(x_all, y_all, lr=10, epochs=100)","key":"xaxtJw2HJ0"},{"type":"outputs","id":"hquLD-ACkJ_5OWDfZqqH_","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"19.74797821044922\n38.09200668334961\n46.314208984375\n40.89773941040039\n73.4378433227539\n58.34128952026367\n51.356876373291016\n55.045501708984375\n54.94329833984375\n56.30958557128906\n70.37184143066406\n56.63667678833008\n56.90707778930664\n45.661888122558594\n50.97232437133789\n55.65245819091797\n57.46098327636719\n78.44556427001953\n70.66008758544922\n51.70524978637695\n48.533164978027344\n80.0494613647461\n90.79659271240234\n61.892642974853516\n52.87863540649414\n41.49900436401367\n63.885189056396484\n69.60615539550781\n60.386714935302734\n76.09366607666016\n40.18576431274414\n53.72964096069336\n48.1932373046875\n46.865108489990234\n48.253753662109375\n53.61216735839844\n77.9145278930664\n75.54542541503906\n65.61190795898438\n78.13446044921875\n83.6716537475586\n79.6883773803711\n59.334083557128906\n74.78559875488281\n50.28561782836914\n53.59624099731445\n35.096195220947266\n50.16322326660156\n73.99742889404297\n86.66049194335938\n70.05807495117188\n78.18916320800781\n48.637943267822266\n77.84318542480469\n56.17559051513672\n44.09672164916992\n70.90714263916016\n79.0201187133789\n67.89301300048828\n65.17256927490234\n68.24624633789062\n63.97649383544922\n90.05917358398438\n91.45114135742188\n60.47791290283203\n70.57051086425781\n57.64970397949219\n44.6708984375\n54.10292053222656\n60.48087692260742\n59.21522903442383\n51.96377944946289\n53.79441452026367\n63.579402923583984\n65.1745376586914\n54.898189544677734\n50.91022872924805\n55.830299377441406\n47.503177642822266\n56.56501770019531\n46.5484504699707\n43.91749954223633\n50.70798110961914\n48.224388122558594\n69.06616973876953\n62.38393020629883\n53.78395080566406\n61.84634780883789\n55.61307907104492\n48.13108825683594\n55.1087532043457\n62.52896499633789\n52.36894226074219\n52.819580078125\n73.38019561767578\n87.60235595703125\n78.37958526611328\n61.38961410522461\n65.26103973388672\n70.43557739257812\n"},"children":[],"key":"tilTzcTy10"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":43,"metadata":{},"data":{"text/plain":{"content":"70.43557739257812","content_type":"text/plain"}}},"children":[],"key":"mqju9wIqx6"}],"key":"F1AnaOiQ1M"}],"key":"sSigtbs48N"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Haha, no thanks. So, we know that a satisfactory learning rate lies between ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZjsqeokBHH"},{"type":"inlineCode","value":"0.001","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YFIuEsaOfv"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IS2HKWavKZ"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tV0igo4dad"},{"type":"text","value":". To find it, we can lay these numbers out, exponentially separated:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uX7SpSokUT"}],"key":"FElIEHLh25"}],"key":"eyzj55i4zI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"step = 1000\nlre = torch.linspace(-3, 0, step)\nlrs = 10**lre\nlrs","key":"IuuvkBE4eg"},{"type":"outputs","id":"JQwtMVlHrszVLDjBNpXcB","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":44,"metadata":{},"data":{"text/plain":{"content":"tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n        1.0000])","content_type":"text/plain"}}},"children":[],"key":"V58SbgS1qU"}],"key":"INESbTvjR4"}],"key":"sY0PeDfpnY"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"then, we plot the loss for each learning rate:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rAeNsQKoLq"}],"key":"pk6A2otV5r"}],"key":"b4IndFzOdB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn()\nlrei = []\nlossi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(1000):\n    batchix = torch.randint(0, x_all.shape[0], (batchsize,))\n    bx, by = x_all[batchix], y_all[batchix]\n    loss = forward_pass(bx, by)\n    # print(loss.item())\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    lr = lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n    lrei.append(lre[i])\n    lossi.append(loss.item())\nplt.figure()\nplt.plot(lrei, lossi)","key":"Hh3XOnchYj"},{"type":"outputs","id":"rBdrzN43JywNcqD59Lwj4","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"e9e22b74e1844fd4b84c5f82be7c853c\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"cf4563592fce8c1c14bc482165ba7665","path":"/build/cf4563592fce8c1c14bc482165ba7665.png"},"text/html":{"content_type":"text/html","hash":"9812705dc3b5ead7acfa74e727626910","path":"/build/9812705dc3b5ead7acfa74e727626910.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"BvtL2VF999"}],"key":"RqX6A1Exp3"}],"key":"FL23mDDIyp"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here, we see the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hj0YLx3Wbc"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q87uOzSPt3"}],"key":"OazHsV7pLS"},{"type":"text","value":" dropping as the exponent of the learning rate starts to increase, then, after a learning rate of around ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wSny8Ubvx2"},{"type":"inlineCode","value":"0.5","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wrv0elzsO9"},{"type":"text","value":", the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WtuHXIuwrB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LCJW699897"}],"key":"T3G1d6lgnO"},{"type":"text","value":" starts to increase. A good rule of thumb is to pick a learning rate whose at a point around which the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"C9LwpB0242"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d5StgGyg39"}],"key":"u4xTNddCRz"},{"type":"text","value":" is the lowest and most stable, before any increasing tendency. Let’s pick the learning rate whose exponent corresponds to the lowest ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NX8RkQTR1q"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yspl1RBKK8"}],"key":"Iy4GZM5ppe"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"najETxd68e"}],"key":"mdjlCfUogI"}],"key":"CR1ZZy8yEy"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lr = 10**lrei[lossi.index(min(lossi))]\nlr","key":"ezZKxrylja"},{"type":"outputs","id":"SPYZtlyKoYSSxt9rCdUQX","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":46,"metadata":{},"data":{"text/plain":{"content":"tensor(0.2309)","content_type":"text/plain"}}},"children":[],"key":"K5otfTA0mT"}],"key":"aoS0Bxcv9G"}],"key":"VFxp54ym1u"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now we have some confidence that this is a fairly good learning rate. Now let’s train for many epochs using this new learning rate!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d5rM7pVtz8"}],"key":"CmJQ5LX2AG"}],"key":"QXazCj9s8s"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"_ = train(x_all, y_all, lr=lr, epochs=10000, print_all_losses=False)","key":"K3Gb7gT0w3"},{"type":"outputs","id":"bk77djTYqZJOOGyP7f_1l","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"2.5242083072662354\n"},"children":[],"key":"kn9MXDuvDR"}],"key":"dKkFRvMMU6"}],"key":"pFfe67NT66"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Nice. We got a much smaller ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rXH9PD5HZl"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tJlGJbGnMT"}],"key":"GD2iprMO90"},{"type":"text","value":" after training. We have dramatically improved on the bigram language model, using this simple ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R60d6JiNzX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mkVDBjmfH4"}],"key":"Clw0V6FF88"},{"type":"text","value":" of only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tA4de6FoL4"},{"type":"text","value":"3481","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NOk1gRBXF0"},{"type":"text","value":" parameters. Now, there’s something we have to be careful with. Although our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k5OZsa16cO"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lawN71opDm"}],"key":"EKjdDd1AFW"},{"type":"text","value":" is the lowest so far, it is not ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CwA65rGSa3"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"exactly","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gkvT9PgqoN"}],"key":"z5DftUt8h6"},{"type":"text","value":" true to say that we now have a better model. The reason is that this is actually a very small model. Even though these kind of models can get much larger by adding more and more parameters, e.g. with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A8jLIiBClt"},{"type":"text","value":"10000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fle2EDw3nn"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NDnTzGhTJ6"},{"type":"text","value":"100000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"huau4aAhos"},{"type":"text","value":" or a million parameters, as the capacity of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qs0EkluMk7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pl6GQQu8wl"}],"key":"bA19mC7s7W"},{"type":"text","value":" grows, it becomes more and more capable of overfitting your training set. What that means is that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oce32pBG0z"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wFrPhcBFlD"}],"key":"h1SM7nyNRQ"},{"type":"text","value":" on the training set (the data that you are training on), will become very very low. As low as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zM8j1p6Ygv"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nXLPPvjHBw"},{"type":"text","value":". But in such a case, all that the model is doing is memorizing your training set exactly, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YAdBElfD5p"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"verbatim","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I5LIcdJ18E"}],"key":"pbOVJuTz0O"},{"type":"text","value":". So, if you were to take this model and it’s working very well, but you try to sample from it, you will only get examples, exactly as they are in the training set. You won’t get any new data. In addition to that, if you try to evaluate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LV13XcOK60"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"D0z8tcDERu"}],"key":"PYqD5JqEDf"},{"type":"text","value":" on some withheld names or other input data (e.g. words), you will actually see that the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YdOOplpzZK"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m0zTwjTR6m"}],"key":"NIB3xSdfcf"},{"type":"text","value":" of those will be very high. And so it is in practice basically not a very good model, since it doesn’t generalize. So, it is standard in the field to split up the dataset into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sfpLaZpySH"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BuDHul8XJX"},{"type":"text","value":" splits, as we call them: the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wNioVixbEu"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eQg7eFfOiB"}],"key":"TYNiCuQSKm"},{"type":"text","value":" split (roughly ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t9Iu4lCFHX"},{"type":"inlineMath","value":"80\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>80</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">80\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">80%</span></span></span></span>","key":"MDPybAoSz0"},{"type":"text","value":" of data), the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QNmosjpc4y"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dev","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kx6K9JDExn"}],"key":"pvIjjYl6ij"},{"type":"text","value":"/","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"frvUU0ickN"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"validation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pQlPgGqWFX"}],"key":"FsyTSjOXI0"},{"type":"text","value":" split (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x57414tenE"},{"type":"inlineMath","value":"10\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>10</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">10%</span></span></span></span>","key":"b54OvokXUf"},{"type":"text","value":") and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N9HnHhGLQC"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"test","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zyy9bJkiRV"}],"key":"t6f1acDzp5"},{"type":"text","value":" split (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z3LTOWbh2Z"},{"type":"inlineMath","value":"10\\%","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>10</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">10\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">10%</span></span></span></span>","key":"GuQLZw7nTe"},{"type":"text","value":").","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BhtLWuPZgw"}],"key":"TWejhnUPBl"}],"key":"cbMjHFgWIX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# training split, dev/validation split, test split\n# 80%, 10%, 10%","key":"SgrhkLPPkG"},{"type":"outputs","id":"d5qn7r_bdJA98_wjfb8Bb","children":[],"key":"WYGrEg2C8L"}],"key":"W8WJ3u4iTO"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, the training split is used to optimize the parameters of the model (for training). The validation split is typically used for development and tuning over all the hyperparameters of the model, such as the learning rate, layer width, embedding size, regularization parameters, and other settings, in order to choose a combination that works best on this split. The test split is used to evaluate the performance of the model at the end (after training). So, we are only evaluating the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qSEtIvsP53"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mjf81bTOyN"}],"key":"o0Vci0vkoH"},{"type":"text","value":" on the test split very very sparingly and very few times. Because, every single time you evaluate your test ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ItJquF1aQm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aARyZR7ln1"}],"key":"uBYHo8P0Xm"},{"type":"text","value":" and you learn something from it, you are basically trying to also train on the test split. So, you are only allowed to evaluate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Rrunl0BkEe"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o89m6Rsvge"}],"key":"OtHpPU2D2o"},{"type":"text","value":" on the test dataset very few times, otherwise you risk overfitting to it as well, as you experiment on your model. Now, let’s actually split our dataset into training, validation and test datasets. Then, we are going to train on the training dataset and only evaluate on the test dataset very very sparingly. Here we go:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"viWbGRQeKu"}],"key":"TfMCwO09O5"}],"key":"IFRPkHDdRY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import random\n\nrandom.seed(42)\nrandom.shuffle(words)\nlenwords = len(words)\nn1 = int(0.8 * lenwords)\nn2 = int(0.9 * lenwords)\nprint(f\"{lenwords=}\")\nprint(f\"{n1} words in training set\")\nprint(f\"{n2 - n1} words in validation set\")\nprint(f\"{lenwords - n2} words in test\")","key":"LGHA0wR6Qv"},{"type":"outputs","id":"TAseUwIKcgDsBTEYCALDK","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"lenwords=32033\n25626 words in training set\n3203 words in validation set\n3204 words in test\n"},"children":[],"key":"yaKXcwCl4m"}],"key":"K6tYjDDseu"}],"key":"gjlUP4RCDi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xtrain, ytrain = build_dataset(words[:n1])\nxval, yval = build_dataset(words[n1:n2])\nxtest, ytest = build_dataset(words[n2:])","key":"DBdNBgvPP1"},{"type":"outputs","id":"2fz24XzUxX-TTOGVNqnlu","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182580, 3]) torch.Size([182580])\ntorch.Size([22767, 3]) torch.Size([22767])\ntorch.Size([22799, 3]) torch.Size([22799])\n"},"children":[],"key":"vHTulAo2kc"}],"key":"rmWggKLddn"}],"key":"nEAdJ58fke"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We now have the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lzvDxCCgcc"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hHNwm361Nm"},{"type":"text","value":" split sets. Great! Let’s now re-define our parameters train, anew, on the training dataset:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZwKJUsW6Dp"}],"key":"WfUuhw8Sn1"}],"key":"peBVlkboNR"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_train = train(xtrain, ytrain, lr=lr, epochs=30000, print_all_losses=False)","key":"ZqUVbWUfFH"},{"type":"outputs","id":"HrQE9NiDvgsy_X4h94E0O","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"2.083573579788208\n"},"children":[],"key":"A5XnKxXK5V"}],"key":"iA6tqcF6RG"}],"key":"jSVjQSnbm0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome. Our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XyGoxcGysS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NWwLJXOs6a"}],"key":"jWHQROQpBW"},{"type":"text","value":" has been trained and the final ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mrrlhwRRks"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ISsnHsvlmd"}],"key":"d4fi72IcWh"},{"type":"text","value":" is actually surprisingly good. Let’s now evaluate the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oTkCqjQoRq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QTt7Le3xSu"}],"key":"UUgVsVeMs8"},{"type":"text","value":" of the validation set (remember, this data was not in the training set on which it was trained):","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AhXJ4T1Mb6"}],"key":"BiOc0npORs"}],"key":"yoyc1tUt8F"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_val = forward_pass(xval, yval)\nloss_val","key":"QocBj9hrKS"},{"type":"outputs","id":"Pk3ZMU4HH6kcyyO1H73rO","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":52,"metadata":{},"data":{"text/plain":{"content":"tensor(2.4294, grad_fn=<NllLossBackward0>)","content_type":"text/plain"}}},"children":[],"key":"MKVA7lADzt"}],"key":"jgeFAkdE0b"}],"key":"lgFdXgUd8b"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Not too bad! Now, as you can see, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qHgARTeavh"},{"type":"inlineCode","value":"loss_train","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RwANoIcGzd"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Au8QEACeLF"},{"type":"inlineCode","value":"loss_val","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EJwkhhYfeh"},{"type":"text","value":" are pretty close. In fact, they are roughly equal. This means that we are ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FHH9AsEPud"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"not","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SjgyCCAaIR"}],"key":"WI2QYgysPx"},{"type":"text","value":" overfitting, but underfitting. It seems that this model is not powerful enough so as not to be purely memorizing the data. Basically, our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"r7JKUmJqH7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XFohfDe8wz"}],"key":"ju6cDil3sz"},{"type":"text","value":" is very tiny. But, we can expect to make performance improvements by scaling up the size of this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xYNG5ZfjWP"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HFs0fD6VRk"}],"key":"KTXSdiJ2H5"},{"type":"text","value":". The easiest way to do this is to redefine our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L16We0evwI"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jgKATQ0PNX"}],"key":"jUCB1FBfQq"},{"type":"text","value":" with more neurons in the hidden layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ig88jtEL6x"}],"key":"gMELKBgciP"}],"key":"ns56LNGFxi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(l1out=300)","key":"t2Wqmpvraa"},{"type":"outputs","id":"klbXJ-MlzLoxitqo-ECqm","children":[],"key":"AEd0uq8N8N"}],"key":"HQFaoYMHin"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Then, let’s re-train and visualize the loss curve:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JKu2JwouBZ"}],"key":"XL7iohkXeE"}],"key":"a21DBKVhFk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi = []\nstepi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(30000):\n    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))\n    bx, by = xtrain[batchix], ytrain[batchix]\n    loss = forward_pass(bx, by)\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    for p in parameters:\n        p.data += -lr * p.grad\n    stepi.append(i)\n    lossi.append(loss.log10().item())\nplt.figure()\nplt.plot(stepi, lossi)","key":"A80kM2g1Xi"},{"type":"outputs","id":"Pce6Kl4PdfrJAGWAHjcef","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"7435f13fbd754dadbafc577840424b04\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"2cbad2c6c621b032a7b473cbccdde177","path":"/build/2cbad2c6c621b032a7b473cbccdde177.png"},"text/html":{"content_type":"text/html","hash":"d90e7262aaa68b89d50c492cd5d117be","path":"/build/d90e7262aaa68b89d50c492cd5d117be.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"DrE8tYZ5hT"}],"key":"yYoX3Q6LZQ"}],"key":"D5MM6VQKUS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, it is a bit noisy, but that is just because of the minibatches!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EENKSgVVy9"}],"key":"RXDOoFbjHX"}],"key":"LbhMRZJmuK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_train = forward_pass(xtrain, ytrain)\nloss_val = forward_pass(xval, yval)\nprint(loss_train)\nprint(loss_val)","key":"EOHh5MYO8m"},{"type":"outputs","id":"0X1FRHNvDqR5PHyADPysF","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(2.4805, grad_fn=<NllLossBackward0>)\ntensor(2.4808, grad_fn=<NllLossBackward0>)\n"},"children":[],"key":"Tn22Tf2I4X"}],"key":"GFdscfWmTb"}],"key":"Dg1J8NOF77"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome, the training loss is actually lower than before, whereas the validation loss is pretty much the same. So, increasing the size of the hidden layer gave us some benefit. Let’s experiment more to see if we can get even lower losses by increasing the embedding layer. First though, let’s visualize the character embeddings:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mI6WhVDntR"}],"key":"zpHm2IYjY4"}],"key":"T76WmlfMb5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# visualize dimensions 0 and 1 of the embedding matrix C for all characters\nplt.figure(figsize=(8, 8))\nplt.scatter(C[:, 0].data, C[:, 1].data, s=200)\nfor i in range(C.shape[0]):\n    plt.text(\n        C[i, 0].item(),\n        C[i, 1].item(),\n        itoc[i],\n        ha=\"center\",\n        va=\"center\",\n        color=\"white\",\n    )\nplt.grid(\"minor\")","key":"O0cFLJ8GQa"},{"type":"outputs","id":"GQ2g1qt8YvOhzRTyb7wTB","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9e5b59f480324492897003bc6557de7b\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"8c426aae1269dc28fa53e703a7eaae9f","path":"/build/8c426aae1269dc28fa53e703a7eaae9f.png"},"text/html":{"content_type":"text/html","hash":"ac1d5d2a7f7cd24ad610bfc9c41fdf7f","path":"/build/ac1d5d2a7f7cd24ad610bfc9c41fdf7f.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"TN26CXQnsQ"}],"key":"gahYHPEqzg"}],"key":"nfz51hMJ13"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The network has basically learned to separate out the characters and cluster them a little bit. For example, it has learned some characters are usually found more closer together than others. Let’s try to improve our model loss by choosing a greater embeddings layer size of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nktjgl2ZXB"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZaNIclyIgT"},{"type":"text","value":" and by increasing the number of epochs to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fPWPtqiOhn"},{"type":"text","value":"200000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uQ5zqdMtBc"},{"type":"text","value":", also we’ll decay the learning rate after ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"abXrOlIFjJ"},{"type":"text","value":"100000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WLRlc8bKGf"},{"type":"text","value":" epochs:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BB5iEa7xHv"}],"key":"jIM8I5kfzL"}],"key":"a7YMKFqwcm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"parameters = define_nn(l1out=200, embsize=10)","key":"IrcpyJd2sn"},{"type":"outputs","id":"G_vJiwz1RNgpuwH5uR_AR","children":[],"key":"MH7tqAvmxT"}],"key":"ViplkqEjkF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi = []\nstepi = []\nfor p in parameters:\n    p.requires_grad = True\nfor i in range(200000):\n    batchix = torch.randint(0, xtrain.shape[0], (batchsize,))\n    bx, by = xtrain[batchix], ytrain[batchix]\n    loss = forward_pass(bx, by)\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    lr = 0.1 if i < 100000 else 0.01\n    for p in parameters:\n        p.data += -lr * p.grad\n    stepi.append(i)\n    lossi.append(loss.log10().item())\nplt.figure()\nplt.plot(stepi, lossi);","key":"O2FZmtApfB"},{"type":"outputs","id":"jEp8U5B8pXtxasbHWWzLk","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"8278e481a5184d788204c09648462766\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"1a716447d3751b1ef4e3d97328f400b4","path":"/build/1a716447d3751b1ef4e3d97328f400b4.png"},"text/html":{"content_type":"text/html","hash":"69e928816945882ac57b7cf5a3ff305e","path":"/build/69e928816945882ac57b7cf5a3ff305e.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"e3O0go5wfI"}],"key":"NkTJWh7z5t"}],"key":"GlX0vpSQfi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"loss_train = forward_pass(xtrain, ytrain)\nloss_val = forward_pass(xval, yval)\nprint(loss_train)\nprint(loss_val)","key":"OkqqXDyT5T"},{"type":"outputs","id":"oOA9tekT75dyW727y-Koy","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"tensor(2.1189, grad_fn=<NllLossBackward0>)\ntensor(2.1583, grad_fn=<NllLossBackward0>)\n"},"children":[],"key":"AzxaVOuiFd"}],"key":"EId01v2j9U"}],"key":"r3BP5G2va4"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Wow, we did it! Both train and validation losses are lower now. Can we go lower? Play around and find out! Now, before we end this lesson, let’s sample from our model:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zXAhMyQuBH"}],"key":"JLOynXdQuo"}],"key":"RoSUs1kmbN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\nfor _ in range(20):    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        emb = C[torch.tensor([context])] # (1,block_size,d)\n        h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n        logits = h @ w2 + b2\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join(itoc[i] for i in out))","key":"W5ZhtLaDqz"},{"type":"outputs","id":"ymi1M-076OvERbztxoYOW","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"mona.\nkayah.\nsee.\nmed.\nryla.\nremmastendrael.\nazeer.\nmelin.\nshivonna.\nkeisen.\nanaraelynn.\nhotelin.\nshaber.\nshiriel.\nkinze.\njenslenter.\nfius.\nkavder.\nyaralyeha.\nkayshayton.\n"},"children":[],"key":"nn3JQrWDJI"}],"key":"aTKOL4nbTi"}],"key":"x69VYu1LCs"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Outro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iRliLTxmEI"}],"identifier":"outro","label":"Outro","html_id":"outro","implicit":true,"key":"j11Z6zzKkx"}],"key":"Ndnx4dLjfv"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, our model now is pretty decent and able to produce suprisingly name-like text, which is what we wanted all along! Next up, we will explore ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bay94xxh7j"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hkT0FTL9jy"}],"key":"o5kyLcXskk"},{"type":"text","value":" internals and other such magic.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZWa6v1WHXK"}],"key":"ivExNcmQpm"}],"key":"NgtIQaOmTj"}],"key":"hbQDOVvnj9"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"2. makemore (part 1): implementing a bigram character-level language model","url":"/micrograduate/makemore1","group":"microgra∇uate"},"next":{"title":"4. makemore (part 3): activations & gradients, batchnorm","url":"/micrograduate/makemore3","group":"microgra∇uate"}}},"domain":"http://localhost:3000"}