{"version":3,"kind":"Notebook","sha256":"10cc35aed28ce253f4849cd83b2710de172d81ae0479bdcbda690be595c94a1d","slug":"micrograduate.makemore5","location":"/micrograduate/makemore5.ipynb","dependencies":[],"frontmatter":{"title":"6. makemore (part 5): building a WaveNet","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"micrograduate-env","language":"python"},"github":"https://github.com/ckaraneen/micrograduate","copyright":"MIT License","source_url":"https://github.com/ckaraneen/micrograduate/blob/main/micrograduate/makemore5.ipynb","edit_url":"https://github.com/ckaraneen/micrograduate/edit/main/micrograduate/makemore5.ipynb","exports":[{"format":"ipynb","filename":"makemore5.ipynb","url":"/build/makemore5-1d9f666389c448cb027e77d41492d5c3.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import sys\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    print(\"Cloning repo...\")\n    !git clone --quiet https://github.com/ckaraneen/micrograduate.git > /dev/null\n    %cd micrograduate\n    print(\"Installing requirements...\")\n    !pip install --quiet uv\n    !uv pip install --system --quiet -r requirements.txt","key":"hEOvAEGWet"},{"type":"outputs","id":"gYY77dXqfkP0mYSV4OCGY","children":[],"key":"BCf9ROg9Ye"}],"key":"TJOkVHJISo"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Intro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rX2C6w0SLj"}],"identifier":"intro","label":"Intro","html_id":"intro","implicit":true,"key":"wcQm3P4EhW"}],"visibility":"show","key":"y9NtprTMZ2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Hi, everyone! Today we are continuing our implementation of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KAPkJQIuUS"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l3Y0hFGsDI"}],"key":"RswE8HQnYY"},{"type":"text","value":", our favorite character-level language model. Now, over the last few lectures, we’ve built up an architecture that is a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZgEKdrRMl0"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"mlp","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wRfs9klnDa"}],"key":"MDlQHrSiaP"},{"type":"text","value":" character-level language model. So we see that it receives ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mTIQoh48O6"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DD61TdcD6D"},{"type":"text","value":" previous characters and tries to predict the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QUaQbHfnuj"},{"type":"inlineMath","value":"4th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">4th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"Eg4vHq2KWz"},{"type":"text","value":" character in a sequence using one hidden layer of neurons with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dN0y137fMw"},{"type":"inlineCode","value":"tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZrmiQlDxKv"},{"type":"text","value":" nonlinearities:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j3R3nqSWXn"}],"key":"z03JQ0ELVn"}],"key":"INVuAqAwEe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"bengio2003nn.jpeg\"))","key":"zGy7hj8Ca6"},{"type":"outputs","id":"1nyHiQ7mOiVhtBIZRcJCj","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"a21abcc7498c74c85d4a3cd5f51b3817","path":"/build/a21abcc7498c74c85d4a3cd5f51b3817.jpeg"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"UiJhM8OEQ2"}],"key":"Y1XM4h9E1e"}],"key":"toE6Ch4h2V"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So what we’d like to do now in this lecture is to complexify this architecture. In particular, we would like to take more characters in a sequence as an input, not just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EcN8hEfVIu"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SjoOI6kwU5"},{"type":"text","value":". In addition to that, we don’t just want to feed them all into a single hidden layer, because that squashes too much information too quickly. Instead, we would like to make a deeper model that progressively fuses this information to make its guess about the next character in a sequence. We’re actually going to arrive at something that looks very much like ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ix1hkGTgMn"},{"type":"link","url":"https://arxiv.org/abs/1609.03499","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HWwN5Qj0ip"}],"key":"Fra1IAMDNT"},{"type":"text","value":", a paper published by DeepMind in 2016","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fvx32aHsaw"}],"urlSource":"https://arxiv.org/abs/1609.03499","key":"qIjRVydXAt"},{"type":"text","value":". Which is a language model basically, but it tries to predict audio sequences instead of character-level sequences or word-level sequences:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FC5z8MYeyi"}],"key":"NYbOR92XMe"}],"key":"CMo7AKCMq4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig1.png\"))","key":"UkeMSu9bsE"},{"type":"outputs","id":"AH6tLI8eqcdwruCMkGPmW","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"3f9530b394e99dfd6655c51628e3e70c","path":"/build/3f9530b394e99dfd6655c51628e3e70c.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"kTNwJSrHSZ"}],"key":"QXRpUj1qZt"}],"key":"mt1wta93Ay"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"But fundamentally, the modeling setup is identical. It is an autoregressive model and it tries to predict the next character in a sequence:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RuGIJB1PmC"}],"key":"h3lIKtDNyH"}],"key":"lFY7Xo2n83"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_eq1.png\"))","key":"xq1mUDqoh2"},{"type":"outputs","id":"uSefDZ6ccDyHP6-2yjpu4","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"45fa30aa01de3ef1b6dadc47c8cfd86f","path":"/build/45fa30aa01de3ef1b6dadc47c8cfd86f.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"RFPDwhGtTk"}],"key":"U8PU0RAvPk"}],"key":"SP2XlCWQpL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And the architecture actually takes this interesting hierarchical sort of approach to predicting the next character in a sequence with this tree-like structure:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nnTQwOJRl8"}],"key":"qlblU38x3d"}],"key":"QAkMiDqin8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))","key":"elAqSmZtBD"},{"type":"outputs","id":"BGvdF_ByyjWn9RYXYt3oV","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"198c39366ff68e8b03ccb06df349f47a","path":"/build/198c39366ff68e8b03ccb06df349f47a.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"KCAldlfiC9"}],"key":"RSHZPxFfoE"}],"key":"nV4BeJvd9A"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And this is the architecture:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nm7MO3km3z"}],"key":"iydvYuuEIj"}],"key":"EdESN8JhPK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig4.png\"))","key":"HdtXA7DAKf"},{"type":"outputs","id":"fjX02hT7k2S3jr0tMPdN2","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"54272b3d206b11fb693990a90d767eee","path":"/build/54272b3d206b11fb693990a90d767eee.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"BYaVToePTk"}],"key":"pMJXjVyPjA"}],"key":"lxdRWTs4fs"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And we’re going to implement it in this lesson. So let’s get started!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MLYFYNSIyN"}],"key":"SChTK4YiuX"}],"key":"B2CVC33fZf"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Starter code walkthrough","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FTZ7MTmnSv"}],"identifier":"starter-code-walkthrough","label":"Starter code walkthrough","html_id":"starter-code-walkthrough","implicit":true,"key":"sdZrfv0Ikr"}],"visibility":"show","key":"bRW17qpOcE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The starter code for this part is very similar to where we ended up in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NeZWccs5C1"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zlgKpcWXut"}],"key":"lBt7OlAuY2"},{"type":"text","value":" (part 3). So very briefly, we are doing imports:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ICDju5DkJL"}],"key":"WgDmOyRz06"}],"key":"eMFuLH63Vw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import random\nrandom.seed(42)\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\nif IN_COLAB:\n    %matplotlib inline\nelse:\n    %matplotlib ipympl\nSEED = 2147483647","key":"nHFhlUGMS3"},{"type":"outputs","id":"8kAPrbPqJlFafN5alaiiE","children":[],"key":"oMxhVwxH7R"}],"key":"bxPnKmMXEC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We are reading our data set of words:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UBOmU866Jf"}],"key":"YgiP5Hvjae"}],"key":"GrnF1B3oNP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# read in all the words\nwords = open(\"names.txt\", \"r\").read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])","key":"GAkRVIIqTK"},{"type":"outputs","id":"jedYL-OPB4NukoHYKWme-","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"},"children":[],"key":"CWH6OlG1L1"}],"key":"C2sNs5XpgJ"}],"key":"OJnZL5SNyS"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And we are processing the dataset of words into lots and lots of individual examples:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y4Sn8kJcct"}],"key":"QMw2GGi8NO"}],"key":"QQnJ2cotjM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(\"\".join(words))))\nctoi = {s: i + 1 for i, s in enumerate(chars)}\nctoi[\".\"] = 0\nitoc = {i: s for s, i in ctoi.items()}\nvocab_size = len(itoc)\nprint(itoc)\nprint(vocab_size)","key":"b5oLo6maDS"},{"type":"outputs","id":"xPqfH8-CZOMupt_eMunHD","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n"},"children":[],"key":"Z0eYHuGtaK"}],"key":"WBwvNgP1IT"}],"key":"LZeHxS3Kgs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def build_dataset(words, block_size):\n    x, y = [], []\n    for w in words:\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = ctoi[ch]\n            x.append(context)\n            y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n    x = torch.tensor(x)\n    y = torch.tensor(y)\n    print(x.shape, y.shape)\n    return x, y","key":"Jr9dbG3gQr"},{"type":"outputs","id":"dxVlU4naAx9iS2bgg2NVT","children":[],"key":"VzSFxYiSW9"}],"key":"tjfdWkIfwh"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def build_all_datasets(block_size):\n    random.shuffle(words)\n    n1 = int(0.8 * len(words))\n    n2 = int(0.9 * len(words))\n    xtrain_dataset = build_dataset(words[:n1], block_size)  # 80%\n    xval_dataset = build_dataset(words[n1:n2], block_size)  # 10%\n    xtest_dataset = build_dataset(words[n2:], block_size)  # 10%\n    return xtrain_dataset, xval_dataset, xtest_dataset","key":"MSKFG8fRi5"},{"type":"outputs","id":"z-lT_5MQxyPnU7ISGpwGA","children":[],"key":"e13pTM4u5E"}],"key":"ihhts59iUv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def print_next_character(xtrain, ytrain):\n    for x, y in zip(xtrain, ytrain):\n        print(\"\".join(itoc[ix.item()] for ix in x), \"-->\", itoc[y.item()])","key":"BqsNPJ9wTa"},{"type":"outputs","id":"BhPg4T9wzxF6bbIVmbVmj","children":[],"key":"VdhEVfDpHl"}],"key":"pMD6CVGzqn"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Specifically many examples of...","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bs99ooJyk4"}],"key":"P20WsbQzKS"}],"key":"HbFnuJJeeK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"block_size = (\n    3  # context length: how many characters do we take to predict the next one?\n)\n(xtrain, ytrain), (xval, yval), (xtest, ytest) = build_all_datasets(block_size)","key":"G3dX2lhUfh"},{"type":"outputs","id":"BEoBbDRd3245IMzdKsre3","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n"},"children":[],"key":"o1IMdvLSdx"}],"key":"MgM8YJPwiK"}],"key":"WcnKVxoSPn"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"... ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ri3geoBYSi"},{"type":"inlineCode","value":"block_size=3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wuhTXR0ewG"},{"type":"text","value":" characters and we are trying to predict the fourth one:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hnuENt4WH2"}],"key":"zZRW5471t7"}],"key":"vU22syfbZY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_next_character(xtrain[:20], ytrain[:20])","key":"DI64o9bRMY"},{"type":"outputs","id":"3YIxpMdj-YeclClv3rnGq","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"... --> y\n..y --> u\n.yu --> h\nyuh --> e\nuhe --> n\nhen --> g\neng --> .\n... --> d\n..d --> i\n.di --> o\ndio --> n\nion --> d\nond --> r\nndr --> e\ndre --> .\n... --> x\n..x --> a\n.xa --> v\nxav --> i\navi --> e\n"},"children":[],"key":"cjAWyKOoNX"}],"key":"OoZUl6FMPG"}],"key":"fHAHps9K7G"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Basically, we are breaking down each of these word into little problems of “given ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mJhS9E8lzM"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YLAQhhPg8T"},{"type":"text","value":" characters, predict the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AVYDBPaN6C"},{"type":"inlineMath","value":"4th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">4th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"K5h1c7TbEB"},{"type":"text","value":" one”. So this is our data set and this is what we’re trying to get the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LmdgkViKW6"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qUsicSvwdF"}],"key":"rqvmzYHWFj"},{"type":"text","value":" to do. Now in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"be5CTlr7da"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"makemore","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"X2AJJZUqZi"}],"key":"LH3WvyVZVE"},{"type":"text","value":" (part 3), we started to develop our code around these following layer modules. We’re doing this because we want to think of these modules as lego building blocks that we can sort of stack up into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GZxk0XWEJq"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H2QIr3jCl6"}],"key":"GQms1VMege"},{"type":"text","value":"s and we can feed data between these layers and stack them up into sort of graphs. Now we also developed these layers to have APIs and signatures very similar to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZTe4eMTF8B"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"those that are found in PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M32evskD5R"}],"urlSource":"https://pytorch.org/docs/stable/nn.html","key":"s1p7Iar7nC"},{"type":"text","value":". And so we have the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eIH3UCQca7"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yBXIqJxjhS"},{"type":"text","value":" layer, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WuH9xb0Sk0"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fWoQXlYNnp"},{"type":"text","value":" layer and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ehbrrb2APa"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nwg6YJ6SU2"},{"type":"text","value":" layer that we developed previously:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"epszHwynR5"}],"key":"wYhXZzAtg0"}],"key":"bbHYmyK6wb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True)  # batch mean\n            xvar = x.var(0, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (\n                    1 - self.momentum\n                ) * self.running_mean + self.momentum * xmean\n                self.running_var = (\n                    1 - self.momentum\n                ) * self.running_var + self.momentum * xvar\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []","key":"MPIiGhFKbD"},{"type":"outputs","id":"PJkunDuy8sX9k4JTSREih","children":[],"key":"JUAglld7JC"}],"key":"jPs3kvrbsm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Αnd ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hnW2gaTXOi"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Db6f4bTk8P"},{"type":"text","value":" just does a matrix multiply in the forward pass of this module, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"p1Y4wT9Tlq"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hBjwtBMnWg"},{"type":"text","value":" of course is this crazy layer that we developed in the previous lecture. What’s crazy about it is... well there’s many things. Number one, it has these running mean and variances that are trained outside of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uWBhTj46rC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"backprop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FHFnyqIz2t"}],"key":"Pm06ZHPujQ"},{"type":"text","value":". They are trained using exponential moving average inside this layer when we call the forward pass. In addition to that, there’s this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NYWyzl22rV"},{"type":"inlineCode","value":"self.training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Su4iYs3VpG"},{"type":"text","value":" flag because the behavior of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hcl54YW3Uy"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TaSPpwrCDN"}],"key":"CK5PUjarzj"},{"type":"text","value":" is different during train time and evaluation time. And so suddenly we have to be very careful that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j7E2mCu4SL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H0XMGJpbq6"}],"key":"mK7cnc3qvI"},{"type":"text","value":" is in its correct state. That it’s in the evaluation state or training state. So that’s something to now keep track of something that sometimes introduces bugs because you forget to put it into the right mode. And finally, we saw that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LUDhklgKdg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NcaCcSk8TG"}],"key":"PrzjgrtKkk"},{"type":"text","value":" couples the statistics or the activations across the examples in the batch. So normally we thought of the batch as just an efficiency thing, but now we are coupling the computation across batch elements and it’s done for the purposes of controlling the activation statistics as we saw in the previous video. So ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lO1mrYxtsy"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YmJ7nla5ld"}],"key":"ue8FZzEdBr"},{"type":"text","value":" is a very weird layer because you have to modulate the training and eval phase. What’s more, you have to wait for the mean and the variance to settle and to actually reach a steady state and a state can become the source of many bugs, usually. And now let’s define the appropriate functions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HAk4YXJrMm"}],"key":"s8vyirVrGY"}],"key":"ZDgxY478YC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# seed rng for reproducability\ntorch.manual_seed(42)","key":"xL1qgQMrs6"},{"type":"outputs","id":"odRGhHn8QqXUXaUb36rFy","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":23,"metadata":{},"data":{"text/plain":{"content":"<torch._C.Generator at 0x7ffa740b3d90>","content_type":"text/plain"}}},"children":[],"key":"hhYvVpgsp2"}],"key":"q1oKEJ6Y1w"}],"key":"AyBIp79DzM"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"n_embd = 10  # the dimensionality of the character embedding vectors\nn_hidden = 200  # the number of neurons in the hidden layer of the MLP\n\n\ndef define_nn(block_size, n_embd, n_hidden):\n    global C\n    C = torch.randn((vocab_size, n_embd))\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    layers = [\n        Linear(n_inputs, n_hidden, bias=False),\n        BatchNorm1d(n_hidden),\n        Tanh(),\n        Linear(n_hidden, n_outputs),\n    ]\n    # parameter init\n    with torch.no_grad():\n        layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = [C] + [p for l in layers for p in l.parameters()]\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters","key":"plGXsSeP1c"},{"type":"outputs","id":"Ho4I3pqQ2BEOPSUqPQeSS","children":[],"key":"smLi1CPDM5"}],"key":"Xu9VV9bXU5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(layers, xb, yb):\n    emb = C[xb]  # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, yb)  # loss function\n    return loss","key":"WIg2ad3C7X"},{"type":"outputs","id":"EcH2h1Ur97NoU2jAshmQK","children":[],"key":"EvA63xt0Bm"}],"key":"rwiMEF7lmz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def backward(parameters, loss):\n    for p in parameters:\n        p.grad = None\n    loss.backward()","key":"puXWCb8CFT"},{"type":"outputs","id":"mrz8B87U_ULyYyuBnLppu","children":[],"key":"ErXORow63Z"}],"key":"gYt029WiZE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def update(parameters, lr):\n    for p in parameters:\n        p.data += -lr * p.grad","key":"oj9UPrkLiL"},{"type":"outputs","id":"Wt8nuD0WzBQx9vrEfVWHX","children":[],"key":"xzoXuHrpcE"}],"key":"qijjpReV7L"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(\n    x,\n    y,\n    layers,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(layers, xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        if break_at_step is not None and i >= break_at_step:\n            break\n    return lossi","key":"r5JQpHoHhP"},{"type":"outputs","id":"xhTctOwv4hgwoXMRpQsNY","children":[],"key":"DTBFnLecOZ"}],"key":"FCfS2cyqR4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def trigger_eval_mode(layers):\n    for l in layers:\n        l.training = False","key":"sGdUPzg9ks"},{"type":"outputs","id":"LotaFBvK-ubpbuXKdLql8","children":[],"key":"qsBfoV8x9U"}],"key":"j5Po8kEUc3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@torch.no_grad()\ndef infer_loss(layers, x, y, prefix=\"\"):\n    loss = forward(layers, x, y)\n    print(f\"{prefix} {loss}\")\n    return loss","key":"FTjI3vg6tB"},{"type":"outputs","id":"5C6NeFLVIXm1zZ0DwKKNA","children":[],"key":"BeXLTEhPhL"}],"key":"BIUlJg1S9W"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def sample_from_model(block_size, layers):\n    for _ in range(20):\n        out = []\n        context = [0] * block_size  # initialize with all ...\n        while True:\n            # forward pass the neural net\n            emb = C[torch.tensor([context])]  # (1, block_size, n_embd)\n            x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n            for l in layers:\n                x = l(x)\n            logits = x\n            probs = F.softmax(logits, dim=1)\n            # sample from the distribution\n            ix = torch.multinomial(probs, num_samples=1).item()\n            # shift the context window and track the samples\n            context = context[1:] + [ix]\n            out.append(ix)\n            # if we sample the special '.' token, break\n            if ix == 0:\n                break\n        print(\"\".join(itoc[i] for i in out))  # decode and print the generated word","key":"rSdzAc6wnc"},{"type":"outputs","id":"uuHlzp5cLQ5Q_1KeGgc6R","children":[],"key":"METAi9rGAG"}],"key":"gGOQo6KqRl"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"These should look somewhat familiar to you by now. Let’s train!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"knTznqVDzx"}],"key":"mqA657rRZB"}],"key":"oMMaVMANuX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"layers, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, layers, parameters)","key":"edCFlaUDOY"},{"type":"outputs","id":"mMSRm3zk4pxjqeFLxQ56-","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"12097\n      0/ 200000: 3.2966\n"},"children":[],"key":"Us4NI3NdmE"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"  10000/ 200000: 2.2322\n  20000/ 200000: 2.4111\n  30000/ 200000: 2.1004\n  40000/ 200000: 2.3157\n  50000/ 200000: 2.2104\n  60000/ 200000: 1.9653\n  70000/ 200000: 1.9767\n  80000/ 200000: 2.6738\n  90000/ 200000: 2.0837\n 100000/ 200000: 2.2730\n 110000/ 200000: 1.7491\n 120000/ 200000: 2.2891\n 130000/ 200000: 2.3443\n 140000/ 200000: 2.1731\n 150000/ 200000: 1.8246\n 160000/ 200000: 1.7614\n 170000/ 200000: 2.2419\n 180000/ 200000: 2.0803\n 190000/ 200000: 2.1326\n"},"children":[],"key":"Bl56xqKAny"}],"key":"mSVJPUyd8U"}],"key":"lBngqH1Ei5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(lossi);","key":"Xv8arTn1ks"},{"type":"outputs","id":"NCvoSFpZ5VaCZD2KTOwId","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"c0ff1cdd47ba47c79578bbd5f0c5f983\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"48e19c2cf41d8f75fc83c18d5baccf0d","path":"/build/48e19c2cf41d8f75fc83c18d5baccf0d.png"},"text/html":{"content_type":"text/html","hash":"a25d5bfbebab9c1f574ead8922d0b080","path":"/build/a25d5bfbebab9c1f574ead8922d0b080.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"huOQ0wSbsZ"}],"key":"fi1T0uwcGe"}],"key":"vVoRZrvo32"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U9tlWWedYX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Pxfa0k2x0x"}],"key":"GELONbVtfw"},{"type":"text","value":" function looks very crazy. We should probably fix this. And that’s because ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CKgSKYS5xS"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dHsyTnKnc8"},{"type":"text","value":" batch elements are too few. And so you can get very lucky or unlucky in any one of these batches, and it creates a very thicc ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JfhYYsOY91"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ngmSAKzEvN"}],"key":"mno941TcLx"},{"type":"text","value":" function. So we’re gonna fix that soon. Now, before we evaluate the trained ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YCQhPBMNg7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G9Q3p53SMi"}],"key":"b3OKwIjUWV"},{"type":"text","value":" by inferring the training and validation ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H6c2q3VQ8G"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z1lAYZv4SJ"}],"key":"n8vuugmMpp"},{"type":"text","value":", we need to remember because of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kzXetptUcv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HroebG2sZG"}],"key":"N0yHbMdva8"},{"type":"text","value":" layers to set all the layers’ ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jWbwspqRLR"},{"type":"inlineCode","value":"training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DAPhGEHvLl"},{"type":"text","value":" flag to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ryZw7KHPLM"},{"type":"inlineCode","value":"False","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t8xtpM8LnD"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BBMtE2WOc0"}],"key":"matX0xU8CI"}],"key":"uEY8iQRon5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"trigger_eval_mode(layers)\ninfer_loss(layers, xtrain, ytrain, prefix=\"train\")\ninfer_loss(layers, xval, yval, prefix=\"val\");","key":"lWgoP3vKNI"},{"type":"outputs","id":"l7_AjNN_2_UTjfntJwmKt","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.0583250522613525\nval 2.1065292358398438\n"},"children":[],"key":"vdaeiacxY6"}],"key":"xmhm9HTwlz"}],"key":"ye1JOGwAuQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We still have a ways to go, as far as the validation ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CjOPtDtyjL"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tijZBfSvOB"}],"key":"cYQQo7ceh4"},{"type":"text","value":" is concerned. But if we sample from our model, we see that we get relatively name-like results that do no exist in the training set:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S5OeD58mne"}],"key":"b88kMOqQ6P"}],"key":"tysWrPIBNw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sample_from_model(block_size=block_size, layers=layers)","key":"qG6B3JZldX"},{"type":"outputs","id":"vjvSXhEKycHZBkCuQTI9k","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"damiara.\nalyzah.\nfard.\nazalee.\nsayah.\nayvi.\nreino.\nsophemuellani.\nciaub.\nalith.\nsira.\nliza.\njah.\ngrancealynna.\njamaur.\nben.\nquan.\ntorie.\ncoria.\ncer.\n"},"children":[],"key":"NRVguVGtFI"}],"key":"J4KHULtFm3"}],"key":"kMkHTAM34a"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"But we can improve our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AYQCMuDlNv"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dOZKDDnasr"}],"key":"XS67aEf7ze"},{"type":"text","value":" and improve our results even further. We’ll start by fixing that thicc loss plot!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Uv3QvlrUZW"}],"key":"sE83n2LZWA"}],"key":"w7dsZKoqEq"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Fixing the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WEMHcgbbdU"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QTBwUld2P4"}],"key":"LgvALcYyGO"},{"type":"text","value":" plot","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AB4Er8lT7b"}],"identifier":"fixing-the-loss-plot","label":"Fixing the loss plot","html_id":"fixing-the-loss-plot","implicit":true,"key":"hSE8EHCAqU"}],"key":"qMb5TWHXGQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"One way to turn this thicc loss plot into a normal one is to only plot the mean. Remember, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kXaIf5bQsb"},{"type":"inlineCode","value":"lossi","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HQdHwlRGO4"},{"type":"text","value":" is a very long list of floats that contains a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sh4lQBTmTM"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"In28GtPfaj"}],"key":"i1ku0RaI0L"},{"type":"text","value":" for each training episode:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O1dm95Sp8u"}],"key":"o0imD0GX4s"}],"key":"nrSweZsz63"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"len(lossi), lossi[:5]","key":"bK9bFM2BBg"},{"type":"outputs","id":"UNPLHFuvDWzcBCjKmCh78","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":36,"metadata":{},"data":{"text/plain":{"content":"(200000,\n [0.5180676579475403,\n  0.5164594054222107,\n  0.507362961769104,\n  0.507546603679657,\n  0.4992470443248749])","content_type":"text/plain"}}},"children":[],"key":"sjivZDIOXx"}],"key":"x8SzDUW5br"}],"key":"vTwOQoCO97"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s segment this very long list into a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f6XCpv2Z5X"},{"type":"inlineMath","value":"2D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">2D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"P8Kn3VhB8E"},{"type":"text","value":" tensor of rows, with each row containing ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ra5WmXSPgh"},{"type":"text","value":"1000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Et9NPkM19E"},{"type":"text","value":" loss values:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lRVqOQuwUa"}],"key":"zl62QaJUpb"}],"key":"x0qgQXITAg"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"t_loss = torch.tensor(lossi).view(-1, 1000)\nt_loss","key":"ZG0VgPUgLv"},{"type":"outputs","id":"E8HzqVq8yl3Qkrz-CekgV","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":37,"metadata":{},"data":{"text/plain":{"content":"tensor([[0.5181, 0.5165, 0.5074,  ..., 0.4204, 0.3860, 0.4014],\n        [0.3937, 0.3930, 0.4177,  ..., 0.3788, 0.3896, 0.4054],\n        [0.3426, 0.4191, 0.3918,  ..., 0.4447, 0.4419, 0.2821],\n        ...,\n        [0.3625, 0.3517, 0.3376,  ..., 0.3266, 0.3191, 0.3271],\n        [0.2550, 0.3659, 0.2968,  ..., 0.2744, 0.3853, 0.3300],\n        [0.3041, 0.2740, 0.3213,  ..., 0.3081, 0.4082, 0.3207]])","content_type":"text/plain"}}},"children":[],"key":"qQglHzZsPc"}],"key":"vVyQnT0NHr"}],"key":"C3nHQcjaVj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, if we take the mean of each row, we end up with a list of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S4zzqxwIuo"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VVdP7YQkMp"}],"key":"HtXz7KfAnh"},{"type":"text","value":" averages:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JC3YzkxlhI"}],"key":"KTjofyXzcI"}],"key":"quVTZBtVb4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"mean_t_loss = t_loss.mean(1)\nmean_t_loss","key":"qe1qLtk3gL"},{"type":"outputs","id":"ZUudCcEw8YfrPneSKFv6A","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":38,"metadata":{},"data":{"text/plain":{"content":"tensor([0.4059, 0.3791, 0.3698, 0.3681, 0.3657, 0.3639, 0.3624, 0.3593, 0.3557,\n        0.3561, 0.3516, 0.3515, 0.3504, 0.3501, 0.3491, 0.3477, 0.3498, 0.3474,\n        0.3494, 0.3449, 0.3456, 0.3440, 0.3452, 0.3461, 0.3429, 0.3456, 0.3458,\n        0.3438, 0.3408, 0.3437, 0.3435, 0.3407, 0.3424, 0.3412, 0.3415, 0.3404,\n        0.3419, 0.3391, 0.3414, 0.3396, 0.3392, 0.3408, 0.3394, 0.3416, 0.3389,\n        0.3390, 0.3376, 0.3407, 0.3364, 0.3376, 0.3393, 0.3362, 0.3371, 0.3349,\n        0.3393, 0.3369, 0.3363, 0.3349, 0.3338, 0.3386, 0.3366, 0.3388, 0.3370,\n        0.3379, 0.3349, 0.3378, 0.3325, 0.3358, 0.3353, 0.3390, 0.3369, 0.3366,\n        0.3354, 0.3350, 0.3375, 0.3347, 0.3352, 0.3352, 0.3318, 0.3359, 0.3348,\n        0.3338, 0.3350, 0.3367, 0.3331, 0.3333, 0.3346, 0.3356, 0.3339, 0.3339,\n        0.3332, 0.3331, 0.3352, 0.3356, 0.3350, 0.3335, 0.3330, 0.3299, 0.3344,\n        0.3350, 0.3318, 0.3295, 0.3328, 0.3336, 0.3345, 0.3341, 0.3319, 0.3342,\n        0.3329, 0.3299, 0.3346, 0.3312, 0.3312, 0.3344, 0.3340, 0.3305, 0.3319,\n        0.3344, 0.3302, 0.3315, 0.3335, 0.3319, 0.3345, 0.3326, 0.3331, 0.3319,\n        0.3317, 0.3331, 0.3316, 0.3313, 0.3319, 0.3340, 0.3306, 0.3329, 0.3306,\n        0.3322, 0.3332, 0.3313, 0.3309, 0.3348, 0.3297, 0.3324, 0.3305, 0.3311,\n        0.3316, 0.3308, 0.3301, 0.3323, 0.3289, 0.3313, 0.3199, 0.3201, 0.3196,\n        0.3233, 0.3184, 0.3179, 0.3180, 0.3172, 0.3175, 0.3176, 0.3200, 0.3194,\n        0.3196, 0.3195, 0.3186, 0.3166, 0.3192, 0.3179, 0.3168, 0.3171, 0.3173,\n        0.3188, 0.3175, 0.3176, 0.3174, 0.3197, 0.3182, 0.3167, 0.3187, 0.3217,\n        0.3165, 0.3187, 0.3144, 0.3165, 0.3183, 0.3187, 0.3179, 0.3161, 0.3182,\n        0.3177, 0.3171, 0.3187, 0.3194, 0.3183, 0.3157, 0.3156, 0.3167, 0.3168,\n        0.3187, 0.3179])","content_type":"text/plain"}}},"children":[],"key":"FWWtdin5kv"}],"key":"iHZ1CPvQ0m"}],"key":"cI8Xb6gcLZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If we plot this tensor list of mean losses, we should get a nicer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SDoUu1UMwn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SbEP250h9a"}],"key":"Kot6gv0JNv"},{"type":"text","value":" plot:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FGv1HLT30F"}],"key":"GvPald9ntJ"}],"key":"gWrpXzodqs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(mean_t_loss);","key":"D1nUOSOphC"},{"type":"outputs","id":"I8bjH9_o9vaN_twmQNpGy","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"42dc20cba22c45458c6f9877637adf70\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"5d9ce90e435b7c612969d574e5fe3c25","path":"/build/5d9ce90e435b7c612969d574e5fe3c25.png"},"text/html":{"content_type":"text/html","hash":"069f0424ea33d6e559bb93a29a286a49","path":"/build/069f0424ea33d6e559bb93a29a286a49.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"iYcH31wKfp"}],"key":"RszUjJcOMy"}],"key":"h0Ull7lUqB"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, the progress we make during training is much more clearly visible! Also, notice the learning rate decay, where the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VEKUcpwgLF"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VfoZAdHzE7"}],"key":"qCP9KXd7oN"},{"type":"text","value":" drops to a even lower minimum. This is the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e9RzlLZO6h"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yd96jCOyOh"}],"key":"lSPwQDVRlE"},{"type":"text","value":" plot we are going to be using going forward.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zjFMcMwM80"}],"key":"A0Mf7wKxOc"}],"key":"EZTdK0IwKr"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"torchifying the code: layers, containers, torch.nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GL0bmrJzOm"}],"identifier":"torchifying-the-code-layers-containers-torch-nn","label":"torchifying the code: layers, containers, torch.nn","html_id":"torchifying-the-code-layers-containers-torch-nn","implicit":true,"key":"oBq8tVJE3Z"}],"visibility":"show","key":"E8IPZJ3JnH"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now it’s time to simplify our forward function a little bit. Notice how the embeddings and flattening operations are calculated outside of the layers:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pQhemUgIlF"}],"key":"EkrMMHGfpe"}],"key":"tdGa3yqhH8"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"def forward(layers, xb, yb):\n    emb = C[xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        ...","position":{"start":{"line":1,"column":1},"end":{"line":7,"column":1}},"key":"lcNf75KmWC"}],"key":"SV3iwxdAL9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To start tidying things up, let’s mirror ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ENe6xsxC02"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn.Embedding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SiVl4jjstd"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding","key":"SnDYomQovy"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RTqY3HPdA2"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn.Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Tn7gnPZc1T"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten","key":"ZZcoKzyyz7"},{"type":"text","value":" with our own incredibly simplified equivalent modules:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zSclrYx1X5"}],"key":"qMZ6PlIsmZ"}],"key":"zZXdqlYX0a"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class Embedding:\n    def __init__(self, n_embd, embd_dim):\n        self.weight = torch.randn((n_embd, embd_dim))\n\n    def __call__(self, ix):\n        self.out = self.weight[ix]\n        return self.out\n\n    def parameters(self):\n        return [self.weight]","key":"BmVvglFTSk"},{"type":"outputs","id":"UnBdeCe7olHbZ9LHMHoan","children":[],"key":"svps4hcLnj"}],"key":"AzWnQQ7ifK"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class Flatten:\n    def __call__(self, x):\n        self.out = x.view(x.shape[0], -1)\n        return self.out\n\n    def parameters(self):\n        return []","key":"ZumLSIdDzx"},{"type":"outputs","id":"qoo7NC6tWHaD33Eccuy4X","children":[],"key":"JOLrjJ4Fly"}],"key":"ke6AlAHO8r"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"These will simply be responsible for indexing and flattening. We can now simplify our forward pass by including the embedding and flattening operations as modules in the definition of the layers:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qGttL056O9"}],"key":"jSICWeBPZS"}],"key":"oXo3mIuIFY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(block_size, n_embd, n_hidden):\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    layers = [\n        Embedding(vocab_size, n_embd),\n        Flatten(),\n        Linear(n_inputs, n_hidden, bias=False),\n        BatchNorm1d(n_hidden),\n        Tanh(),\n        Linear(n_hidden, n_outputs),\n    ]\n    # parameter init\n    with torch.no_grad():\n        layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = [p for l in layers for p in l.parameters()]\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters","key":"caMni8Oi6K"},{"type":"outputs","id":"xBTMa5N8FNNYDFwOS0RS1","children":[],"key":"is6YJuUSan"}],"key":"sAwiAL2K1d"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(layers, xb, yb):\n    x = xb\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, yb)  # loss function\n    return loss","key":"ewoLt06JfU"},{"type":"outputs","id":"45ynUUwNrn6L9Yw1oWM6L","children":[],"key":"oo8JEkyb6B"}],"key":"LP4fIWXVp6"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Awesome. Now we can even further simplify our forward pass by replacing the list that contains our layers with our simplified implementation of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tIy4IEYTmz"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn.Sequential","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hVb1r0AvJu"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential","key":"XfrkZ3dDql"},{"type":"text","value":" container: this object contains layers and the functionality to iteratively pass data through them. Meaning that we now define a bunch of layers as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kwow70y5DO"},{"type":"inlineCode","value":"Sequential","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H6tmHpmIx7"},{"type":"text","value":" object (i.e. a model) through which we can pass input data (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t6BTpqhJbV"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qZQihvTB1Y"},{"type":"text","value":"), without the need to explicitly loop.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jxxS7HbUUK"}],"key":"xjReyZ6v7z"}],"key":"jrSAmdH7oz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class Sequential:\n    def __init__(self, layers):\n        self.layers = layers\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        self.out = x\n        return self.out\n\n    def parameters(self):\n        # get parameters of all layers and stretch them out into one list\n        return [p for layer in self.layers for p in layer.parameters()]","key":"YBKK29qFeo"},{"type":"outputs","id":"4py6vVSK5W7eAlAhyXUNf","children":[],"key":"cCGxLZDYJX"}],"key":"QetiPJHJok"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s now further simplify our functions by replacing the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Box2AnWA3k"},{"type":"inlineCode","value":"layers","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HE5UhEAJpr"},{"type":"text","value":" list with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LrH65miUaV"},{"type":"inlineCode","value":"model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GaU4WrY9M1"},{"type":"text","value":", a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QiwvZq3STs"},{"type":"inlineCode","value":"Sequential","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O7G5E8aWLv"},{"type":"text","value":" object:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MesgmKUe5y"}],"key":"sAQZWJ7Am6"}],"key":"hwwZo937h2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(block_size, n_embd, n_hidden):\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    model = Sequential(\n        [\n            Embedding(vocab_size, n_embd),\n            Flatten(),\n            Linear(n_inputs, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            Linear(n_hidden, n_outputs),\n        ]\n    )\n    # parameter init\n    with torch.no_grad():\n        model.layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = model.parameters()\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return model, parameters","key":"hjIAUFJ6S0"},{"type":"outputs","id":"tG993ZAkrX6z5rhroj87H","children":[],"key":"G6wO26euh9"}],"key":"Zao1YDdH5y"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def forward(model, xb, yb):\n    logits = model(xb)\n    loss = F.cross_entropy(logits, yb)  # loss function\n    return loss","key":"sWI0s8w7PR"},{"type":"outputs","id":"CKFJ29oHEENpdx_tGbCc_","children":[],"key":"JUCmEiFaWP"}],"key":"C4JKXLs2Xo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def train(\n    x,\n    y,\n    model,\n    parameters,\n    initial_lr=0.1,\n    maxsteps=200000,\n    batchsize=32,\n    break_at_step=None,\n):\n    lossi = []\n    for i in range(maxsteps):\n        # minibatch construct\n        bix = torch.randint(0, x.shape[0], (batchsize,))\n        xb, yb = x[bix], y[bix]\n        loss = forward(model, xb, yb)\n        backward(parameters, loss)\n        lr = initial_lr if i < 150000 else initial_lr / 10\n        update(parameters, lr=lr)\n        # track stats\n        if i % 10000 == 0:  # print every once in a while\n            print(f\"{i:7d}/{maxsteps:7d}: {loss.item():.4f}\")\n        lossi.append(loss.log10().item())\n        if break_at_step is not None and i >= break_at_step:\n            break\n    return lossi","key":"rDfPY2s2GR"},{"type":"outputs","id":"zq4LXSP0SP88yYbQ01-DH","children":[],"key":"mwidZ4kWFJ"}],"key":"XWOSV7bJgU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def trigger_eval_mode(model):\n    for l in model.layers:\n        l.training = False","key":"c7m9n9um3A"},{"type":"outputs","id":"t0BLsLGQdZIq6AGpRCbo_","children":[],"key":"VXsNxltPJj"}],"key":"KTYMDvPAek"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@torch.no_grad()\ndef infer_loss(model, x, y, prefix=\"\"):\n    loss = forward(model, x, y)\n    print(f\"{prefix} {loss}\")\n    return loss","key":"BXxO4lVbOZ"},{"type":"outputs","id":"sTxgZjHC1k_qGZpGbMtsn","children":[],"key":"a1LQQShX0O"}],"key":"KzssuJxnmi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def sample_from_model(block_size, model):\n    for _ in range(20):\n        out = []\n        context = [0] * block_size  # initialize with all ...\n        while True:\n            # forward pass the neural net\n            logits = model(torch.tensor([context]))\n            probs = F.softmax(logits, dim=1)\n            # sample from the distribution\n            ix = torch.multinomial(probs, num_samples=1).item()\n            # shift the context window and track the samples\n            context = context[1:] + [ix]\n            out.append(ix)\n            # if we sample the special '.' token, break\n            if ix == 0:\n                break\n        print(\"\".join(itoc[i] for i in out))  # decode and print the generated word","key":"LzxwLf0GBf"},{"type":"outputs","id":"iCIGlPIqd_JPTUNZIfu3w","children":[],"key":"clxzNmun5i"}],"key":"sKWgTlvOk1"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And let’s verify that our new definitions work by re-training our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fGL2xNkhKz"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AqGntgY8VL"}],"key":"OklAbpAy1A"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mmw0gNLkdR"}],"key":"NJ3epJDn0v"}],"key":"yIkr03puwC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)","key":"eNWzS3KEeA"},{"type":"outputs","id":"bqO2EdmKHlmhBm93OA5sw","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"12097\n      0/ 200000: 3.3055\n  10000/ 200000: 2.1954\n  20000/ 200000: 2.2630\n  30000/ 200000: 2.0618\n  40000/ 200000: 2.0468\n  50000/ 200000: 2.1775\n  60000/ 200000: 2.1750\n  70000/ 200000: 1.9390\n  80000/ 200000: 2.1816\n  90000/ 200000: 2.0516\n 100000/ 200000: 2.0578\n 110000/ 200000: 2.2706\n 120000/ 200000: 2.3313\n 130000/ 200000: 2.1557\n 140000/ 200000: 2.0983\n 150000/ 200000: 1.9418\n 160000/ 200000: 1.9421\n 170000/ 200000: 2.1256\n 180000/ 200000: 2.2467\n 190000/ 200000: 1.6821\n"},"children":[],"key":"KyP3Id8S4b"}],"key":"FMagvkTSUa"}],"key":"VANdDWU5Gz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));","key":"w3byCRn523"},{"type":"outputs","id":"-bgFQoW5ynQpyCjJ2a7YB","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"3bb3fea1b9d14a5aa8d6287c63f7ddac\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"dc071225b20724ced3b45fdcad310314","path":"/build/dc071225b20724ced3b45fdcad310314.png"},"text/html":{"content_type":"text/html","hash":"528384c689a08e25b51452974422c1bb","path":"/build/528384c689a08e25b51452974422c1bb.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"zxoif7fn5f"}],"key":"rVxZ49U654"}],"key":"kA1ta7zE6t"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"trigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");","key":"M0w2s8Bl9S"},{"type":"outputs","id":"R_vNItOVYYh94aSV_ulop","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 2.0581207275390625\nval 2.105104684829712\n"},"children":[],"key":"of9C9VxMkc"}],"key":"dMDqUpNMnk"}],"key":"qdopkMZ5Rl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sample_from_model(block_size, model)","key":"B5IRJ69vNZ"},{"type":"outputs","id":"LJ6KznAwdb69KBB6UXQqA","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"masea.\niman.\nryy.\nayee.\nhavajine.\nmiliakendalikain.\namagntanton.\naviona.\njah.\nwiseegh.\navon.\nman.\ntovi.\nsullessa.\nmarcuz.\njazia.\nabellabell.\nathin.\nahkiara.\nkrister.\n"},"children":[],"key":"bNmgHeBflS"}],"key":"HbaEEVaSwU"}],"key":"Dfta4tf0as"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Cool. Now it’s time to decrease the loss even further by scaling up our model to make it bigger and deeper!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A2D9IWgDEK"}],"key":"Wmagz5Akmc"}],"key":"VdG8wkS2Yo"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZOIJRsRIwe"}],"key":"SBNrttAu8x"},{"type":"text","value":" overview","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QRLi9beru3"}],"identifier":"wavenet-overview","label":"WaveNet overview","html_id":"wavenet-overview","implicit":true,"key":"N8mFXWRqpo"}],"visibility":"show","key":"H6O4ZQXUdR"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Currently, we are using this architecture here:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UthdqO5sI8"}],"key":"IBmINSpmDK"}],"key":"IMWQQEwiS8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"bengio2003nn.jpeg\"))","key":"QA4oRCT5TL"},{"type":"outputs","id":"TW5ZREGDVvPq4LPLfe0w3","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/jpeg":{"content_type":"image/jpeg","hash":"a21abcc7498c74c85d4a3cd5f51b3817","path":"/build/a21abcc7498c74c85d4a3cd5f51b3817.jpeg"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"Zu1lZpxdwG"}],"key":"cVHxosTl8D"}],"key":"fXj4CwQRnw"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"where we are taking in some number of characters, going into a single hidden layer, and then going to the prediction of the next character. The problem here is we don’t have a naive way of making this bigger in a productive way. We could, of course, use our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CdbrzVYwac"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gkjs8Fr2qT"}],"key":"tH88pDRDjT"},{"type":"text","value":". We could use our layers, sort of like building block materials to introduce additional layers here and make the network deeper. But it is still the case that we are crushing all of the characters into a single layer all the way at the beginning. And even if we make this a layer bigger by adding neurons, it’s still kind of like silly to squash all that information so fast in a single step. What we’d like to do instead is we’d like our network to look a lot more like this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oL4vu73pzD"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GTPRPtk3Bw"}],"key":"qVgHuBCPAC"},{"type":"text","value":" case:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"drA0Cu0mJY"}],"key":"FWE2lrWS7k"}],"key":"EgKAUqSNBt"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))","key":"MUSgt1SAAa"},{"type":"outputs","id":"lt34W0TyJT2q12IBnkSPA","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"198c39366ff68e8b03ccb06df349f47a","path":"/build/198c39366ff68e8b03ccb06df349f47a.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"pm0qsKgWZu"}],"key":"Ottvb4AZ9M"}],"key":"AzDj0hvbQL"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So you see in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wGABKOLKw6"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oJtrk12M8l"}],"key":"yXkGedPi69"},{"type":"text","value":", when we are trying to make the prediction for the next character in a sequence a function of the previous characters that feed in. But it is not the case that all of these different characters are just crushed to a single layer and then you have a sandwich. They are crushed slowly. So in particular, we take two characters and we fuse them into sort of like a bigram representation. And we do that for all these characters consecutively. And then we take the bigrams and we fuse those into four character level chunks. And then we fuse again. And so we do that in this tree-like hierarchical manner. So we fuse the information from the previous context ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rspBlb81vu"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"gradually","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wnZQCD3iIY"}],"key":"bYIc6l7f1a"},{"type":"text","value":", as the network deepens. This is the kind of architecture that we want to implement. Now in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GuB9pbhyEH"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JcIg0kZJ9n"}],"key":"IgI4rn0cCX"},{"type":"text","value":" case, this is a visualization of a stack of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OOJUWBCtvm"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dilated","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NGW7fGAMyI"}],"key":"CKaw6EaoGh"},{"type":"text","value":" causal convolution layers. And this makes it sound very scary, but actually the idea is quite simple. And the fact that it’s a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lycjujy4ny"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"dilated","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WMYzWi8ChH"}],"key":"IImQT2vTBc"},{"type":"text","value":" causal convolution layer is really just an implementation detail to make everything fast. We’re going to see that later. But for now, let’s just keep going. We’re going to keep the basic idea of it, which is this progressive fusion. So we want to make the network deeper, and at each level, we want to fuse only two consecutive elements. Two characters, then two bigrams, then two fourgrams, and so on. So let’s implement this.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fqE6LNZDlO"}],"key":"GhcAZdoRXX"}],"key":"oot4JS5ZUj"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Bumping the context size to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yx7GGA2IWf"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AHM5TqaPmM"}],"identifier":"bumping-the-context-size-to-8","label":"Bumping the context size to 8","html_id":"bumping-the-context-size-to-8","implicit":true,"key":"iplKmyNwgm"}],"key":"XACd824kUI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Okay, so first up, let me scroll to where we built the dataset, and let’s change the block size from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WB0PZSSWkB"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pU8apBV8sJ"},{"type":"text","value":" to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VffiPX2Vge"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ly4XamPpKY"},{"type":"text","value":". So we’re going to be taking ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cGCcfbGKzU"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JMnr0RAdln"},{"type":"text","value":" characters of context to predict the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"f7XqGGhkUH"},{"type":"inlineMath","value":"9th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>9</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">9th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">9</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"GBPi5DgDUO"},{"type":"text","value":" character:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QYdB3sU5uR"}],"key":"jzOvFwKty4"}],"key":"q2w9NOUuAQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"block_size = 8\n(xtrain, ytrain), (xval, yval), (xtest, ytest) = build_all_datasets(block_size)","key":"sef8Eoy9pf"},{"type":"outputs","id":"M1j1iSRJz3Sk0N3AtJe_j","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([182473, 8]) torch.Size([182473])\ntorch.Size([22827, 8]) torch.Size([22827])\ntorch.Size([22846, 8]) torch.Size([22846])\n"},"children":[],"key":"QXCzilsOKC"}],"key":"zruND7UOqu"}],"key":"czdYQdFfSg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So the dataset now looks like this:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qeS8Y1wBMB"}],"key":"ROAhMBLapH"}],"key":"gYzZIOJzQN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_next_character(xtrain[:20], ytrain[:20])","key":"hsO16v886q"},{"type":"outputs","id":"No0bLsiaq-8NClqIMdUR-","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"........ --> c\n.......c --> a\n......ca --> t\n.....cat --> h\n....cath --> y\n...cathy --> .\n........ --> k\n.......k --> e\n......ke --> n\n.....ken --> a\n....kena --> d\n...kenad --> i\n..kenadi --> .\n........ --> a\n.......a --> m\n......am --> i\n.....ami --> .\n........ --> l\n.......l --> a\n......la --> r\n"},"children":[],"key":"hFyhOn190H"}],"key":"ZYOsXQpNVa"}],"key":"zMQhC5lfuv"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"These ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GOtKBLRzyd"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jcxePQ9yC3"},{"type":"text","value":" characters are going to be processed in the above tree-like structure. Let’s find out how to implement this hierarchical scheme! But before doing that, let’s train our simple fully-connected ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vTP6VKgHoC"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UrzJ0nN5c7"}],"key":"ofte22TjMw"},{"type":"text","value":" with this new dataset and see how well it performs:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q7v6F9gaSP"}],"key":"guLY6ykn3M"}],"key":"n9se3rROR4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)","key":"KV9Zm1E6Gg"},{"type":"outputs","id":"8UebAiQtQn9HHDhj-Ptp5","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"22097\n      0/ 200000: 3.3024\n  10000/ 200000: 2.1462\n  20000/ 200000: 2.2304\n  30000/ 200000: 2.1978\n  40000/ 200000: 2.3442\n  50000/ 200000: 2.1926\n  60000/ 200000: 2.4338\n  70000/ 200000: 2.0021\n  80000/ 200000: 2.0781\n  90000/ 200000: 1.7328\n 100000/ 200000: 2.2064\n 110000/ 200000: 1.9591\n 120000/ 200000: 1.9200\n 130000/ 200000: 1.7876\n 140000/ 200000: 2.0151\n 150000/ 200000: 1.9124\n 160000/ 200000: 1.9154\n 170000/ 200000: 2.4858\n 180000/ 200000: 2.0312\n 190000/ 200000: 1.7150\n"},"children":[],"key":"KfGxX5NzVa"}],"key":"chJaKMRvMH"}],"key":"z5M6Ut0LUf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));","key":"dSoo0vSNHM"},{"type":"outputs","id":"K6PM1l1Q-hAqn5Kixl0f3","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ff615d43a98747bd9c51619102571855\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"165817da5ba45451a2b5cc72d6b0048c","path":"/build/165817da5ba45451a2b5cc72d6b0048c.png"},"text/html":{"content_type":"text/html","hash":"d15454142d80cffdbabe07305558aecc","path":"/build/d15454142d80cffdbabe07305558aecc.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Jy4FFcG4Wg"}],"key":"gQ8yod0Opq"}],"key":"mf38Xj2WCY"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"trigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");","key":"gJ8B5Km8MC"},{"type":"outputs","id":"6xiObZb2SFTEQ1E1VY8MZ","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 1.9159809350967407\nval 2.0343399047851562\n"},"children":[],"key":"xVB9zlfFe7"}],"key":"D9BaufiC2X"}],"key":"Vg4P0CZyMk"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Interesting! The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M4U9L7aJ1R"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j18rjNvsj8"}],"key":"DQ258GGjwt"},{"type":"text","value":" has improved compared to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xAgrDgSCXB"},{"type":"inlineCode","value":"block_size = 3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xtMpNsb6w0"},{"type":"text","value":" case. Let’s log our losses so far:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lNfkRI9M2d"}],"key":"P7JbpimEQH"}],"key":"AaAVeOCZnd"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"code","lang":"python","value":"# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034","position":{"start":{"line":1,"column":1},"end":{"line":4,"column":1}},"key":"FMk4eSUG6Q"}],"visibility":"show","key":"DrSXhNe6id"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Also, if we sample from the model, we can see the names improving qualitatively as well:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TH3vBh6fUd"}],"key":"TmouDo3RUg"}],"key":"XLglWkvSID"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sample_from_model(block_size, model)","key":"Jfsg4wsObR"},{"type":"outputs","id":"ViE5ploM3E_5IP0UuKRdZ","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"kobi.\npran.\nmarlecm.\nlunghan.\ncamillo.\nshatar.\nelizee.\nlumarius.\nderis.\nbrook.\nmadaniy.\nyarel.\nmilaal.\naylen.\nnikora.\nniani.\nsahanlaa.\nelaya.\nmalixa.\ndalioluw.\n"},"children":[],"key":"o0zEXKSeyp"}],"key":"lW4APbcpha"}],"key":"QdHa76223X"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So we could, of course, spend a lot of time here tuning things and scaling up our network further. But let’s continue and let’s implement the hierarchical model and treat this as just a rough baseline performance. There’s a lot of optimization left on the table in terms of some of the hyperparameters that you’re hopefully getting a sense of now.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CD8jMvPHVk"}],"key":"IlWYmyG379"}],"key":"I5VSlsYYRn"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Implementing ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b0WfJ0lOlj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DDUABcivuY"}],"key":"VUlz0gj7uL"}],"identifier":"implementing-wavenet","label":"Implementing WaveNet","html_id":"implementing-wavenet","implicit":true,"key":"YTJcag0WWg"}],"visibility":"show","key":"vfVAkvudcn"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s now create a bit of a scratch space for us to just look at the forward pass of the  ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ibaD4Km84q"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IIbo6BXKFy"}],"key":"LIWdRNkFtL"},{"type":"text","value":" and inspect the shape of the tensors along the way of the forward pass:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cSt3VcmP1y"}],"key":"bANNgIeta2"}],"key":"sum6GQcPNj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# let's look at a batch of just 4 examples\nix = torch.randint(0, xtrain.shape[0], (4,))\nxb, yb = xtrain[ix], ytrain[ix]\nlogits = model(xb)\nprint(xb.shape)\nxb","key":"Eiu8UiNJZZ"},{"type":"outputs","id":"DPeFrLqkQ0konGxLB3Er4","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"torch.Size([4, 8])\n"},"children":[],"key":"hj3UcetaR4"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":63,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0,  0,  0,  0,  0,  0,  0, 12],\n        [ 0,  0,  0,  0,  0,  0, 18,  5],\n        [ 0,  0,  0, 11,  1, 12,  9, 14],\n        [ 0,  0,  0,  0,  0, 11,  9, 18]])","content_type":"text/plain"}}},"children":[],"key":"HUM9KjH5yx"}],"key":"Dx5FiyuXj6"}],"key":"Kkd3y3DH2C"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here we are just temporarily, for debugging purposes, creating a batch of just, say, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AIXEijdOGx"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lvi3JLserS"},{"type":"text","value":" examples. So ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YKtHuk774P"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a1TG6JsqPL"},{"type":"text","value":" random integers. Then, we are plucking out those rows from our training set. And then we are passing into the model the input ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fz5UcSY3Ir"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MN8rC0pS03"},{"type":"text","value":". Now the shape of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jwaNM9Mldd"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rNiicSUVlI"},{"type":"text","value":" here, because we only have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LzDVX1svxV"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n2BiduoGkP"},{"type":"text","value":" examples. And ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IUASUODV0X"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DR3ZMW9r4y"},{"type":"text","value":" is the current block size. So ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xYUOKxMjZV"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sBZWqsgtXG"},{"type":"text","value":" contains ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fIkMv4xVgJ"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AMK3Q7H0VC"},{"type":"text","value":" rows/examples of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lsZm4b2G0m"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QdFPwaiNHQ"},{"type":"text","value":"  characters each. And each integer tensor row of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NwtQ8YQXXO"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"celdFnSpL0"},{"type":"text","value":" just contains the identities of those characters. Therefore, the first layer of our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NiyqAMTX2O"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qhaPbT1hdP"}],"key":"CTWFWaSgYV"},{"type":"text","value":" is the embedding layer. So passing ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YaodOatZll"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kiNMsDjRm5"},{"type":"text","value":", this integer tensor, through the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FaNpqIUxkl"},{"type":"inlineCode","value":"Embedding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yOeRLBpbaM"},{"type":"text","value":" layer creates an output:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fQfjeWlAAX"}],"key":"rhgUo9GU3a"}],"key":"lDKagPJ0AZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model.layers[0].out.shape  # output of Embedding layer","key":"jvAt6uxYIh"},{"type":"outputs","id":"JDg7d1nQ22NJZxyVHXN_V","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":64,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 8, 10])","content_type":"text/plain"}}},"children":[],"key":"XnF5EIXEEQ"}],"key":"RgdzouwS8O"}],"key":"hSz43SIZ7o"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So our embedding table ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cXRRT618vG"},{"type":"inlineCode","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fax3Tw1UjT"},{"type":"text","value":" has, for each character, a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZtfaHeDyQR"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jKTHEQar9A"},{"type":"text","value":"-dimensional vector (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ecv3xH5hlA"},{"type":"inlineCode","value":"n_embd=10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QAzTrtvH7D"},{"type":"text","value":") that we are trying to learn. What the layer does here is it plucks out the embedding vector for each one of these integers (of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kpxEEow0cq"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xperl8nv91"},{"type":"text","value":" and organizes it all in a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ij6bQJhsQN"},{"type":"inlineMath","value":"4\\times8\\times10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">4\\times8\\times10</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">10</span></span></span></span>","key":"vuGvH5MztS"},{"type":"text","value":" tensor. So all of these integers are translated into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zDnQ4Rwe82"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WnqZqUp0mS"},{"type":"text","value":"-dimensional vectors inside this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mNOweUYfPT"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KSi9DQDoET"},{"type":"text","value":"-dimensional tensor now.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iMAgPjZ4M0"}],"key":"K7FoTJp4UH"}],"key":"pvfqT3qHkS"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model.layers[1].out.shape  # output of Flatten layer","key":"gMnU3OrTyd"},{"type":"outputs","id":"o-oXKi9xJzy_WukqFXeqv","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":65,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 80])","content_type":"text/plain"}}},"children":[],"key":"uwMHJZnuC4"}],"key":"d2uoQtn1B2"}],"key":"FH0ROxNCBc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now passing that through the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RdzCnWz4mE"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ccrhRNERJO"},{"type":"text","value":" layer, as you recall, what this does is it views this tensor as just a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EnIXA2j17a"},{"type":"inlineMath","value":"4\\times80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4\\times80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">80</span></span></span></span>","key":"I9Tb3gXxAO"},{"type":"text","value":" tensor. And what that effectively does is that all these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vAtsxPaNcv"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J3k0kNbf4B"},{"type":"text","value":"-dimensional embeddings for all these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hyplZsKKGe"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UiT6kEBhC7"},{"type":"text","value":" characters just end up being stretched out into a long row. And that looks kind of like a concatenation operation, basically. So by viewing the tensor differently, we now have a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wdvcFfhN65"},{"type":"inlineMath","value":"4\\times80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4\\times80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">80</span></span></span></span>","key":"BAXsYxJi73"},{"type":"text","value":". And inside this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hLV1CyNdub"},{"type":"text","value":"80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g3EBTwFh5O"},{"type":"text","value":", it’s all the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cN9n7RCxrJ"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lOuZJgtAa7"},{"type":"text","value":"-dimensional vectors just concatenated next to each other.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ipS6J7tspy"}],"key":"nKjAry68TG"}],"key":"YltoMoowo2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model.layers[2].out.shape  # output of Linear layer","key":"LKqROLK0Co"},{"type":"outputs","id":"SMYYRtXhLrzt2FkVX-haZ","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":66,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 200])","content_type":"text/plain"}}},"children":[],"key":"hB6nVLNgUz"}],"key":"xSf3RqdRuF"}],"key":"MBKldYOtud"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And the linear layer, of course, takes ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ugGkLQzHth"},{"type":"text","value":"80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"opi6yC8ta3"},{"type":"text","value":" and creates ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LygW3JEFbO"},{"type":"text","value":"200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hADGyzNzm5"},{"type":"text","value":" channels just via matrix multiplication. So far, so good. Now let’s see something surprising. Let’s look at the insides of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FrcmKKir9u"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nfZMrAgSZI"},{"type":"text","value":" layer and remind ourselves how it works:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DfPi3O6YNo"}],"key":"iyrWkrvuqv"}],"key":"wuJ7JhguEe"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"class Linear:\n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n...","position":{"start":{"line":1,"column":1},"end":{"line":13,"column":1}},"key":"Qj12OKlnww"}],"key":"kG4oLXhDUg"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JrDzmS7C1C"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"duuibatAtB"},{"type":"text","value":" layer here in a forward pass takes the input ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QEcdF8JZ2Y"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yme1z0kg6y"},{"type":"text","value":", multiplies it with a weight and then optionally adds a bias. And the weight is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hgLSnzOMMD"},{"type":"inlineMath","value":"2D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">2D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"ZOWDggTCLr"},{"type":"text","value":", as defined here, and the bias is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fnWT3JsZU2"},{"type":"inlineMath","value":"1D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">1D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">1</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"s5McLPhsAw"},{"type":"text","value":". So effectively, in terms of the shapes involved, what’s happening inside this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aOKbJS2gvr"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RvM0qKIZuz"},{"type":"text","value":" layer looks like this right now. And we’re using random numbers here, but just to illustrate the shapes and what happens:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VLHf9g4RhT"}],"key":"GREP08beKU"}],"key":"j7XKEuJYPr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)).shape","key":"ohtmgYXEoY"},{"type":"outputs","id":"JNdxjfkSsOGmMxb4u00Ft","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":67,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 200])","content_type":"text/plain"}}},"children":[],"key":"xbDb5tkpu0"}],"key":"GEwNbevgoP"}],"key":"LK4zCTMttI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Basically, a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"t2g4HhIhVl"},{"type":"inlineMath","value":"4\\times80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4\\times80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">80</span></span></span></span>","key":"OAdhhC8S3Z"},{"type":"text","value":" comes into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BVlwE8FMPA"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CMpMF8cEzo"},{"type":"text","value":" layer, gets multiplied by a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ydvQkISkum"},{"type":"inlineMath","value":"80\\times200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>80</mn><mo>×</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">80\\times200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">80</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">200</span></span></span></span>","key":"db5zWY6oMm"},{"type":"text","value":" weight matrix inside, and then there’s a plus ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v5BbB0kkIu"},{"type":"text","value":"200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bXu4dJDIcu"},{"type":"text","value":" bias. And the shape of the whole thing that comes out of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ha2BNJoLSa"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Tb5ajkCV75"},{"type":"text","value":" layer is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QoKqztCWjw"},{"type":"inlineMath","value":"4\\times200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4\\times200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">200</span></span></span></span>","key":"WqJHY754Im"},{"type":"text","value":", as we see here. Notice, by the way, that the matrix multiplication here will create a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yVHwZiYCIN"},{"type":"inlineMath","value":"4x200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>x</mi><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4x200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">200</span></span></span></span>","key":"FxmVPTAlzI"},{"type":"text","value":" tensor, and then when adding ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yogo3RC68z"},{"type":"text","value":"200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TbTtSxSEpe"},{"type":"text","value":" there’s a broadcasting happening here, but since ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tv4d76zwij"},{"type":"inlineMath","value":"4x200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>x</mi><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4x200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">200</span></span></span></span>","key":"eutiLlZy0V"},{"type":"text","value":" broadcasts with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EaB285srNp"},{"type":"text","value":"200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ALvVczkFWX"},{"type":"text","value":", everything works here. So now the surprising thing is how this works. Specifically, something you may not expect is that this input here, that is being matrix-multiplied, doesn’t actually have to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jcpo0QHQxu"},{"type":"inlineMath","value":"2D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">2D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"W3QChyzGTn"},{"type":"text","value":". This matrix multiply operator in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XK5IQnFOGo"},{"type":"inlineCode","value":"PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ahZxaVcfqt"},{"type":"text","value":" is quite powerful, and in fact, you can actually pass in higher dimensional arrays or tensors, and everything works fine. So for example, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HEjVlJd990"},{"type":"inlineCode","value":"torch.randn(4, 80)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xesNTPlZBx"},{"type":"text","value":" could instead be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vcPwITpUWB"},{"type":"inlineCode","value":"torch.randn(4, 5, 80)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yeIYIOBpii"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S14Qvr20Mj"},{"type":"inlineMath","value":"4\\times5\\times80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>5</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4\\times5\\times80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">80</span></span></span></span>","key":"W3GXzciSwU"},{"type":"text","value":") and the result in that case would become ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v153gKN8ro"},{"type":"inlineMath","value":"4\\times5\\times200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>5</mn><mo>×</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4\\times5\\times200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">200</span></span></span></span>","key":"itndBsTf7B"},{"type":"text","value":". You can add as many dimensions as you like to the left of the last dimension of the input tensor (here, dimension ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SQuCFzWB0C"},{"type":"text","value":"80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NMcfj05eyD"},{"type":"text","value":"). And so effectively, what’s happening is that the matrix multiplication only works on a matrix multiplication on the last dimension, and the dimensions before it in the input tensor are left unchanged. So basically, these dimensions to the left of the last dimension are all treated as just a batch dimension. So we can have multiple batch dimensions (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U2GHz0cZoR"},{"type":"inlineCode","value":"torch.randn(4, 5, 6, 7, 80)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dI3pJkUI5F"},{"type":"text","value":"), and then in parallel over all those dimensions, we are doing the matrix multiplication only on the last dimension. So this is quite convenient, because we can use that in our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sxXeShoFZt"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K6LMypkytX"}],"key":"nMQL6247pY"},{"type":"text","value":" now. Remember that we have these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jd44eUHSjn"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DcGwfzXfLM"},{"type":"text","value":" characters coming in, e.g.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HFrJ4t8OJH"}],"key":"u7koEUDcq2"},{"type":"code","lang":"python","value":"# 1 2 3 4 5 6 7 8","position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"key":"OZGfRNvYN6"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"And we don’t want to now flatten all of it out into a large ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"MMSOUggJTB"},{"type":"text","value":"8","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PKXAlgvWtI"},{"type":"text","value":"-dimensional vector, because we don’t want to matrix multiply ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"zPd02hPQjZ"},{"type":"text","value":"80","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"czvMkRnSdH"},{"type":"text","value":" into a weight matrix multiply immediately. Instead, we want to group these like this:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"UG7oBNg71d"}],"key":"IfF3gf3u1X"},{"type":"code","lang":"python","value":"# (1 2) (3 4) (5 6) (7 8)","position":{"start":{"line":9,"column":1},"end":{"line":11,"column":1}},"key":"DyN1Yglfj9"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"So every consecutive two elements should now basically be flattened and multiplied by a weight matrix. But the idea is that all of these four groups here, we’d like to process in parallel. So it’s kind of like a extra batch dimension that we can introduce. And then we can, in parallel, basically process all of these bigram groups in the four extra batch dimension of an individual example, and also over the actual batch dimension of the four examples. So let’s see what this is all about and how that works. Right now, we take a ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"jO7JeK79oI"},{"type":"inlineMath","value":"4\\times80","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4\\times80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">80</span></span></span></span>","key":"jqN64L6aGp"},{"type":"text","value":" and multiply it by ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"G3anBZKJ3s"},{"type":"inlineMath","value":"80\\times200","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>80</mn><mo>×</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">80\\times200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">80</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">200</span></span></span></span>","key":"iSyzvUHzOP"},{"type":"text","value":" in the linear layer. Effectively, what we want is instead of ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"bmkFWTdWab"},{"type":"text","value":"8","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"HZvmYUr4J4"},{"type":"text","value":" characters (","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"wOjUZquo8p"},{"type":"text","value":"80","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"VjPaTxR4lW"},{"type":"text","value":" embedding numbers) coming in, we only want ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"LTac2B0Ldd"},{"type":"text","value":"2","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"qoUNAOtDo6"},{"type":"text","value":" characters (","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"SNbHFZs1Yq"},{"type":"text","value":"20","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"JRnJNRavqQ"},{"type":"text","value":" embedding numbers) to come in. Therefore, if we want that, we can’t have a ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"qW8EBXwmWB"},{"type":"inlineMath","value":"4x80","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>x</mi><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4x80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">80</span></span></span></span>","key":"eGk4NBQuGy"},{"type":"text","value":" feeding into the ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"WtMgpuuKo3"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"UINxcSxGET"},{"type":"text","value":" layer, but instead ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"QqH2qf4llM"},{"type":"text","value":"4","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"JQ3YqdkAU7"},{"type":"text","value":" groups of ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"zOWNjqNFyZ"},{"type":"text","value":"2","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"fAHqSBw5kR"},{"type":"text","value":" characters to be feeding in, like this:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"j6fmhOV5Vh"}],"key":"dsDsb9dzXd"}],"key":"h0OxTnRypI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape","key":"Mte23Aq7qC"},{"type":"outputs","id":"a-pFy0CzZ5xjJLbZQZvsu","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":68,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 4, 200])","content_type":"text/plain"}}},"children":[],"key":"LZFJ54iXqo"}],"key":"H3iOqxUDy2"}],"key":"GJZwUG6fV7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Therefore, what we would want to do now is change the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iF2cE2UIwf"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DnHRBpPKUS"},{"type":"text","value":" layer so that it doesn’t output a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Tqrr7n4Wya"},{"type":"inlineMath","value":"4x80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>x</mi><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4x80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">80</span></span></span></span>","key":"xaGcZAoJ0X"},{"type":"text","value":" but a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ttIB1IMTpW"},{"type":"inlineMath","value":"4x4x20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>x</mi><mn>4</mn><mi>x</mi><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">4x4x20</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">20</span></span></span></span>","key":"ImpZTYvvDq"},{"type":"text","value":" where basically in each row tensor of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XNEJkA0m5C"},{"type":"inlineCode","value":"xb","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eQxBDwPfDP"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bSowRxlM9x"}],"key":"RnY7HZCEMg"}],"key":"Lqdmj1NiaI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"xb","key":"pzoGDbcUW9"},{"type":"outputs","id":"npZVu9rBNIuGEVmsQUUgi","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":69,"metadata":{},"data":{"text/plain":{"content":"tensor([[ 0,  0,  0,  0,  0,  0,  0, 12],\n        [ 0,  0,  0,  0,  0,  0, 18,  5],\n        [ 0,  0,  0, 11,  1, 12,  9, 14],\n        [ 0,  0,  0,  0,  0, 11,  9, 18]])","content_type":"text/plain"}}},"children":[],"key":"WBbrp2Dn7S"}],"key":"bzjvYLe4Py"}],"key":"K8CXq8wl1E"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"every two consecutive characters (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pQZQva0LLA"},{"type":"inlineMath","value":"(0, 0), (10, 21), (12,  9), (5, 1)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0</mn><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mo stretchy=\"false\">(</mo><mn>10</mn><mo separator=\"true\">,</mo><mn>21</mn><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mo stretchy=\"false\">(</mo><mn>12</mn><mo separator=\"true\">,</mo><mn>9</mn><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mo stretchy=\"false\">(</mo><mn>5</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(0, 0), (10, 21), (12,  9), (5, 1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">10</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">21</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">12</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">9</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">5</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>","key":"iVNCNTwcL5"},{"type":"text","value":") are packed in on the very last dimension (i.e. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ga0PUWA0dt"},{"type":"text","value":"20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"btTFYI9fVB"},{"type":"text","value":"). So that the first dimension (i.e. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BjzMN1AK91"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LWj4oFWOlm"},{"type":"text","value":") is the first batch dimension and the second dimension (i.e. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"q6b3TLJAz4"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ox9ssbjWLK"},{"type":"text","value":") is the second batch dimension. And this is where we want to get to:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CUkp5t2upp"}],"key":"TSzREAS8O0"}],"key":"Y2oGiLkD9e"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape","key":"voaRpv8yNw"},{"type":"outputs","id":"entSPxg0SUjWMehXR4lUz","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":70,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 4, 200])","content_type":"text/plain"}}},"children":[],"key":"ke6tCaajMH"}],"key":"OM17ttUBPe"}],"key":"m5J4N5VvtK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now we have to change our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JGHWDDrJ0A"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XiC6fzLfNB"},{"type":"text","value":" layer (so that it doesn’t fully flatten out the examples, but creates a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o04HKt9DxD"},{"type":"inlineMath","value":"4\\times4\\times20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">4\\times4\\times20</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">20</span></span></span></span>","key":"RskWGid2C9"},{"type":"text","value":" instead of a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RwKyV9cSAq"},{"type":"inlineMath","value":"4\\times80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">4\\times80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">80</span></span></span></span>","key":"ZpGu7ZlOYF"},{"type":"text","value":") and our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KQMDGETSYw"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UuJA2akkFt"},{"type":"text","value":" layer (to expect ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Qp6IyMBlJi"},{"type":"text","value":"20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d9pwUIWrJX"},{"type":"text","value":" instead of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EzwaDtA4Uy"},{"type":"text","value":"80","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ocGA0QW4Bh"},{"type":"text","value":"). So let’s see how this could be implemented. Basically, right now we have an input that is a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i9ZBNjepB8"},{"type":"inlineMath","value":"4\\times8\\times10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">4\\times8\\times10</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">10</span></span></span></span>","key":"yT5HkSTITS"},{"type":"text","value":" that feeds into the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nP6xCioLLD"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WbGGWYMDha"},{"type":"text","value":" layer, and currently the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WaMaPVvdwK"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AZMXlJ3LAm"},{"type":"text","value":" layer just stretches it out:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Lc66EhsU5Q"}],"key":"FnXZfPjXdr"},{"type":"code","lang":"python","value":"class Flatten:\n    def __call__(self, x):\n        self.out = x.view(x.shape[0], -1)\n        return self.out\n...","position":{"start":{"line":3,"column":1},"end":{"line":9,"column":1}},"key":"J8GXx4bgwl"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"through the ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"bkziEfHgT7"},{"type":"inlineCode","value":"view","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"NiyW9K4Ph8"},{"type":"text","value":" operation. Effectively what it does now is:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"eUbZwDUVgr"}],"key":"u9ahJNV6ke"}],"key":"Yw9iV2oR5t"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"e = torch.randn(\n    4, 8, 10\n)  # goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated\ne.view(4, -1).shape  # yields 4x80","key":"G40GPPNECp"},{"type":"outputs","id":"DkPXhdOKMRNJILUlacvFw","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":71,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 80])","content_type":"text/plain"}}},"children":[],"key":"GrkM2Ifcoo"}],"key":"AZc0QkHpcQ"}],"key":"bOhrq788bB"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"But we want to just view the same tensor as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QXt7JZQRHb"},{"type":"inlineMath","value":"4x4x20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>x</mi><mn>4</mn><mi>x</mi><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">4x4x20</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">4</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">20</span></span></span></span>","key":"MvExQAHYY5"},{"type":"text","value":" instead, so:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zvTpS0pq4W"}],"key":"PSVy59rPrQ"}],"key":"cyTxkIW62f"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"e.view(4, 4, -1).shape  # yields 4x4x20: this is what we want!","key":"tsiycYpNTK"},{"type":"outputs","id":"hKh1bIk5-y3It2RwdlY9s","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":72,"metadata":{},"data":{"text/plain":{"content":"torch.Size([4, 4, 20])","content_type":"text/plain"}}},"children":[],"key":"xRMHbgPrnl"}],"key":"CnV0QWe285"}],"key":"ZxP7WN0VYC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Easy, right? Let’s now rewrite ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A9B4nxOoix"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QanE6ll9TR"},{"type":"text","value":", but since ours will now start to depart from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yL8DL0X2Cv"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn.Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WI2zhVse8H"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten","key":"VN2PrnQGzn"},{"type":"text","value":", we’ll rename it to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZOHtk7zdxT"},{"type":"inlineCode","value":"FlattenConsecutive","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"a4l5mHGFfn"},{"type":"text","value":" just to make sure that our APIs are somewhat similar but not the same:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DozhVnbvsb"}],"key":"Ga8ZmQnSLJ"}],"key":"ZsKOzo462f"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class FlattenConsecutive:\n    def __init__(self, n):\n        self.n = n\n\n    def __call__(self, x):\n        b, t, c = x.shape\n        x = x.view(b, t // self.n, c * self.n)\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        self.out = x\n        return self.out\n\n    def parameters(self):\n        return []","key":"PvMVrGLhLr"},{"type":"outputs","id":"5-k_Fr026PKf_QsbYSq43","children":[],"key":"DYYz8GySsl"}],"key":"e4lJiRNjh2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B2iA6WEnB9"},{"type":"inlineCode","value":"FlattenConsecutive","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vJUHVhKhtO"},{"type":"text","value":" takes in and flattens only some ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ifqgQfTiIy"},{"type":"inlineCode","value":"n","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"P1bSFTyeVE"},{"type":"text","value":" consecutive elements and puts them into the last dimension. In ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AIlactWLpL"},{"type":"inlineCode","value":"__call__","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YnLL7tn82F"},{"type":"text","value":" we parse the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qzzsCQEQiu"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ohAATvxFA3"},{"type":"text","value":" dimensions of the input ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fXILXQSpXs"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zbiAM24Iro"},{"type":"text","value":" as ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hN4xbh9oeN"},{"type":"inlineCode","value":"b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N1Ybw8X8aa"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jXQl6OAcLU"},{"type":"inlineCode","value":"c","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GpE5sdtMec"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CxDO8DdDmL"},{"type":"inlineCode","value":"t","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A54S42oPh7"},{"type":"text","value":" (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Kqn0KLV8ck"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F8r6W6pct9"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xLKs9KcOnF"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g7I4fDv0ue"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kt8lDZ3dtO"},{"type":"text","value":"10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jDusqASrjZ"},{"type":"text","value":") and then we view ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nrGNK2PhAK"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UhjY8LpLZw"},{"type":"text","value":" as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jM62OIs3gb"},{"type":"inlineCode","value":"b","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SnJIye6aY3"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GsUrxogiE8"},{"type":"inlineCode","value":"t // n","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bPgkF79T7D"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A86Zvrq89P"},{"type":"inlineCode","value":"c * n","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nN7fvSQNhW"},{"type":"text","value":" tensor (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fixPo49M0c"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tpqwqCbaeQ"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o4XxEUHl2C"},{"type":"inlineMath","value":"8/2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">8/2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">8/2</span></span></span></span>","key":"Z8OSR39SNT"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h8K86slre6"},{"type":"inlineMath","value":"10 \\cdot 2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>10</mn><mo>⋅</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">10 \\cdot 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">10</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span></span></span></span>","key":"Zx7tJxPugi"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RLlYqCXMzK"},{"type":"emphasis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"a.k.a.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rrp8jh8cPY"}],"key":"JDHBhpz63E"},{"type":"text","value":": ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MRdwL3y6vy"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dr1wOPTos7"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mHN3f14Hhp"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h7v8n7xzTR"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CMilQvJr6q"},{"type":"text","value":"20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PXprvisF5O"},{"type":"text","value":"). Last but not least, we check whether the middle dimension of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RTyZbN7VKJ"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kLTQQPnAHc"},{"type":"text","value":" (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RMno9rdLbW"},{"type":"inlineCode","value":"x.shape[1]","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ISaCT2L9D0"},{"type":"text","value":") is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UgENsAn1gS"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B9quujoRXZ"},{"type":"text","value":" and if so, then we simply squeeze out that dimension (e.g. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PrguXTIpOq"},{"type":"inlineMath","value":"4\\times1\\times10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>1</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">4\\times1\\times10</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">10</span></span></span></span>","key":"BZb9zKU0V3"},{"type":"text","value":" would become ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tTy6q3NZYv"},{"type":"inlineMath","value":"4\\times10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">4\\times10</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">10</span></span></span></span>","key":"DYA7xowYqE"},{"type":"text","value":"). Let’s now replace ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BVPsggHRZE"},{"type":"inlineCode","value":"Flatten","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EVjo7mtWVv"},{"type":"text","value":" with our new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DoWIpoeqmo"},{"type":"inlineCode","value":"FlattenConsecutive","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MeqM3DPVy8"},{"type":"text","value":", while maintaining the same functionality:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"loLKqRR6P8"}],"key":"UAdZHLOloM"}],"key":"Y6ijjZl2c4"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(block_size, n_embd, n_hidden):\n    n_inputs = n_embd * block_size\n    n_outputs = vocab_size\n    model = Sequential(\n        [\n            Embedding(vocab_size, n_embd),\n            FlattenConsecutive(block_size),\n            Linear(n_inputs, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            Linear(n_hidden, n_outputs),\n        ]\n    )\n    # parameter init\n    with torch.no_grad():\n        model.layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = model.parameters()\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return model, parameters","key":"iu0OeUDZ6D"},{"type":"outputs","id":"kXE2Y-5vAnRvRgTXLm9SO","children":[],"key":"KdKRNrwOPB"}],"key":"DzSA8zMGdJ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, let’s define the model and verify that the shapes of the layer outputs are the same after feeding one batch of data into it:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eEr6bVAoDy"}],"key":"yh5G1FV3Z8"}],"key":"Mrn5naBXTW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, _ = define_nn(block_size, n_embd, n_hidden)\nprint(xb.shape)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))","key":"AQ03gEfzW6"},{"type":"outputs","id":"_pLCmXQi7xQlpWZvE5-oz","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"22097\ntorch.Size([4, 8])\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 80)\nLinear : (4, 200)\nBatchNorm1d : (4, 200)\nTanh : (4, 200)\nLinear : (4, 27)\n"},"children":[],"key":"bAVGOk7e6r"}],"key":"ZP4NBukKxI"}],"key":"XcpnUjYmMm"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, we see the shapes as we expect them after every single layer in its output. Now, let’s try to restructure it and do it hierarchically:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Yk6Y65e1NN"}],"key":"puPgoUq1UA"}],"key":"SQRKYybyCo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def define_nn(block_size, n_embd, n_hidden):\n    n_consec = 2\n    n_inputs = n_embd * n_consec\n    n_outputs = vocab_size\n    model = Sequential(\n        [\n            Embedding(vocab_size, n_embd),\n            FlattenConsecutive(n_consec),\n            Linear(n_inputs, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            FlattenConsecutive(n_consec),\n            Linear(n_hidden * n_consec, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            FlattenConsecutive(n_consec),\n            Linear(n_hidden * n_consec, n_hidden, bias=False),\n            BatchNorm1d(n_hidden),\n            Tanh(),\n            Linear(n_hidden, n_outputs),\n        ]\n    )\n    # parameter init\n    with torch.no_grad():\n        model.layers[-1].weight *= 0.1  # last layer make less confident\n    parameters = model.parameters()\n    print(sum(p.nelement() for p in parameters))  # number of parameters in total\n    for p in parameters:\n        p.requires_grad = True\n    return model, parameters","key":"rIf8ljqgVS"},{"type":"outputs","id":"xotejLgGbnKbXjAL81JXj","children":[],"key":"PBhORebBkD"}],"key":"EnQ3YEpPX3"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now, let’s inspect the numbers in between after a forward pass on a new ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B4zlA5Q2rn"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yI75Q9hkbT"}],"key":"HnY9MUQmWR"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jciitY594O"}],"key":"YYahRVFDwZ"}],"key":"WBasJIf3SV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, _ = define_nn(block_size, n_embd, n_hidden)\nprint(xb.shape)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))","key":"ot2HFQgaqP"},{"type":"outputs","id":"SibbkqEQjTdHRX0WBNpKO","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"170897\ntorch.Size([4, 8])\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 4, 20)\nLinear : (4, 4, 200)\nBatchNorm1d : (4, 4, 200)\nTanh : (4, 4, 200)\nFlattenConsecutive : (4, 2, 400)\nLinear : (4, 2, 200)\nBatchNorm1d : (4, 2, 200)\nTanh : (4, 2, 200)\nFlattenConsecutive : (4, 400)\nLinear : (4, 200)\nBatchNorm1d : (4, 200)\nTanh : (4, 200)\nLinear : (4, 27)\n"},"children":[],"key":"G2RxN1nuOV"}],"key":"bWzvQH9eg5"}],"key":"WjrKVBNeP7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"So ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LiCabUQUfO"},{"type":"inlineMath","value":"4\\times8\\times20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">4\\times8\\times20</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">8</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">20</span></span></span></span>","key":"aJfN5F3gvt"},{"type":"text","value":" was flattened consecutively into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JiP7p75xGx"},{"type":"inlineMath","value":"4\\times4\\times20","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">4\\times4\\times20</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">20</span></span></span></span>","key":"Dn3jUbLSCI"},{"type":"text","value":". Through the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OBmnxtxqea"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sBYR9GhzM3"},{"type":"text","value":" layer, this was projected into ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SdIZBjsjbx"},{"type":"inlineMath","value":"4\\times4\\times200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4\\times4\\times200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">200</span></span></span></span>","key":"JRwtmUoI5i"},{"type":"text","value":". And then ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uIOlMfv4OZ"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g2KLBL8Dzk"},{"type":"text","value":" just works out of the box and so does ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FPQyBVxyHU"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tfgRMciSox"},{"type":"text","value":", which is element-wise. Then we crushed it again. So we flattened consecutively once more and ended up with a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qXYi5PuC6P"},{"type":"inlineMath","value":"4\\times2\\times400","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>400</mn></mrow><annotation encoding=\"application/x-tex\">4\\times2\\times400</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">400</span></span></span></span>","key":"NZDi612R1n"},{"type":"text","value":" now. Then ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x76L2mDkSl"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zcr4K6kTUb"},{"type":"text","value":" brought it back down to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PIbAKebFlL"},{"type":"inlineMath","value":"4\\times2\\times200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4\\times2\\times200</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">200</span></span></span></span>","key":"gscSk8cQFf"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"va1h6gaGNJ"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LbF1n5nNSj"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Wc0rWcHyFh"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mxEedQHX4q"},{"type":"text","value":" didn’t change the shape and for the last flattening,\nit squeezed out that dimension of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GyLVxs7vAA"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AQo3fNn7BP"},{"type":"text","value":", we end up with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OnRHtw4Oom"},{"type":"inlineMath","value":"4\\times400","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>400</mn></mrow><annotation encoding=\"application/x-tex\">4\\times400</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">400</span></span></span></span>","key":"EfJoWDqYWM"},{"type":"text","value":". And then ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IoFfpT1kmu"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W8K00t5pOO"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"l8ILEFxXnG"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v84W8VMoPp"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XslxCFX0HY"},{"type":"inlineCode","value":"Tanh","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VG8J2WjE8c"},{"type":"text","value":" and the last ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NE8gKCqVTs"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kllDAYVNLh"},{"type":"text","value":" yield our logits that end up in the same shape as they were before. Now, we actually have a nice three-layer ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mpJD9iGkA4"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lG4fQoUDK8"}],"key":"i2IcSoTjtd"},{"type":"text","value":" that basically corresponds to this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ba4Mfv13JN"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xp2tx5xXXA"}],"key":"CaaLf5pA1O"},{"type":"text","value":" network:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w6MwCRf2Z4"}],"key":"dm5fGzDJTW"}],"key":"XoMteWKSvB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))","key":"g6Kczib5Qj"},{"type":"outputs","id":"oiP4PEBds0ek5ru3bO3pV","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"198c39366ff68e8b03ccb06df349f47a","path":"/build/198c39366ff68e8b03ccb06df349f47a.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"I5nD6Xg6nX"}],"key":"keUu7VpfPc"}],"key":"zkzd06LIve"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"with the only difference that we are using a blocksize of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tDyXf7E65j"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m2GrBKiJX4"},{"type":"text","value":" instead of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Du1H3ZAzkP"},{"type":"text","value":"16","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U46MKKWYsA"},{"type":"text","value":", as depicted above. Now with a new architecture, we just have to kind of figure out some good channel numbers (numbers of hidden units) to use here. If we decrease the number to:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XaR0KQdVx7"}],"key":"yrRIv5F9aA"}],"key":"b3KFX14bZB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"n_hidden = 68","key":"tCtXFFvuwn"},{"type":"outputs","id":"zPsYenbuif8rVU_Smp-BE","children":[],"key":"NfxzzReiXI"}],"key":"HCyJUTIk41"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"then the total number of parameters comes out to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s1gjeeeJop"},{"type":"text","value":"22000","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k7Aq2rbFYH"},{"type":"text","value":": exactly the same that we had before (when ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NblFTPez1a"},{"type":"inlineCode","value":"n_hidden=200","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yg5VOHdMHY"},{"type":"text","value":"). So we have the same amount of capacity with this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xctAwmGq9a"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ClGEJ7g01r"}],"key":"JcXI2U9CRI"},{"type":"text","value":" in terms of the number of parameters. But the question is whether we are utilizing those parameters in a more efficient architecture.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sz0HZDh3hd"}],"key":"b9NDjargRU"}],"key":"DF39HHVRBA"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Training the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FMfAUzfNSD"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xNFCkFy6ut"}],"key":"LMpcqdU8dE"},{"type":"text","value":": first pass","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sRjYVbcnhl"}],"identifier":"training-the-wavenet-first-pass","label":"Training the WaveNet: first pass","html_id":"training-the-wavenet-first-pass","implicit":true,"key":"HyU9bljO5v"}],"visibility":"show","key":"BDturEEB8q"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s train this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jIV7bxS062"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B2dG3e8RUC"}],"key":"Zp1y3oTrbA"},{"type":"text","value":" and see the results:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cM7oYqnzay"}],"key":"rQPZ3TcjNs"}],"key":"zJqFQ9elaN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)","key":"JdKDnH1Bz7"},{"type":"outputs","id":"hIUjiNwOylitp0n_MakjR","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"22397\n      0/ 200000: 3.2978\n  10000/ 200000: 2.1271\n  20000/ 200000: 2.0807\n  30000/ 200000: 1.6842\n  40000/ 200000: 2.0252\n  50000/ 200000: 2.3853\n  60000/ 200000: 2.4678\n  70000/ 200000: 1.7907\n  80000/ 200000: 2.2092\n  90000/ 200000: 2.3790\n 100000/ 200000: 1.7643\n 110000/ 200000: 1.6553\n 120000/ 200000: 1.9414\n 130000/ 200000: 1.9827\n 140000/ 200000: 1.7703\n 150000/ 200000: 1.8300\n 160000/ 200000: 1.6640\n 170000/ 200000: 1.9619\n 180000/ 200000: 1.7971\n 190000/ 200000: 1.9981\n"},"children":[],"key":"HoRwlR5eow"}],"key":"WNf7Gv7Kan"}],"key":"M450TI345Y"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));","key":"CR24H1zlKi"},{"type":"outputs","id":"YyHNSJ9CDSFvb46Y7BfuG","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ebbb1fbb33064be7876271d293fca166\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"cf9b940575805b57c2be2a06f27ba028","path":"/build/cf9b940575805b57c2be2a06f27ba028.png"},"text/html":{"content_type":"text/html","hash":"ed8a713dd897205bb2b423de5d813551","path":"/build/ed8a713dd897205bb2b423de5d813551.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"Tsqjm429xf"}],"key":"tL2rbEyAxC"}],"key":"PkE83Mra5R"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"trigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");","key":"zEiuk9iQoh"},{"type":"outputs","id":"sUxg6bDc9-3qAGDNPoDk8","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 1.9376758337020874\nval 2.026397943496704\n"},"children":[],"key":"szBFX91Ge2"}],"key":"g4UZXY6oUV"}],"key":"gbVf79Fys0"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"code","lang":"python","value":"# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n# flat -> hierachical (22K params): train 1.937, val 2.026","position":{"start":{"line":1,"column":1},"end":{"line":5,"column":1}},"key":"pgl0waQGR9"}],"visibility":"show","key":"JSa6rX6uR4"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As you can see, changing from the flat to hierachical model (while keeping the same number of parameters) is not giving us any noticeable significant benefit in terms of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CXA55Jpjfb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fHujhr9smj"}],"key":"Pr9MUP7aVr"},{"type":"text","value":".  That said, there are two things to point out. Number one, we didn’t really “torture” the architecture here very much. And there’s a bunch of hyperparameter search that we could do in terms of how we allocate our budget of parameters to what layers. Number two, we still may have a bug inside the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mbmw6HIKWo"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KkXeh5OqBz"},{"type":"text","value":" layer. So let’s take a look at that because it runs, but doesn’t do the right thing.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NA5t1uOfNo"}],"key":"ZTz0ePjw98"}],"key":"Br7loKCMbP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Fixing the BatchNorm1d bug","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zNfeJfqrqT"}],"identifier":"fixing-the-batchnorm1d-bug","label":"Fixing the BatchNorm1d bug","html_id":"fixing-the-batchnorm1d-bug","implicit":true,"key":"FzOLqPlz14"}],"key":"kooKcm1zMG"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"If we train for just one step and we print the layer output shapes:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Eo5SBjHWBw"}],"key":"l4x5iSMPgj"}],"key":"HwnQpFzsiH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"_ = train(xtrain, ytrain, model, parameters, break_at_step=1)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))","key":"iCFu2UFOPT"},{"type":"outputs","id":"g0J86atmiuoKjXgaYQVfL","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"      0/ 200000: 2.0969\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 4, 20)\nLinear : (4, 4, 68)\nBatchNorm1d : (4, 4, 68)\nTanh : (4, 4, 68)\nFlattenConsecutive : (4, 2, 136)\nLinear : (4, 2, 68)\nBatchNorm1d : (4, 2, 68)\nTanh : (4, 2, 68)\nFlattenConsecutive : (4, 136)\nLinear : (4, 68)\nBatchNorm1d : (4, 68)\nTanh : (4, 68)\nLinear : (4, 27)\n"},"children":[],"key":"tV2BKWfzlo"}],"key":"MnUMe9KuIE"}],"key":"qByOFcgdC0"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"currently, it looks like the BatchNorm is receiving an input that is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jNOLGZr9c3"},{"type":"inlineMath","value":"32\\times4\\times68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>32</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>68</mn></mrow><annotation encoding=\"application/x-tex\">32\\times4\\times68</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">32</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">68</span></span></span></span>","key":"wFBvfGHrzn"},{"type":"text","value":", right? Let’s take a look at the implementation of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FtkTbppCgM"},{"type":"inlineCode","value":"BatchNorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eOz1yX6ETn"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BScg0OM05K"}],"key":"Y7aAhX5LPh"},{"type":"code","lang":"python","value":"class BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n  \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True) # batch mean\n            xvar = x.var(0, keepdim=True) # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n  \n    def parameters(self):\n        return [self.gamma, self.beta]","position":{"start":{"line":3,"column":1},"end":{"line":35,"column":1}},"key":"A5wCCbkLI6"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"It assumed, in the way we wrote it and at the time, that the input ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"bQtuVQ9fZl"},{"type":"inlineCode","value":"x","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"YLBgjJgdET"},{"type":"text","value":" is ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"kJLiUf7ZKt"},{"type":"inlineMath","value":"2D","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">2D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"sksxQdTFoq"},{"type":"text","value":". So it was ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"kN4LzKXdKb"},{"type":"inlineMath","value":"N \\times D","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">N \\times D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"ICXS0r4cbr"},{"type":"text","value":", where ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"keEdIfBTJz"},{"type":"inlineMath","value":"N","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>","key":"ae7ineMki5"},{"type":"text","value":" was the batch size. So that’s why we only reduced the mean and the variance over the ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ICsrrTRumj"},{"type":"inlineMath","value":"0th","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">0th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">0</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"VqZ6feckx8"},{"type":"text","value":" dimension. But now ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"XB9XemMaaC"},{"type":"inlineCode","value":"x","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ebjc2z49BH"},{"type":"text","value":" will basically become ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"Kug3NtQ3Fq"},{"type":"inlineMath","value":"3D","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">3D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">3</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"DGTTbHkFfq"},{"type":"text","value":". So what’s happening inside the ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ukWkQJSncq"},{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"vRyOKW3ba6"}],"key":"gCceAYGhez"},{"type":"text","value":" layer right now? And how come it’s working at all and not giving any errors? The reason for that is basically because everything broadcasts properly, but the ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"PtchMav6Bo"},{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"aNjYq0BNnP"}],"key":"KW1rIeHg2x"},{"type":"text","value":" is not doing what we want it to do. So in particular, let’s basically think through what’s happening inside the ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"KHEvIDlZpm"},{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"yrGBT477Tf"}],"key":"eYec9d43x7"},{"type":"text","value":". Let’s look at what’s happening here in a simplified example:","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"HAESFUHYzw"}],"key":"wjmY2VjGCH"}],"key":"pPbfekbEMO"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"e = torch.randn(32, 4, 68)\nemean = e.mean(0, keepdim=True)  # 1, 4, 68\nevar = e.var(0, keepdim=True)  # 1, 4, 68\nehat = (e - emean) / torch.sqrt(evar + 1e-5)  # 32, 4, 68\nehat.shape","key":"HsFtUY59Sw"},{"type":"outputs","id":"EtCre-6tboP2E6pl3DLTP","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":84,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 4, 68])","content_type":"text/plain"}}},"children":[],"key":"gbCKyc47od"}],"key":"QbgLKYII7Z"}],"key":"fhHClBquat"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So we’re receiving an input of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MhWFkywlyi"},{"type":"inlineMath","value":"32\\times4\\times68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>32</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>68</mn></mrow><annotation encoding=\"application/x-tex\">32\\times4\\times68</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">32</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">68</span></span></span></span>","key":"eZgQ1G6L9l"},{"type":"text","value":". And then we are doing here ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n8VvaruLXB"},{"type":"inlineCode","value":"x.mean()","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"V0tqGjljvp"},{"type":"text","value":", but we have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RA9lK0bG7p"},{"type":"inlineCode","value":"e","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MWThb1x3SN"},{"type":"text","value":" instead of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i2MhiVSuq8"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xedCD63eNv"},{"type":"text","value":". But we’re doing the mean over ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oPowMLMzq0"},{"type":"text","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xJ94lx2ncL"},{"type":"text","value":" and that’s actually giving us ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c83vUY7BDt"},{"type":"inlineMath","value":"1\\times4\\times68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>68</mn></mrow><annotation encoding=\"application/x-tex\">1\\times4\\times68</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">68</span></span></span></span>","key":"LYMdxpoyGb"},{"type":"text","value":". So we’re doing the mean only over the very first dimension. And it’s giving us a mean and a variance that still maintains the middle dimension in between (i.e. ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e2q9fcdt4o"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"O7rIX93VHx"},{"type":"text","value":"). So these means are only taken over ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GFO6gD8lJE"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N3MQroi5Io"},{"type":"text","value":" numbers in the first dimension. And then, when we perform the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ajx7mE86rO"},{"type":"inlineCode","value":"ehat","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"etnZKxoM4J"},{"type":"text","value":" assignment, everything broadcasts correctly still. But basically what ends up happening is when we also look at the running mean:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fCVleW0d7k"}],"key":"tYOp16I677"}],"key":"VqmJsEkoY3"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model.layers[3].running_mean.shape","key":"f2p3PWlf6I"},{"type":"outputs","id":"m7G_GG2LfxU9o03jMeUWg","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":85,"metadata":{},"data":{"text/plain":{"content":"torch.Size([1, 4, 68])","content_type":"text/plain"}}},"children":[],"key":"IrAMejQOkP"}],"key":"YsAVUVTOv9"}],"key":"z9uQ6QWcXQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"the shape of this running mean now is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QhQFwpVKPt"},{"type":"inlineMath","value":"1\\times4\\times68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>68</mn></mrow><annotation encoding=\"application/x-tex\">1\\times4\\times68</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">68</span></span></span></span>","key":"SpMpVUISLG"},{"type":"text","value":". Instead of it being just a size of dimension, because we have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"slGpONK8mC"},{"type":"text","value":"68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gIaRmFyZXK"},{"type":"text","value":" channels, we expect to have ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YrpXaYm6ei"},{"type":"text","value":"68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jMxIygx7y7"},{"type":"text","value":" means and variances that we’re maintaining. But actually, we have an array of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Tr6eEObsYx"},{"type":"inlineMath","value":"4\\times68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>68</mn></mrow><annotation encoding=\"application/x-tex\">4\\times68</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">68</span></span></span></span>","key":"jWn54B6KDq"},{"type":"text","value":". And so basically what this is telling us is this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tSKiCk5kKx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ngd5fjtecS"}],"key":"ID2itDvUYC"},{"type":"text","value":" is currently working in parallel over ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KYJzzDTGHh"},{"type":"inlineMath","value":"4\\times68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mo>×</mo><mn>68</mn></mrow><annotation encoding=\"application/x-tex\">4\\times68</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">4</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">68</span></span></span></span>","key":"eL7DBpp2zZ"},{"type":"text","value":" instead of just ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rPAbG2FBwQ"},{"type":"text","value":"68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z1iecwuBjU"},{"type":"text","value":" channels. So basically we are maintaining this. We are maintaining statistics for every one of these four positions individually and independently. And instead, what we want to do is we want to treat this middle ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Azn6uUEqzQ"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zftdV8S3yl"},{"type":"text","value":" dimension as a batch dimension, just like the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H9NPBQ12gV"},{"type":"inlineMath","value":"0th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">0th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">0</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"pinSiZJtD8"},{"type":"text","value":" dimension. So as far as the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lQfD91HSyW"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XcA1rSrFg3"}],"key":"qAI8rCwIri"},{"type":"text","value":" is concerned, it doesn’t want to average... We don’t want to average over ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j5FiV8KVji"},{"type":"text","value":"32","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cv0aIFLISc"},{"type":"text","value":" numbers. But instead, we want to now average over ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mDocEiD2PX"},{"type":"inlineMath","value":"32\\times4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>32</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">32\\times4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">32</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">4</span></span></span></span>","key":"vDAWPalvXG"},{"type":"text","value":" numbers for every single one of these ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o1bbDDL51o"},{"type":"text","value":"68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A9YmDjohOl"},{"type":"text","value":" channels. Since ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pOKsG7bC9Z"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.mean.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.mean","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZnyxtJLAlM"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.mean.html","key":"oLNS9gxW4w"},{"type":"text","value":" allows us to reduce over multiple (and not just one) dimensions at the same time, we’ll do just that and reduce over both the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BWNw64vBBS"},{"type":"inlineMath","value":"0th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">0th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">0</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"tbkoCcnfo6"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AjyaDZ4MdG"},{"type":"inlineMath","value":"1st","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mi>s</mi><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">1st</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">t</span></span></span></span>","key":"vQ9PwDR7dG"},{"type":"text","value":" dimensions:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UII8enIISH"}],"key":"A73KOtDDHV"}],"key":"bJKkEqv72y"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"e = torch.randn(32, 4, 68)\nemean = e.mean((0, 1), keepdim=True)  # 1, 1, 68\nevar = e.var((0, 1), keepdim=True)  # 1, 1, 68\nehat = (e - emean) / torch.sqrt(evar + 1e-5)  # 32, 4, 68\nehat.shape","key":"q8EGzbNZAS"},{"type":"outputs","id":"_B6_siBKI-qWMbrpzc10Q","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":86,"metadata":{},"data":{"text/plain":{"content":"torch.Size([32, 4, 68])","content_type":"text/plain"}}},"children":[],"key":"EQM5gyfH5M"}],"key":"kfKCXVhe5h"}],"key":"qJoIwF8sEj"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Although the final shape of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"smS8m6n5Ni"},{"type":"inlineCode","value":"ehat","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"euoaxJmpWb"},{"type":"text","value":" remains the same, we see now that:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dPdPIsKYJc"}],"key":"dBjjdGf7P9"}],"key":"hjx1Tu3Wsk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"emean.shape, evar.shape","key":"wdrKpQdWrq"},{"type":"outputs","id":"R2nMvYbyKa8IsSrW_YQJW","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":87,"metadata":{},"data":{"text/plain":{"content":"(torch.Size([1, 1, 68]), torch.Size([1, 1, 68]))","content_type":"text/plain"}}},"children":[],"key":"rsYrbupzlY"}],"key":"Qiup9TER15"}],"key":"d57iywsonz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"instead of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EPphpwlW9g"},{"type":"inlineCode","value":"1, 4, 68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mMSkieYgMf"},{"type":"text","value":", since we reduced over both of the batch dimensions, it yields only ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Oldu8RsWHy"},{"type":"text","value":"68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AscPwmuE1S"},{"type":"text","value":" numbers total for each tensor, with a bunch of spurious leftover ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WKIY0RAmG5"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ma6bzB8eTz"},{"type":"text","value":" dimensions remaining. Therefore, this is what should be happening with our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F1VXV3hPvk"},{"type":"inlineCode","value":"running_mean","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o0SQk0IE88"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xt6rvMClUK"},{"type":"inlineCode","value":"running_var","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LnMSO1aZCr"},{"type":"text","value":" tensors inside our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s08YyPMgjl"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iZuM62vHe4"},{"type":"text","value":" implementation. So the change is pretty straightforward:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kPN5HRbI69"}],"key":"ltCerD8MBQ"}],"key":"iyMxCV7OJu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            if x.ndim == 2:\n                dim = 0\n            elif x.ndim == 3:\n                dim = (0, 1)\n            xmean = x.mean(dim, keepdim=True)  # batch mean\n            xvar = x.var(dim, keepdim=True)  # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (\n                    1 - self.momentum\n                ) * self.running_mean + self.momentum * xmean\n                self.running_var = (\n                    1 - self.momentum\n                ) * self.running_var + self.momentum * xvar\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]","key":"hTrXc8hnPo"},{"type":"outputs","id":"SpiyR7rBXYp3yJfSzxfm_","children":[],"key":"Mo1FettlbA"}],"key":"nLDPGQa3ef"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Basically, now in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eDE6RJG45K"},{"type":"inlineCode","value":"__call__","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bdSzp5f5Bc"},{"type":"text","value":" we are checking the dimensionality of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H4gBZ02dQP"},{"type":"inlineCode","value":"x","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lwpgmKiT9H"},{"type":"text","value":" and based on it we are determining the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J2Ub5k9PdW"},{"type":"inlineCode","value":"dim","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"shMvkbgvdo"},{"type":"text","value":" parameters to be passed to the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"n5w1A7Ab92"},{"type":"inlineCode","value":"mean","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HQ9x0M6rvZ"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VC5Q7NbLjg"},{"type":"inlineCode","value":"var","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VbSkDZDHxA"},{"type":"text","value":" functions. Now, to point out one more thing. We’re actually departing from the API of PyTorch here a little bit, because when you go read the documentation of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y6L3uevu7W"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn.BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"chXBTimYdb"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","key":"SphXH64lwa"},{"type":"text","value":", it says:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rYTOfgfOr7"}],"key":"juUS2C3k9o"},{"type":"blockquote","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Input: ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yMep00cTuh"},{"type":"inlineMath","value":"(N,C)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N,C)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mclose\">)</span></span></span></span>","key":"JclBx7m4Si"},{"type":"text","value":" or ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Vm01TLR2WV"},{"type":"inlineMath","value":"(N,C,L)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N,C,L)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mclose\">)</span></span></span></span>","key":"p2tj33EJIB"},{"type":"text","value":", where ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"fam9jbLiH4"},{"type":"inlineMath","value":"N","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>","key":"pV6IUYHyrK"},{"type":"text","value":" is the batch size, ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qdiQvB5qCg"},{"type":"inlineMath","value":"C","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"c7cCcIF2Ek"},{"type":"text","value":" is the number of features or channels, and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"rvKs1tgtxy"},{"type":"inlineMath","value":"L","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span>","key":"mofEByLcWJ"},{"type":"text","value":" is the sequence length","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"j2KhffcXM9"}],"key":"cQF4uWNmtu"}],"key":"d6TtuGxMhW"}],"key":"bV0V3ZI2dl"}],"key":"fiENKGlOSA"}],"key":"zeKtm3CXGd"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Notice, the input to this layer can either be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JWUwTWT3YS"},{"type":"inlineMath","value":"N","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>","key":"bmussbcDKX"},{"type":"text","value":" (batch size) ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z4Eo5Ce18M"},{"type":"inlineMath","value":"\\times","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>×</mo></mrow><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord\">×</span></span></span></span>","key":"B215hDGa31"},{"type":"text","value":" ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TwC3tqEnHV"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"KSO55jZ8aE"},{"type":"text","value":" (number of features or channels) or it actually does accept three-dimensional inputs, but it expects it to be ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GWCMyqrWxO"},{"type":"inlineMath","value":"N\\times C \\times L","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi><mo>×</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">N\\times C \\times L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span>","key":"uHDRWOG5hv"},{"type":"text","value":" (sequence legth). So this is a problem because you see how ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YMcIXvdC1N"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"WUce02joS6"},{"type":"text","value":" is nested here in the middle. And so when it gets ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"M91UBUNNnW"},{"type":"inlineMath","value":"3D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">3D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">3</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"p5lDJudxAN"},{"type":"text","value":" inputs, this ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eQ4os87Bkg"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vgp2nBu8RW"}],"key":"dveE7ZDnMM"},{"type":"text","value":" layer will reduce over ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"G5ZHcirRHt"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wBhQPhc9Pd"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zkkDo29TMM"},{"type":"inlineCode","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bCwLutYclf"},{"type":"text","value":" instead of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ENUsQh3H6Y"},{"type":"inlineCode","value":"0","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zKw6fcYAjv"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k10CfAqgqI"},{"type":"inlineCode","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K5gi59gYL5"},{"type":"text","value":". So basically, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"B5Kay4g0Ae"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn.BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hfR20bOiNz"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","key":"HcNXVroGIO"},{"type":"text","value":" layer assumes that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U37eQwsWTs"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"pgQvNgJmaY"},{"type":"text","value":" will always be the first dimension, whereas we assume here that ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PJexZQJZVg"},{"type":"inlineMath","value":"C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"ty3N4JzPIX"},{"type":"text","value":" is the last dimension, and there are some number of batch dimensions beforehand. And so, it expects ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fgCnCxtj1S"},{"type":"inlineMath","value":"N\\times C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">N\\times C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"abRFRugKtf"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MOpPbRoCjF"},{"type":"inlineMath","value":"N\\times C\\times L","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi><mo>×</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">N\\times C\\times L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span>","key":"hnd2T98C3Y"},{"type":"text","value":", whereas we expect ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nwHnPYeQjx"},{"type":"inlineMath","value":"N\\times C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">N\\times C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"yI3mDTzfQU"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ni0inbkNkM"},{"type":"inlineMath","value":"N\\times L\\times C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>L</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">N\\times L\\times C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"yFm6dhPQht"},{"type":"text","value":". So just a small deviation from the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W91ewxfFiD"},{"type":"inlineCode","value":"PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rDhpoyh3ZQ"},{"type":"text","value":" API. Now, after updating our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"od15kd36PH"},{"type":"inlineCode","value":"BatchNorm1d","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e0U6ndnLH3"},{"type":"text","value":", if we redefine our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XPzPj08oZA"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ca1cRYA94d"}],"key":"RzZECr4pGo"},{"type":"text","value":" and run for one step:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ga5FjyEwU7"}],"key":"oTAehjiFAh"}],"key":"ZnVSEDRXzu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, parameters = define_nn(block_size, n_embd, n_hidden)\n_ = train(xtrain, ytrain, model, parameters, break_at_step=1)\nmodel(xb)\nfor l in model.layers:\n    print(l.__class__.__name__, \":\", tuple(l.out.shape))","key":"IyhwALEtBz"},{"type":"outputs","id":"Md6zZBR1AZjTellgNjzN9","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"22397\n      0/ 200000: 3.2907\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 4, 20)\nLinear : (4, 4, 68)\nBatchNorm1d : (4, 4, 68)\nTanh : (4, 4, 68)\nFlattenConsecutive : (4, 2, 136)\nLinear : (4, 2, 68)\nBatchNorm1d : (4, 2, 68)\nTanh : (4, 2, 68)\nFlattenConsecutive : (4, 136)\nLinear : (4, 68)\nBatchNorm1d : (4, 68)\nTanh : (4, 68)\nLinear : (4, 27)\n"},"children":[],"key":"wMLWUyfTGk"}],"key":"fcgO6Tipyb"}],"key":"g205CyxD3d"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"the shapes are of course the same as before, but now if we inspect the shape of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LuQjB0ULyD"},{"type":"inlineCode","value":"running_mean","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZSpLNmRQvx"},{"type":"text","value":" inside the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"edtNfCIvFj"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"batchnorm","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SKBacn5qz3"}],"key":"PwUmYuwxm1"},{"type":"text","value":" layer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tB7UHl8tgT"}],"key":"gMRzdbERqu"}],"key":"vOgbwOUihL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model.layers[3].running_mean.shape","key":"eOPBasvcVA"},{"type":"outputs","id":"WY-3xyDd_Jbd2i3wZdLyj","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":90,"metadata":{},"data":{"text/plain":{"content":"torch.Size([1, 1, 68])","content_type":"text/plain"}}},"children":[],"key":"CFVAILpj7X"}],"key":"X90FoJSn6x"}],"key":"Thxcb1AUJ9"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So correctly now we are only maintaining ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bx1D1PxK9N"},{"type":"text","value":"68","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IwcC5uSIbB"},{"type":"text","value":" means, for every one of our channels, and we are treating the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GlYUpk1dKp"},{"type":"inlineMath","value":"0th","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mi>t</mi><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">0th</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord\">0</span><span class=\"mord mathnormal\">t</span><span class=\"mord mathnormal\">h</span></span></span></span>","key":"omf5NkVQ9j"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vgmvUnsDTf"},{"type":"inlineMath","value":"1st","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mi>s</mi><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">1st</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">t</span></span></span></span>","key":"PsPyjqVGJU"},{"type":"text","value":" dimension as batch dimensions, which is exactly what we want!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xlbkujrPdG"}],"key":"VCGyGDOrfh"}],"key":"QyH6CvmOJ5"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Re-training the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YBVOZ36Fka"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YZ58hVluvl"}],"key":"GYdtHVFYrc"},{"type":"text","value":" after bug fix","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QdHW8nWxgP"}],"identifier":"re-training-the-wavenet-after-bug-fix","label":"Re-training the WaveNet after bug fix","html_id":"re-training-the-wavenet-after-bug-fix","implicit":true,"key":"ezP3yfyevj"}],"key":"PZFv4dADEQ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So let’s retrain the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IxL1pfLBER"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lUNZa34jqA"}],"key":"YXOSXGltLG"},{"type":"text","value":" now, after the bug fix:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U1FSG4HrSh"}],"key":"OM4ragftfe"}],"key":"yjwQRA9K8A"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, parameters = define_nn(block_size, n_embd, n_hidden)\nlossi = train(xtrain, ytrain, model, parameters)","key":"QiscgcPSSD"},{"type":"outputs","id":"v1wOk2k06POvTU3aJHXed","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"22397\n      0/ 200000: 3.3054\n  10000/ 200000: 2.2512\n  20000/ 200000: 2.3514\n  30000/ 200000: 2.5115\n  40000/ 200000: 1.6012\n  50000/ 200000: 2.1728\n  60000/ 200000: 1.8859\n  70000/ 200000: 2.1417\n  80000/ 200000: 2.0348\n  90000/ 200000: 1.7458\n 100000/ 200000: 1.7257\n 110000/ 200000: 1.9065\n 120000/ 200000: 2.0347\n 130000/ 200000: 2.1898\n 140000/ 200000: 2.2163\n 150000/ 200000: 1.7984\n 160000/ 200000: 1.4846\n 170000/ 200000: 1.7952\n 180000/ 200000: 2.0809\n 190000/ 200000: 2.2824\n"},"children":[],"key":"DNvF1awo03"}],"key":"IrG2ld5Qgo"}],"key":"pWWgF297tV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));","key":"iiqGzGC8CD"},{"type":"outputs","id":"5I172WlCoJ_-8lzp63JEt","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"089f12aea1ff4c02959e39af0a8a4ef2\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"cdaf79a36040dbd960f88cb29bd58539","path":"/build/cdaf79a36040dbd960f88cb29bd58539.png"},"text/html":{"content_type":"text/html","hash":"a83b309522d993ec6ccf1704230ffdf5","path":"/build/a83b309522d993ec6ccf1704230ffdf5.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"utM3F7V40t"}],"key":"aiPqppoNA5"}],"key":"iwNBN1Lh66"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"trigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");","key":"dxXLXi1Hdk"},{"type":"outputs","id":"XdXRNWHZjqiA-sHp_DSWO","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 1.911558985710144\nval 2.019017219543457\n"},"children":[],"key":"rDGJ9TUb5D"}],"key":"NiG2ocucub"}],"key":"QEecWipdP4"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And we can see that we are getting a tiny improvement in our training and validation losses:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IilQFQVIq9"}],"key":"sgqhiRtAKW"}],"key":"APvoqxI4IY"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"code","lang":"python","value":"# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n# flat -> hierachical (22K params): train 1.937, val 2.026\n# fix bug in batchnorm: train 1.911, val 2.019","position":{"start":{"line":1,"column":1},"end":{"line":6,"column":1}},"key":"JPWP1RRW0b"}],"visibility":"show","key":"tP7BSAN6aC"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The reason this improvement is to be expected is that now we have less numbers going into the estimates of the mean and variance which allows everything to be more stable and less wiggly.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IbAyF2vpg8"}],"key":"h3U4pUXtWU"}],"key":"HRGST2Gs5j"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Scaling up our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rDhgt9UZf1"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TvIsqNl18Q"}],"key":"g3DhEYVVHe"}],"identifier":"scaling-up-our-wavenet","label":"Scaling up our WaveNet","html_id":"scaling-up-our-wavenet","implicit":true,"key":"R6yrgc8yfS"}],"key":"sXFxxStRKE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And with this more general architecture in place, we are now set up to push the performance further by increasing the size of the network:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ad06cNNlB7"}],"key":"GwBy7gv2BW"}],"key":"zRS31EvrZr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"n_embd = 24  # the dimensionality of the character embedding vectors\nn_hidden = 128  # the number of neurons in the hidden layer of the MLP","key":"pjP9nvhtJ2"},{"type":"outputs","id":"fgu1nC5eXhWwyFACUKkAl","children":[],"key":"c4Xc8xZX3G"}],"key":"BS7ZwvZ72S"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"And using the exact same architecture, we now have","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eYSdUFMwld"}],"key":"ZzlgCdTK22"}],"key":"ppCrmXTfhQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model, parameters = define_nn(block_size, n_embd, n_hidden)","key":"oToLWpbGuk"},{"type":"outputs","id":"LnxM898eqZDNNrRml9lcf","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"76579\n"},"children":[],"key":"FAVmodB2hO"}],"key":"dsRso74naS"}],"key":"FTOC4xnHQZ"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"76579","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N5olfWQT05"},{"type":"text","value":" parameters and the training takes a lot longer:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LFGLiR1MGg"}],"key":"WC9nDWL0tG"}],"key":"Zo5xRZWPfT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"lossi = train(xtrain, ytrain, model, parameters)","key":"XqvcjxU6Y4"},{"type":"outputs","id":"m5JcGXfQSpoHEOQiSWqb9","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"      0/ 200000: 3.3060\n  10000/ 200000: 2.1627\n  20000/ 200000: 1.8125\n  30000/ 200000: 2.1831\n  40000/ 200000: 1.9874\n  50000/ 200000: 2.3684\n  60000/ 200000: 2.1482\n  70000/ 200000: 1.7558\n  80000/ 200000: 1.8260\n  90000/ 200000: 1.8867\n 100000/ 200000: 1.9521\n 110000/ 200000: 1.9662\n 120000/ 200000: 1.9416\n 130000/ 200000: 2.0037\n 140000/ 200000: 1.9890\n 150000/ 200000: 1.7099\n 160000/ 200000: 1.8770\n 170000/ 200000: 1.5360\n 180000/ 200000: 1.4641\n 190000/ 200000: 1.8875\n"},"children":[],"key":"Iqn4Qi78hC"}],"key":"e6pnLGeGYa"}],"key":"CtD4pOX56P"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"but we do get a nice curve:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MOd9KlEST7"}],"key":"SsseCrvtYY"}],"key":"DO8cfKpxzd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plt.figure()\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));","key":"mN6rNBKDqU"},{"type":"outputs","id":"hjD4A6parAdfpOPzFTA9i","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"338ac53bc3b84c37828d28bf9e4e8ce7\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"image/png":{"content_type":"image/png","hash":"13aaa7d5b0f0198790fa6b2332620b23","path":"/build/13aaa7d5b0f0198790fa6b2332620b23.png"},"text/html":{"content_type":"text/html","hash":"99231f3c921b2f703abfc487ea65ae31","path":"/build/99231f3c921b2f703abfc487ea65ae31.html"},"text/plain":{"content":"Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …","content_type":"text/plain"}}},"children":[],"key":"CQkcyFJCpk"}],"key":"bSvx7YtojJ"}],"key":"FH4U8yTP7W"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"and we are now getting even better performance:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sEe6rYXBl9"}],"key":"yAguMeuK7q"}],"key":"kBfjpvftKs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"trigger_eval_mode(model)\ninfer_loss(model, xtrain, ytrain, prefix=\"train\")\ninfer_loss(model, xval, yval, prefix=\"val\");","key":"XWdRg3CINu"},{"type":"outputs","id":"j05KI2Q-ITXusZnmyTwLj","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"train 1.7666022777557373\nval 1.9965752363204956\n"},"children":[],"key":"lb3P2PR0Kf"}],"key":"DDu93jSdr1"}],"key":"PU1fO4TEZp"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, to compare to previous performances:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HnCcAiCIpT"}],"key":"rD4zFIIoRQ"}],"key":"FkafZAyIQt"},{"type":"block","kind":"notebook-content","data":{"tags":[]},"children":[{"type":"code","lang":"python","value":"# original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n# context: 3 -> 8 (22K params): train 1.915, val 2.034\n# flat -> hierachical (22K params): train 1.937, val 2.026\n# fix bug in batchnorm: train 1.911, val 2.019\n# scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.768, val 1.990","position":{"start":{"line":1,"column":1},"end":{"line":7,"column":1}},"key":"UTHGqpCZgY"}],"visibility":"show","key":"LbBmtDOYSK"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"However because the experiments are starting to take longer to train, we are a little bit in the dark with respect to the correct setting of the hyperparameters here and the learning rates and so on. And so we are missing sort of like an experimental harness on which we could run a number of experiments and really tune this architecture very well.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TgSjPm3v5o"}],"key":"iNyERNZrPd"}],"key":"mdOmTr309i"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet but with dilated causal convolutions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SJWONjgZlA"}],"identifier":"wavenet-but-with-dilated-causal-convolutions","label":"WaveNet but with dilated causal convolutions","html_id":"wavenet-but-with-dilated-causal-convolutions","implicit":true,"key":"Pq4cX9iRdo"}],"key":"TEXrtJGdL8"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So let’s conclude now with a few notes. We basically improved our performance noticeably from a val ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uajOP3W3Iw"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ors6n6FTXp"}],"key":"Ho6daJjbWa"},{"type":"text","value":" of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tX5eJeMWbI"},{"type":"text","value":"2.10","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L5QNU2KfKg"},{"type":"text","value":" to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ra4oR4GZzw"},{"type":"text","value":"1.99","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oPsxK6LEZI"},{"type":"text","value":". But this shouldn’t be the focus as we are kind of in the dark. We have no experimental harness, we are just guessing and checking. And this whole thing is pretty terrible to be honest. We are just looking at the training ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YuGzc5tXh6"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EVNa15Wso5"}],"key":"LojFH23UgU"},{"type":"text","value":", whereas we should be looking at the training and validation ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uxcXMd7rNc"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"loss","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EDqk4Dt19K"}],"key":"U5VNeBuqqK"},{"type":"text","value":" together. That said, we did implement the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z4Mzs7MDpb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RwOmzNEpFH"}],"key":"yXluz8yZrS"},{"type":"text","value":" architecture from the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VEeZYs8qQC"},{"type":"link","url":"https://arxiv.org/abs/1609.03499","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"paper","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"N0kx5lmV1k"}],"urlSource":"https://arxiv.org/abs/1609.03499","key":"dHLINNphGJ"},{"type":"text","value":":","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"I87lnpEvXo"}],"key":"lQvx5HU3Zw"}],"key":"H8JtaapmGp"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))","key":"xe7To2hJl5"},{"type":"outputs","id":"C9Ls1JjCgIK9jbybiTZdt","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"198c39366ff68e8b03ccb06df349f47a","path":"/build/198c39366ff68e8b03ccb06df349f47a.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"B5iYPD9Fvu"}],"key":"MlrsOcX9BJ"}],"key":"LuF7nnjBoR"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"But we did not implement this specific forward pass of it:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RhbtfmeTBj"}],"key":"qEfSbjws2R"}],"key":"QukJCWUvuj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig4.png\"))","key":"nwqSjzauH0"},{"type":"outputs","id":"D1LLro8nqjOsi4QeAgcOD","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"54272b3d206b11fb693990a90d767eee","path":"/build/54272b3d206b11fb693990a90d767eee.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"tjaEPbJdAr"}],"key":"lv9Su94k9s"}],"key":"bKLAhgodED"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"where you have a more complicated kind of gated linear layer with residual connections, skip connections and so on... So we did not implement this, but only the tree-like model. All things considered, let’s briefly go over how what we’ve done here relates to convolutional neural networks as used in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pzErOYU0ZF"},{"type":"link","url":"https://arxiv.org/abs/1609.03499","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"WaveNet paper","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TC4ClUUjcF"}],"urlSource":"https://arxiv.org/abs/1609.03499","key":"cLyg1mErrP"},{"type":"text","value":". Basically the use of convolutions is strictly for efficiency. It doesn’t actually change the model we’ve implemented. So, here for example:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rypzBAqcFE"}],"key":"SSgQmZXXTh"}],"key":"LPSJJuCCMl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_next_character(xtrain[46:54], ytrain[46:54])","key":"uxnurnEgGQ"},{"type":"outputs","id":"mU8T32tmSCcXZvBXL6SzL","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"........ --> a\n.......a --> n\n......an --> a\n.....ana --> l\n....anal --> i\n...anali --> s\n..analis --> a\n.analisa --> .\n"},"children":[],"key":"UL6nQxZ4Bz"}],"key":"YlXxKrsGCN"}],"key":"xUwgcetxl8"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"we see the name ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cm1Vato1oR"},{"type":"inlineCode","value":"analisa","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yvuRVcMvOO"},{"type":"text","value":" from our training set and it has ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"psH6zkYzKg"},{"type":"text","value":"7","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vsevLSTRwT"},{"type":"text","value":" letters, so that is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uF5GGxrh7M"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cOwci2XSfC"},{"type":"text","value":" rows which correspond to independent examples of that name. Now, we can forward any one of these rows independently:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kKcNTZhgkx"}],"key":"u87Gr5HN2v"}],"key":"QanGlb0ZHd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# forward a single example\nsingle_example = xtrain[[7]]  # index by [[7]] to get an extra batch dimension\nlogits = model(single_example)\nlogits.shape","key":"RalSd1xS0F"},{"type":"outputs","id":"0dCbie8cGmlKIT5hFgcw8","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":102,"metadata":{},"data":{"text/plain":{"content":"torch.Size([1, 27])","content_type":"text/plain"}}},"children":[],"key":"mXr11FTxG0"}],"key":"SW2Uaj8Lb7"}],"key":"ICNif66xnM"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now imagine that instead of just a single example, you would like to forward all of the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"T5rMqK9M0q"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i8Y0aB26U7"},{"type":"text","value":" examples that make up the name into the network at the same time:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JQfgeIIlis"}],"key":"dC158kwS5I"}],"key":"faW0DtxVTV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# forward all of them\nlogits = torch.zeros(8, 27)\nfor i in range(7):\n    logits[i] = model(xtrain[[7 + i]])\nlogits.shape","key":"pfqKi7ru1t"},{"type":"outputs","id":"y05CUZtS_Erz-0MSz_K2g","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":103,"metadata":{},"data":{"text/plain":{"content":"torch.Size([8, 27])","content_type":"text/plain"}}},"children":[],"key":"UZYHELiPtp"}],"key":"zKw6XXprFT"}],"key":"nR5cBR6FSz"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Of course, as we’ve implemented this right now, this is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PfOOYi7Gnn"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uwNKigClza"},{"type":"text","value":" independent calls to our model. But what convolutions allow you to do is they allow you to “slide” this model efficiently over the input sequence. And so this for loop we just wrote out can be done not “outside”, through iteration, in Python, but “inside” of kernels in ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fBlzI9HXOn"},{"type":"link","url":"https://en.wikipedia.org/wiki/CUDA","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"CUDA","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ETXelLHmFy"}],"urlSource":"https://en.wikipedia.org/wiki/CUDA","data":{"page":"CUDA","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"gVF8ZBk2Nn"},{"type":"text","value":". And so this for loop gets hidden into the convolution. So basically you can think of the convolution as a for loop applying a little linear filter over space of some input sequence. And in our case the space we’re interested in is one dimensional. And we are interested in sliding these filter over the input data. This diagram is quite helpful for understanding actually:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QY01e59Lqv"}],"key":"XFVj82eiSD"}],"key":"JSqtD4lQLv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from IPython.display import Image, display\n\ndisplay(Image(filename=\"wavenet_fig3.png\"))","key":"Ky7A5XcDUJ"},{"type":"outputs","id":"h6gsZh_jIq60fmfwhlKJt","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"198c39366ff68e8b03ccb06df349f47a","path":"/build/198c39366ff68e8b03ccb06df349f47a.png"},"text/plain":{"content":"<IPython.core.display.Image object>","content_type":"text/plain"}}},"children":[],"key":"kYWroudbeL"}],"key":"K3whTKNroA"}],"key":"Fhk7N7pEo2"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Here, you can see highlighted with black arrows a single tree of the calculation we just described. So depicted here, calculating a single orange node at the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MFhusbkIAX"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Output","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TZLno79XCb"}],"key":"LzFuJPAoTw"},{"type":"text","value":" layer corresponds to us in our example forwarding a single example and getting out a single output. But what convolutions allow you to do is it allows you to take this black tree-like structure and kind of like slide it over the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c60Y08GvAm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Input","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WTHGgTli9r"}],"key":"l0ABQGhvdf"},{"type":"text","value":" sequence (blue nodes) and calculate all of the orange outputs at the same time. In the above figure, this sliding action is represented by the dashed connections between the nodes. In our example:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WFcTRQbvzw"}],"key":"Mu6xtJSPqf"}],"key":"DN1OFFOt3A"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print_next_character(xtrain[46:54], ytrain[46:54])","key":"Qh9sJeba7W"},{"type":"outputs","id":"ZSwADkHXewFvFUpiBkvlL","children":[{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"........ --> a\n.......a --> n\n......an --> a\n.....ana --> l\n....anal --> i\n...anali --> s\n..analis --> a\n.analisa --> .\n"},"children":[],"key":"RR3UpyyVTN"}],"key":"MGksZpufHZ"}],"key":"iWl3lNFe03"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"this sliding operation would correspond to calculating all the above ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vctDptBtJ5"},{"type":"text","value":"8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U588A1Sjqy"},{"type":"text","value":" outputs at all the positions of the name (like we did in an explicit loop) at the same time. And the reason this is much more efficient is because the for loop is inside the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dVXc9nXANh"},{"type":"inlineCode","value":"CUDA","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FzgZKuNk9N"},{"type":"text","value":" kernels. That makes it efficient. Also, notice the node re-use in the diagram were for example in the first ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OSc5NEADg7"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Hidden Layer","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SMT7dMI6Ap"}],"key":"yit34UNkLS"},{"type":"text","value":" each white node is the right child of the white node above it (in the second ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TExWpSv2BM"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Hidden Layer","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SfUXYunuCO"}],"key":"PvJP1YLCJ8"},{"type":"text","value":"), but also the left child of another white node (also in the second ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CmXXeOrDcb"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Hidden Layer","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VdN8U4ay8w"}],"key":"M8iWJa47Za"},{"type":"text","value":"). In the first ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SyQya7H4RY"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Hidden Layer","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KKnZEfZSRz"}],"key":"gzHEix2mPB"},{"type":"text","value":", each node and its value is used twice. Therefore, in our above example snippet, with our naive way we would have to recalculate the value that corresponds to such a node, whereas with such a convolutional ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ldd7e08v3D"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RTDBk2WHUL"}],"key":"T2JyxNWpQ3"},{"type":"text","value":" we are allowed to reuse it. So, in the convolutional ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DNkZBT46ei"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uCt7h2mGPf"}],"key":"mhdOvB8K3q"},{"type":"text","value":" you can think of the linear layer as filters. And we take these filters and their linear filters and we slide them over the input sequence and we calculate the first layer, the second layer, the third layer and then the output layer of the sandwich and it is all done very efficiently using these convolutions.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"msBRdfOFsv"}],"key":"sP9c27NpIi"}],"key":"VN4ZnsMurH"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Summary","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xwCx43caZB"}],"identifier":"summary","label":"Summary","html_id":"summary","implicit":true,"key":"Fke8W6OYiS"}],"key":"IYDMbenQYk"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Another thing to take away from this lecture is having modeled our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rPqYCkQBQm"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mseIQkGYl2"}],"key":"f2EyYlPit6"},{"type":"text","value":" lego blocks: the module classes (","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HsplItuAHS"},{"type":"inlineCode","value":"Linear","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cjvDEt8M5X"},{"type":"text","value":", etc.) after modules from ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wqPahMV0mU"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"torch.nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Cy2tMyeb2C"}],"urlSource":"https://pytorch.org/docs/stable/nn.html","key":"xev32FFt43"},{"type":"text","value":". So it is now very easy to start using modules directly from PyTorch, from hereon. The next thing I hope you got a bit of a sense of is what the development process of building deep neural networks looks like. Which I think was relatively representative to some extent. So number one, we are spending a lot of time in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"azNGJDDYHh"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"documentation page of PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TWvIcddZsS"}],"urlSource":"https://pytorch.org/docs/stable/nn.html","key":"uqMaSEKIGJ"},{"type":"text","value":". And we’re reading through all the layers, looking at documentations, what are the shapes of the inputs, what they can be, what does the layer do, and so on. Unfortunately, however, the PyTorch documentation is not very good, at least not at the time these lectures were implemented. The PyTorch developers spend a ton of time on hardcore engineering of all kinds of distributed primitives, etc. But no one is rigorously maintaining documentation. It will lie to you, it will be wrong, it will be incomplete, it will be unclear. So unfortunately, it is what it is and you just kind of have to do your best with what they give us. Also, there’s a ton of trying to make the shapes work. And there’s a lot of gymnastics around these multi-dimensional arrays. Are they ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JJFqrFPxVx"},{"type":"inlineMath","value":"2D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">2D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"W1UWhHNQka"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yTLstR7QTv"},{"type":"inlineMath","value":"3D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">3D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">3</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"malSDdzKXx"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PITCGECQuJ"},{"type":"inlineMath","value":"4D","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">4D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">4</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"mDDYSR7mbG"},{"type":"text","value":"? What shapes do the layers take? Is it ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cmi0WpQCy4"},{"type":"inlineMath","value":"N\\times C\\times L","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi><mo>×</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">N\\times C\\times L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span></span></span></span>","key":"XTSmpp7s2l"},{"type":"text","value":" or ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UyrNLtJCVE"},{"type":"inlineMath","value":"N\\times L\\times C","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>L</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">N\\times L\\times C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span>","key":"kaS945kELq"},{"type":"text","value":"? And you’re permuting and viewing, and it just gets pretty messy. And so that brings me to number three. It’s often helpful to first prototype these layers and implementations in jupyter notebooks and make sure that all the shapes work out, initially making sure everything is correct. And then, once you’re satisfied with the functionality, you can copy-paste the code into your actual code base or repository (e.g. in VSCode). So these are roughly only some notes on the development process of working with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wgJduAp7nB"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WInn5Niofq"}],"key":"uCSIlItSZm"},{"type":"text","value":"s.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rjZevG5JUk"}],"key":"yFXCBgALRZ"}],"key":"kz8DWyVOXi"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Outro","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JgAoVev4I5"}],"identifier":"outro","label":"Outro","html_id":"outro","implicit":true,"key":"T13p5r0wH3"}],"key":"G6DWQrv7Q7"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Lastly, this lecture unlocks a lot of potential further lectures because, number one, we have to convert our ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Mz0j11dMCs"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"nn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vJiRnGBMea"}],"key":"uNpq4hXES3"},{"type":"text","value":" to actually use these dilated causal convolutional layers, so implementing the convnet. Number two, we potentially start to get into what this means, where are residual connections and skip connections and why are they useful. Number three, as we already mentioned, we don’t have any experimental harness. So right now, we are just guessing and checking everything. This is not representative of typical deep learning workflows. You usually have to set up your evaluation harness. You have lots of arguments that your script can take. You’re more comfortably kicking off a lot of experiments. You’re looking at a lot of plots of training and validation losses, and you’re looking at what is working and what is not working. And you’re working on this like population level, and you’re doing all these hyperparameter searches. So we’ve done none of that so far. So how to set that up and how to make it good, I think is a whole other topic. And number four, we should probably cover RNNs, LSTMs, GRUs, and of course Transformers. So many places to go, and we’ll cover that in the future. That’s all for now. Bye! :)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HC2VaaRDiB"}],"key":"KD2BS6Y7tz"}],"key":"ZSOCfEoTDD"}],"key":"sHioe4byUT"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"5. makemore (part 4): becoming a backprop ninja","url":"/micrograduate/makemore4","group":"microgra∇uate"},"next":{"title":"7. picoGPT: implementing a tiny GPT from scratch","url":"/micrograduate/picogpt","group":"microgra∇uate"}}},"domain":"http://localhost:3000"}